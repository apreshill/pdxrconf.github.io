+++
url = "/speakers"
+++
<style>
table{width:100%;display:inline-table;table-layout: fixed; width: 90%;}
td{vertical-align:top;}
.portrait {display: inline-block;top:0px; margin:10px;padding:10px;}
.portrait img {max-height:200px; width:auto; max-width:100%; height:auto; margin: 20px 0 0 0;}
.portrait p {max-height:200px; width:auto; margin: 20px 0 0 0;}
.details {display:inline-block; margin:10px ;padding:10px}
.portraitContainer {width:200px;}
.detailsContainer {width:100%;}
.speakerContainer { transition: all .2s ease-in-out; }
.speakerContainer:hover { transform: scale(1.01); }

@media all and (max-width:740px) {
tr {display: table;width:200%;}               
td {display: table-row; width:100%}
.portraitContainer {width:100%;}
.detailsContainer {width:100%;}
}
</style>

<h2>Session Talks</h2>
<ul>
	<table>
	<col span=1 class="portraitContainer">
	<col span=1 class="detailsContainer">
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="kate_hertweck"> Kate Hertweck </h3>
		<!-- <img style="" src="../img/speakers_2019/kate_hertweck.jpg"/> -->
		</div></td>
		<td><div class="details"/>
		<div>
			<h4> R We There Yet? Building Communities of Practice Around R and Topics in Biology</h4>
			<p> R meetups and communities are thriving in cities around the world, but even identifying other R users at your workplace can be surprisingly difficult. While it's possible to develop expert-level R coding skills in isolation, it's much easier (and far more fun!) to improve your coding skills in cooperative communities of practice, encompassing users of various skills levels and working on different types of problems. What does it take to develop communities of practice at an institution or company? How do you assess what members of a community need or prefer? In this talk, I'll discuss my experiences supporting emerging communities of practice for coding skills at a large non-profit organization with many R users. I'll identify common impediments to community development, but also provide specific recommendations for facilitating and encouraging investment and cohesion in cooperative learning. </p>
		</div>
		
		<!--<div class="biography">
			<h4> Biography: </h4>
			<p> Coming Soon...</p>
		</div> -->
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="robert_amezquita"> Robert Amezquita </h3>
		<!-- <img src="../img/speakers_2019/Gagan_2.jpg"/> -->
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> The Role of Data Science in Translational Cancer Research: From Desk, to Bench, to Bedside </h4>
			<p> The guiding mission of Fred Hutch is the elimination cancer. Over the course of more than 40 years, we have been redefining what's possible in cancer research, and translating these findings into cures. Fred Hutch has been a pioneer of immunotherapies that have improved patient outcomes in ways we have never seen before. However, these novel therapies provide robust cures in only a subset of patients with specific types of cancer. Thus, there remain significant challenges in deciphering how to extend these cures to all cancers and all patients. To overcome these challenges, the Hutch has created the Translation Data Science Interdiscipinary Research Center (TDS IRC) to harness joint advances in statistics, computational science, and biology through tight-knit collaboration. Here, we will discuss how we infuse data science throughout the bench-to-bedside discovery cycle to fuel new research opportunities. </p>
		</div>
		
		<!--<div class="biography">
			<h4> Biography: </h4>
			<p> Coming Soon...</p>
		</div>-->
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="heather_nolis_and_sai_nuthalapati"> Heather Nolis and<br> Sai Nuthalapati </h3>
		<!-- <img src="../img/speakers_2019/Gagan_2.jpg"/> -->
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> How To Talk So Engineers Will Listen: R in Production at T-Mobile </h4>
			<p> When we hired the very first data scientist for the AI @ T-Mobile team, nobody in our organization used R. Less than 8 weeks later, we had R models running in production environments. In lots of organizations, R isn't valued as a "real" programming language. At T-Mobile by sitting data scientists and engineers together we have cultivated a environment of mutual respect. </p>
			<p>In this talk, we will walk through our typical engineering product development workflow at T-Mobile, where the heart of the product is a model created in R. We will also cover the differences in how engineers at data scientists work. By doing so, we hope to empower R users to consider themselves as engineers and convey the vocabulary necessary to make engineers stop and listen. </p>
		</div>
		
		<!--<div class="biography">
			<h4> Biography: </h4>
			<p> Coming Soon...</p>
		</div>-->
		</td>
	</tr>	
	
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="bethany_yollin"> Bethany Yollin </h3>
		<img src="../img/speakers_2019/bethany_yollin.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Creating Interactive GIS Applications with Shiny and Leaflet </h4>
			<p> The Shiny web application framework by RStudio enables data scientists to create and share interactive data analytics. Since Shiny was introduced in 2012, countless open-source contributors have published packages that allow R developers to use Javascript libraries in their Shiny applications. One such Javascript library is leaflet, a platform for creating interactive maps. This talk will introduce how Shiny, leaflet and other geospatial mapping packages can work together to create beautiful web applications capable of providing information-rich visualizations. This talk will also briefly touch on "Dockerizing" and deploying Shiny applications in a High Availability production environment using GCP (Google Cloud Platform). Lastly, if time permits, there will be an opportunity to demo some applications that put all these concepts and technologies together. </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Bethany Yollin is a data scientist working in the transportation industry. With an educational background in geography and applied mathematics, she enjoys developing fun and informative web applications using Shiny. She lives in Seattle, Washington.</p>
		</div>
		</td>
	</tr>	
	<!--<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="clara_yuan"> Clara Yuan </h3>
		<img src="../img/speakers_2019/Gagan_2.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Surge Pricing: An Application of Segmented Regression in Marketplace Pricing </h4>
			<p> Coming Soon... </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Coming Soon...</p>
		</div>
		</td>
	</tr>-->	
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="edward_flinchem"> Edward P. Flinchem </h3>
		<img src="../img/speakers_2019/ed_flinchem.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Bayesian NLP in R on Clinical Text: Predictions from Electronic Health Records </h4>
			<p> Healthcare in the US follows a crisis first, response second pattern. Consequences include high costs and potentially avoidable human suffering. Alternatively, many envision healthcare adopting data-informed patterns, so as to predict poor outcomes and deliver care proactively, thereby mitigating future suffering, controlling costs, and providing better care for more people. The intervention side of that vision is a vast scope and not my topic, but I will demonstrate that the predictive aspect is practical today with concrete examples and simple, transparent models constructed and visualized in R and trained on text extracted from electronic health records (EHRs).</p>
			<p>The frontier of machine learning in healthcare is the analysis of unstructured text in EHRs. Naive Bayesian (NB) modeling is a core method for machine learning, well known in document classification, for example. I demonstrate the utility of NB models, with clinical text as input, to predict hospitalizations, emergency department usage, and mortality. I discuss simplicity and transparency as factors critical to gaining the support of stakeholders in adopting models.</p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Ed Flinchem is a Principal Data Scientist with the Davita Medical Group, a major provider of primary and specialty care in six states. Ed's focus is on developing predictive machine learning models to optimize value based care delivery with an emphasis on forecasting high risk, high complexity outcomes. Ed leverages both the structured and unstructured data (free text) of electronic health records, in the service of providing better healthcare, for more people, at lower cost. Ed has served the Davita Medical Group and its subsidiary, The Everett Clinic, since 2017. </p> 
			<p> Over his 27 year career, Ed has served in industry, government, academic, and startup roles. As Chief Data Scientist at TurboPatent, Ed applied machine learning to the text of patent applications to predict rejections by the US Patent Office. Ed co-invented the predictive text input method, T9, a product based on machine learning and one of the most widely distributed pieces of software in history, used daily by billions of persons texting on their mobile phones. Prior to developing T9, Ed served in academic and government labs developing software to advance research and teaching in physical oceanography and geophysics, acquiring expertise in large scale data analysis, statistics, geographical information systems, satellite remote sensing, fluid dynamics, and digital signal processing. </p>
			<p> Ed earned his B.A. in physics at Brown University in 1985, followed by graduate study in physical oceanography and geophysics at the University of Washington. He has authored over 20 publications, including 5 journal articles and 15 patent applications. </p>
		</div>
		</td>
	</tr>
	
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="eina_ooka"> Eina Ooka </h3>
		<img src="../img/speakers_2019/eina_ooka.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Time Series Forcasting with Keras: LSTM vs ConvNN </h4>
			<p> When we look for examples of Long Short Term Memory (LSTM), they usually concern natural language processing. Similarly, Convolutional Neural Networks (ConvNN) usually concern image processing. As such, most popular applications of deep learning are not time series forecasting. How can we then effectively apply these methods to time series forecasting? To answer this I have built hourly solar generation forecasts with different methods in Keras. As a practitioner in the power utility industry, I will talk about different deep learning architectures suitable for time series forecasting and how they compare to traditional statistical methods. </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Eina Ooka is a senior quantitative analyst at the Energy Authority. She develops multivariate stochastic forecasting models for electric power markets and utility portfolios, utilizing both statistical and data science methods. She develops production-level models in R, all the way from R&D to the shiny app deployment that can be utilized throughout the company for portfolio management. </p>
		</div>
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="michael_frasco"> Michael Frasco </h3>
		<img src="../img/speakers_2019/michael_frasco.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Deploying Machine Learning in R with Amazon SageMaker </h4>
			<p> Too often, data scientists build potentially high-impact models in R on their personal laptops that never see the light of day in production due to deployment obstacles. At Convoy, we leverage machine learning to manage hundreds of marketplaces each day, so building a robust and frictionless platform to deploy models is critical to our success. Convoy uses Amazon SageMaker to minimize the amount of code that data scientists need to write to go from researching and training a model locally in R to deploying the same model in production. As a result, our team has ownership over the end-to-end machine learning pipeline and can rapidly deliver impact on the Convoy product. In this talk, we'll discuss the central challenge of deploying machine learning in production, how the Plumber package allows us to serve our models in production, and the benefits that Amazon SageMaker provides in this project. </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Michael Frasco is a data scientist at Convoy, a private company in Seattle that operates a marketplace for trucking services. At Convoy, he builds internal tools that enable collaboration across the company and make other data scientists more productive. Michael received an MS in Statistics from the University of Chicago, where he fell in love with Bayesian statistics. In his free time, Michael enjoys watching basketball, exploring the pacific Northwest, and reading books from the library. </p>
		</div>
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="kevin_kuo"> Kevin Kuo </h3>
		<img src="../img/speakers_2019/kevin_kuo.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> The latest drops from the Tensorflow + R ecosystem </h4>
			<p> We provide a quick overview of the R interface to the TensorFlow ecosystem and move on to  recent developments, including support for TF 2.0. We introduce the tfprobability package which enables building probabilistic models and demonstrate some applications. </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Kevin is a software engineer at RStudio building open source packages for machine learning development and deployment. </p>
		</div>
		</td>
	</tr>
	
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="bryan_mayer"> Bryan Mayer </h3>
		<!-- <img src="../img/speakers_2019/Gagan_2.jpg"/> -->
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Reproducible Data Processing in Team Workflows with DataPackageR </h4>
			<p> As data is cleaned and updated throughout a research project, it can be easy for each user to generate their own versions. With many instances of data, confusion often surfaces for naming conventions (i.e., date and initial suffixes) and could result in multiple "final" versions, potentially hindering the analysis work flow and making reproducibility difficult. In this talk, I will motivate and demonstrate the use of DataPackageR: an R package that transforms the data processing pipeline into a version-controlled data package. With data packages generated by DataPackageR, raw data remains immutable, read-only input that is processed into analysis datasets once the package is built. To maintain reproducibility, pre-specified user-generated R markdown scripts are rendered, simultaneously generating processed data and documentation vignettes. The data package version is also automatically updated with each build. The R dataset can then be consumed downstream, with the data version established, by the original author, teammates, or other researchers by simply installing and loading the data package. As part of the presentation, I will demonstrate real-life application where we package laboratory-generated HIV data for downstream clinical trial analysis. </p>
		</div>
		<!--<div class="biography">
			<h4> Biography: </h4>
			<p> Coming Soon...</p>
		</div>-->
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="javier_luraschi"> Javier Luraschi </h3>
		<img src="../img/speakers_2019/javier_luraschi.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Cluster Computing Made Easy with Spark and R </h4>
			<p> Have you ever found yourself waiting hours for R to finish analyzing data, running out of memory or spending hours fine tunning your model parameters? Are you interested in using large datasets in R but you are not sure where to start? Fear not, this talk will introduce you to the exciting world of cluster computer using Apache Spark with the ease of use of R. </p>
			<p> This talk will start by introducing techniques to make code faster and explain where Apache Spark fits. It will introduce Apache Spark and then the sparklyr package, which provides an interface to Apache Spark for R. </p>
			<p> You will learn how to install and use Spark from R with familiar packages like dplyr, DBI and broom. You will then learn how to make use of the modeling functionality available in Spark and advanced functions like processing graphs, using other machine learning frameworks, process real time data, run custom R code using new features introduced in sparklyr 1.0.0 and upcoming extension currently being developed. </p>
			<p>This talk should be of interest to new users that are unfamiliar to cluster computing, intermediate users that have used Apache Spark with R and advanced users interested in learning the latest best practices and features available in Spark with R. </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Javier is a software engineer with experience in technologies ranging from desktop, web, mobile and backend; to augmented reality and deep learning applications. He previously worked in Microsoft Research and SAP and holds a double degree in Mathematics and Software Engineering. Javier is the creator of sparklyr, r2d3, cloudml and other R packages. </p>
		</div>
		</td>
	</tr>	
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="gagandeep_singh"> Gagandeep Singh </h3>
		<img src="../img/speakers_2019/Gagan_2.jpg"/></div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Building Data Science Infrastructure at Enterprise Level</h4>
			<p> Modern day organizations are spending considerable amount of resources in data science research and development and the need for having a dedicated data science department in house has increased exponentially. Companies are looking at external partners with dedicated competence and experience in this field to assist in building a comprehensive ‘one place all tools’ solution. We, as data science specialty consultants, have established partnerships with popular data science platforms providers like RStudio and Jupyterhub. We were brought in by a multinational leading biotechnology company to design, develop and deploy an integrated data science development platform for their team of over 100 data scientists. The ask was to build a comprehensive solution- where users can use either R and Python and to develop model and share results through a common platform.</p>
			<p>The biggest concern for us was to provide a solution which can handle multitude of user sessions, but also provide high performance computing and resources at the same time. Safe option was to build a high availability, load balanced environment, though it will create troubles in the future as numbers of users keep on increasing and resources need to be optimized. We decided to take a two-pronged approach, where a kubernetes backed containerized solution will be the primary interface and a load balanced product for backing up additional load. Users launch their own containers for each processing session and kubernetes takes care of backend resource allocations. They can run both Jupyter notebooks (through Python IDE) and R scripts(through R IDE) in the container and perform multiple assignments concurrently. The publishing platform provides a cohesive product to share results through shiny applications, HTML reports, or even python code. Connect is enabled to run both Python and R. It has also been configured to schedule reports to be sent as emails. We have also built R IDE in a load balanced High availability environment. Here R IDE’s internal load balancer works with AWS’s load balancer to accommodate backup and smooth operations in case any of the server(s) goes down. The publishing platform is also configured with high availability, which means multiple servers are simultaneously serving the user’s publishing needs by using a common database. We have also integrated a high availability Package Manager in the mix, which enables the administrator to establish control over package access and downloads. Users can also utilize Package Manager to access different versions of R packages. Our instance of Package Manager is also capable of serving internally developed packages by connecting to the original git source, which eliminates the need for additional administration. </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p>Gagandeep is a Senior Data Scientist at ProCogia, a Seattle based data science consulting firm. An avid user of R since his graduate school days, he has developed R based data science solutions at multiple Fortune 500 clients. He is a certified RStudio Administrative Professional and a regular contributor to the eastide Seattle useR meetup. Through his talk at Cascadia, Gagandeep wants to evangelize the strength of R and related products as a production ready enterprise solution.</p>
		</div>
		</td>
	</tr>
	
	
	</table>
</ul>



<h2>Lightning Talks</h2>

<ul>
	<table>
	<col width="15">
	<col width="80">
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="brittany_barker"> Brittany Barker </h3>
		<!--<img src="../img/speakers_2019/Gagan_2.jpg"/>-->
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Modeling in R to safeguard U.S. agricultural and natural resources from invasive pests </h4>
			<p> A primary goal of the US Department of Agriculture (USDA) is to safeguard US agricultural and natural resources through early detection of exotic plant pests and weeds. We have developed a spatial modeling platform in R for the prediction of life cycle events (phenology) and climate suitability of invasive insect species in the continental US. This platform combines gridded weather/climate data with insect temperature response parameters to produce maps that can depict the potential range of a species and the timing of pest events (e.g., when adults will emerge and start laying eggs). Products from the model will be used to guide USDA supported trapping programs for at least 16 insect species. We plan to place a version of the model online, and to share the source code as open source software. </p>
		</div>
		<!--<div class="biography">
			<h4> Biography: </h4>
			<p> Coming Soon...</p>
		</div>-->
		</td>
	</tr>
	<!-- <tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="joseph_scheidt"> Joseph Scheidt </h3>
		<img src="../img/speakers_2019/Gagan_2.jpg"/>
		<p>Coming Soon...</p>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Improving Performance Metrics with R </h4>
			<p> Coming Soon... </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Coming Soon...</p>
		</div>
		</td>
	</tr> -->
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="scott_came"> Scott Came </h3>
		<img src="../img/speakers_2019/scott_came.png"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Analyzing Legislative Activity with R </h4>
			<p> During the 2019 session of the Washington Legislature, I used R to curate a dataset of bills and roll call vote results (i.e., capturing how each Representative and Senator voted on each bill). This involved using httr and the tidyverse to harvest data from the Legislative Service Center's XML data feed, and munge the data into data frames suitable for analysis and visualization.  I used the sf package and ggplot to create a cartogram of how legislators voted, and also performed analyses of caucus loyalty. I published results/visualizations periodically throughout the session on my Twitter timeline ( @scottcame ).</p>
			<p>This talk will provide a very quick walkthrough of the R code and a glimpse of some of the results.</p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Scott Came is the principal consultant in Olympia-based Cascadia Analytics LLC, where he provides data engineering and data science consulting services to clients in the Northwest and nationally.  His 25-year career has spanned software engineering, data science, and executive management, mostly in the public and non-profit sectors. He also enjoys analyzing and visualizing data to explore interesting topics in sport (baseball mostly), elections, politics, and education policy. </p>
		</div>
		</td>
	</tr>
	<<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="tiernan_martin"> Tiernan Martin </h3>
		<img src="../img/speakers_2019/tiernan_martin.jpg"/> 
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> DRAKE-AGE: Lessions Learned While Package-ing {drake} </h4>
			<p> Coming Soon... </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p>Tiernan Martin is a data analyst and program manager at Futurewise, an urban planning policy organization based in Seattle, Washington. His research and analytical work spans a variety of topics including gentrification, affordable housing, and bicycle/pedestrian transportation.</p>
			<p> tw: @maynardandking</p>
		</div>
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="dror_berel"> Dror Berel </h3>
		<img src="../img/speakers_2019/dror_berel.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Scope Creep and other Software design lessons learned the hard way... </h4>
			<p> 5 lessons on how to deal with scope creep, from a data-science / machine learning perspective.</li>
			<ul>
			<li>Lesson #1: Begin at the end! Define what your scope is. Do you need to extend it?</li>
			<li>Lesson #2: Do not reinvent the wheel! There are other experts that know how to do it better than you!</li>
			<li>Lesson #3: Found a gap? Be creative, but keep it simple!</li>
			<li>Lesson #4: Do not be afraid to refactor!</li>
			<li>Lesson #5: go to lesson #1</li>
			</ul>
			<p>Couple of case studies will be demonstrated in the context of machine learning, and genomic data analysis.More details at my recent blog: https://medium.com/@drorberel/scope-creep-and-other-software-design-lessons-learned-the-hard-way-edacf021965b</p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Dror Berel enjoys analyzing and exploring data, especially using his favorite open-source tool: R. With a background in statistics and biology, he was an early adopter of R and started using it nearly 18 years ago; he's thrilled to see how it's grown since then. With work experience in biostatistics cores at several medical research institutes, he gained experience in collecting and analyzing clinical and public-health data, communicating it with collaborators and publishing at peer-reviewed journals. </p>
		</div>
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="jacqueline_nolis"> Jacqueline Nolis </h3>
		<img src="../img/speakers_2019/jacqueline-portrait.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Adding shine to Shiny: improving the look of your UI </h4>
			<p> Shiny is a package that makes it incredibly easy to create a graphical user interface around R code and quickly share prototypes with colleagues. By default, Shiny apps end up all having the same grey and blue look - which is boring and often comes across as sloppy. From themes to editing CSS files and even creating your own website templates, there are so many ways to improve the styling and design of your shiny app. In this talk I'll walk through these methods, pros and pitfalls. By the end, you should be ready to add some pizzaz to your apps! </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Dr. Jacqueline Nolis is the Principal Data Scientist at Nolis, LLC. She has over a decade of experience in the data science industry, working with companies ranging from DSW and Union bank to T-Mobile and Airbnb. Her academic research covered optimization under uncertainty with a specialization in electric vehicle routing, which yielded a PhD in Industrial Engineering from ASU. Previously, Jacqueline was the Director of Insights and Analytics at Lenati and a Lead of Advanced Analytics at Promontory Financial Group. </p>
		</div>
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="edward_borasky"> M. Edward (Ed) Borasky </h3>
		<img src="../img/speakers_2019/edward_borasky.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Archetypal Ballers and Ternary Plots - Evaluating Basketball Players via Unsupervised Learning </h4>
			<p> Archetypal analysis is a dimensionality reduction technique that reduces an 18-dimensional box score dataset down to three dimensions by representing each player's skills as a linear combination of the skills of the extreme players - the best and the worst. Combined with ternary plot visualization, archetypal analysis offers both insight and quantitative evaluation of players and teams.</p>
			<p>This talk describes a new R package for archetypal analysis of men's and women's college and professional basketball players obtained from the web, with examples from the 2019 NCAA "March Madness" tournaments, the 2018-2018 NBA season and the 2018 WNBA season.</p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> M. Edward (Ed) Borasky is a retired scientific applications / operating systems programmer and open source aficionado, He has been using Linux and R for almost 20 years. He is a volunteer at Hack Oregon, where he builds transit operations databases and APIs. </p>
		</div>
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="mark_druffel"> Mark Druffel </h3>
		<!--<img src="../img/speakers_2019/Gagan_2.jpg"/>-->
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Executing Full Stack Data Strategy </h4>
			<p> Coming Soon... </p>
		</div>
		<!--<div class="biography">
			<h4> Biography: </h4>
			<p> Coming Soon...</p>
		</div>-->
		</td>
	</tr>
	<tr class="speakerContainer">
		<td><div class="portrait">
		<h3 id="ryan_hafen"> Ryan Hafen </h3>
		<img src="../img/speakers_2019/ryan_hafen.jpg"/>
		</div></td>
		<td><div class="details"/>
		<div class="abstract">
			<h4> Visualizing geo-temporal data in R - geofacts and geovis </h4>
			<p> A common type of data encountered in data analysis is geo-temporal, where data is observed for different geographic regions at different points in time. In this talk I will introduce two R packages that help visualize this type of data, geofacet and geovis. The geofacet package provides a mechanism to easily arrange a grid of time series (or any other visualizations) according to the underlying geography. The geovis package produces geographical maps that allow interactive viewing of geographies at multiple resolutions (e.g. country, state, municipality) and over time. I will illustrate the use of the packages using data from 14 million birth records in Brazil. </p>
		</div>
		<div class="biography">
			<h4> Biography: </h4>
			<p> Ryan Hafen is a data scientist working as an independent consultant. He works on tools, methodology, and applications in exploratory analysis, visualization, computational statistics, statistical model building, and machine learning on large, complex datasets. Ryan is active in the data science open source community, mainly working on projects in R and JavaScript.</p>
		</div>
		</td>
	</tr>
	
	</table>
</ul>
