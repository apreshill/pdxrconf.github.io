<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Package on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/package/</link>
    <description>Recent content in Package on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 17 Oct 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/package/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data from Public Bicycle Hire Systems</title>
      <link>https://ropensci.org/blog/2017/10/17/bikedata/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/17/bikedata/</guid>
      <description>
        
        

&lt;p&gt;A new rOpenSci package provides access to data to which users may already have directly contributed, and for which contribution is fun, keeps you fit, and &lt;a href=&#34;http://www.bmj.com/content/357/bmj.j1456&#34;&gt;helps make the world a better place&lt;/a&gt;. The data come from using public bicycle hire schemes, and the package is called &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt;. Public bicycle hire systems operate in many cities throughout the world, and most systems collect (generally anonymous) data, minimally consisting of the times and locations at which every single bicycle trip starts and ends. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package provides access to data from all cities which openly publish these data, currently including &lt;a href=&#34;https://tfl.gov.uk/modes/cycling/santander-cycles&#34;&gt;London, U.K.&lt;/a&gt;, and in the U.S.A., &lt;a href=&#34;https://www.citibikenyc.com&#34;&gt;New York&lt;/a&gt;, &lt;a href=&#34;https://bikeshare.metro.net&#34;&gt;Los Angeles&lt;/a&gt;, &lt;a href=&#34;https://www.rideindego.com&#34;&gt;Philadelphia&lt;/a&gt;, &lt;a href=&#34;https://www.divvybikes.com&#34;&gt;Chicago&lt;/a&gt;, &lt;a href=&#34;https://www.thehubway.com&#34;&gt;Boston&lt;/a&gt;, and &lt;a href=&#34;https://www.capitalbikeshare.com&#34;&gt;Washington DC&lt;/a&gt;. The package will expand as more cities openly publish their data (with the newly enormously expanded San Francisco system &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/2&#34;&gt;next on the list&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;why-bikedata&#34;&gt;Why bikedata?&lt;/h3&gt;

&lt;p&gt;The short answer to that question is that the package provides access to what is arguably one of the most spatially and temporally detailed databases of finely-scaled human movement throughout several of the world&amp;rsquo;s most important cities. Such data are likely to prove invaluable in the increasingly active and well-funded attempt to develop a science of cities. Such a science does not yet exist in any way comparable to most other well-established scientific disciplines, but the importance of developing a science of cities is indisputable, and reflected in such enterprises as the NYU-based &lt;a href=&#34;http://cusp.nyu.edu&#34;&gt;Center for Urban Science and Progress&lt;/a&gt;, or the UCL-based &lt;a href=&#34;https://www.ucl.ac.uk/bartlett/casa/&#34;&gt;Centre for Advanced Spatial Analysis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;People move through cities, yet at present anyone faced with the seemingly fundamental question of how, when, and where people do so would likely have to draw on some form of private data (typically operators of transport systems or mobile phone providers). There are very few open, public data providing insight into this question. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package aims to be one contribution towards filling this gap. The data accessed by the package are entirely open, and are constantly updated, typically on a monthly basis. The package thus provides ongoing insight into the dynamic changes and reconfigurations of these cities. Data currently available via the package amounts to several tens of Gigabytes, and will expand rapidly both with time, and with the inclusion of more cities.&lt;/p&gt;

&lt;h3 id=&#34;why-are-these-data-published&#34;&gt;Why are these data published?&lt;/h3&gt;

&lt;p&gt;In answer to that question, all credit must rightfully go to &lt;a href=&#34;http://www.theregister.co.uk/2011/01/11/transport_for_london_foi/&#34;&gt;Adrian Short&lt;/a&gt;, who submitted a Freedom of Information request in 2011 to Transport for London for usage statistics from the relatively new, and largely publicly-funded, bicycle scheme. This request from one individual ultimately resulted in the data being openly published on an ongoing basis. All U.S. systems included in &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; commenced operation subsequent to that point in time, and many of them have openly published their data from the very beginning. The majority of the world&amp;rsquo;s public bicycle hire systems (&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_bicycle-sharing_systems&#34;&gt;see list here&lt;/a&gt;) nevertheless do not openly publish data, notably including very large systems in China, France, and Spain. One important aspiration of the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package is to demonstrate the positive benefit for the cities themselves of openly and easily facilitating complex analyses of usage data, which brings us to &amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;what-s-important-about-these-data&#34;&gt;What&amp;rsquo;s important about these data?&lt;/h3&gt;

&lt;p&gt;As mentioned, the data really do provide uniquely valuable insights into the movement patterns and behaviour of people within some of the world&amp;rsquo;s major cities. While the more detailed explorations below demonstrate the kinds of things that can be done with the package, the variety of insights these data facilitate is best demonstrated through considering the work of other people, exemplified by &lt;a href=&#34;http://toddwschneider.com/posts/a-tale-of-twenty-two-million-citi-bikes-analyzing-the-nyc-bike-share-system/&#34;&gt;Todd Schneider&amp;rsquo;s high-profile blog piece&lt;/a&gt; on the New York City system. Todd&amp;rsquo;s analyses clearly demonstrate how these data can provide insight into where and when people move, into inter-relationships between various forms of transport, and into relationships with broader environmental factors such as weather. As cities evolve, and public bicycle hire schemes along with them, data from these systems can play a vital role in informing and guiding the ongoing processes of urban development. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package greatly facilitates analysing such processes, not only through making data access and aggregation enormously easier, but through enabling analyses from any one system to be immediately applied to, and compared with, any other systems.&lt;/p&gt;

&lt;h3 id=&#34;how-it-works&#34;&gt;How it works&lt;/h3&gt;

&lt;p&gt;The package currently focusses on the data alone, and provides functionality for downloading, storage, and aggregation. The data are stored in an &lt;code&gt;SQLite3&lt;/code&gt; database, enabling newly published data to be continually added, generally with one simple line of code. It&amp;rsquo;s as easy as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;store_bikedata (city = &amp;quot;chicago&amp;quot;, bikedb = &amp;quot;bikedb&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the nominated database (&lt;code&gt;bikedb&lt;/code&gt;) already holds data for Chicago, only new data will be added, otherwise all historical data will be downloaded and added. All bicycle hire systems accessed by &lt;code&gt;bikedata&lt;/code&gt; have fixed docking stations, and the primary means of aggregation is in terms of &amp;ldquo;trip matrices&amp;rdquo;, which are square matrices of numbers of trips between all pairs of stations, extracted with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chi&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that most parameters are highly flexible in terms of formatting, so pretty much anything starting with &lt;code&gt;&amp;quot;ch&amp;quot;&lt;/code&gt; will be recognised as Chicago. Of course, if the database only contains data for Chicago, the &lt;code&gt;city&lt;/code&gt; parameter may be omitted entirely. Trip matrices may be filtered by time, through combinations of year, month, day, hour, minute, or even second, as well as by demographic characteristics such as gender or date of birth for those systems which provide such data. (These latter data are freely provided by users of the systems, and there can be no guarantee of their accuracy.) These can all be combined in calls like the following, which further demonstrates the highly flexible ways of specifying the various parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (&amp;quot;bikedb&amp;quot;, city = &amp;quot;london, innit&amp;quot;,
                       start_date = 20160101, end_date = &amp;quot;16,02,28&amp;quot;,
                       start_time = 6, end_time = 24,
                       birth_year = 1980:1990, gender = &amp;quot;f&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The second mode of aggregation is as daily time series, via the &lt;code&gt;bike_daily_trips()&lt;/code&gt; function. See &lt;a href=&#34;https://ropensci.github.io/bikedata/articles/bikedata.html&#34;&gt;the vignette&lt;/a&gt; for further details.&lt;/p&gt;

&lt;h3 id=&#34;what-can-be-done-with-these-data&#34;&gt;What can be done with these data?&lt;/h3&gt;

&lt;p&gt;Lots of things. How about examining how far people ride. This requires getting the distances between all pairs of docking stations as routed through the street network, to yield a distance matrix corresponding to the trip matrix. The latest version of &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; has a brand new function to perform exactly that task, so it&amp;rsquo;s as easy as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;ropensci/bikedata&amp;quot;) # to install latest version
dists &amp;lt;- bike_distmat (bikedb = bikedb, city = &amp;quot;chicago&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are distances as routed through the underlying street network, with street types prioritised for bicycle travel. The network is extracted from OpenStreetMap using the &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;rOpenSci &lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;, and the distances are calculated using a brand new package called &lt;a href=&#34;https://cran.r-project.org/package=dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; (Distances on Directed Graphs). (Disclaimer: It&amp;rsquo;s my package, and this is a shameless plug for it - please use it!)&lt;/p&gt;

&lt;p&gt;The distance matrix extracted with &lt;code&gt;bike_distmat&lt;/code&gt; is between all stations listed for a given system, which &lt;code&gt;bike_tripmat&lt;/code&gt; will return trip matrices only between those stations in operation over a specified time period. Because systems expand over time, the two matrices will generally not be directly comparable, so it is necessary to submit both to the &lt;code&gt;bikedata&lt;/code&gt; function &lt;code&gt;match_matrices()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 636 636
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mats &amp;lt;- match_matrices (trips, dists)
trips &amp;lt;- mats$trip
dists &amp;lt;- mats$dist
dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 581 581
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;identical (rownames (trips), rownames (dists))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Distances can then be visually related to trip numbers to reveal their distributional form. These matrices contain too many values to plot directly, so the &lt;code&gt;hexbin&lt;/code&gt; package is used here to aggregate in a &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library (hexbin)
library (ggplot2)
dat &amp;lt;- data.frame (distance = as.vector (dmat),
                   number = as.vector (trips))
ggplot (dat, aes (x = distance, y = number)) +
    stat_binhex(aes(fill = log (..count..))) +
    scale_x_log10 (breaks = c (0.1, 0.5, 1, 2, 5, 10, 20),
                   labels = c (&amp;quot;0.1&amp;quot;, &amp;quot;0.5&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;20&amp;quot;)) +
    scale_y_log10 (breaks = c (10, 100, 1000)) +
    scale_fill_gradientn(colours = c(&amp;quot;seagreen&amp;quot;,&amp;quot;goldenrod1&amp;quot;),
                         name = &amp;quot;Frequency&amp;quot;, na.value = NA) +
    guides (fill = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/chicago.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The central region of the graph (yellow hexagons) reveals that numbers of trips generally decrease roughly exponentially with increasing distance (noting that scales are logarithmic), with most trip distances lying below 5km. What is the &amp;ldquo;average&amp;rdquo; distance travelled in Chicago? The easiest way to calculate this is as a weighted mean,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum (as.vector (dmat) * as.vector (trips) / sum (trips), na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 2.510285
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;giving a value of just over 2.5 kilometres. We could also compare differences in mean distances between cyclists who are registered with a system and causal users. These two categories may be loosely considered to reflect &amp;ldquo;residents&amp;rdquo; and &amp;ldquo;non-residents&amp;rdquo;. Let&amp;rsquo;s wrap this in a function so we can use it for even cooler stuff in a moment.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean &amp;lt;- function (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chicago&amp;quot;)
{
    tm &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
    tm_memb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = TRUE)
    tm_nomemb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = FALSE)
    stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
    dists &amp;lt;- bike_distmat (bikedb = bikedb, city = city)
    mats &amp;lt;- match_mats (dists, tm_memb)
    tm_memb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm_nomemb)
    tm_nomemb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm)
    tm &amp;lt;- mats$trip
    dists &amp;lt;- mats$dists

    d0 &amp;lt;- sum (as.vector (dists) * as.vector (tm) / sum (tm), na.rm = TRUE)
    dmemb &amp;lt;- sum (as.vector (dists) * as.vector (tmemb) / sum (t_memb), na.rm = TRUE)
    dnomemb &amp;lt;- sum (as.vector (dists) * as.vector (tm_nomemb) / sum (tm_nomemb), na.rm = TRUE)
    res &amp;lt;- c (d0, dmemb / dnomemb)
    names (res) &amp;lt;- c (&amp;quot;dmean&amp;quot;, &amp;quot;ratio_memb_non&amp;quot;)
    return (res)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Differences in distances ridden between &amp;ldquo;resident&amp;rdquo; and &amp;ldquo;non-resident&amp;rdquo; cyclists can then be calculated with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean (bikedb = bikedb, city = &amp;quot;ch&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##          dmean ratio_memb_non
##       2.510698       1.023225
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And system members cycle slightly longer distances than non-members. (Do not at this point ask about statistical tests - these comparisons are made between millions&amp;ndash;often tens of millions&amp;ndash;of points, and statistical significance may always be assumed to be negligibly small.) Whatever the reason for this difference between &amp;ldquo;residents&amp;rdquo; and others, we can use this exact same code to compare equivalent distances for all cities which record whether users are members or not (which is all cities except London and Washington DC).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cities &amp;lt;- c (&amp;quot;ny&amp;quot;, &amp;quot;ch&amp;quot;, &amp;quot;bo&amp;quot;, &amp;quot;la&amp;quot;, &amp;quot;ph&amp;quot;) # NYC, Chicago, Boston, LA, Philadelphia
sapply (cities, function (i) dmean (bikedb = bikedb, city = i))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                       ny       ch       bo       la       ph
## dmean          2.8519131 2.510285 2.153918 2.156919 1.702372
## ratio_memb_non 0.9833729 1.023385 1.000635 1.360099 1.130929
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we thus discover that Boston manifests the greatest equality in terms of distances cycled between residents and non-residents, while LA manifests the greatest difference. New York City is the only one of these five in which non-members of the system actually cycle further than members. (And note that these two measures can&amp;rsquo;t be statistically compared in any direct way, because mean distances are also affected by relative numbers of member to non-member trips.) These results likely reflect a host of (scientifically) interesting cultural and geo-spatial differences between these cities, and demonstrate how the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package (combined with &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt;&lt;/a&gt;) can provide unique insight into differences in human behaviour between some of the most important cities in the U.S.&lt;/p&gt;

&lt;h3 id=&#34;visualisation&#34;&gt;Visualisation&lt;/h3&gt;

&lt;p&gt;Many users are likely to want to visualise how people use a given bicycle system, and in particular are likely to want to produce maps. This is also readily done with the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt;, which can route and aggregate transit flows for a particular mode of transport throughout a street network. Let&amp;rsquo;s plot bicycle flows for the Indego System of Philadelphia PA. First get the trip matrix, along with the coordinates of all bicycle stations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;gmost/dodgr&amp;quot;) # to install latest version
city &amp;lt;- &amp;quot;ph&amp;quot;
# store_bikedata (bikedb = bikedb, city = city) # if not already done
trips &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
xy &amp;lt;- stns [, which (names (stns) %in% c (&amp;quot;longitude&amp;quot;, &amp;quot;latitude&amp;quot;))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows of cyclists are calculated between those &lt;code&gt;xy&lt;/code&gt;points, so the &lt;code&gt;trips&lt;/code&gt; table has to match the &lt;code&gt;stns&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;indx &amp;lt;- match (stns$stn_id, rownames (trips))
trips &amp;lt;- trips [indx, indx]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; can be used to extract the underlying street network surrounding those &lt;code&gt;xy&lt;/code&gt; points (expanded here by 50%):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;net &amp;lt;- dodgr_streetnet (pts = xy, expand = 0.5) %&amp;gt;%
    weight_streetnet (wt_profile = &amp;quot;bicycle&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then need to align the bicycle station coordinates in &lt;code&gt;xy&lt;/code&gt; to the nearest points (or &amp;ldquo;vertices&amp;rdquo;) in the street network:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;verts &amp;lt;- dodgr_vertices (net)
pts &amp;lt;- verts$id [match_pts_to_graph (verts, xy)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows between these points can then be mapped onto the underlying street network with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;flow &amp;lt;- dodgr_flows (net, from = pts, to = pts, flow = trips) %&amp;gt;%
    merge_directed_flows ()
net &amp;lt;- net [flow$edge_id, ]
net$flow &amp;lt;- flow$flow
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; documentation&lt;/a&gt; for further details of how this works. We&amp;rsquo;re now ready to plot those flows, but before we do, let&amp;rsquo;s overlay them on top of the rivers of Philadelphia, extracted with rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;q &amp;lt;- opq (&amp;quot;Philadelphia pa&amp;quot;)
rivers1 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;waterway&amp;quot;, value = &amp;quot;river&amp;quot;, value_exact = FALSE) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers2 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;natural&amp;quot;, value = &amp;quot;water&amp;quot;) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers &amp;lt;- c (rivers1, rivers2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally plot the map, using rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmplotr&#34;&gt;&lt;code&gt;osmplotr&lt;/code&gt; package&lt;/a&gt; to prepare a base map with the underlying rivers, and the &lt;code&gt;ggplot2::geom_segment()&lt;/code&gt; function to add the line segments with colours and widths weighted by bicycle flows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#gtlibrary (osmplotr)
require (ggplot2)
bb &amp;lt;- get_bbox (c (-75.22, 39.91, -75.10, 39.98))
cols &amp;lt;- colorRampPalette (c (&amp;quot;lawngreen&amp;quot;, &amp;quot;red&amp;quot;)) (30)
map &amp;lt;- osm_basemap (bb, bg = &amp;quot;gray10&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_multipolygons, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_lines, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_colourbar (zlims = range (net$flow / 1000), col = cols)
map &amp;lt;- map + geom_segment (data = net, size = net$flow / 50000,
                           aes (x = from_lon, y = from_lat, xend = to_lon, yend = to_lat,
                                colour = flow, size = flow)) +
    scale_colour_gradient (low = &amp;quot;lawngreen&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;none&amp;quot;)
print_osm_map (map)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/ph.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The colour bar on the right shows thousands of trips, with the map revealing the relatively enormous numbers crossing the South Street Bridge over the Schuylkill River, leaving most other flows coloured in the lower range of green or yellows. This map thus reveals that anyone wanting to see Philadelphia&amp;rsquo;s Indego bikes in action without braving the saddle themselves would be best advised to head straight for the South Street Bridge.&lt;/p&gt;

&lt;h3 id=&#34;future-plans&#34;&gt;Future plans&lt;/h3&gt;

&lt;p&gt;Although the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; greatly facilitates the production of such maps, the code is nevertheless rather protracted, and it would probably be very useful to convert much of the code in the preceding section to an internal &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; function to map trips between pairs of stations onto corresponding flows through the underlying street networks.&lt;/p&gt;

&lt;p&gt;Beyond that point, and the list of currently open issues awaiting development on the &lt;a href=&#34;https://github.com/ropensci/bikedata/issues&#34;&gt;github repository&lt;/a&gt;, future development is likely to depend very much on how users use the package, and on what extra features people might want. How can you help? A great place to start might be the official &lt;a href=&#34;https://ropensci.org/blog/blog/2017/10/02/hacktoberfest&#34;&gt;Hacktoberfest issue&lt;/a&gt;, helping to import the next lot of data from &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/34&#34;&gt;San Francisco&lt;/a&gt;. Or just use the package, and open up a new issue in response to any ideas that might pop up, no matter how minor they might seem. See the &lt;a href=&#34;https://github.com/ropensci/bikedata/blob/master/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for general advice.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Finally, this package wouldn&amp;rsquo;t be what it is without my co-author &lt;a href=&#34;https://github.com/richardellison&#34;&gt;Richard Ellison&lt;/a&gt;, who greatly accelerated development through encouraging C rather than C++ code for the SQL interfaces. &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maëlle Salmon&lt;/a&gt; majestically guided the entire review process, and made the transformation of the package to its current polished form a joy and a pleasure. I remain indebted to both &lt;a href=&#34;https://github.com/chucheria&#34;&gt;Bea Hernández&lt;/a&gt; and &lt;a href=&#34;https://github.com/eamcvey&#34;&gt;Elaine McVey&lt;/a&gt; for offering their time to extensively test and &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/116&#34;&gt;review the package&lt;/a&gt; as part of rOpenSci&amp;rsquo;s onboarding process. The review process has made the package what it is, and for that I am grateful to all involved!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Accessing patent data with the patentsview package</title>
      <link>https://ropensci.org/blog/2017/09/19/patentsview/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/19/patentsview/</guid>
      <description>
        
        

&lt;h3 id=&#34;why-care-about-patents&#34;&gt;Why care about patents?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Patents play a critical role in incentivizing innovation, without
which we wouldn&amp;rsquo;t have much of the technology we rely on everyday&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What does your iPhone, Google&amp;rsquo;s PageRank algorithm, and a butter
substitute called Smart Balance all have in common?&lt;/p&gt;

&lt;!-- These are open source images taken from: https://pixabay.com/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/iphone.png&#34; width=&#34;15%&#34;&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/google.jpg&#34; width=&#34;25%&#34;&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/butter.png&#34; width=&#34;25%&#34;&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;They all probably wouldn&amp;rsquo;t be here if not for patents. A patent
provides its owner with the ability to make money off of something that
they invented, without having to worry about someone else copying their
technology. Think Apple would spend millions of dollars developing the
iPhone if Samsung could just come along and &lt;a href=&#34;http://www.reuters.com/article/us-apple-samsung-elec-appeal-idUSKCN1271LF&#34;&gt;rip it
off&lt;/a&gt;?
Probably not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Patents offer a great opportunity for data analysis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are two primary reasons for this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Patent data is public&lt;/strong&gt;. In return for the exclusive right to
profit off an invention, an individual/company has to publicly
disclose the details of their invention to the rest of the world.
&lt;a href=&#34;http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;amp;Sect2=HITOFF&amp;amp;p=1&amp;amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;amp;r=11&amp;amp;f=G&amp;amp;l=50&amp;amp;co1=AND&amp;amp;d=PTXT&amp;amp;s1=dog&amp;amp;OS=dog&amp;amp;RS=dog&#34;&gt;Examples of those
details&lt;/a&gt;
include the patent&amp;rsquo;s title, abstract, technology classification,
assigned organizations, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patent data can answer questions that people care about&lt;/strong&gt;.
Companies (especially big ones like IBM and Google) have a vested
interest in extracting insights from patents, and spend a lot of
time/resources trying figure out how to best manage their
intellectual property (IP) rights. They&amp;rsquo;re plagued by questions like
&amp;ldquo;who should I sell my underperforming patents to,&amp;rdquo; &amp;ldquo;which technology
areas are open to new innovations,&amp;rdquo; &amp;ldquo;what&amp;rsquo;s going to be the next big
thing in the world of buttery spreads,&amp;rdquo; etc. Patents offer a way to
provide data-driven answers to these questions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Combined, these two things make patents a prime target for data
analysis. However, until recently it was hard to get at the data inside
these documents. One had to either collect it manually using the
official &lt;a href=&#34;https://en.wikipedia.org/wiki/United_States_Patent_and_Trademark_Office&#34;&gt;United States Patent and Trademark
Office&lt;/a&gt;
(USPTO) &lt;a href=&#34;http://patft.uspto.gov/netahtml/PTO/search-adv.htm&#34;&gt;search
engine&lt;/a&gt;, or figure
out a way to download, parse, and model huge XML data dumps. Enter
PatentsView.&lt;/p&gt;

&lt;h3 id=&#34;patentsview-and-the-patentsview-package&#34;&gt;PatentsView and the &lt;code&gt;patentsview&lt;/code&gt; package&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.patentsview.org/web/#viz/relationships&#34;&gt;PatentsView&lt;/a&gt; is one
of USPTO&amp;rsquo;s new initiatives intended to increase the usability and value
of patent data. One feature of this project is a publicly accessible API
that makes it easy to programmatically interact with the data. A few of
the reasons why I like the API (and PatentsView more generally):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The API is free (no credential required) and currently doesn&amp;rsquo;t
impose rate limits/bandwidth throttling.&lt;/li&gt;
&lt;li&gt;The project offers &lt;a href=&#34;http://www.patentsview.org/download/&#34;&gt;bulk downloads of patent
data&lt;/a&gt; on their website (in a
flat file format), for those who want to be closest to the data.&lt;/li&gt;
&lt;li&gt;Both the API and the bulk download data contain disambiguated
entities such as inventors, assignees, organizations, etc. In other
words, the API will tell you whether it thinks that John Smith on
patent X is the same person as John Smith on patent Y. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;patentsview&lt;/code&gt; R package is a wrapper around the PatentsView API. It
contains a function that acts as a client to the API (&lt;code&gt;search_pv()&lt;/code&gt;) as
well as several supporting functions. Full documentation of the package
can be found on its
&lt;a href=&#34;https://ropensci.github.io/patentsview/index.html&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;

&lt;p&gt;You can install the stable version of &lt;code&gt;patentsview&lt;/code&gt; from CRAN:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;patentsview&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version from GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (!require(devtools)) install.packages(&amp;quot;devtools&amp;quot;)

devtools::install_github(&amp;quot;ropensci/patentsview&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;getting-started&#34;&gt;Getting started&lt;/h3&gt;

&lt;p&gt;The package has one main function, &lt;code&gt;search_pv()&lt;/code&gt;, that makes it easy to
send requests to the API. There are two parameters to &lt;code&gt;search_pv()&lt;/code&gt; that
you&amp;rsquo;re going to want to think about just about every time you call it -
&lt;code&gt;query&lt;/code&gt; and &lt;code&gt;fields&lt;/code&gt;. You tell the API how you want to filter the patent
data with &lt;code&gt;query&lt;/code&gt;, and which fields you want to retrieve with
&lt;code&gt;fields&lt;/code&gt;. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h4 id=&#34;query&#34;&gt;&lt;code&gt;query&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Your query has to use the &lt;a href=&#34;http://www.patentsview.org/api/query-language.html&#34;&gt;PatentsView query
language&lt;/a&gt;, which is
a JSON-based syntax that is similar to the one used by Lucene. You can
write the query directly and pass it as a string to &lt;code&gt;search_pv()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(patentsview)

qry_1 &amp;lt;- &#39;{&amp;quot;_gt&amp;quot;:{&amp;quot;patent_year&amp;quot;:2007}}&#39;
search_pv(query = qry_1, fields = NULL) # This will retrieve a default set of fields
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_id    : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_number: chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_title : chr [1:25] &amp;quot;Sealing device for body suit and sealin&amp;quot;..
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;Or you can use the domain specific language (DSL) provided in the
&lt;code&gt;patentsview&lt;/code&gt; package to help you write the query:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;qry_2 &amp;lt;- qry_funs$gt(patent_year = 2007) # All DSL functions are in the qry_funs list
qry_2 # qry_2 is the same as qry_1
#&amp;gt; {&amp;quot;_gt&amp;quot;:{&amp;quot;patent_year&amp;quot;:2007}}

search_pv(query = qry_2)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_id    : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_number: chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_title : chr [1:25] &amp;quot;Sealing device for body suit and sealin&amp;quot;..
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;qry_1&lt;/code&gt; and &lt;code&gt;qry_2&lt;/code&gt; will result in the same HTTP call to the API. Both
queries search for patents in USPTO that were published after 2007.
There are three gotchas to look out for when writing a query:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Field is queryable.&lt;/strong&gt; The API has 7 endpoints (the default
endpoint is &amp;ldquo;patents&amp;rdquo;), and each endpoint has its own set of fields
that you can filter on. &lt;em&gt;The fields that you can filter on are not
necessarily the same as the ones that you can retrieve.&lt;/em&gt; In other
words, the fields that you can include in &lt;code&gt;query&lt;/code&gt; (e.g.,
&lt;code&gt;patent_year&lt;/code&gt;) are not necessarily the same as those that you can
include in &lt;code&gt;fields&lt;/code&gt;. To see which fields you can query on, look in
the &lt;code&gt;fieldsdf&lt;/code&gt; data frame (&lt;code&gt;View(patentsview::fieldsdf)&lt;/code&gt;) for fields
that have a &amp;ldquo;y&amp;rdquo; indicator in their &lt;code&gt;can_query&lt;/code&gt; column for your given
endpoint.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Correct data type for field.&lt;/strong&gt; If you&amp;rsquo;re filtering on a field in
your query, you have to make sure that the value you are filtering
on is consistent with the field&amp;rsquo;s data type. For example,
&lt;code&gt;patent_year&lt;/code&gt; has type &amp;ldquo;integer,&amp;rdquo; so if you pass 2007 as a string
then you&amp;rsquo;re going to get an error (&lt;code&gt;patent_year = 2007&lt;/code&gt; is good,
&lt;code&gt;patent_year = &amp;quot;2007&amp;quot;&lt;/code&gt; is no good). You can find a field&amp;rsquo;s data type
in the &lt;code&gt;fieldsdf&lt;/code&gt; data frame.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comparison function works with field&amp;rsquo;s data type.&lt;/strong&gt; The comparison
function(s) that you use (e.g., the greater-than function shown
above, &lt;code&gt;qry_funs$gt()&lt;/code&gt;) must be consistent with the field&amp;rsquo;s data
type. For example, you can&amp;rsquo;t use the &amp;ldquo;contains&amp;rdquo; function on fields
of type &amp;ldquo;integer&amp;rdquo; (&lt;code&gt;qry_funs$contains(patent_year = 2007)&lt;/code&gt; will
throw an error). See &lt;code&gt;?qry_funs&lt;/code&gt; for more details.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In short, use the &lt;code&gt;fieldsdf&lt;/code&gt; data frame when you write a query and you
should be fine. Check out the &lt;a href=&#34;https://ropensci.github.io/patentsview/articles/writing-queries.html&#34;&gt;writing queries
vignette&lt;/a&gt;
for more details.&lt;/p&gt;

&lt;h4 id=&#34;fields&#34;&gt;&lt;code&gt;fields&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Up until now we have been using the default value for &lt;code&gt;fields&lt;/code&gt;. This
results in the API giving us some small set of default fields. Let&amp;rsquo;s see
about retrieving some more fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;search_pv(
  query = qry_funs$gt(patent_year = 2007),
  fields = c(&amp;quot;patent_abstract&amp;quot;, &amp;quot;patent_average_processing_time&amp;quot;,
             &amp;quot;inventor_first_name&amp;quot;, &amp;quot;inventor_total_num_patents&amp;quot;)
)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame (with list column(s) inside) on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_abstract               : chr [1:25] &amp;quot;A sealing device for a&amp;quot;..
#&amp;gt;   ..$ patent_average_processing_time: chr [1:25] &amp;quot;1324&amp;quot; ...
#&amp;gt;   ..$ inventors                     :List of 25
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The fields that you can retrieve depends on the endpoint that you are
hitting. We&amp;rsquo;ve been using the &amp;ldquo;patents&amp;rdquo; endpoint thus far, so all of
these are retrievable:
&lt;code&gt;fieldsdf[fieldsdf$endpoint == &amp;quot;patents&amp;quot;, &amp;quot;field&amp;quot;]&lt;/code&gt;. You can also use
&lt;code&gt;get_fields()&lt;/code&gt; to list the retrievable fields for a given endpoint:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;search_pv(
  query = qry_funs$gt(patent_year = 2007),
  fields = get_fields(endpoint = &amp;quot;patents&amp;quot;, groups = c(&amp;quot;patents&amp;quot;, &amp;quot;inventors&amp;quot;))
)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame (with list column(s) inside) on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  31 variables:
#&amp;gt;   ..$ patent_abstract                       : chr [1:25] &amp;quot;A sealing devi&amp;quot;..
#&amp;gt;   ..$ patent_average_processing_time        : chr [1:25] &amp;quot;1324&amp;quot; ...
#&amp;gt;   ..$ patent_date                           : chr [1:25] &amp;quot;2008-01-01&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_city       : chr [1:25] &amp;quot;Cambridge&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_country    : chr [1:25] &amp;quot;US&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_id         : chr [1:25] &amp;quot;b9fc6599e3d60c&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_assignee_latitude   : chr [1:25] &amp;quot;42.3736&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_location_id: chr [1:25] &amp;quot;42.3736158|-71&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_assignee_longitude  : chr [1:25] &amp;quot;-71.1097&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_state      : chr [1:25] &amp;quot;MA&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_city       : chr [1:25] &amp;quot;Lucca&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_country    : chr [1:25] &amp;quot;IT&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_id         : chr [1:25] &amp;quot;6416028-3&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_latitude   : chr [1:25] &amp;quot;43.8376&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_location_id: chr [1:25] &amp;quot;43.8376211|10.&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_inventor_longitude  : chr [1:25] &amp;quot;10.4951&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_state      : chr [1:25] &amp;quot;Tuscany&amp;quot; ...
#&amp;gt;   ..$ patent_id                             : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_kind                           : chr [1:25] &amp;quot;B1&amp;quot; ...
#&amp;gt;   ..$ patent_number                         : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_num_cited_by_us_patents        : chr [1:25] &amp;quot;5&amp;quot; ...
#&amp;gt;   ..$ patent_num_claims                     : chr [1:25] &amp;quot;25&amp;quot; ...
#&amp;gt;   ..$ patent_num_combined_citations         : chr [1:25] &amp;quot;35&amp;quot; ...
#&amp;gt;   ..$ patent_num_foreign_citations          : chr [1:25] &amp;quot;0&amp;quot; ...
#&amp;gt;   ..$ patent_num_us_application_citations   : chr [1:25] &amp;quot;0&amp;quot; ...
#&amp;gt;   ..$ patent_num_us_patent_citations        : chr [1:25] &amp;quot;35&amp;quot; ...
#&amp;gt;   ..$ patent_processing_time                : chr [1:25] &amp;quot;792&amp;quot; ...
#&amp;gt;   ..$ patent_title                          : chr [1:25] &amp;quot;Sealing device&amp;quot;..
#&amp;gt;   ..$ patent_type                           : chr [1:25] &amp;quot;utility&amp;quot; ...
#&amp;gt;   ..$ patent_year                           : chr [1:25] &amp;quot;2008&amp;quot; ...
#&amp;gt;   ..$ inventors                             :List of 25
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s look at a quick example of pulling and analyzing patent data.
We&amp;rsquo;ll look at patents from the last ten years that are classified below
the &lt;a href=&#34;https://worldwide.espacenet.com/classification#!/CPC=H04L63/02&#34;&gt;H04L63/00 CPC
code&lt;/a&gt;.
Patents in this area relate to &amp;ldquo;network architectures or network
communication protocols for separating internal from external
traffic.&amp;rdquo; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; CPC codes offer a quick and dirty way to find patents of
interest, though getting a sense of their hierarchy can be tricky.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download the data&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(patentsview)

# Write a query:
query &amp;lt;- with_qfuns( # with_qfuns is basically just: with(qry_funs, ...)
  and(
    begins(cpc_subgroup_id = &#39;H04L63/02&#39;),
    gte(patent_year = 2007)
  )
)

# Create a list of fields:
fields &amp;lt;- c(
  c(&amp;quot;patent_number&amp;quot;, &amp;quot;patent_year&amp;quot;),
  get_fields(endpoint = &amp;quot;patents&amp;quot;, groups = c(&amp;quot;assignees&amp;quot;, &amp;quot;cpcs&amp;quot;))
)

# Send HTTP request to API&#39;s server:
pv_res &amp;lt;- search_pv(query = query, fields = fields, all_pages = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;See where the patents are coming from (geographically)&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(leaflet)
library(htmltools)
library(dplyr)
library(tidyr)

data &amp;lt;-
  pv_res$data$patents %&amp;gt;%
    unnest(assignees) %&amp;gt;%
    select(assignee_id, assignee_organization, patent_number,
           assignee_longitude, assignee_latitude) %&amp;gt;%
    group_by_at(vars(-matches(&amp;quot;pat&amp;quot;))) %&amp;gt;%
    mutate(num_pats = n()) %&amp;gt;%
    ungroup() %&amp;gt;%
    select(-patent_number) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(popup = paste0(&amp;quot;&amp;lt;font color=&#39;Black&#39;&amp;gt;&amp;quot;,
                          htmlEscape(assignee_organization), &amp;quot;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Patents:&amp;quot;,
                          num_pats, &amp;quot;&amp;lt;/font&amp;gt;&amp;quot;)) %&amp;gt;%
    mutate_at(vars(matches(&amp;quot;_l&amp;quot;)), as.numeric) %&amp;gt;%
    filter(!is.na(assignee_id))

leaflet(data) %&amp;gt;%
  addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %&amp;gt;%
  addCircleMarkers(lng = ~assignee_longitude, lat = ~assignee_latitude,
                   popup = ~popup, ~sqrt(num_pats), color = &amp;quot;yellow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Plot the growth of the field&amp;rsquo;s topics over time&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(ggplot2)
library(RColorBrewer)

data &amp;lt;-
  pv_res$data$patents %&amp;gt;%
    unnest(cpcs) %&amp;gt;%
    filter(cpc_subgroup_id != &amp;quot;H04L63/02&amp;quot;) %&amp;gt;% # remove patents categorized into only top-level category of H04L63/02
    mutate(
      title = case_when(
        grepl(&amp;quot;filtering&amp;quot;, .$cpc_subgroup_title, ignore.case = T) ~
          &amp;quot;Filtering policies&amp;quot;,
        .$cpc_subgroup_id %in% c(&amp;quot;H04L63/0209&amp;quot;, &amp;quot;H04L63/0218&amp;quot;) ~
          &amp;quot;Architectural arrangements&amp;quot;,
        grepl(&amp;quot;Firewall traversal&amp;quot;, .$cpc_subgroup_title, ignore.case = T) ~
          &amp;quot;Firewall traversal&amp;quot;,
        TRUE ~
          .$cpc_subgroup_title
      )
    ) %&amp;gt;%
    mutate(title = gsub(&amp;quot;.*(?=-)-&amp;quot;, &amp;quot;&amp;quot;, title, perl = TRUE)) %&amp;gt;%
    group_by(title, patent_year) %&amp;gt;%
    count() %&amp;gt;%
    ungroup() %&amp;gt;%
    mutate(patent_year = as.numeric(patent_year))

ggplot(data = data) +
  geom_smooth(aes(x = patent_year, y = n, colour = title), se = FALSE) +
  scale_x_continuous(&amp;quot;\nPublication year&amp;quot;, limits = c(2007, 2016),
                     breaks = 2007:2016) +
  scale_y_continuous(&amp;quot;Patents\n&amp;quot;, limits = c(0, 700)) +
  scale_colour_manual(&amp;quot;&amp;quot;, values = brewer.pal(5, &amp;quot;Set2&amp;quot;)) +
  theme_bw() + # theme inspired by https://hrbrmstr.github.io/hrbrthemes/
  theme(panel.border = element_blank(), axis.ticks = element_blank())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;learning-more&#34;&gt;Learning more&lt;/h3&gt;

&lt;p&gt;For analysis examples that go into a little more depth, check out the
&lt;a href=&#34;https://ropensci.github.io/patentsview/articles/citation-networks.html&#34;&gt;data applications
vignettes&lt;/a&gt;
on the package&amp;rsquo;s website. If you&amp;rsquo;re just interested in &lt;code&gt;search_pv()&lt;/code&gt;,
there are
&lt;a href=&#34;https://ropensci.github.io/patentsview/articles/examples.html&#34;&gt;examples&lt;/a&gt;
on the site for that as well. To contribute to the package or report an
issue, check out the &lt;a href=&#34;https://github.com/ropensci/patentsview/issues&#34;&gt;issues page on
GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;d like to thank the package&amp;rsquo;s two reviewers, &lt;a href=&#34;https://github.com/poldham&#34;&gt;Paul
Oldham&lt;/a&gt; and &lt;a href=&#34;http://blog.haunschmid.name/&#34;&gt;Verena
Haunschmid&lt;/a&gt;, for taking the time to review
the package and providing helpful feedback. I&amp;rsquo;d also like to thank
&lt;a href=&#34;http://www.masalmon.eu/&#34;&gt;Maëlle Salmon&lt;/a&gt; for shepherding the package
along the rOpenSci review process, as well &lt;a href=&#34;https://scottchamberlain.info/&#34;&gt;Scott
Chamberlain&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/stefaniebutland&#34;&gt;Stefanie
Butland&lt;/a&gt; for their miscellaneous
help.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;This is both good and bad, as there are errors in the disambiguation. The algorithm that is responsible for the disambiguation was created by the winner of the &lt;a href=&#34;http://www.patentsview.org/workshop/&#34;&gt;PatentsView Inventor Disambiguation Technical Workshop&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;These two parameters end up getting translated into a MySQL query by the API&amp;rsquo;s server, which then gets sent to a back-end database. &lt;code&gt;query&lt;/code&gt; and &lt;code&gt;fields&lt;/code&gt; are used to create the query&amp;rsquo;s &lt;code&gt;WHERE&lt;/code&gt; and &lt;code&gt;SELECT&lt;/code&gt; clauses, respectively.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;There is a slightly more in-depth definition that says that these are patents &amp;ldquo;related to the (logical) separation of traffic/(sub-) networks to achieve protection.&amp;rdquo;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>Onboarding visdat, a tool for preliminary visualisation of whole dataframes</title>
      <link>https://ropensci.org/blog/2017/08/22/visdat/</link>
      <pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/08/22/visdat/</guid>
      <description>
        
        

&lt;blockquote&gt;
&lt;p&gt;Take a look at the data&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a phrase that comes up when you first get a dataset.&lt;/p&gt;

&lt;p&gt;It is also ambiguous. Does it mean to do some exploratory modelling? Or make some histograms, scatterplots, and boxplots? Is it both?&lt;/p&gt;

&lt;p&gt;Starting down either path, you often encounter the non-trivial growing pains of working with a new dataset. The mix ups of data types - height in cm coded as a factor, categories are numerics with decimals, strings are datetimes, and somehow datetime is one long number. And let&amp;rsquo;s not forget everyone&amp;rsquo;s favourite: missing data.&lt;/p&gt;

&lt;p&gt;These growing pains often get in the way of your basic modelling or graphical exploration. So, sometimes you can&amp;rsquo;t even start to take a look at the data, and that is frustrating.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/ropensci/visdat&#34;&gt;&lt;code&gt;visdat&lt;/code&gt;&lt;/a&gt; package aims to make this preliminary part of analysis easier. It focuses on creating visualisations of whole dataframes, to make it easy and fun for you to &amp;ldquo;get a look at the data&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Making &lt;code&gt;visdat&lt;/code&gt; was fun, and it was easy to use. But I couldn&amp;rsquo;t help but think that maybe &lt;code&gt;visdat&lt;/code&gt; could be more.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I felt like the code was a little sloppy, and that it could be better.&lt;/li&gt;
&lt;li&gt;I wanted to know whether others found it useful.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What I needed was someone to sit down and read over it, and tell me what they thought. And hey, a publication out of this would certainly be great.&lt;/p&gt;

&lt;p&gt;Too much to ask, perhaps? No. Turns out, not at all. This is what the rOpenSci &lt;a href=&#34;https://github.com/ropensci/onboarding&#34;&gt;onboarding process&lt;/a&gt; provides.&lt;/p&gt;

&lt;h3 id=&#34;ropensci-onboarding-basics&#34;&gt;rOpenSci onboarding basics&lt;/h3&gt;

&lt;p&gt;Onboarding a package onto rOpenSci is an open peer review of an R package. If successful, the package is migrated to rOpenSci, with the option of putting it through an accelerated publication with &lt;a href=&#34;http://joss.theoj.org/&#34;&gt;JOSS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What&amp;rsquo;s in it for the author?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Feedback on your package&lt;/li&gt;
&lt;li&gt;Support from rOpenSci members&lt;/li&gt;
&lt;li&gt;Maintain ownership of your package&lt;/li&gt;
&lt;li&gt;Publicity from it being under rOpenSci&lt;/li&gt;
&lt;li&gt;Contribute something to rOpenSci&lt;/li&gt;
&lt;li&gt;Potentially a publication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;What can rOpenSci do that CRAN cannot?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The rOpenSci onboarding process provides a stamp of quality on a package that you do not necessarily get when a package is on CRAN &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Here&amp;rsquo;s what rOpenSci does that CRAN cannot:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Assess documentation readability / usability&lt;/li&gt;
&lt;li&gt;Provide a code review to find weak points / points of improvement&lt;/li&gt;
&lt;li&gt;Determine whether a package is overlapping with another.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So I submitted &lt;code&gt;visdat&lt;/code&gt; to the onboarding process. For me, I did this for three reasons.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;So &lt;code&gt;visdat&lt;/code&gt; could become a better package&lt;/li&gt;
&lt;li&gt;Pending acceptance, I would get a publication in JOSS&lt;/li&gt;
&lt;li&gt;I get to contribute back to rOpenSci&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Submitting the package was actually quite easy - you go to &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/new&#34;&gt;submit an issue&lt;/a&gt; on the onboarding page on GitHub, and it provides a magical template for you to fill out &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, with no submission gotchas - this could be the future &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Within 2 days of submitting the issue, I had a response from the editor, &lt;a href=&#34;https://github.com/noamross&#34;&gt;Noam Ross&lt;/a&gt;, and two reviewers assigned, &lt;a href=&#34;https://github.com/batpigandme&#34;&gt;Mara Averick&lt;/a&gt;, and &lt;a href=&#34;https://github.com/seaaan&#34;&gt;Sean Hughes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/87&#34;&gt;submitted &lt;strong&gt;visdat&lt;/strong&gt;&lt;/a&gt; and waited, somewhat apprehensively. &lt;em&gt;What would the reviewers think?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In fact, Mara Averick wrote a post: &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/22/first-package-review&#34;&gt;&amp;ldquo;So you (don&amp;rsquo;t) think you can review a package&amp;rdquo;&lt;/a&gt; about her experience evaluating &lt;code&gt;visdat&lt;/code&gt; as a first-time reviewer.&lt;/p&gt;

&lt;h3 id=&#34;getting-feedback&#34;&gt;Getting feedback&lt;/h3&gt;

&lt;h4 id=&#34;unexpected-extras-from-the-review&#34;&gt;Unexpected extras from the review&lt;/h4&gt;

&lt;p&gt;Even before the review started officially, I got some great concrete feedback from Noam Ross, the editor for the &lt;code&gt;visdat&lt;/code&gt; submission.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Noam used the &lt;a href=&#34;https://github.com/MangoTheCat/goodpractice&#34;&gt;goodpractice&lt;/a&gt; package, to identify bad code patterns and other places to immediately improve upon in a concrete way. This resulted in me:

&lt;ul&gt;
&lt;li&gt;Fixing error prone code such as using &lt;code&gt;1:length(...)&lt;/code&gt;, or &lt;code&gt;1:nrow(...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Improving testing using the visualisation testing software &lt;a href=&#34;https://github.com/lionel-/vdiffr&#34;&gt;&lt;code&gt;vdiffr&lt;/code&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Reducing long code lines to improve readability&lt;/li&gt;
&lt;li&gt;Defining global variables to avoid a NOTE (&amp;ldquo;no visible binding for global variable&amp;rdquo;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So before the review even started, &lt;code&gt;visdat&lt;/code&gt; is in better shape, with 99% test coverage, and clearance from &lt;code&gt;goodpractice&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;the-feedback-from-reviewers&#34;&gt;The feedback from reviewers&lt;/h4&gt;

&lt;p&gt;I received prompt replies from the reviewers, and I got to hear really nice things like  &amp;ldquo;I think &lt;code&gt;visdat&lt;/code&gt; is a very worthwhile project and have already started using it in my own work.&amp;ldquo;, and &amp;ldquo;Having now put it to use in a few of my own projects, I can confidently say that it is an incredibly useful early step in the data analysis workflow. &lt;code&gt;vis_miss()&lt;/code&gt;, in particular, is helpful for scoping the task at hand &amp;hellip;&amp;ldquo;. In addition to these nice things, there was also great critical feedback from Sean and Mara.&lt;/p&gt;

&lt;p&gt;A common thread in both reviews was that the way I initially had &lt;code&gt;visdat&lt;/code&gt; set up was to have the first row of the dataset at the bottom left, and the variable names at the bottom. However, this doesn&amp;rsquo;t reflect what a dataframe typically looks like - with the names of the variables at the top, and the first row also at the top. There was also suggestions to add the percentage of missing data in each column.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-compare-1.png&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-compare-2.png&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-compare-3.png&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-compare-4.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;On the left are the old &lt;code&gt;visdat&lt;/code&gt; and vismiss plots, and on the right are the new &lt;code&gt;visdat&lt;/code&gt; and vismiss plots.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Changing this makes the plots make a lot more sense, and read better.&lt;/p&gt;

&lt;p&gt;Mara made me aware of the warning and error messages that I had let crop up in the package. This was something I had grown to accept - the plot worked, right? But Mara pointed out that from a user perspective, seeing these warnings and messages can be a negative experience for the user, and something that might stop them from using it - how do they know if their plot is accurate with all these warnings? Are they using it wrong?&lt;/p&gt;

&lt;p&gt;Sean gave practical advice on reducing code duplication, explaining how to write general construction method to prepare the data for the plots. Sean also explained how to write C++ code to improve the speed of &lt;code&gt;vis_guess()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;From both reviewers I got nitty gritty feedback about my writing - places where documentation that was just a bunch of notes I made, or where I had reversed the order of a statement.&lt;/p&gt;

&lt;h3 id=&#34;what-did-i-think&#34;&gt;What did I think?&lt;/h3&gt;

&lt;p&gt;I think that getting feedback in general on your own work can be a bit hard to take sometimes. We get attached to our ideas, we&amp;rsquo;ve seen them grow from little thought bubbles all the way to &amp;ldquo;all growed up&amp;rdquo; R packages. I was apprehensive about getting feedback on &lt;code&gt;visdat&lt;/code&gt;. But the feedback process from rOpenSci was, as Tina Turner put it, &lt;a href=&#34;https://www.youtube.com/watch?v=mNU3aIJs88g&#34;&gt;&amp;ldquo;simply the best&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Boiling down the onboarding review process down to a few key points, I would say it is &lt;strong&gt;transparent&lt;/strong&gt;, &lt;strong&gt;friendly&lt;/strong&gt;, and &lt;strong&gt;thorough&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Having the entire review process on GitHub means that everyone is accountable for what they say, and means that you can track exactly what everyone said about it &lt;em&gt;in one place&lt;/em&gt;. No email chain hell with (mis)attached documents, accidental reply-alls or single replies. The whole internet is cc&amp;rsquo;d in on this discussion.&lt;/p&gt;

&lt;p&gt;Being an rOpenSci initiative, the process is incredibly &lt;strong&gt;friendly&lt;/strong&gt; and respectful of everyone involved. Comments are upbeat, but are also, importantly &lt;strong&gt;thorough&lt;/strong&gt;, providing constructive feedback.&lt;/p&gt;

&lt;h3 id=&#34;so-what-does-visdat-look-like&#34;&gt;So what does &lt;code&gt;visdat&lt;/code&gt; look like?&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(visdat)

vis_dat(airquality)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-example-1.png&#34; alt=&#34;visdat-example&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This shows us a visual analogue of our data, the variable names are shown on the top, and the class of each variable is shown, along with where missing data.&lt;/p&gt;

&lt;p&gt;You can focus in on missing data with &lt;code&gt;vis_miss()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vis_miss(airquality)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-miss-aq-1.png&#34; alt=&#34;vis-miss-example&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This shows only missing and present information in the data. In addition to &lt;code&gt;vis_dat()&lt;/code&gt; it shows the percentage of missing data for each variable and also the overall amount of missing data. &lt;code&gt;vis_miss()&lt;/code&gt; will also indicate when a dataset has no missing data at all, or a very small percentage.&lt;/p&gt;

&lt;h3 id=&#34;the-future-of-visdat&#34;&gt;The future of &lt;code&gt;visdat&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;There are some really exciting changes coming up for &lt;code&gt;visdat&lt;/code&gt;. The first is making a plotly version of all of the figures that provides useful tooltips and interactivity. The second and third changes to bring in later down the track are to include the idea of visualising expectations, where the user can search their data for particular things, such as particular characters like &amp;ldquo;~&amp;rdquo; or values like -99, or -0, or conditions &amp;ldquo;x &amp;gt; 101&amp;rdquo;, and visualise them. Another final idea is to make it easy to visually compare two dataframes of differing size. We also want to work on providing consistent palettes for particular datatypes. For example, character, numerics, integers, and datetime would all have different (and consistently different) colours.&lt;/p&gt;

&lt;p&gt;I am very interested to hear how people use &lt;code&gt;visdat&lt;/code&gt; in their work, so if you have suggestions or feedback I would love to hear from you! The best way to leave feedback is by &lt;a href=&#34;https://github.com/ropensci/visdat/issues/new&#34;&gt;filing an issue&lt;/a&gt;, or perhaps sending me an email at nicholas [dot] tierney [at] gmail [dot] com.&lt;/p&gt;

&lt;h3 id=&#34;the-future-of-your-r-package&#34;&gt;The future of your R package?&lt;/h3&gt;

&lt;p&gt;If you have an R package you should give some serious thought about submitting it to the rOpenSci through their onboarding process. There are very clear guidelines on their &lt;a href=&#34;https://github.com/ropensci/onboarding&#34;&gt;onboarding GitHub page&lt;/a&gt;. If you aren&amp;rsquo;t sure about &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md&#34;&gt;package fit&lt;/a&gt;, you can submit a &lt;a href=&#34;https://github.com/ropensci/onboarding/issues?q=is%3Aissue+label%3A0%2Fpresubmission&#34;&gt;pre-submission enquiry&lt;/a&gt; - the editors are nice and friendly, and a positive experience awaits you!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:3&#34;&gt;CRAN is an essential part of what makes the r-project successful and certainly without CRAN R simply would not be the language that it is today. The tasks provided by the rOpenSci onboarding require human hours, and there just isn&amp;rsquo;t enough spare time and energy amongst CRAN managers.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:1&#34;&gt;Never used GitHub? Don&amp;rsquo;t worry, creating an account is easy, and the template is all there for you. You provide very straightforward information, and it&amp;rsquo;s all there at once.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;With some journals, the submission process means you aren&amp;rsquo;t always clear what information you need ahead of time. Gotchas include things like &amp;ldquo;what is the residential address of every co-author&amp;rdquo;, or getting everyone to sign a copyright notice.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>So you (don&#39;t) think you can review a package</title>
      <link>https://ropensci.org/blog/2017/08/22/first-package-review/</link>
      <pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/08/22/first-package-review/</guid>
      <description>
        
        

&lt;p&gt;Contributing to an open-source community &lt;em&gt;without&lt;/em&gt; contributing code is an oft-vaunted idea that can seem nebulous. Luckily, putting vague ideas into action is one of the strengths of the &lt;a href=&#34;https://ropensci.org/community/&#34;&gt;rOpenSci Community&lt;/a&gt;, and their package onboarding system offers a chance to do just that.&lt;/p&gt;

&lt;p&gt;This was my first time reviewing a package, and, as with so many things in life, I went into it worried that I&amp;rsquo;d somehow ruin the package-reviewing process— not just the package itself, but the actual onboarding infrastructure&amp;hellip;maybe even rOpenSci on the whole.&lt;/p&gt;

&lt;p&gt;Barring the destruction of someone else&amp;rsquo;s hard work and/or an entire organization, I was fairly confident that I&amp;rsquo;d have little to offer in the way of useful advice. &lt;em&gt;What if I have absolutely nothing to say other than, yes, this is, in fact, a package?!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/np59m8Z.png&#34; alt=&#34;rOpenSci package review: what I imagined&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So, step one (for me) was: confess my inadequacies and seek advice. It turns out that much of the advice vis-à-vis &lt;em&gt;how to review a package&lt;/em&gt; is baked right into the documents. The &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/reviewer_template.md&#34;&gt;reviewer template&lt;/a&gt; is a great trail map, the utility of which is fleshed out in the &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/reviewing_guide.md&#34;&gt;rOpenSci Package Reviewing Guide&lt;/a&gt;. Giving these a thorough read, and perusing a recommended review or two (links in the reviewing guide) will probably have you raring to go. But, if you&amp;rsquo;re feeling particularly neurotic (as I almost always am), the rOpenSci &lt;a href=&#34;https://github.com/ropensci/onboarding#-editors-and-reviewers&#34;&gt;onboarding editors&lt;/a&gt; and larger community are endless founts of wisdom and resources.&lt;/p&gt;

&lt;h3 id=&#34;visdat&#34;&gt;&lt;code&gt;visdat&lt;/code&gt; 📦👀&lt;/h3&gt;

&lt;p&gt;I knew nothing about &lt;a href=&#34;https://github.com/njtierney&#34;&gt;Nicholas Tierney&lt;/a&gt;&amp;rsquo;s &lt;a href=&#34;http://visdat.njtierney.com/&#34;&gt;&lt;code&gt;visdat&lt;/code&gt;&lt;/a&gt; package prior to receiving my &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/87#issuecomment-270428584&#34;&gt;invitation to review&lt;/a&gt; it. So the first (coding-y) thing I did was play around with it in the same way I do for other cool R packages I encounter. This is a totally unstructured mish-mash of running examples, putting my own data in, and seeing what happens. In addition to being amusing, it&amp;rsquo;s a good way to sort of &amp;ldquo;ground-truth&amp;rdquo; the package&amp;rsquo;s mission, and make sure there isn&amp;rsquo;t some super helpful feature that&amp;rsquo;s going unsung.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re &lt;em&gt;not&lt;/em&gt; familiar with &lt;code&gt;visdat&lt;/code&gt;, it &amp;ldquo;provides a quick way for the user to visually examine the structure of their data set, and, more specifically, where and what kinds of data are missing.&amp;rdquo;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; With early-stage EDA (exploratory data analysis), you&amp;rsquo;re really trying to get a &lt;em&gt;feel&lt;/em&gt; of your data. So, knowing that I couldn&amp;rsquo;t be much help in the &lt;em&gt;&amp;ldquo;here&amp;rsquo;s how you could make this faster with C++&amp;rdquo;&lt;/em&gt; department, I decided to fully embrace my role as &lt;em&gt;&amp;ldquo;naïve user&amp;rdquo;&lt;/em&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h4 id=&#34;questions-i-kept-in-mind-as-del-myself-del-resident-naïf&#34;&gt;Questions I kept in mind as &lt;del&gt;myself&lt;/del&gt;  resident naïf:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;What did I think this thing would do? Did it do it?&lt;/li&gt;
&lt;li&gt;What are things that scare me off?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The latter question is key, and, while I don&amp;rsquo;t have data to back this up, can be a sort of &amp;ldquo;silent&amp;rdquo; usability failure when left unexamined. Someone who tinkers with a package, but finds it confusing doesn&amp;rsquo;t necessarily stop to give feedback. There&amp;rsquo;s also a pseudo &lt;em&gt;curse-of-knowledge&lt;/em&gt; component. While messages and warnings are easily parsed, suppressed, dealt with, and/or dismissed by the veteran R user/programmer, unexpected, brightly-coloured text can easily scream &lt;em&gt;Oh my gosh you broke it all!!&lt;/em&gt; to those with less experience.&lt;/p&gt;

&lt;h3 id=&#34;myriad-lessons-learned&#34;&gt;Myriad lessons learned 💡&lt;/h3&gt;

&lt;p&gt;I can&amp;rsquo;t speak for Nick per the utility or lack thereof of my review (you can see &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/22/visdat&#34;&gt;his take here&lt;/a&gt;, but I &lt;em&gt;can&lt;/em&gt; vouch for the package-reviewing experience as a means of methodically inspecting the innards of an R package. Methodical is really the operative word here. Though &lt;em&gt;&amp;ldquo;read the docs,&amp;rdquo;&lt;/em&gt; or &lt;em&gt;&amp;ldquo;look at the code&amp;rdquo;&lt;/em&gt; sounds straight-forward enough, it&amp;rsquo;s not always easy to coax oneself into going through the task piece-by-piece without an end goal in mind. While a desire to contribute to open-source software is noble enough (and is how I &lt;em&gt;personaly&lt;/em&gt; ended up involved in this process&amp;ndash; with some help/coaxing from &lt;a href=&#34;https://twitter.com/noamross&#34;&gt;Noam Ross&lt;/a&gt;), it&amp;rsquo;s also an abstraction that can leave one feeling overwhelmed, and not knowing where to begin.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;There are also &lt;a href=&#34;https://github.com/ropensci/onboarding#why-review-packages-for-ropensci&#34;&gt;self-serving bonus points&lt;/a&gt; that one simply can&amp;rsquo;t avoid, should you go the rOpenSci-package-reviewing route&amp;ndash; especially if package development is new to you.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; Heck, the &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/reviewing_guide.md&#34;&gt;package reviewing guide&lt;/a&gt; alone was illuminating.&lt;/p&gt;

&lt;p&gt;Furthermore, the wise-sage 🦉 &lt;a href=&#34;https://github.com/ropensci/onboarding#associate-editors&#34;&gt;rOpenSci onboarding editors&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; are excellent matchmakers, and ensure that you&amp;rsquo;re actually reviewing a package authored by someone who &lt;em&gt;wants&lt;/em&gt; their package to be reviewed. This sounds simple enough, but it&amp;rsquo;s a comforting thought to know that your feedback isn&amp;rsquo;t totally unsolicited.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Yes, I&amp;rsquo;m quoting my own review.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;So, basically just playing myself&amp;hellip; Also I knew that, if nothing more, I can proofread and copy edit.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;There &lt;em&gt;are&lt;/em&gt; lots of good resources out there re. overcoming this obstacle, though (e.g. &lt;a href=&#34;http://www.firsttimersonly.com/&#34;&gt;First Timers Only&lt;/a&gt;; or &lt;a href=&#34;https://twitter.com/cvwickham&#34;&gt;Charlotte Wickham&lt;/a&gt;&amp;rsquo;s &lt;a href=&#34;http://cwick.co.nz/talks/collab-code-user17/#/&#34;&gt;Collaborative Coding&lt;/a&gt; from useR!2017 is esp. 👍 for the R-user).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;OK, so I don&amp;rsquo;t have a parallel world wherein a very experienced package-developer version of me is running around getting &lt;em&gt;less&lt;/em&gt; out of the process, &lt;em&gt;but&lt;/em&gt; if you already deeply understand package structure, you&amp;rsquo;re unlikely to stumble upon quite so many basic &amp;ldquo;a-ha&amp;rdquo; moments.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;👋 &lt;a href=&#34;https://github.com/noamross&#34;&gt;Noam Ross&lt;/a&gt;, &lt;a href=&#34;https://github.com/sckott&#34;&gt;Scott Chamberlain&lt;/a&gt;, &lt;a href=&#34;https://github.com/karthik&#34;&gt;Karthik Ram&lt;/a&gt;, &amp;amp; &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maëlle Salmon&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>New package tokenizers joins rOpenSci</title>
      <link>https://ropensci.org/blog/2016/08/23/tokenizers-joins-ropensci/</link>
      <pubDate>Tue, 23 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2016/08/23/tokenizers-joins-ropensci/</guid>
      <description>
        
        &lt;p&gt;The R package ecosystem for natural language processing has been flourishing in recent days. R packages for text analysis have usually been based on the classes provided by the &lt;a href=&#34;https://cran.r-project.org/package=NLP/&#34;&gt;NLP&lt;/a&gt; or &lt;a href=&#34;https://cran.r-project.org/package=tm/&#34;&gt;tm&lt;/a&gt; packages. Many of them depend on Java. But recently there have been a number of new packages for text analysis in R, most notably &lt;a href=&#34;https://github.com/dselivanov/text2vec&#34;&gt;text2vec&lt;/a&gt;, &lt;a href=&#34;https://github.com/kbenoit/quanteda&#34;&gt;quanteda&lt;/a&gt;, and &lt;a href=&#34;https://github.com/juliasilge/tidytext&#34;&gt;tidytext&lt;/a&gt;. These packages are built on top of &lt;a href=&#34;http://www.rcpp.org/&#34;&gt;Rcpp&lt;/a&gt; instead of &lt;a href=&#34;https://cran.r-project.org/package=rJava/&#34;&gt;rJava&lt;/a&gt;, which makes them much more reliable and portable. And instead of the classes based on NLP, which I have never thought to be particularly idiomatic for R, they use standard R data structures. The text2vec and quanteda packages both rely on the sparse matrices provided by the rock solid &lt;a href=&#34;https://cran.r-project.org/package=Matrix/&#34;&gt;Matrix&lt;/a&gt; package. The tidytext package is idiosyncratic (in the best possible way!) for doing all of its work in data frames rather than matrices, but a data frame is about as standard as you can get. For a long time when I would recommend R to people, I had to add the caveat that they should use Python if they were primarily interested in text analysis. But now I no longer feel the need to hedge.&lt;/p&gt;

&lt;p&gt;Still there is a lot of duplicated effort between these packages on the one hand and a lot of incompatibilities between the packages on the other. The R ecosystem for text analysis is not exactly coherent or consistent at the moment.&lt;/p&gt;

&lt;p&gt;My small contribution to the new text analysis ecosystem is the tokenizers package, which was recently accepted into rOpenSci after a careful &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/33&#34;&gt;peer review&lt;/a&gt; by &lt;a href=&#34;https://kevinushey.github.io/&#34;&gt;Kevin Ushey&lt;/a&gt;. A new version of the package is &lt;a href=&#34;https://cran.r-project.org/package=tokenizers/&#34;&gt;on CRAN&lt;/a&gt;. (Also check out the
Jeroen Ooms&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/hunspell&#34;&gt;hunspell&lt;/a&gt; package, which is a part of rOpensci.)&lt;/p&gt;

&lt;p&gt;One of the basic tasks in any NLP pipeline is turning texts (which humans can read) into tokens (which machines can compute with). For example, you might break a text into words or into &lt;a href=&#34;https://en.wikipedia.org/wiki/N-gram&#34;&gt;n-grams&lt;/a&gt;. Here is an example using the &lt;a href=&#34;https://memory.loc.gov/ammem/snhtml/snhome.html&#34;&gt;former slave interviews&lt;/a&gt; from the Great Depression era Federal Writers&amp;rsquo; Project. (A data package with those interviews is in development &lt;a href=&#34;https://github.com/lmullen/WPAnarratives&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# devtools::install_github(&amp;quot;lmullen/WPAnarratives&amp;quot;)
# install.packages(&amp;quot;tokenizers&amp;quot;)
library(WPAnarratives)
library(tokenizers)

text &amp;lt;- head(wpa_narratives$text, 5)
class(text)

## [1] &amp;quot;character&amp;quot;

words &amp;lt;- tokenize_words(text, lowercase = TRUE)
str(words)

## List of 5
##  $ : chr [1:1141] &amp;quot;_he&amp;quot; &amp;quot;loved&amp;quot; &amp;quot;young&amp;quot; &amp;quot;marster&amp;quot; ...
##  $ : chr [1:1034] &amp;quot;_old&amp;quot; &amp;quot;joe&amp;quot; &amp;quot;can&amp;quot; &amp;quot;keep&amp;quot; ...
##  $ : chr [1:824] &amp;quot;_jesus&amp;quot; &amp;quot;has&amp;quot; &amp;quot;my&amp;quot; &amp;quot;chillun&amp;quot; ...
##  $ : chr [1:779] &amp;quot;charity&amp;quot; &amp;quot;anderson&amp;quot; &amp;quot;who&amp;quot; &amp;quot;believes&amp;quot; ...
##  $ : chr [1:350] &amp;quot;dat&amp;quot; &amp;quot;was&amp;quot; &amp;quot;one&amp;quot; &amp;quot;time&amp;quot; ...

ngrams &amp;lt;- tokenize_ngrams(text, n_min = 3, n = 5)
str(ngrams)

## List of 5
##  $ : chr [1:3414] &amp;quot;_he loved young&amp;quot; &amp;quot;_he loved young marster&amp;quot; &amp;quot;_he loved young marster john_&amp;quot; &amp;quot;loved young marster&amp;quot; ...
##  $ : chr [1:3093] &amp;quot;_old joe can&amp;quot; &amp;quot;_old joe can keep&amp;quot; &amp;quot;_old joe can keep his&amp;quot; &amp;quot;joe can keep&amp;quot; ...
##  $ : chr [1:2463] &amp;quot;_jesus has my&amp;quot; &amp;quot;_jesus has my chillun&amp;quot; &amp;quot;_jesus has my chillun counted_&amp;quot; &amp;quot;has my chillun&amp;quot; ...
##  $ : chr [1:2328] &amp;quot;charity anderson who&amp;quot; &amp;quot;charity anderson who believes&amp;quot; &amp;quot;charity anderson who believes she&amp;quot; &amp;quot;anderson who believes&amp;quot; ...
##  $ : chr [1:1041] &amp;quot;dat was one&amp;quot; &amp;quot;dat was one time&amp;quot; &amp;quot;dat was one time when&amp;quot; &amp;quot;was one time&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Practically all text analysis packages provide their own functions for tokenizing text, so why do R users need this package?&lt;/p&gt;

&lt;p&gt;First, these tokenizers are reasonably fast. The basic string operations are handled by the &lt;a href=&#34;https://cran.r-project.org/package=stringi/&#34;&gt;stringi&lt;/a&gt; package, which is quick while also doing the correct thing across encodings and locales. And &lt;a href=&#34;http://dsnotes.com/&#34;&gt;Dmitriy Selivanov&lt;/a&gt; (author of the text2vec package) has written the n-gram and skip n-gram tokenizers in C++ so that those are fast too. It is probably possible to write tokenizers with better performance, but these are fast enough for even large scale text mining efforts.&lt;/p&gt;

&lt;p&gt;The second and more important reason is that these tokenizers are consistent. They all take either a character vector of any length, or a list where each element is a character vector of length one. The idea is that each element of the input comprises a text. Then each function returns a list with the same length as the input vector, where each element in the list contains the tokens generated by the function. If the input character vector or list is named, then the names are preserved, so that the names can serve as identifiers.&lt;/p&gt;

&lt;p&gt;And third, the tokenizers are reasonably comprehensive, including functions for characters, lines, words, word stems, sentences, paragraphs, n-grams, skip n-grams, and regular expressions.&lt;/p&gt;

&lt;p&gt;My hope is that developers of other text analysis packages for R will rely on this package to provide tokenizers. (So far only tidytext has taken me up on that, but I also have to re-write my own &lt;a href=&#34;https://github.com/ropensci/textreuse&#34;&gt;textreuse&lt;/a&gt; package now.) But even if natural language packages do not take the package as a formal dependency, most packages let you pass in your own tokenizing functions. So users can reap the benefits of a consistent set of tokenizers by using the functions in this package. The success of the &amp;ldquo;&lt;a href=&#34;https://twitter.com/hadleywickham/status/751805589425000450&#34;&gt;tidyverse&lt;/a&gt;&amp;rdquo; has shown the power of buying into a convention for the structure of data and the inputs and outputs of functions. My hope is that the tokenizers package is a step in that direction for text analysis in R.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
