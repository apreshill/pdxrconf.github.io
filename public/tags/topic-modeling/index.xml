<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Topic Modeling on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/topic-modeling/</link>
    <description>Recent content in Topic Modeling on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 16 Apr 2014 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/topic-modeling/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Topic modeling in R</title>
      <link>https://ropensci.org/blog/2014/04/16/topic-modeling-in-r/</link>
      <pubDate>Wed, 16 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2014/04/16/topic-modeling-in-r/</guid>
      <description>
        
        

&lt;p&gt;&lt;em&gt;Editor&amp;rsquo;s note: This is the first in a series of posts from rOpenSci&amp;rsquo;s &lt;a href=&#34;http://ropensci.github.io/hackathon/&#34;&gt;recent hackathon&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I recently had the pleasure of participating in &lt;a href=&#34;https://github.com/ropensci/hackathon/&#34;&gt;rOpenSci&amp;rsquo;s hackathon&lt;/a&gt;. To be honest, I was quite nervous to work among such notables, but I immediately felt welcome thanks to a warm and personable group. &lt;a href=&#34;http://alyssafrazee.com/&#34;&gt;Alyssa Frazee&lt;/a&gt; has a &lt;a href=&#34;http://simplystatistics.org/2014/04/10/the-ropensci-hackathon-ropenhack/&#34;&gt;great post summarizing the event&lt;/a&gt;, so check that out if you haven&amp;rsquo;t already. Once again, many thanks to rOpenSci for making it possible!&lt;/p&gt;

&lt;p&gt;In addition to learning and socializing at the hackathon, I wanted to ensure my time was productive, so I worked on a mini-project related to my research in text mining. rOpenSci has plethora of &lt;a href=&#34;http://ropensci.org/packages/index.html&#34;&gt;R packages&lt;/a&gt; for extracting literary content off the web, including &lt;a href=&#34;https://github.com/ropensci/elife&#34;&gt;elife&lt;/a&gt;, which is a lightweight interface to the &lt;a href=&#34;http://dev.elifesciences.org/&#34;&gt;elife API&lt;/a&gt;. This package is not yet available on CRAN, but we can easily install from GitHub thanks to devtools.&lt;/p&gt;

&lt;h4 id=&#34;installing-the-package&#34;&gt;Installing the package&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;library(devtools)
install_github(&amp;quot;ropensci/elife&amp;quot;)
library(elife)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;brief-overview-of-topic-models&#34;&gt;Brief Overview of Topic Models&lt;/h4&gt;

&lt;p&gt;My research in text mining is focused on a particular type of &lt;a href=&#34;http://en.wikipedia.org/wiki/Topic_model&#34;&gt;topic model&lt;/a&gt; known as &lt;a href=&#34;http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#34;&gt;Latent Dirichlet Allocation&lt;/a&gt; (LDA). In general, a topic model discovers topics (e.g., hidden themes) within a collection of documents. For example, if a given document is generated from a hypothetical &amp;ldquo;statistics topic&amp;rdquo;, there might be a 10% chance a given word in that document is &amp;ldquo;model&amp;rdquo;, a 5% chance that word is &amp;ldquo;probability&amp;rdquo;, a 1% that word is &amp;ldquo;algorithm&amp;rdquo;, etc. Whereas, if a document is generated from a hypothetical &amp;ldquo;computer science topic&amp;rdquo;, there might be a 4% chance a given word in that document is &amp;ldquo;model&amp;rdquo;, a 2% chance that word is &amp;ldquo;probability&amp;rdquo;, a 16% that word is &amp;ldquo;algorithm&amp;rdquo;, etc. In other words, each topic is defined by a probability mass function over each possible word.&lt;/p&gt;

&lt;p&gt;LDA takes this example one step further and allows for each document to be generated from a mixture of topics. For example, a particular document could be 60% statistics, 10% computer science, 20% mathematics, etc. Whereas, a different document could be 30% statistics, 30% computer science, 15% mathematics, etc. Within the LDA literature, fitting models to abstracts of academic articles is quite common, so I thought it would be neat to do the same with abstracts from elife articles.&lt;/p&gt;

&lt;h4 id=&#34;get-all-the-elife-abstracts&#34;&gt;Get all the elife abstracts!&lt;/h4&gt;

&lt;p&gt;In order to grab all the abstracts, first we&amp;rsquo;ll grab all the &lt;a href=&#34;http://en.wikipedia.org/wiki/Digital_object_identifier&#34;&gt;DOIs&lt;/a&gt; that point to currently available articles. Note that we can do more complicated queries of specific articles with &lt;code&gt;searchelife&lt;/code&gt; (the &lt;code&gt;help(searchelife)&lt;/code&gt; page has some nice examples).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dois &amp;lt;- searchelife(terms = &amp;quot;*&amp;quot;, searchin = &amp;quot;article_title&amp;quot;, boolean = &amp;quot;matches&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;dois&lt;/code&gt; can now be used to obtain all sorts of meta data associated with these articles using &lt;code&gt;elife_doi&lt;/code&gt;. In this case, I just want the abstracts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;abs &amp;lt;- sapply(dois, function(x) elife_doi(x, ret = &amp;quot;abstract&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here, we have what we need to fit the topic model. I don&amp;rsquo;t want to focus on technical details here, but if you are interested in the statistics involved, I recommend reading &lt;a href=&#34;http://cpsievert.github.io/xkcd/&#34;&gt;my post on xkcd comics&lt;/a&gt;. This post also covers the method I use to determine an optimal number of topics. I&amp;rsquo;ve provided all the code used to fit the model &lt;a href=&#34;https://github.com/cpsievert/cpsievert.github.com/blob/master/elife/elife.R&#34;&gt;here&lt;/a&gt;, but let&amp;rsquo;s skip to the fun part and jump right into exploring the model output.&lt;/p&gt;

&lt;p&gt;The window below is an interactive visualization of the LDA output derived from elife abstracts. The aim of this visualization is to aid interpretation of topics. Topic interpretation tends to be difficult since each topic is defined by a probability distribution with support over many of words. With this interactive visualization, one can focus on the most &amp;ldquo;relevant&amp;rdquo; words for any topic by hovering/clicking over the appropriate circle. We will define &amp;ldquo;relevance&amp;rdquo; shortly, but for now, go ahead and click on the circle towards the bottom labeled &amp;ldquo;11&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://gallery.shinyapps.io/LDAelife/&#34; target=&#34;_blank&#34;&gt;Go To Application&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;&lt;div class=&#34;col-sm-10 col-lg-10&#34;&gt;
&lt;iframe src=&#34;https://gallery.shinyapps.io/LDAelife/&#34; width=&#34;1200&#34; height = &#34;800&#34;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;div class=&#34;col-sm-8 col-sm-offset-2&#34;&gt;&lt;/p&gt;

&lt;p&gt;Now that topic 11 is selected, in the bar chart to the right, we see &amp;ldquo;relat&amp;rdquo;, &amp;ldquo;evolut&amp;rdquo;, and &amp;ldquo;similar&amp;rdquo; are the top 3 most relevant words. Towards the bottom of the bar chart, we see &amp;ldquo;resid&amp;rdquo; which is the 30th most relevant. Note that before the model was fit &lt;a href=&#34;http://en.wikipedia.org/wiki/Stemming&#34;&gt;stemming&lt;/a&gt; was performed. Thus, a word like &amp;ldquo;relat&amp;rdquo; could stand for &amp;ldquo;relation&amp;rdquo;, &amp;ldquo;relations&amp;rdquo;, &amp;ldquo;relationship&amp;rdquo;, etc. You might now be thinking: &amp;ldquo;That&amp;rsquo;s great, I can see this topic is related to evolutionary biology, but why are these words ranked in this order?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;The topic specific word rankings are determined by a measure known as &lt;em&gt;relevance&lt;/em&gt;. Relevance is a compromise between the probability of a word given the topic (the width of the red bars) and the probability within topic divided by the overall frequency of the word (the ratio of red to gray). Note the &amp;ldquo;Value of lambda&amp;rdquo; slider which controls this compromise. A value of 1 for lambda will rank words solely on the width of the red bars (which tends to over-rank common words). A value of 0 for lambda will rank words solely on the ratio of red to gray (which tends to over-rank rare words). A recent study has shown evidence for an optimal value of lambda around 0.6 which is the default value.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;topic landscape&amp;rdquo; on the left-hand side provides a sense of topic similarity by approximating distances between topics. To produce the point locations, pairwise distances between topic specific word distributions are computed according to the currently selected measure in the &amp;ldquo;Topical Distance Calculation&amp;rdquo; menu. Those distances are next scaled down to two dimensions using the currently select algorithm in the &amp;ldquo;Multidimensional Scaling Method&amp;rdquo; menu. By default, the circle sizes are proportional to the prevalence of each topic in the collection of text.&lt;/p&gt;

&lt;p&gt;Hovering over labels on the bar chart allows us to explore different contexts for the same word. Upon hovering over a word, circles in the topic landscape will change according to the distribution over topics for that given word. For example, if we hover over &amp;ldquo;evolut&amp;rdquo; (the 2nd most relevant word for topic 11 when lambda is 0.6), we see that it has large mass under topic 11 and 20. If we now click on topic 20, the most relevant words suggest this is a &amp;ldquo;population genetics&amp;rdquo; topic. Interestingly, if we hover over &amp;ldquo;adapt&amp;rdquo; (the 29th most relevant word for topic 20 when lambda is 0.6), we see that it has large mass under topic 1 and 20. Now, if we hover over topic 1, we see a &amp;ldquo;general life science&amp;rdquo; topic.&lt;/p&gt;

&lt;p&gt;There are certainly many other things to discover using this interactive visualization. I hope you take the time to explore and leave a comment with findings or questions below. If you are interested in text mining, or open science in general, definitely check out &lt;a href=&#34;http://ropensci.org/packages/index.html&#34;&gt;rOpenSci&amp;rsquo;s R packages&lt;/a&gt;. If you&amp;rsquo;d like to make a similar interactive visualization, check out the &lt;a href=&#34;https://github.com/cpsievert/LDAvis/&#34;&gt;LDAvis&lt;/a&gt; package.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
