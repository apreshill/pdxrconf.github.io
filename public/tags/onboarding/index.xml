<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Onboarding on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/onboarding/</link>
    <description>Recent content in Onboarding on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 29 Nov 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/onboarding/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Announcing a New rOpenSci Software Review Collaboration</title>
      <link>https://ropensci.org/blog/2017/11/29/review-collaboration-mee/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/29/review-collaboration-mee/</guid>
      <description>
        
        

&lt;p&gt;rOpenSci is pleased to announce a new collaboration with the &lt;a href=&#34;http://besjournals.onlinelibrary.wiley.com/hub/journal/10.1111/(ISSN)2041-210X/&#34;&gt;Methods in Ecology and Evolution (MEE)&lt;/a&gt;, a journal of the &lt;a href=&#34;http://www.britishecologicalsociety.org/&#34;&gt;British Ecological Society&lt;/a&gt;, published by Wiley press &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Publications destined for MEE that include the development of a scientific R package will now have the option of a joint review process whereby the R package is reviewed by rOpenSci, followed by fast-tracked review of the manuscript by MEE. Authors opting for this process will be recognized via a mark on both web and print versions of their paper.&lt;/p&gt;

&lt;p&gt;We are very excited for this partnership to improve the rigor of both scientific software and software publications and to provide greater recognition to developers in the fields of ecology and evolution.  It is a natural outgrowth of our interest in supporting scientists in developing and maintaining software, and of MEE&amp;rsquo;s mission of vetting and disseminating tools and methods for the research community. The collaboration formalizes and eases a path already pursued by researchers: The &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12593/abstract&#34;&gt;&lt;strong&gt;rotl&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12469/abstract&#34;&gt;&lt;strong&gt;RNexML&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00247.x/abstract&#34;&gt;&lt;strong&gt;treebase&lt;/strong&gt;&lt;/a&gt; packages were all developed or reviewed by rOpenSci and subsequently had associated manuscripts published in MEE.&lt;/p&gt;

&lt;h3 id=&#34;about-ropensci-software-review&#34;&gt;About rOpenSci software review&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; is a diverse community of researchers from academia, non-profit, government, and industry who collaborate to develop and maintain tools and practices around open data and reproducible research. The rOpenSci suite of tools is made of core infrastructure software developed and maintained by the &lt;a href=&#34;https://ropensci.org/about#team&#34;&gt;project staff&lt;/a&gt;. The suite also contains numerous packages that are contributed by members of the broader R community. The volume of community submissions has grown considerably over the years necessitating a formal system of review quite analogous to that of a peer reviewed academic journal.&lt;/p&gt;

&lt;p&gt;rOpenSci welcomes full software submissions that fit within our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;aims and scope&lt;/a&gt;, with the option of a pre-submission inquiry in cases when the scope of a submission is not immediately obvious. This software peer review framework, known as the rOpenSci Onboarding process, operates with three editors and one editor in chief who carefully vet all incoming submissions. After an editorial review, editors solicit detailed, public and signed reviews from two reviewers, and the path to acceptance from then on is similar to a standard journal review process. Details about the system are described in &lt;a href=&#34;https://ropensci.org/blog/2016/03/28/software-review/&#34;&gt;various&lt;/a&gt; &lt;a href=&#34;https://ropensci.org/blog/2017/09/01/nf-softwarereview/&#34;&gt;blog&lt;/a&gt; &lt;a href=&#34;https://ropensci.org/blog/2017/09/11/software-review-update/&#34;&gt;posts&lt;/a&gt; by the editorial team.&lt;/p&gt;

&lt;h3 id=&#34;collaboration-with-journals&#34;&gt;Collaboration with journals&lt;/h3&gt;

&lt;p&gt;This is our second collaboration with a journal. Since late 2015, rOpenSci has partnered with the &lt;a href=&#34;http://joss.theoj.org/&#34;&gt;Journal of Open Source software (JOSS)&lt;/a&gt;, an open access journal that publishes brief articles on research software. Packages accepted to rOpenSci can be submitted for fast-track publication at JOSS, in which JOSS editors may evaluate based on rOpenSci&amp;rsquo;s reviews alone. As rOpenSci&amp;rsquo;s review criteria is significantly more stringent and designed to be compatible with JOSS, these packages are generally accepted without additional review. We have had great success with this partnership providing rOpenSci authors with an additional venue to publicize and archive their work. Given this success, we are keen on expanding to other journals and fields where there is potential for software reviewed and created by rOpenSci to play a significant role in supporting scientific findings.&lt;/p&gt;

&lt;h3 id=&#34;the-details&#34;&gt;The details&lt;/h3&gt;

&lt;p&gt;Our new partnership with MEE broadly resembles that with JOSS, with the major difference that MEE, rather than rOpenSci, leads review of the manuscript component.  Authors with R packages and associated manuscripts that fit the Aims and Scope for both &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;rOpenSci&lt;/a&gt; and &lt;a href=&#34;http://www.methodsinecologyandevolution.org/view/0/aimsAndScope.html&#34;&gt;MEE&lt;/a&gt; are encouraged to first submit to rOpenSci. The &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12593/abstract&#34;&gt;&lt;strong&gt;rotl&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12469/abstract&#34;&gt;&lt;strong&gt;RNexML&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00247.x/abstract&#34;&gt;&lt;strong&gt;treebase&lt;/strong&gt;&lt;/a&gt; packages are all great examples of such packages. MEE editors may also refer authors to this option if authors submit an appropriate manuscript to MEE first.&lt;/p&gt;

&lt;p&gt;On submission to rOpenSci, authors can use our updated &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/issue_template.md&#34;&gt;submission template&lt;/a&gt; to choose MEE as a publication venue. Following acceptance by rOpenSci, the associated manuscript will be reviewed by an expedited process at MEE, with reviewers and editors having the knowledge that the software has already been reviewed and the public reviews available to them.&lt;/p&gt;

&lt;p&gt;Should the manuscript be accepted, a footnote will appear in the web version and the first page of the print version of the MEE article indicating that the software as well as the manuscript has been peer-reviewed, with a link to the rOpenSci open reviews.&lt;/p&gt;

&lt;p&gt;As with any collaboration, there may be a few hiccups early on and we welcome ideas to make the process more streamlined and efficient. We look forward to the community&amp;rsquo;s submissions and to your participation in this process.&lt;/p&gt;

&lt;p&gt;Many thanks to MEE&amp;rsquo;s Assistant Editor Chris Grieves and Senior Editor Bob O&amp;rsquo;Hara for working with us on this collaboration.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;See also MEE&amp;rsquo;s post from today at &lt;a href=&#34;https://methodsblog.wordpress.com/2017/11/29/software-review/&#34;&gt;https://methodsblog.wordpress.com/2017/11/29/software-review/&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>Data from Public Bicycle Hire Systems</title>
      <link>https://ropensci.org/blog/2017/10/17/bikedata/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/17/bikedata/</guid>
      <description>
        
        

&lt;p&gt;A new rOpenSci package provides access to data to which users may already have directly contributed, and for which contribution is fun, keeps you fit, and &lt;a href=&#34;http://www.bmj.com/content/357/bmj.j1456&#34;&gt;helps make the world a better place&lt;/a&gt;. The data come from using public bicycle hire schemes, and the package is called &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt;. Public bicycle hire systems operate in many cities throughout the world, and most systems collect (generally anonymous) data, minimally consisting of the times and locations at which every single bicycle trip starts and ends. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package provides access to data from all cities which openly publish these data, currently including &lt;a href=&#34;https://tfl.gov.uk/modes/cycling/santander-cycles&#34;&gt;London, U.K.&lt;/a&gt;, and in the U.S.A., &lt;a href=&#34;https://www.citibikenyc.com&#34;&gt;New York&lt;/a&gt;, &lt;a href=&#34;https://bikeshare.metro.net&#34;&gt;Los Angeles&lt;/a&gt;, &lt;a href=&#34;https://www.rideindego.com&#34;&gt;Philadelphia&lt;/a&gt;, &lt;a href=&#34;https://www.divvybikes.com&#34;&gt;Chicago&lt;/a&gt;, &lt;a href=&#34;https://www.thehubway.com&#34;&gt;Boston&lt;/a&gt;, and &lt;a href=&#34;https://www.capitalbikeshare.com&#34;&gt;Washington DC&lt;/a&gt;. The package will expand as more cities openly publish their data (with the newly enormously expanded San Francisco system &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/2&#34;&gt;next on the list&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;why-bikedata&#34;&gt;Why bikedata?&lt;/h3&gt;

&lt;p&gt;The short answer to that question is that the package provides access to what is arguably one of the most spatially and temporally detailed databases of finely-scaled human movement throughout several of the world&amp;rsquo;s most important cities. Such data are likely to prove invaluable in the increasingly active and well-funded attempt to develop a science of cities. Such a science does not yet exist in any way comparable to most other well-established scientific disciplines, but the importance of developing a science of cities is indisputable, and reflected in such enterprises as the NYU-based &lt;a href=&#34;http://cusp.nyu.edu&#34;&gt;Center for Urban Science and Progress&lt;/a&gt;, or the UCL-based &lt;a href=&#34;https://www.ucl.ac.uk/bartlett/casa/&#34;&gt;Centre for Advanced Spatial Analysis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;People move through cities, yet at present anyone faced with the seemingly fundamental question of how, when, and where people do so would likely have to draw on some form of private data (typically operators of transport systems or mobile phone providers). There are very few open, public data providing insight into this question. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package aims to be one contribution towards filling this gap. The data accessed by the package are entirely open, and are constantly updated, typically on a monthly basis. The package thus provides ongoing insight into the dynamic changes and reconfigurations of these cities. Data currently available via the package amounts to several tens of Gigabytes, and will expand rapidly both with time, and with the inclusion of more cities.&lt;/p&gt;

&lt;h3 id=&#34;why-are-these-data-published&#34;&gt;Why are these data published?&lt;/h3&gt;

&lt;p&gt;In answer to that question, all credit must rightfully go to &lt;a href=&#34;http://www.theregister.co.uk/2011/01/11/transport_for_london_foi/&#34;&gt;Adrian Short&lt;/a&gt;, who submitted a Freedom of Information request in 2011 to Transport for London for usage statistics from the relatively new, and largely publicly-funded, bicycle scheme. This request from one individual ultimately resulted in the data being openly published on an ongoing basis. All U.S. systems included in &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; commenced operation subsequent to that point in time, and many of them have openly published their data from the very beginning. The majority of the world&amp;rsquo;s public bicycle hire systems (&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_bicycle-sharing_systems&#34;&gt;see list here&lt;/a&gt;) nevertheless do not openly publish data, notably including very large systems in China, France, and Spain. One important aspiration of the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package is to demonstrate the positive benefit for the cities themselves of openly and easily facilitating complex analyses of usage data, which brings us to &amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;what-s-important-about-these-data&#34;&gt;What&amp;rsquo;s important about these data?&lt;/h3&gt;

&lt;p&gt;As mentioned, the data really do provide uniquely valuable insights into the movement patterns and behaviour of people within some of the world&amp;rsquo;s major cities. While the more detailed explorations below demonstrate the kinds of things that can be done with the package, the variety of insights these data facilitate is best demonstrated through considering the work of other people, exemplified by &lt;a href=&#34;http://toddwschneider.com/posts/a-tale-of-twenty-two-million-citi-bikes-analyzing-the-nyc-bike-share-system/&#34;&gt;Todd Schneider&amp;rsquo;s high-profile blog piece&lt;/a&gt; on the New York City system. Todd&amp;rsquo;s analyses clearly demonstrate how these data can provide insight into where and when people move, into inter-relationships between various forms of transport, and into relationships with broader environmental factors such as weather. As cities evolve, and public bicycle hire schemes along with them, data from these systems can play a vital role in informing and guiding the ongoing processes of urban development. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package greatly facilitates analysing such processes, not only through making data access and aggregation enormously easier, but through enabling analyses from any one system to be immediately applied to, and compared with, any other systems.&lt;/p&gt;

&lt;h3 id=&#34;how-it-works&#34;&gt;How it works&lt;/h3&gt;

&lt;p&gt;The package currently focusses on the data alone, and provides functionality for downloading, storage, and aggregation. The data are stored in an &lt;code&gt;SQLite3&lt;/code&gt; database, enabling newly published data to be continually added, generally with one simple line of code. It&amp;rsquo;s as easy as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;store_bikedata (city = &amp;quot;chicago&amp;quot;, bikedb = &amp;quot;bikedb&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the nominated database (&lt;code&gt;bikedb&lt;/code&gt;) already holds data for Chicago, only new data will be added, otherwise all historical data will be downloaded and added. All bicycle hire systems accessed by &lt;code&gt;bikedata&lt;/code&gt; have fixed docking stations, and the primary means of aggregation is in terms of &amp;ldquo;trip matrices&amp;rdquo;, which are square matrices of numbers of trips between all pairs of stations, extracted with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chi&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that most parameters are highly flexible in terms of formatting, so pretty much anything starting with &lt;code&gt;&amp;quot;ch&amp;quot;&lt;/code&gt; will be recognised as Chicago. Of course, if the database only contains data for Chicago, the &lt;code&gt;city&lt;/code&gt; parameter may be omitted entirely. Trip matrices may be filtered by time, through combinations of year, month, day, hour, minute, or even second, as well as by demographic characteristics such as gender or date of birth for those systems which provide such data. (These latter data are freely provided by users of the systems, and there can be no guarantee of their accuracy.) These can all be combined in calls like the following, which further demonstrates the highly flexible ways of specifying the various parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (&amp;quot;bikedb&amp;quot;, city = &amp;quot;london, innit&amp;quot;,
                       start_date = 20160101, end_date = &amp;quot;16,02,28&amp;quot;,
                       start_time = 6, end_time = 24,
                       birth_year = 1980:1990, gender = &amp;quot;f&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The second mode of aggregation is as daily time series, via the &lt;code&gt;bike_daily_trips()&lt;/code&gt; function. See &lt;a href=&#34;https://ropensci.github.io/bikedata/articles/bikedata.html&#34;&gt;the vignette&lt;/a&gt; for further details.&lt;/p&gt;

&lt;h3 id=&#34;what-can-be-done-with-these-data&#34;&gt;What can be done with these data?&lt;/h3&gt;

&lt;p&gt;Lots of things. How about examining how far people ride. This requires getting the distances between all pairs of docking stations as routed through the street network, to yield a distance matrix corresponding to the trip matrix. The latest version of &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; has a brand new function to perform exactly that task, so it&amp;rsquo;s as easy as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;ropensci/bikedata&amp;quot;) # to install latest version
dists &amp;lt;- bike_distmat (bikedb = bikedb, city = &amp;quot;chicago&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are distances as routed through the underlying street network, with street types prioritised for bicycle travel. The network is extracted from OpenStreetMap using the &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;rOpenSci &lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;, and the distances are calculated using a brand new package called &lt;a href=&#34;https://cran.r-project.org/package=dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; (Distances on Directed Graphs). (Disclaimer: It&amp;rsquo;s my package, and this is a shameless plug for it - please use it!)&lt;/p&gt;

&lt;p&gt;The distance matrix extracted with &lt;code&gt;bike_distmat&lt;/code&gt; is between all stations listed for a given system, which &lt;code&gt;bike_tripmat&lt;/code&gt; will return trip matrices only between those stations in operation over a specified time period. Because systems expand over time, the two matrices will generally not be directly comparable, so it is necessary to submit both to the &lt;code&gt;bikedata&lt;/code&gt; function &lt;code&gt;match_matrices()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 636 636
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mats &amp;lt;- match_matrices (trips, dists)
trips &amp;lt;- mats$trip
dists &amp;lt;- mats$dist
dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 581 581
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;identical (rownames (trips), rownames (dists))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Distances can then be visually related to trip numbers to reveal their distributional form. These matrices contain too many values to plot directly, so the &lt;code&gt;hexbin&lt;/code&gt; package is used here to aggregate in a &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library (hexbin)
library (ggplot2)
dat &amp;lt;- data.frame (distance = as.vector (dmat),
                   number = as.vector (trips))
ggplot (dat, aes (x = distance, y = number)) +
    stat_binhex(aes(fill = log (..count..))) +
    scale_x_log10 (breaks = c (0.1, 0.5, 1, 2, 5, 10, 20),
                   labels = c (&amp;quot;0.1&amp;quot;, &amp;quot;0.5&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;20&amp;quot;)) +
    scale_y_log10 (breaks = c (10, 100, 1000)) +
    scale_fill_gradientn(colours = c(&amp;quot;seagreen&amp;quot;,&amp;quot;goldenrod1&amp;quot;),
                         name = &amp;quot;Frequency&amp;quot;, na.value = NA) +
    guides (fill = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/chicago.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The central region of the graph (yellow hexagons) reveals that numbers of trips generally decrease roughly exponentially with increasing distance (noting that scales are logarithmic), with most trip distances lying below 5km. What is the &amp;ldquo;average&amp;rdquo; distance travelled in Chicago? The easiest way to calculate this is as a weighted mean,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum (as.vector (dmat) * as.vector (trips) / sum (trips), na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 2.510285
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;giving a value of just over 2.5 kilometres. We could also compare differences in mean distances between cyclists who are registered with a system and causal users. These two categories may be loosely considered to reflect &amp;ldquo;residents&amp;rdquo; and &amp;ldquo;non-residents&amp;rdquo;. Let&amp;rsquo;s wrap this in a function so we can use it for even cooler stuff in a moment.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean &amp;lt;- function (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chicago&amp;quot;)
{
    tm &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
    tm_memb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = TRUE)
    tm_nomemb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = FALSE)
    stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
    dists &amp;lt;- bike_distmat (bikedb = bikedb, city = city)
    mats &amp;lt;- match_mats (dists, tm_memb)
    tm_memb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm_nomemb)
    tm_nomemb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm)
    tm &amp;lt;- mats$trip
    dists &amp;lt;- mats$dists

    d0 &amp;lt;- sum (as.vector (dists) * as.vector (tm) / sum (tm), na.rm = TRUE)
    dmemb &amp;lt;- sum (as.vector (dists) * as.vector (tmemb) / sum (t_memb), na.rm = TRUE)
    dnomemb &amp;lt;- sum (as.vector (dists) * as.vector (tm_nomemb) / sum (tm_nomemb), na.rm = TRUE)
    res &amp;lt;- c (d0, dmemb / dnomemb)
    names (res) &amp;lt;- c (&amp;quot;dmean&amp;quot;, &amp;quot;ratio_memb_non&amp;quot;)
    return (res)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Differences in distances ridden between &amp;ldquo;resident&amp;rdquo; and &amp;ldquo;non-resident&amp;rdquo; cyclists can then be calculated with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean (bikedb = bikedb, city = &amp;quot;ch&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##          dmean ratio_memb_non
##       2.510698       1.023225
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And system members cycle slightly longer distances than non-members. (Do not at this point ask about statistical tests - these comparisons are made between millions&amp;ndash;often tens of millions&amp;ndash;of points, and statistical significance may always be assumed to be negligibly small.) Whatever the reason for this difference between &amp;ldquo;residents&amp;rdquo; and others, we can use this exact same code to compare equivalent distances for all cities which record whether users are members or not (which is all cities except London and Washington DC).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cities &amp;lt;- c (&amp;quot;ny&amp;quot;, &amp;quot;ch&amp;quot;, &amp;quot;bo&amp;quot;, &amp;quot;la&amp;quot;, &amp;quot;ph&amp;quot;) # NYC, Chicago, Boston, LA, Philadelphia
sapply (cities, function (i) dmean (bikedb = bikedb, city = i))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                       ny       ch       bo       la       ph
## dmean          2.8519131 2.510285 2.153918 2.156919 1.702372
## ratio_memb_non 0.9833729 1.023385 1.000635 1.360099 1.130929
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we thus discover that Boston manifests the greatest equality in terms of distances cycled between residents and non-residents, while LA manifests the greatest difference. New York City is the only one of these five in which non-members of the system actually cycle further than members. (And note that these two measures can&amp;rsquo;t be statistically compared in any direct way, because mean distances are also affected by relative numbers of member to non-member trips.) These results likely reflect a host of (scientifically) interesting cultural and geo-spatial differences between these cities, and demonstrate how the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package (combined with &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt;&lt;/a&gt;) can provide unique insight into differences in human behaviour between some of the most important cities in the U.S.&lt;/p&gt;

&lt;h3 id=&#34;visualisation&#34;&gt;Visualisation&lt;/h3&gt;

&lt;p&gt;Many users are likely to want to visualise how people use a given bicycle system, and in particular are likely to want to produce maps. This is also readily done with the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt;, which can route and aggregate transit flows for a particular mode of transport throughout a street network. Let&amp;rsquo;s plot bicycle flows for the Indego System of Philadelphia PA. First get the trip matrix, along with the coordinates of all bicycle stations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;gmost/dodgr&amp;quot;) # to install latest version
city &amp;lt;- &amp;quot;ph&amp;quot;
# store_bikedata (bikedb = bikedb, city = city) # if not already done
trips &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
xy &amp;lt;- stns [, which (names (stns) %in% c (&amp;quot;longitude&amp;quot;, &amp;quot;latitude&amp;quot;))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows of cyclists are calculated between those &lt;code&gt;xy&lt;/code&gt;points, so the &lt;code&gt;trips&lt;/code&gt; table has to match the &lt;code&gt;stns&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;indx &amp;lt;- match (stns$stn_id, rownames (trips))
trips &amp;lt;- trips [indx, indx]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; can be used to extract the underlying street network surrounding those &lt;code&gt;xy&lt;/code&gt; points (expanded here by 50%):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;net &amp;lt;- dodgr_streetnet (pts = xy, expand = 0.5) %&amp;gt;%
    weight_streetnet (wt_profile = &amp;quot;bicycle&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then need to align the bicycle station coordinates in &lt;code&gt;xy&lt;/code&gt; to the nearest points (or &amp;ldquo;vertices&amp;rdquo;) in the street network:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;verts &amp;lt;- dodgr_vertices (net)
pts &amp;lt;- verts$id [match_pts_to_graph (verts, xy)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows between these points can then be mapped onto the underlying street network with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;flow &amp;lt;- dodgr_flows (net, from = pts, to = pts, flow = trips) %&amp;gt;%
    merge_directed_flows ()
net &amp;lt;- net [flow$edge_id, ]
net$flow &amp;lt;- flow$flow
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; documentation&lt;/a&gt; for further details of how this works. We&amp;rsquo;re now ready to plot those flows, but before we do, let&amp;rsquo;s overlay them on top of the rivers of Philadelphia, extracted with rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;q &amp;lt;- opq (&amp;quot;Philadelphia pa&amp;quot;)
rivers1 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;waterway&amp;quot;, value = &amp;quot;river&amp;quot;, value_exact = FALSE) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers2 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;natural&amp;quot;, value = &amp;quot;water&amp;quot;) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers &amp;lt;- c (rivers1, rivers2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally plot the map, using rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmplotr&#34;&gt;&lt;code&gt;osmplotr&lt;/code&gt; package&lt;/a&gt; to prepare a base map with the underlying rivers, and the &lt;code&gt;ggplot2::geom_segment()&lt;/code&gt; function to add the line segments with colours and widths weighted by bicycle flows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#gtlibrary (osmplotr)
require (ggplot2)
bb &amp;lt;- get_bbox (c (-75.22, 39.91, -75.10, 39.98))
cols &amp;lt;- colorRampPalette (c (&amp;quot;lawngreen&amp;quot;, &amp;quot;red&amp;quot;)) (30)
map &amp;lt;- osm_basemap (bb, bg = &amp;quot;gray10&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_multipolygons, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_lines, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_colourbar (zlims = range (net$flow / 1000), col = cols)
map &amp;lt;- map + geom_segment (data = net, size = net$flow / 50000,
                           aes (x = from_lon, y = from_lat, xend = to_lon, yend = to_lat,
                                colour = flow, size = flow)) +
    scale_colour_gradient (low = &amp;quot;lawngreen&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;none&amp;quot;)
print_osm_map (map)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/ph.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The colour bar on the right shows thousands of trips, with the map revealing the relatively enormous numbers crossing the South Street Bridge over the Schuylkill River, leaving most other flows coloured in the lower range of green or yellows. This map thus reveals that anyone wanting to see Philadelphia&amp;rsquo;s Indego bikes in action without braving the saddle themselves would be best advised to head straight for the South Street Bridge.&lt;/p&gt;

&lt;h3 id=&#34;future-plans&#34;&gt;Future plans&lt;/h3&gt;

&lt;p&gt;Although the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; greatly facilitates the production of such maps, the code is nevertheless rather protracted, and it would probably be very useful to convert much of the code in the preceding section to an internal &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; function to map trips between pairs of stations onto corresponding flows through the underlying street networks.&lt;/p&gt;

&lt;p&gt;Beyond that point, and the list of currently open issues awaiting development on the &lt;a href=&#34;https://github.com/ropensci/bikedata/issues&#34;&gt;github repository&lt;/a&gt;, future development is likely to depend very much on how users use the package, and on what extra features people might want. How can you help? A great place to start might be the official &lt;a href=&#34;https://ropensci.org/blog/blog/2017/10/02/hacktoberfest&#34;&gt;Hacktoberfest issue&lt;/a&gt;, helping to import the next lot of data from &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/34&#34;&gt;San Francisco&lt;/a&gt;. Or just use the package, and open up a new issue in response to any ideas that might pop up, no matter how minor they might seem. See the &lt;a href=&#34;https://github.com/ropensci/bikedata/blob/master/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for general advice.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Finally, this package wouldn&amp;rsquo;t be what it is without my co-author &lt;a href=&#34;https://github.com/richardellison&#34;&gt;Richard Ellison&lt;/a&gt;, who greatly accelerated development through encouraging C rather than C++ code for the SQL interfaces. &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maëlle Salmon&lt;/a&gt; majestically guided the entire review process, and made the transformation of the package to its current polished form a joy and a pleasure. I remain indebted to both &lt;a href=&#34;https://github.com/chucheria&#34;&gt;Bea Hernández&lt;/a&gt; and &lt;a href=&#34;https://github.com/eamcvey&#34;&gt;Elaine McVey&lt;/a&gt; for offering their time to extensively test and &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/116&#34;&gt;review the package&lt;/a&gt; as part of rOpenSci&amp;rsquo;s onboarding process. The review process has made the package what it is, and for that I am grateful to all involved!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>googleLanguageR - Analysing language through the Google Cloud Machine Learning APIs</title>
      <link>https://ropensci.org/blog/2017/10/03/googlelanguager/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/03/googlelanguager/</guid>
      <description>
        
        

&lt;!-- open source image taken from: https://upload.wikimedia.org/wikipedia/commons/2/21/Bell_System_switchboard.jpg --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/switchboard.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the greatest assets human beings possess is the power of speech and language, from which almost all our other accomplishments flow. To be able to analyse communication offers us a chance to gain a greater understanding of one another.&lt;/p&gt;

&lt;p&gt;To help you with this, &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt;&lt;/a&gt; is an R package that allows you to perform speech-to-text transcription, neural net translation and natural language processing via the &lt;a href=&#34;https://cloud.google.com/products/machine-learning/&#34;&gt;Google Cloud machine learning services&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An introduction to the package is below, but you can find out more details at the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-s-bet&#34;&gt;Google&amp;rsquo;s bet&lt;/h3&gt;

&lt;p&gt;Google predicts that machine learning is to be a fundamental feature of business, and so they are looking to become the infrastructure that makes machine learning possible. Metaphorically speaking: If machine learning is electricity, then Google wants to be the pylons carrying it around the country.&lt;/p&gt;

&lt;!-- open source image taken from: https://pixabay.com/en/pylon-sky-electricity-tower-2515429/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/pylon.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Google may not be the only company with such ambitions, but one advantage Google has is the amount of data it possesses. Twenty years of web crawling has given it an unprecedented corpus to train its models.  In addition, its recent moves into voice and video gives it one of the biggest audio and speech datasets, all of which have been used to help create machine learning applications within its products such as search and Gmail. Further investment in machine learning is shown by Google&amp;rsquo;s purchase of &lt;a href=&#34;https://deepmind.com/&#34;&gt;Deepmind&lt;/a&gt;, a UK based A.I. research firm that recently was in the news for defeating the top Go champion with its neural network trained Go bot.  Google has also taken an open-source route with the creation and publication of &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt;, a leading machine learning framework.&lt;/p&gt;

&lt;p&gt;Whilst you can create your own machine learning models, for those users who haven&amp;rsquo;t the expertise, data or time to do so, Google also offers an increasing range of machine learning APIs that are pre-trained, such as image and video recognition or job search.  &lt;code&gt;googleLanguageR&lt;/code&gt; wraps the subset of those machine learning APIs that are language flavoured - Cloud Speech, Translation and Natural Language.&lt;/p&gt;

&lt;p&gt;Since they carry complementary outputs that can be used in each other&amp;rsquo;s input, all three of the APIs are included in one package. For example, you can transcribe a recording of someone speaking in Danish, translate that to English and then identify how positive or negative the writer felt about its content (sentiment analysis) then identify the most important concepts and objects within the content (entity analysis).&lt;/p&gt;

&lt;h3 id=&#34;motivations&#34;&gt;Motivations&lt;/h3&gt;

&lt;h4 id=&#34;fake-news&#34;&gt;Fake news&lt;/h4&gt;

&lt;p&gt;One reason why I started looking at this area was the growth of &amp;lsquo;fake news&amp;rsquo;, and its effect on political discourse on social media. I wondered if there was some way to put metrics on how much a news story fuelled one&amp;rsquo;s own bias within your own filter bubble.  The entity API provides a way to perform entity and sentiment analysis at scale on tweets, and by then comparing different users and news sources preferences the hope is to be able to judge how much they are in agreement with your own bias, views and trusted reputation sources.&lt;/p&gt;

&lt;h4 id=&#34;make-your-own-alexa&#34;&gt;Make your own Alexa&lt;/h4&gt;

&lt;p&gt;Another motivating application is the growth of voice commands that will become the primary way of user interface with technology.  Already, &lt;a href=&#34;https://www.thinkwithgoogle.com/data-gallery/detail/google-app-voice-search/&#34;&gt;Google reports up to 20% of search in its app&lt;/a&gt; is via voice search.  I&amp;rsquo;d like to be able to say &amp;ldquo;R, print me out that report for client X&amp;rdquo;.  A Shiny app that records your voice, uploads to the API then parses the return text into actions gives you a chance to create your very own Alexa-like infrastructure.&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;text-align:center&#34;&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/alexa.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The voice activated internet connected speaker, Amazon&amp;rsquo;s Alexa - image from www.amazon.co.uk&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;translate-everything&#34;&gt;Translate everything&lt;/h4&gt;

&lt;p&gt;Finally, I live and work in Denmark.  As Danish is only spoken by less than 6 million people, applications that work in English may not be available in Danish very quickly, if at all.  The API&amp;rsquo;s translation service is the one that made the news in 2016 for &lt;a href=&#34;https://research.googleblog.com/2016/09/a-neural-network-for-machine.html&#34;&gt;&amp;ldquo;inventing its own language&amp;rdquo;&lt;/a&gt;, and offers much better English to Danish translations that the free web version and may make services available in Denmark sooner.&lt;/p&gt;

&lt;h3 id=&#34;using-the-library&#34;&gt;Using the library&lt;/h3&gt;

&lt;p&gt;To use these APIs within R, you first need to do a one-time setup to create a Google Project, add a credit card and authenticate which is &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/#installation&#34;&gt;detailed on the package website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After that, you feed in the R objects you want to operate upon.  The &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/127&#34;&gt;rOpenSci review&lt;/a&gt; helped to ensure that this can scale up easily, so that you can feed in large character vectors which the library will parse and rate limit as required.  The functions also work within &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; pipe syntax.&lt;/p&gt;

&lt;h4 id=&#34;speech-to-text&#34;&gt;Speech-to-text&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/speech.html&#34;&gt;Cloud Speech API&lt;/a&gt; is exposed via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_speech.html&#34;&gt;&lt;code&gt;gl_speech&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It supports multiple audio formats and languages, and you can either feed a sub-60 second audio file directly, or perform asynchrnous requests for longer audio files.&lt;/p&gt;

&lt;p&gt;Example code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)

my_audio &amp;lt;- &amp;quot;my_audio_file.wav&amp;quot;
gl_speech(my_audio)
#  A tibble: 1 x 3
#  transcript confidence                 words
#* &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;                &amp;lt;list&amp;gt;
#1 Hello Mum  0.9227779 &amp;lt;data.frame [19 x 3]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;translation&#34;&gt;Translation&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/translation.html&#34;&gt;Cloud Translation API&lt;/a&gt; lets you translate text via &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_translate.html&#34;&gt;&lt;code&gt;gl_translate&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you are charged per character, one tip here if you are working with lots of different languages is to perform detection of language offline first using another rOpenSci package, &lt;a href=&#34;https://github.com/ropensci/cld2&#34;&gt;&lt;code&gt;cld2&lt;/code&gt;&lt;/a&gt;.  That way you can avoid charges for text that is already in your target language i.e. English.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
library(cld2)
library(purrr)

my_text &amp;lt;- c(&amp;quot;Katten sidder på måtten&amp;quot;, &amp;quot;The cat sat on the mat&amp;quot;)

## offline detect language via cld2
detected &amp;lt;- map_chr(my_text, detect_language)
# [1] &amp;quot;DANISH&amp;quot;  &amp;quot;ENGLISH&amp;quot;

## get non-English text
translate_me &amp;lt;- my_text[detected != &amp;quot;ENGLISH&amp;quot;]

## translate
gl_translate(translate_me)
## A tibble: 1 x 3
#                 translatedText detectedSourceLanguage                    text
#*                         &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;
#1 The cat is sitting on the mat                     da Katten sidder på måtten
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/nlp.html&#34;&gt;Natural Language API&lt;/a&gt; reveals the structure and meaning of text, accessible via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_nlp.html&#34;&gt;&lt;code&gt;gl_nlp&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It returns several analysis:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Entity analysis&lt;/em&gt; - finds named entities (currently proper names and common nouns) in the text along with entity types, salience, mentions for each entity, and other properties. If possible, will also return metadata about that entity such as a Wikipedia URL.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Syntax&lt;/em&gt; - analyzes the syntax of the text and provides sentence boundaries and tokenization along with part of speech tags, dependency trees, and other properties.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sentiment&lt;/em&gt; - the overall sentiment of the text, represented by a magnitude [0, +inf] and score between -1.0 (negative sentiment) and 1.0 (positive sentiment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all useful to get an understanding of the meaning of a sentence, and has potentially the greatest number of applications of the APIs featured.  With entity analysis, auto categorisation of text is possible; the syntax returns let you pull out nouns and verbs for parsing into other actions; and the sentiment analysis allows you to get a feeling for emotion within text.&lt;/p&gt;

&lt;p&gt;A demonstration is below which gives an idea of what output you can generate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
quote &amp;lt;- &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
nlp &amp;lt;- gl_nlp(quote)

str(nlp)
#List of 6
# $ sentences        :List of 1
#  ..$ :&#39;data.frame&#39;:	1 obs. of  4 variables:
#  .. ..$ content    : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
#  .. ..$ beginOffset: int 0
#  .. ..$ magnitude  : num 0.6
#  .. ..$ score      : num -0.6
# $ tokens           :List of 1
#  ..$ :&#39;data.frame&#39;:	20 obs. of  17 variables:
#  .. ..$ content       : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;things&amp;quot; &amp;quot;are&amp;quot; &amp;quot;infinite&amp;quot; ...
#  .. ..$ beginOffset   : int [1:20] 0 4 11 15 23 25 29 38 42 48 ...
#  .. ..$ tag           : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NOUN&amp;quot; &amp;quot;VERB&amp;quot; &amp;quot;ADJ&amp;quot; ...
#  .. ..$ aspect        : chr [1:20] &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; ...
#  .. ..$ case          : chr [1:20] &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; ...
#  .. ..$ form          : chr [1:20] &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; ...
#  .. ..$ gender        : chr [1:20] &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; ...
#  .. ..$ mood          : chr [1:20] &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;INDICATIVE&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; ...
#  .. ..$ number        : chr [1:20] &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;PLURAL&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; ...
#  .. ..$ person        : chr [1:20] &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; ...
#  .. ..$ proper        : chr [1:20] &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; ...
#  .. ..$ reciprocity   : chr [1:20] &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; ...
#  .. ..$ tense         : chr [1:20] &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;PRESENT&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; ...
#  .. ..$ voice         : chr [1:20] &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; ...
#  .. ..$ headTokenIndex: int [1:20] 1 2 2 2 2 6 2 6 9 6 ...
#  .. ..$ label         : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NSUBJ&amp;quot; &amp;quot;ROOT&amp;quot; &amp;quot;ACOMP&amp;quot; ...
#  .. ..$ value         : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;thing&amp;quot; &amp;quot;be&amp;quot; &amp;quot;infinite&amp;quot; ...
# $ entities         :List of 1
#  ..$ :Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	6 obs. of  9 variables:
#  .. ..$ name         : chr [1:6] &amp;quot;human stupidity&amp;quot; &amp;quot;things&amp;quot; &amp;quot;universe&amp;quot; &amp;quot;universe&amp;quot; ...
#  .. ..$ type         : chr [1:6] &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; ...
#  .. ..$ salience     : num [1:6] 0.1662 0.4771 0.2652 0.2652 0.0915 ...
#  .. ..$ mid          : Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ wikipedia_url: Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ magnitude    : num [1:6] NA NA NA NA NA NA
#  .. ..$ score        : num [1:6] NA NA NA NA NA NA
#  .. ..$ beginOffset  : int [1:6] 42 4 29 86 29 86
#  .. ..$ mention_type : chr [1:6] &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; ...
# $ language         : chr &amp;quot;en&amp;quot;
# $ text             : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
# $ documentSentiment:Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	1 obs. of  2 variables:
#  ..$ magnitude: num 0.6
#  ..$ score    : num -0.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;This package is 10 times better due to the efforts of the rOpenSci reviewers &lt;a href=&#34;http://enpiar.com/&#34;&gt;Neal Richardson&lt;/a&gt; and &lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt;, who have whipped the documentation, outputs and test cases into the form they are today in &lt;code&gt;0.1.0&lt;/code&gt;.  Many thanks to them.&lt;/p&gt;

&lt;p&gt;Hopefully, this is just the beginning and the package can be further improved by its users - if you do give the package a try and find a potential improvement, &lt;a href=&#34;https://github.com/ropensci/googleLanguageR/issues&#34;&gt;raise an issue on GitHub&lt;/a&gt; and we can try to implement it.  I&amp;rsquo;m excited to see what users can do with these powerful tools.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>rrricanes to Access Tropical Cyclone Data</title>
      <link>https://ropensci.org/blog/2017/09/27/rrricanes/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/27/rrricanes/</guid>
      <description>
        
        

&lt;h3 id=&#34;what-is-rrricanes&#34;&gt;What is rrricanes&lt;/h3&gt;

&lt;h4 id=&#34;why-write-rrricanes&#34;&gt;Why Write rrricanes?&lt;/h4&gt;

&lt;p&gt;There is a tremendous amount of weather data available on the internet. Much of it is in raw format and not very easy to obtain. Hurricane data is no different. When one thinks of this data they may be inclined to think it is a bunch of map coordinates with some wind values and not much else. A deeper look will reveal structural and forecast data. An even deeper look will find millions of data points from hurricane reconnaissance, computer forecast models, ship and buoy observations, satellite and radar imagery, &amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; is an attempt to bring this data together in a way that doesn&amp;rsquo;t just benefit R users, but other languages as well.&lt;/p&gt;

&lt;p&gt;I began learning R in 2015 and immediately had wished I had a hurricane-specific dataset when Hurricane Patricia became a harmless, but historic hurricane roaming the Pacific waters. I found this idea revisited again as Hurricane Matthew took aim at Florida and the southeast in 2016. Unable to use R to study and consolidate Matthew&amp;rsquo;s data in R led me to begin learning package development. Thus, the birth of &lt;code&gt;rrricanes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In this article, I will take you on a lengthy tour of the most important features of &lt;code&gt;rrricanes&lt;/code&gt; and what the data means. If you have a background working with hurricane data, most of this will be redundant. My aim here is to cover the big ideas behind the package and explain them under the assumption you, the reader, are unfamiliar with the data offered.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; &lt;strong&gt;is not intended to be used in emergency situations&lt;/strong&gt;. I write this article as areas I have lived or currently live are under the gun from Hurricane Harvey and &lt;code&gt;rrricanes&lt;/code&gt; is unable to obtain data due to external issues (I will describe these later). It is designed with the intent of answering questions and exploring ideas outside of a time-sensitive environment.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; will not be available in CRAN for quite some time. The current schedule is May 15, 2018 (the &amp;ldquo;start&amp;rdquo; of the East Pacific hurricane season). This year is soley for testing under real-time conditions.&lt;/p&gt;

&lt;h4 id=&#34;and-rrricanesdata&#34;&gt;And rrricanesdata&lt;/h4&gt;

&lt;p&gt;The NHC archives text products dating back to at least 1998 (some earlier years exist but yet to be implemented in this package). Accessing this data is a time-consuming process on any computer. A limit of 4 requests per second is put in place to avoid being banned (or restricted) from the archives. So, if a hurricane has 20 text products you wish to pull and parse, this will take 5 seconds. Most cyclones have more and some, far more.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; is a compliment package to &lt;code&gt;rrricanes&lt;/code&gt;. &lt;code&gt;rrricanesdata&lt;/code&gt; contains post-scraped datasets of the archives for all available storms with the exception of advisories issued in the current month.This means you can explore the various datasets without the wait.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; will be updated monthly if an advisory has been issued the previous month. There will be regular monthly updates approximately from May through November - the typical hurricane season. In some cases, a cyclone may develop in the off-season. &lt;code&gt;rrricanesdata&lt;/code&gt; will be updated on the same schedule.&lt;/p&gt;

&lt;h4 id=&#34;eli5-the-data&#34;&gt;ELI5 the Data&lt;/h4&gt;

&lt;p&gt;This package covers tropical cyclones that have developed in the Atlantic basin (north Atlantic ocean) or East Pacific basin (northeast Pacific east of 140#&amp;deg;W). Central Pacific (140#&amp;deg;W - 180#&amp;deg;W) may be mixed in if listed in the NHC archives.&lt;/p&gt;

&lt;p&gt;While traditionally the hurricane season for each basin runs from mid-May or June through November, some cyclones have developed outside of this time frame.&lt;/p&gt;

&lt;p&gt;Every tropical cylone (any tropical low whether classified as a tropical depression, tropical storm or hurricane) contains a core set of text products officially issued from the National Hurricane Center. These products are issued every six hours.&lt;/p&gt;

&lt;p&gt;Much of this data has changed in format over the years. Some products have been discontinued and replaced by new products or wrapped into existing products. Some of these products are returned in raw text format; it is not cleaned and may contain HTML characters. Other products are parsed with every piece of data extracted and cleaned.&lt;/p&gt;

&lt;p&gt;I have done my best to ensure data is high quality. But, I cannot guarantee it is perfect. If you do believe you have found an error, please &lt;a href=&#34;https://github.com/ropensci/rrricanes/issues&#34;&gt;let me know&lt;/a&gt;; even if it seems small. I would rather be notified of a false error than ignore a true one.&lt;/p&gt;

&lt;h4 id=&#34;the-products&#34;&gt;The Products&lt;/h4&gt;

&lt;p&gt;Each advisory product is listed below with an abbreviation in parentheses. Unless otherwise noted, these products are issued every six hours. Generally, the times issued are 03:00, 09:00, 15:00 and 21:00 UTC. Some products may be issued in three-hour increments and, sometimes, two-hour increments. &lt;code&gt;update&lt;/code&gt; can be issued at any time.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Storm Discussion (&lt;code&gt;discus&lt;/code&gt;) - These are technical discussions centered on the current structure of the cyclone, satellite presentation, computer forecast model tendencies and more. These products are not parsed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Forecast/Adivsory (&lt;code&gt;fstadv&lt;/code&gt;) - This data-rich product lists the current location of the cyclone, its wind structure, forecast and forecast wind structure.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Public Advisory (&lt;code&gt;public&lt;/code&gt;) - These are general text statements issued for the public-at-large. Information in these products is a summary of the Forecast/Advisory product along with any watches and warnings issued, changed, or cancelled. Public Advisory products are the only regularly-scheduled product that may be issued intermittently (every three hours and, occasionally, every two hours) when watches and warnings are in effect. These products are not parsed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Wind Speed Probabilities (&lt;code&gt;wndprb&lt;/code&gt;) - These products list the probability of a minimum sustained wind speed expected in a given forecast window. This product replaces the Strike Probabilities product beginning in 2006 (see below).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Updates (&lt;code&gt;update&lt;/code&gt;) - Tropical Cyclone Updates may be issued at any time if a storm is an immediate threat to land or if the cyclone undergoes a significant change of strength or structure. The information in this product is general. These products are not parsed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;discontinued-products&#34;&gt;Discontinued Products&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Strike Probabilities (&lt;code&gt;prblty&lt;/code&gt;) - List the probability of a tropical cyclone passing within 65 nautical miles of a location within a forecast window. Replaced in 2006 by the Wind Speed Probabilities product.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Position Estimates (&lt;code&gt;posest&lt;/code&gt;) - Typically issued as a storm is threatening land but generally rare (see Hurricane Ike 2008, Key AL092008). It is generally just an update of the current location of the cyclone. After the 2011 hurricane season, this product was discontinued; Updates are now issued in their place. These products are not parsed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;primary-key&#34;&gt;Primary Key&lt;/h4&gt;

&lt;p&gt;Every cyclone has a &lt;code&gt;Key&lt;/code&gt;. However, not all products contain this value (&lt;code&gt;prblty&lt;/code&gt;, for example). Products issued during and after the 2005 hurricane season contain this variable.&lt;/p&gt;

&lt;p&gt;Use &lt;code&gt;Key&lt;/code&gt; to tie datasets together. If &lt;code&gt;Key&lt;/code&gt; does not exist, you will need to use a combination of &lt;code&gt;Name&lt;/code&gt; and &lt;code&gt;Date&lt;/code&gt;, depending on your requirements. Keep in mind that, unless a name is retired, names are recycled every seven years. For example, there are multiple cyclones named Katrina but you may want to isolate on Katrina, 2005.&lt;/p&gt;

&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; will not be submitted to CRAN until prior to the hurricane season, 2018. It can be installed via github using &lt;code&gt;devtools&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/rrricanes&amp;quot;, build_vignettes = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;optional-supporting-packages&#34;&gt;Optional Supporting Packages&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; uses a drat repository to host the large, pre-processed datasets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;rrricanesdata&amp;quot;,
                 repos = &amp;quot;https://timtrice.github.io/drat/&amp;quot;,
                 type = &amp;quot;source&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To use high resolution tracking charts, you may also wish to install the `rnaturalearthhires&amp;rsquo; package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;rnaturalearthhires&amp;quot;,
                 repos = &amp;quot;http://packages.ropensci.org&amp;quot;,
                 type = &amp;quot;source&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Linux users may also need to install:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;libgdal-dev&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;libproj-dev&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;libxml2-dev&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;get-a-list-of-storms&#34;&gt;Get a List of Storms&lt;/h3&gt;

&lt;p&gt;We start exploring &lt;code&gt;rrricanes&lt;/code&gt; by finding a storm (or storms) we wish to analyze. For this, we use &lt;code&gt;get_storms&lt;/code&gt;. There are two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;years&lt;/code&gt; Between 1998 and current year&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;basins&lt;/code&gt; One or both &amp;ldquo;AL&amp;rdquo; and &amp;ldquo;EP&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An empty call to the function will return storms for both the Atlantic and East Pacific basin for the current year.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
library(rrricanes)
get_storms() %&amp;gt;% print(n = nrow(.))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 33 x 4
##     Year                           Name Basin
##    &amp;lt;dbl&amp;gt;                          &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
##  1  2017          Tropical Storm Arlene    AL
##  2  2017            Tropical Storm Bret    AL
##  3  2017           Tropical Storm Cindy    AL
##  4  2017       Tropical Depression Four    AL
##  5  2017             Tropical Storm Don    AL
##  6  2017           Tropical Storm Emily    AL
##  7  2017             Hurricane Franklin    AL
##  8  2017                 Hurricane Gert    AL
##  9  2017               Hurricane Harvey    AL
## 10  2017 Potential Tropical Cyclone Ten    AL
## 11  2017                 Hurricane Irma    AL
## 12  2017                 Hurricane Jose    AL
## 13  2017                Hurricane Katia    AL
## 14  2017                  Hurricane Lee    AL
## 15  2017                Hurricane Maria    AL
## 16  2017          Tropical Storm Adrian    EP
## 17  2017         Tropical Storm Beatriz    EP
## 18  2017          Tropical Storm Calvin    EP
## 19  2017                 Hurricane Dora    EP
## 20  2017               Hurricane Eugene    EP
## 21  2017             Hurricane Fernanda    EP
## 22  2017            Tropical Storm Greg    EP
## 23  2017    Tropical Depression Eight-E    EP
## 24  2017               Hurricane Hilary    EP
## 25  2017                Hurricane Irwin    EP
## 26  2017   Tropical Depression Eleven-E    EP
## 27  2017            Tropical Storm Jova    EP
## 28  2017              Hurricane Kenneth    EP
## 29  2017           Tropical Storm Lidia    EP
## 30  2017                 Hurricane Otis    EP
## 31  2017                  Hurricane Max    EP
## 32  2017                Hurricane Norma    EP
## 33  2017           Tropical Storm Pilar    EP
## # ... with 1 more variables: Link &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Function &lt;code&gt;get_storms&lt;/code&gt; returns four variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Year - year of the cyclone.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Name - name of the cyclone.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Basin - basin the cyclone developed (AL for Atlantic, EP for east Pacific).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Link - URL to the cyclone&amp;rsquo;s archive page.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The variables &lt;code&gt;Name&lt;/code&gt; and &lt;code&gt;Link&lt;/code&gt; are the only variables that could potentially change. For example, you&amp;rsquo;ll notice a &lt;code&gt;Name&lt;/code&gt; value of &lt;u&gt;Potential Tropical Cyclone Ten&lt;/u&gt;. If this storm became a tropical storm then it would receive a new name and the link to the archive page would change as well.&lt;/p&gt;

&lt;p&gt;For this example we will explore &lt;u&gt;Hurricane Harvey&lt;/u&gt;.&lt;/p&gt;

&lt;h3 id=&#34;text-products&#34;&gt;Text Products&lt;/h3&gt;

&lt;h4 id=&#34;current-data&#34;&gt;Current Data&lt;/h4&gt;

&lt;p&gt;Once we have identified the storms we want to retrieve we can begin working on getting the products. In the earlier discussion of the available products, recall I used abbreviations such as &lt;code&gt;discus&lt;/code&gt;, &lt;code&gt;fstadv&lt;/code&gt;, etc. These are the terms we will use when obtaining data.&lt;/p&gt;

&lt;p&gt;The easiest method to getting storm data is the function &lt;code&gt;get_storm_data&lt;/code&gt;. This function can take multiple storm archive URLs and return multiple datasets within a list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ds &amp;lt;- get_storms() %&amp;gt;%
  filter(Name == &amp;quot;Hurricane Harvey&amp;quot;) %&amp;gt;%
  pull(Link) %&amp;gt;%
  get_storm_data(products = c(&amp;quot;discus&amp;quot;, &amp;quot;fstadv&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This process may take some time (particularly, &lt;code&gt;fstadv&lt;/code&gt; products). This is because the NHC website allows no more than 80 connections every 10 seconds. &lt;code&gt;rrricanes&lt;/code&gt; processes four links every half second.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; uses the &lt;code&gt;dplyr&lt;/code&gt; progress bar to keep you informed of the status. You can turn this off by setting option &lt;code&gt;dplyr.show_progress&lt;/code&gt; to FALSE.&lt;/p&gt;

&lt;p&gt;An additional option is &lt;code&gt;rrricanes.working_msg&lt;/code&gt;; FALSE by default. This option will show a message for each advisory currently being worked. I primarily added it to help find products causing problems but you may find it useful at some point.&lt;/p&gt;

&lt;p&gt;At this point, we have a list - &lt;code&gt;ds&lt;/code&gt; - of dataframes. Each dataframe is named after the product.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(ds)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;discus&amp;quot; &amp;quot;fstadv&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;discus&lt;/code&gt; is one of the products that isn&amp;rsquo;t parsed; the full text of the product is returned.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(ds$discus)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	43 obs. of  6 variables:
##  $ Status  : chr  &amp;quot;Potential Tropical Cyclone&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; ...
##  $ Name    : chr  &amp;quot;Nine&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##  $ Adv     : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Date    : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 21:00:00&amp;quot; ...
##  $ Key     : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Contents: chr  &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nPotential Tropical Cyclone Nine Discussion Number   1\nNWS National&amp;quot;| __truncated__ &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nTropical Storm Harvey Discussion Number   2\nNWS National Hurricane&amp;quot;| __truncated__ &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nTropical Storm Harvey Discussion Number   3\nNWS National Hurricane&amp;quot;| __truncated__ &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nTropical Storm Harvey Discussion Number   4\nNWS National Hurricane&amp;quot;| __truncated__ ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;fstadv&lt;/code&gt; dataframes, however, are parsed and contain the bulk of the information for the storm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(ds$fstadv)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	43 obs. of  117 variables:
##  $ Status       : chr  &amp;quot;Potential Tropical Cyclone&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; ...
##  $ Name         : chr  &amp;quot;Nine&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##  $ Adv          : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Date         : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 21:00:00&amp;quot; ...
##  $ Key          : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Lat          : num  13.1 13 13 13.1 13.1 13.4 13.7 13.8 13.9 14.1 ...
##  $ Lon          : num  -54.1 -55.8 -57.4 -59.1 -61.3 -62.9 -64.1 -65.9 -68.1 -70 ...
##  $ Wind         : num  30 35 35 35 35 35 35 35 35 30 ...
##  $ Gust         : num  40 45 45 45 45 45 45 45 45 40 ...
##  $ Pressure     : num  1008 1004 1005 1004 1005 ...
##  $ PosAcc       : num  30 30 30 30 30 40 40 30 30 30 ...
##  $ FwdDir       : num  270 270 270 270 270 275 275 275 275 275 ...
##  $ FwdSpeed     : num  15 16 16 16 18 18 16 18 19 19 ...
##  $ Eye          : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SeasNE       : num  NA 60 60 60 75 150 60 60 45 NA ...
##  $ SeasSE       : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasSW       : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasNW       : num  NA 60 60 60 60 60 45 60 45 NA ...
##  $ NE64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SE64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SW64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NW64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NE50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SE50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SW50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NW50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NE34         : num  NA 30 50 50 60 60 60 60 0 NA ...
##  $ SE34         : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SW34         : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ NW34         : num  NA 30 50 50 60 60 60 60 60 NA ...
##  $ Hr12FcstDate : POSIXct, format: &amp;quot;2017-08-18 00:00:00&amp;quot; &amp;quot;2017-08-18 06:00:00&amp;quot; ...
##  $ Hr12Lat      : num  13.1 13.1 13.2 13.2 13.3 13.6 14 14 14.1 14.3 ...
##  $ Hr12Lon      : num  -56.4 -58.3 -59.9 -61.7 -63.8 -65.7 -66.8 -68.7 -70.9 -73 ...
##  $ Hr12Wind     : num  30 35 35 35 35 35 35 35 35 30 ...
##  $ Hr12Gust     : num  40 45 45 45 45 45 45 45 45 40 ...
##  $ Hr12NE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NE34     : num  NA 30 50 50 60 60 60 60 60 NA ...
##  $ Hr12SE34     : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ Hr12SW34     : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ Hr12NW34     : num  NA 30 50 50 60 60 60 60 60 NA ...
##  $ Hr24FcstDate : POSIXct, format: &amp;quot;2017-08-18 12:00:00&amp;quot; &amp;quot;2017-08-18 18:00:00&amp;quot; ...
##  $ Hr24Lat      : num  13.2 13.4 13.6 13.5 13.6 13.9 14.3 14.3 14.4 14.6 ...
##  $ Hr24Lon      : num  -59.8 -61.6 -63.3 -65.2 -67.3 -69.3 -70.4 -72.7 -74.9 -77 ...
##  $ Hr24Wind     : num  35 40 40 40 40 40 40 40 40 35 ...
##  $ Hr24Gust     : num  45 50 50 50 50 50 50 50 50 45 ...
##  $ Hr24NE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NE34     : num  50 40 50 50 60 60 60 60 60 60 ...
##  $ Hr24SE34     : num  0 0 0 0 30 0 0 0 0 0 ...
##  $ Hr24SW34     : num  0 0 0 0 30 0 0 0 0 0 ...
##  $ Hr24NW34     : num  50 40 50 50 60 60 60 60 60 60 ...
##  $ Hr36FcstDate : POSIXct, format: &amp;quot;2017-08-19 00:00:00&amp;quot; &amp;quot;2017-08-19 06:00:00&amp;quot; ...
##  $ Hr36Lat      : num  13.5 13.7 13.9 13.9 14 14.2 14.5 14.6 14.9 15.2 ...
##  $ Hr36Lon      : num  -63.2 -65.1 -67 -68.8 -71.1 -73 -74.3 -76.7 -78.7 -80.5 ...
##  $ Hr36Wind     : num  40 45 45 40 40 40 40 45 45 40 ...
##  $ Hr36Gust     : num  50 55 55 50 50 50 50 55 55 50 ...
##  $ Hr36NE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NE34     : num  60 60 60 60 60 60 60 70 70 60 ...
##  $ Hr36SE34     : num  0 30 30 30 30 0 0 0 0 0 ...
##  $ Hr36SW34     : num  0 30 30 30 30 0 0 0 0 0 ...
##  $ Hr36NW34     : num  60 60 60 60 60 60 60 70 70 60 ...
##  $ Hr48FcstDate : POSIXct, format: &amp;quot;2017-08-19 12:00:00&amp;quot; &amp;quot;2017-08-19 18:00:00&amp;quot; ...
##  $ Hr48Lat      : num  13.9 14 14.1 14.1 14.3 14.5 14.8 15.2 15.7 16 ...
##  $ Hr48Lon      : num  -66.7 -68.8 -70.9 -72.7 -75 -76.7 -78.1 -80.1 -82.4 -83.8 ...
##  $ Hr48Wind     : num  45 45 50 50 50 45 45 50 50 45 ...
##  $ Hr48Gust     : num  55 55 60 60 60 55 55 60 60 55 ...
##  $ Hr48NE50     : num  NA NA 30 30 30 NA NA 30 30 NA ...
##  $ Hr48SE50     : num  NA NA 0 0 0 NA NA 0 0 NA ...
##  $ Hr48SW50     : num  NA NA 0 0 0 NA NA 0 0 NA ...
##  $ Hr48NW50     : num  NA NA 30 30 30 NA NA 30 30 NA ...
##  $ Hr48NE34     : num  60 60 60 60 70 60 60 90 90 70 ...
##  $ Hr48SE34     : num  30 30 30 30 30 30 0 50 50 0 ...
##  $ Hr48SW34     : num  30 30 30 30 30 30 0 40 40 0 ...
##  $ Hr48NW34     : num  60 60 60 60 70 60 60 70 70 70 ...
##  $ Hr72FcstDate : POSIXct, format: &amp;quot;2017-08-20 12:00:00&amp;quot; &amp;quot;2017-08-20 18:00:00&amp;quot; ...
##  $ Hr72Lat      : num  14.5 14.5 14.8 15 15 15.5 16.5 17 17.5 18 ...
##  $ Hr72Lon      : num  -74.5 -76.5 -78.6 -80.2 -82 -83.5 -84.7 -86.5 -88 -89 ...
##  $ Hr72Wind     : num  55 55 60 60 60 60 55 55 60 45 ...
##  $ Hr72Gust     : num  65 65 75 75 75 75 65 65 75 55 ...
##   [list output truncated]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each product can also be accessed on its own. For example, if you only wish to view &lt;code&gt;discus&lt;/code&gt; products, use the &lt;code&gt;get_discus&lt;/code&gt; function. &lt;code&gt;fstadv&lt;/code&gt; products can be accessed with &lt;code&gt;get_fstadv&lt;/code&gt;. Every products specific function is preceeded by &lt;code&gt;get_&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To understand the variable definitions, access the help file for each of these functions (i.e., &lt;code&gt;?get_fstadv&lt;/code&gt;). They contain full definitions on the variables and their purpose.&lt;/p&gt;

&lt;p&gt;As you can see, the &lt;code&gt;fstadv&lt;/code&gt; dataframe is very wide. There may be instances you only want to focus on specific pieces of the product. I&amp;rsquo;ve developed tidy functions to help trim these datasets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_fcst&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_fcst_wr&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_fstadv&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_wr&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These datasets exist in &lt;code&gt;rrricanesdata&lt;/code&gt; as &lt;code&gt;fcst&lt;/code&gt;, &lt;code&gt;fcst_wr&lt;/code&gt;, &lt;code&gt;adv&lt;/code&gt;, and &lt;code&gt;wr&lt;/code&gt;, respectively (see below).&lt;/p&gt;

&lt;p&gt;Most tropical cyclone forecast/advisory products will contain multiple forecast points. Initially, only three-day forecasts were issued. Beginning the with the 2003 season, 96 hour (five-day) forecasts were issued.&lt;/p&gt;

&lt;p&gt;If a storm is not expected to survive the full forecast period, then only relevant forecasts will be issued.&lt;/p&gt;

&lt;p&gt;We use &lt;code&gt;tidy_fcst&lt;/code&gt; to return these forecast points in a tidy fashion from &lt;code&gt;fstadv&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_fcst(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	283 obs. of  8 variables:
##  $ Key     : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv     : num  1 1 1 1 1 1 1 2 2 2 ...
##  $ Date    : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 15:00:00&amp;quot; ...
##  $ FcstDate: POSIXct, format: &amp;quot;2017-08-18 00:00:00&amp;quot; &amp;quot;2017-08-18 12:00:00&amp;quot; ...
##  $ Lat     : num  13.1 13.2 13.5 13.9 14.5 15.5 17 13.1 13.4 13.7 ...
##  $ Lon     : num  -56.4 -59.8 -63.2 -66.7 -74.5 -82 -87.5 -58.3 -61.6 -65.1 ...
##  $ Wind    : num  30 35 40 45 55 65 65 35 40 45 ...
##  $ Gust    : num  40 45 50 55 65 80 80 45 50 55 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wind radius values are issued with parameters of 34, 50 and 64. These values are the radius to which minimum one-minute sustained winds can be expected or exist.&lt;/p&gt;

&lt;p&gt;A tropical depression will not have associated wind radius values since the maximum winds of a depression are 30 knots. If a tropical storm has winds less than 50 knots, then it will only have wind radius values for the 34-knot wind field. If winds are greater than 50 knots, then it will have wind radius values for 34 and 50 knot winds. A hurricane will have all wind radius fields.&lt;/p&gt;

&lt;p&gt;Wind radius values are further seperated by quadrant; NE (northeast), SE, SW and NW. Not all quadrants will have values; particularly if the cyclone is struggling to organize. For example, you will often find a minimal hurricane only has hurricane-force winds (64 knots) in the northeast quadrant.&lt;/p&gt;

&lt;p&gt;When appropriate, a forecast/advisory product will contain these values for the current position and for each forecast position. Use &lt;code&gt;tidy_wr&lt;/code&gt; and &lt;code&gt;tidy_fcst_wr&lt;/code&gt;, respectively, for these variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_wr(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	56 obs. of  8 variables:
##  $ Key      : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv      : num  2 3 4 5 6 7 8 9 15 16 ...
##  $ Date     : POSIXct, format: &amp;quot;2017-08-17 21:00:00&amp;quot; &amp;quot;2017-08-18 03:00:00&amp;quot; ...
##  $ WindField: num  34 34 34 34 34 34 34 34 34 34 ...
##  $ NE       : num  30 50 50 60 60 60 60 0 100 80 ...
##  $ SE       : num  0 0 0 0 0 0 0 0 0 30 ...
##  $ SW       : num  0 0 0 0 0 0 0 0 0 20 ...
##  $ NW       : num  30 50 50 60 60 60 60 60 50 50 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_fcst_wr(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	246 obs. of  9 variables:
##  $ Key      : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv      : num  1 1 1 1 1 2 2 2 2 2 ...
##  $ Date     : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 15:00:00&amp;quot; ...
##  $ FcstDate : POSIXct, format: &amp;quot;2017-08-18 12:00:00&amp;quot; &amp;quot;2017-08-19 00:00:00&amp;quot; ...
##  $ WindField: num  34 34 34 34 50 34 34 34 34 34 ...
##  $ NE       : num  50 60 60 80 30 30 40 60 60 80 ...
##  $ SE       : num  0 0 30 40 0 0 0 30 30 40 ...
##  $ SW       : num  0 0 30 40 0 0 0 30 30 40 ...
##  $ NW       : num  50 60 60 80 30 30 40 60 60 80 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, you may only want to focus on current storm details. For this, we use &lt;code&gt;tidy_fstadv&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_fstadv(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	43 obs. of  18 variables:
##  $ Key     : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv     : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Date    : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 21:00:00&amp;quot; ...
##  $ Status  : chr  &amp;quot;Potential Tropical Cyclone&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; ...
##  $ Name    : chr  &amp;quot;Nine&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##  $ Lat     : num  13.1 13 13 13.1 13.1 13.4 13.7 13.8 13.9 14.1 ...
##  $ Lon     : num  -54.1 -55.8 -57.4 -59.1 -61.3 -62.9 -64.1 -65.9 -68.1 -70 ...
##  $ Wind    : num  30 35 35 35 35 35 35 35 35 30 ...
##  $ Gust    : num  40 45 45 45 45 45 45 45 45 40 ...
##  $ Pressure: num  1008 1004 1005 1004 1005 ...
##  $ PosAcc  : num  30 30 30 30 30 40 40 30 30 30 ...
##  $ FwdDir  : num  270 270 270 270 270 275 275 275 275 275 ...
##  $ FwdSpeed: num  15 16 16 16 18 18 16 18 19 19 ...
##  $ Eye     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SeasNE  : num  NA 60 60 60 75 150 60 60 45 NA ...
##  $ SeasSE  : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasSW  : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasNW  : num  NA 60 60 60 60 60 45 60 45 NA ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In release 0.2.1, &lt;code&gt;tidy_fstadv&lt;/code&gt; will be renamed to &lt;code&gt;tidy_adv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One final note on the data: all speed variables are measured in knots, distance variables in nautical miles, and pressure variables in millibars. Functions &lt;code&gt;knots_to_mph&lt;/code&gt; and &lt;code&gt;mb_to_in&lt;/code&gt; are available for speed/pressure conversions. Function &lt;code&gt;nm_to_sm&lt;/code&gt; to convert nautical miles to survey miles will be included in release 0.2.1.&lt;/p&gt;

&lt;h4 id=&#34;archived-data&#34;&gt;Archived Data&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; was built to make it easier to get pre-processed datasets. As mentioned earlier, &lt;code&gt;rrricanesdata&lt;/code&gt; will be updated the first of every month if any advisory was issued for the previous month. (As I am now writing this portion in September, all of Hurricane Harvey&amp;rsquo;s advisories - the last one issued the morning of August 31 - exist in &lt;code&gt;rrricanesdata&lt;/code&gt; release 0.0.1.4.)&lt;/p&gt;

&lt;p&gt;As with &lt;code&gt;rrricanes&lt;/code&gt;, &lt;code&gt;rrricanesdata&lt;/code&gt; is not available in CRAN (nor will be due to size limitations).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll load all datasets with the call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rrricanesdata)
data(list = data(package = &amp;quot;rrricanesdata&amp;quot;)$results[,3])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All core product datasets are available. The dataframes &lt;code&gt;adv&lt;/code&gt;, &lt;code&gt;fcst&lt;/code&gt;, &lt;code&gt;fcst_wr&lt;/code&gt; and &lt;code&gt;wr&lt;/code&gt; are the dataframes created by &lt;code&gt;tidy_fstadv&lt;/code&gt;, &lt;code&gt;tidy_fcst&lt;/code&gt;, &lt;code&gt;tidy_fcst_wr&lt;/code&gt; and &lt;code&gt;tidy_wr&lt;/code&gt;, respectively.&lt;/p&gt;

&lt;h3 id=&#34;tracking-charts&#34;&gt;Tracking Charts&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; also comes with helper functions to quickly generate tracking charts. These charts use &lt;code&gt;rnaturalearthdata&lt;/code&gt; (for high resolution maps, use package &lt;code&gt;rnaturalearthhires&lt;/code&gt;). These charts are not required - &lt;a href=&#34;https://twitter.com/hrbrmstr/status/900762714477350913&#34;&gt;Bob Rudis demonstrates&lt;/a&gt; demonstrates succintly - so feel free to experiment.&lt;/p&gt;

&lt;p&gt;You can generate a default plot for the entire globe with &lt;code&gt;tracking_chart&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tracking_chart()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-11-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You may find this handy when examining cyclones that cross basins (from the Atlantic to east Pacific such as Hurricane Otto, 2016).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tracking_chart&lt;/code&gt; takes three parameters (in addition to dots for other &lt;code&gt;ggplot&lt;/code&gt; calls):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;countries&lt;/code&gt; - By default, show country borders&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;states&lt;/code&gt; - By default, show state borders&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;res&lt;/code&gt; - resolution; default is 110nm.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We do not see countries and states in the map above because of the ggplot defaults. Let&amp;rsquo;s try it again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-12-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can &amp;ldquo;zoom in&amp;rdquo; on each basin with helper functions &lt;code&gt;al_tracking_chart&lt;/code&gt; and &lt;code&gt;ep_tracking_chart&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-13-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ep_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-14-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;gis-data&#34;&gt;GIS Data&lt;/h3&gt;

&lt;p&gt;GIS data exists for some cyclones and varies by year. This is a relatively new archive by the NHC and is inconsistent from storm to storm.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;gis&amp;rdquo; functions are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_advisory&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_latest&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_prob_storm_surge&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_windfield&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_wsp&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another area of inconsistency with these products is how they are organized. For example, &lt;code&gt;gis_advisory&lt;/code&gt;, &lt;code&gt;gis_prob_storm_surge&lt;/code&gt; and &lt;code&gt;gis_windfield&lt;/code&gt; can be retrieved with a storm &lt;code&gt;Key&lt;/code&gt; (unique identifier for every cyclone; see &lt;code&gt;fstadv$Key&lt;/code&gt;). Except for &lt;code&gt;gis_prob_storm_surge&lt;/code&gt;, you can even pass an advisory number (see &lt;code&gt;fstadv$Adv&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;gis_wsp&lt;/code&gt; requires a datetime value; to access a specific GIS package for a storm&amp;rsquo;s advisory you would need to use a variable such as &lt;code&gt;fstadv$Date&lt;/code&gt;, subtract three hours and convert to &amp;ldquo;%Y%m%d%H&amp;rdquo; format (&amp;ldquo;%m&amp;rdquo;, &amp;ldquo;%d&amp;rdquo;, and &amp;ldquo;%H&amp;rdquo; are optional).&lt;/p&gt;

&lt;p&gt;All above functions only return URL&amp;rsquo;s to their respective datasets. This was done to allow you to validate the quantity of datasets you wish to retrieve as, in some cases, the dataset may not exist at all or there may be several available. Use &lt;code&gt;gis_download&lt;/code&gt; with the requested URL&amp;rsquo;s to retrieve your datasets.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go through each of these. First, let&amp;rsquo;s get the &lt;code&gt;Key&lt;/code&gt; of Hurricane Harvey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Remember that ds already and only contains data for Hurricane Harvey
key &amp;lt;- ds$fstadv %&amp;gt;% pull(Key) %&amp;gt;% first()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;gis-advisory&#34;&gt;gis_advisory&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;gis_advisory&lt;/code&gt; returns a dataset package containing past and forecast plot points and lines, a forecast cone (area representing where the cyclone could track), wind radius data and current watches and warnings.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;gis_advisory&lt;/code&gt; takes two parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Key&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;advisory&lt;/code&gt; (optional)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we leave out advisory we get all related datasets for Hurricane Harvey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- gis_advisory(key = key)
length(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 77
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(x, n = 5L)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_001.zip&amp;quot;
## [2] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_001A.zip&amp;quot;
## [3] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_002.zip&amp;quot;
## [4] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_002A.zip&amp;quot;
## [5] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_003.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, there is quite a bit (and why the core gis functions only return URLs rather than the actual datasets). Let&amp;rsquo;s trim this down a bit. Sneaking a peek (&lt;a href=&#34;http://www.nhc.noaa.gov/archive/2017/HARVEY_graphics.php?product=5day_cone_with_line_and_wind&#34;&gt;cheating&lt;/a&gt;) I find advisory 19 seems a good choice.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_advisory(key = key, advisory = 19)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_019.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Good; there is a data package available for this advisory. Once you have confirmed the package you want to retrieve, use &lt;code&gt;gis_download&lt;/code&gt; to get the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_advisory(key = key, advisory = 19) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_5day_lin&amp;quot;
## with 1 features
## It has 7 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_5day_pgn&amp;quot;
## with 1 features
## It has 7 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_5day_pts&amp;quot;
## with 8 features
## It has 23 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_ww_wwlin&amp;quot;
## with 5 features
## It has 8 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see what we have.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(gis)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## List of 4
##  $ al092017_019_5day_lin:Formal class &#39;SpatialLinesDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 4 slots
##   .. ..@ data       :&#39;data.frame&#39;:	1 obs. of  7 variables:
##   .. .. ..$ STORMNAME: chr &amp;quot;Harvey&amp;quot;
##   .. .. ..$ STORMTYPE: chr &amp;quot;HU&amp;quot;
##   .. .. ..$ ADVDATE  : chr &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot;
##   .. .. ..$ ADVISNUM : chr &amp;quot;19&amp;quot;
##   .. .. ..$ STORMNUM : num 9
##   .. .. ..$ FCSTPRD  : num 120
##   .. .. ..$ BASIN    : chr &amp;quot;AL&amp;quot;
##   .. ..@ lines      :List of 1
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:8, 1:2] -94.6 -95.6 -96.5 -97.1 -97.3 -97.5 -97 -95 25.2 26.1 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;0&amp;quot;
##   .. ..@ bbox       : num [1:2, 1:2] -97.5 25.2 -94.6 29.5
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
##  $ al092017_019_5day_pgn:Formal class &#39;SpatialPolygonsDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. ..@ data       :&#39;data.frame&#39;:	1 obs. of  7 variables:
##   .. .. ..$ STORMNAME: chr &amp;quot;Harvey&amp;quot;
##   .. .. ..$ STORMTYPE: chr &amp;quot;HU&amp;quot;
##   .. .. ..$ ADVDATE  : chr &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot;
##   .. .. ..$ ADVISNUM : chr &amp;quot;19&amp;quot;
##   .. .. ..$ STORMNUM : num 9
##   .. .. ..$ FCSTPRD  : num 120
##   .. .. ..$ BASIN    : chr &amp;quot;AL&amp;quot;
##   .. ..@ polygons   :List of 1
##   .. .. ..$ :Formal class &#39;Polygons&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. .. .. .. ..@ Polygons :List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Polygon&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. .. .. .. .. .. .. ..@ labpt  : num [1:2] -95.4 29.2
##   .. .. .. .. .. .. .. ..@ area   : num 51.7
##   .. .. .. .. .. .. .. ..@ hole   : logi FALSE
##   .. .. .. .. .. .. .. ..@ ringDir: int 1
##   .. .. .. .. .. .. .. ..@ coords : num [1:1482, 1:2] -94.6 -94.7 -94.7 -94.7 -94.7 ...
##   .. .. .. .. ..@ plotOrder: int 1
##   .. .. .. .. ..@ labpt    : num [1:2] -95.4 29.2
##   .. .. .. .. ..@ ID       : chr &amp;quot;0&amp;quot;
##   .. .. .. .. ..@ area     : num 51.7
##   .. ..@ plotOrder  : int 1
##   .. ..@ bbox       : num [1:2, 1:2] -100 24.9 -91 33
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
##  $ al092017_019_5day_pts:Formal class &#39;SpatialPointsDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. ..@ data       :&#39;data.frame&#39;:	8 obs. of  23 variables:
##   .. .. ..$ ADVDATE  : chr [1:8] &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; ...
##   .. .. ..$ ADVISNUM : chr [1:8] &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; ...
##   .. .. ..$ BASIN    : chr [1:8] &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; ...
##   .. .. ..$ DATELBL  : chr [1:8] &amp;quot;10:00 PM Thu&amp;quot; &amp;quot;7:00 AM Fri&amp;quot; &amp;quot;7:00 PM Fri&amp;quot; &amp;quot;7:00 AM Sat&amp;quot; ...
##   .. .. ..$ DVLBL    : chr [1:8] &amp;quot;H&amp;quot; &amp;quot;H&amp;quot; &amp;quot;M&amp;quot; &amp;quot;M&amp;quot; ...
##   .. .. ..$ FCSTPRD  : num [1:8] 120 120 120 120 120 120 120 120
##   .. .. ..$ FLDATELBL: chr [1:8] &amp;quot;2017-08-24 7:00 PM Thu CDT&amp;quot; &amp;quot;2017-08-25 7:00 AM Fri CDT&amp;quot; &amp;quot;2017-08-25 7:00 PM Fri CDT&amp;quot; &amp;quot;2017-08-26 7:00 AM Sat CDT&amp;quot; ...
##   .. .. ..$ GUST     : num [1:8] 90 115 135 120 85 45 45 45
##   .. .. ..$ LAT      : num [1:8] 25.2 26.1 27.2 28.1 28.6 28.5 28.5 29.5
##   .. .. ..$ LON      : num [1:8] -94.6 -95.6 -96.5 -97.1 -97.3 -97.5 -97 -95
##   .. .. ..$ MAXWIND  : num [1:8] 75 95 110 100 70 35 35 35
##   .. .. ..$ MSLP     : num [1:8] 973 9999 9999 9999 9999 ...
##   .. .. ..$ SSNUM    : num [1:8] 1 2 3 3 1 0 0 0
##   .. .. ..$ STORMNAME: chr [1:8] &amp;quot;Hurricane Harvey&amp;quot; &amp;quot;Hurricane Harvey&amp;quot; &amp;quot;Hurricane Harvey&amp;quot; &amp;quot;Hurricane Harvey&amp;quot; ...
##   .. .. ..$ STORMNUM : num [1:8] 9 9 9 9 9 9 9 9
##   .. .. ..$ STORMSRC : chr [1:8] &amp;quot;Tropical Cyclone&amp;quot; &amp;quot;Tropical Cyclone&amp;quot; &amp;quot;Tropical Cyclone&amp;quot; &amp;quot;Tropical Cyclone&amp;quot; ...
##   .. .. ..$ STORMTYPE: chr [1:8] &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; &amp;quot;MH&amp;quot; &amp;quot;MH&amp;quot; ...
##   .. .. ..$ TCDVLP   : chr [1:8] &amp;quot;Hurricane&amp;quot; &amp;quot;Hurricane&amp;quot; &amp;quot;Major Hurricane&amp;quot; &amp;quot;Major Hurricane&amp;quot; ...
##   .. .. ..$ TAU      : num [1:8] 0 12 24 36 48 72 96 120
##   .. .. ..$ TCDIR    : num [1:8] 315 9999 9999 9999 9999 ...
##   .. .. ..$ TCSPD    : num [1:8] 9 9999 9999 9999 9999 ...
##   .. .. ..$ TIMEZONE : chr [1:8] &amp;quot;CDT&amp;quot; &amp;quot;CDT&amp;quot; &amp;quot;CDT&amp;quot; &amp;quot;CDT&amp;quot; ...
##   .. .. ..$ VALIDTIME: chr [1:8] &amp;quot;25/0000&amp;quot; &amp;quot;25/1200&amp;quot; &amp;quot;26/0000&amp;quot; &amp;quot;26/1200&amp;quot; ...
##   .. ..@ coords.nrs : num(0)
##   .. ..@ coords     : num [1:8, 1:2] -94.6 -95.6 -96.5 -97.1 -97.3 -97.5 -97 -95 25.2 26.1 ...
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : NULL
##   .. .. .. ..$ : chr [1:2] &amp;quot;coords.x1&amp;quot; &amp;quot;coords.x2&amp;quot;
##   .. ..@ bbox       : num [1:2, 1:2] -97.5 25.2 -94.6 29.5
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;coords.x1&amp;quot; &amp;quot;coords.x2&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
##  $ al092017_019_ww_wwlin:Formal class &#39;SpatialLinesDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 4 slots
##   .. ..@ data       :&#39;data.frame&#39;:	5 obs. of  8 variables:
##   .. .. ..$ STORMNAME: chr [1:5] &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##   .. .. ..$ STORMTYPE: chr [1:5] &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; ...
##   .. .. ..$ ADVDATE  : chr [1:5] &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; ...
##   .. .. ..$ ADVISNUM : chr [1:5] &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; ...
##   .. .. ..$ STORMNUM : num [1:5] 9 9 9 9 9
##   .. .. ..$ FCSTPRD  : num [1:5] 120 120 120 120 120
##   .. .. ..$ BASIN    : chr [1:5] &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; ...
##   .. .. ..$ TCWW     : chr [1:5] &amp;quot;TWA&amp;quot; &amp;quot;HWA&amp;quot; &amp;quot;TWR&amp;quot; &amp;quot;TWR&amp;quot; ...
##   .. ..@ lines      :List of 5
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] -97.7 -97.4 -97.2 24.3 25.2 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;0&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] -97.2 -97.2 -97.3 26 26.1 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;1&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] -97.2 -97.2 -97.3 26 26.1 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;2&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:10, 1:2] -95.6 -95.3 -95.1 -95.1 -94.8 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;3&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:16, 1:2] -97.3 -97.3 -97.3 -97.4 -97.4 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;4&amp;quot;
##   .. ..@ bbox       : num [1:2, 1:2] -97.7 24.3 -94.4 29.8
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We get four spatial dataframes - points, polygons and lines.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(gis)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;al092017_019_5day_lin&amp;quot; &amp;quot;al092017_019_5day_pgn&amp;quot; &amp;quot;al092017_019_5day_pts&amp;quot;
## [4] &amp;quot;al092017_019_ww_wwlin&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the expection of point spatial dataframes (which can be converted to dataframe using &lt;code&gt;tibble::as_data_frame&lt;/code&gt;, use helper function &lt;code&gt;shp_to_df&lt;/code&gt; to convert the spatial dataframes to dataframes.&lt;/p&gt;

&lt;h4 id=&#34;forecast-track&#34;&gt;Forecast Track&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)
al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_path(data = shp_to_df(gis$al092017_019_5day_lin), aes(x = long, y = lat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-21-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Use &lt;code&gt;geom_path&lt;/code&gt; instead of &lt;code&gt;geom_line&lt;/code&gt; to keep the positions in order.&lt;/p&gt;

&lt;p&gt;You can &amp;ldquo;zoom in&amp;rdquo; even further using &lt;code&gt;ggplot2::coord_equal&lt;/code&gt;. For that, we need to know the limits of our objects (minimum and maximum latitude and longitude) or bounding box. Thankfully, the &lt;code&gt;sp&lt;/code&gt; package can get us this information with the &lt;code&gt;bbox&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;But, we don&amp;rsquo;t want to use the &amp;ldquo;al092017_019_5day_lin&amp;rdquo; dataset. Our &lt;code&gt;gis&lt;/code&gt; dataset contains a forecast cone which expands well beyond the lines dataset.  Take a look:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp::bbox(gis$al092017_019_5day_lin)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##     min   max
## x -97.5 -94.6
## y  25.2  29.5
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp::bbox(gis$al092017_019_5day_pgn)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##          min       max
## x -100.01842 -90.96327
## y   24.86433  33.01644
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, let&amp;rsquo;s get the bounding box of our forecast cone dataset and zoom in on our map.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$al092017_019_5day_pgn)
al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_path(data = shp_to_df(gis$al092017_019_5day_lin),
            aes(x = long, y = lat)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-24-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s much better. For simplicity I&amp;rsquo;m going to save the base map, &lt;code&gt;bp&lt;/code&gt;, without the line plot.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp &amp;lt;- al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;forecast-points&#34;&gt;Forecast Points&lt;/h4&gt;

&lt;p&gt;Forecast points identify each forecast position along with forecast winds and date. Remember that for point spatial dataframes you use &lt;code&gt;tibble::as_data_frame&lt;/code&gt; rather than &lt;code&gt;sp_to_df&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_point(data = tibble::as_data_frame(gis$al092017_019_5day_pts),
             aes(x = long, y = lat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you ran the code above you would get an error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error in FUN(X[[i]], ...) : object &#39;long&#39; not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Why? The variable &lt;code&gt;long&lt;/code&gt; does not exist as it does in other GIS datasets; it is &lt;code&gt;lon&lt;/code&gt;. This is one of the inconsistencies I was referring to previously. Additionally, the variables are all uppercase.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(gis$al092017_019_5day_pts)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;ADVDATE&amp;quot;   &amp;quot;ADVISNUM&amp;quot;  &amp;quot;BASIN&amp;quot;     &amp;quot;DATELBL&amp;quot;   &amp;quot;DVLBL&amp;quot;
##  [6] &amp;quot;FCSTPRD&amp;quot;   &amp;quot;FLDATELBL&amp;quot; &amp;quot;GUST&amp;quot;      &amp;quot;LAT&amp;quot;       &amp;quot;LON&amp;quot;
## [11] &amp;quot;MAXWIND&amp;quot;   &amp;quot;MSLP&amp;quot;      &amp;quot;SSNUM&amp;quot;     &amp;quot;STORMNAME&amp;quot; &amp;quot;STORMNUM&amp;quot;
## [16] &amp;quot;STORMSRC&amp;quot;  &amp;quot;STORMTYPE&amp;quot; &amp;quot;TCDVLP&amp;quot;    &amp;quot;TAU&amp;quot;       &amp;quot;TCDIR&amp;quot;
## [21] &amp;quot;TCSPD&amp;quot;     &amp;quot;TIMEZONE&amp;quot;  &amp;quot;VALIDTIME&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s try it again.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_point(data = tibble::as_data_frame(gis$al092017_019_5day_pts),
             aes(x = LON, y = LAT))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-28-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Better.&lt;/p&gt;

&lt;h4 id=&#34;forecast-cone&#34;&gt;Forecast Cone&lt;/h4&gt;

&lt;p&gt;A forecast cone identifies the probability of error in a forecast. Forecasting tropical cyclones is tricky business - errors increase the further out a forecast is issued. Theoretically, any area within a forecast cone is at risk of seeing cyclone conditions within the given period of time.&lt;/p&gt;

&lt;p&gt;Generally, a forecast cone package contains two subsets: 72-hour forecast cone and 120-hour forecast cone. This is identified in the dataset under the variable &lt;code&gt;FCSTPRD&lt;/code&gt;. Let&amp;rsquo;s take a look at the 72-hour forecast period:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_polygon(data = shp_to_df(gis$al092017_019_5day_pgn) %&amp;gt;%
                 filter(FCSTPRD == 72),
               aes(x = long, y = lat, color = FCSTPRD))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-29-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nothing there!&lt;/p&gt;

&lt;p&gt;As mentioned earlier, these are experimental products issued by the NHC and they do contain inconsistencies. To demonstrate, I&amp;rsquo;ll use Hurricane Ike advisory 42.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- gis_advisory(key = &amp;quot;AL092008&amp;quot;, advisory = 42) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_5day_lin&amp;quot;
## with 2 features
## It has 9 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_5day_pgn&amp;quot;
## with 2 features
## It has 9 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_5day_pts&amp;quot;
## with 13 features
## It has 20 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_ww_wwlin&amp;quot;
## with 5 features
## It has 10 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = shp_to_df(df$al092008_042_5day_pgn) %&amp;gt;%
                 filter(FCSTPRD == 72),
                  aes(x = long, y = lat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-30-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We do, however, have a 120-hour forecast cone for Hurricane Harvey. Let&amp;rsquo;s go ahead and plot that.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_polygon(data = gis$al092017_019_5day_pgn,
               aes(x = long, y = lat), alpha = 0.15)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-31-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s an odd-looking forecast cone, for sure. But this demonstrates the entire area that Harvey could have potentially traveled.&lt;/p&gt;

&lt;h4 id=&#34;watches-and-warnings&#34;&gt;Watches and Warnings&lt;/h4&gt;

&lt;p&gt;Our last dataset in this package is &amp;ldquo;al092017_09_ww_wlin&amp;rdquo;. These are the current watches and warnings in effect. This is a spatial lines dataframe that needs &lt;code&gt;shp_to_df&lt;/code&gt;. Again, we use &lt;code&gt;geom_path&lt;/code&gt; instead of &lt;code&gt;geom_line&lt;/code&gt;. And we want to group our paths by the variable &lt;code&gt;TCWW&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_path(data = shp_to_df(gis$al092017_019_ww_wwlin),
            aes(x = long, y = lat, group = group, color = TCWW), size = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-32-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The paths won&amp;rsquo;t follow our coastlines exactly but you get the idea. The abbreviations don&amp;rsquo;t really give much information, either. Convert &lt;code&gt;TCWW&lt;/code&gt; to factor and provide better labels for your legend.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ww_wlin &amp;lt;- shp_to_df(gis$al092017_019_ww_wwlin)
ww_wlin$TCWW &amp;lt;- factor(ww_wlin$TCWW,
                              levels = c(&amp;quot;TWA&amp;quot;, &amp;quot;TWR&amp;quot;, &amp;quot;HWA&amp;quot;, &amp;quot;HWR&amp;quot;),
                              labels = c(&amp;quot;Tropical Storm Watch&amp;quot;,
                                         &amp;quot;Tropical Storm Warning&amp;quot;,
                                         &amp;quot;Hurricane Watch&amp;quot;,
                                         &amp;quot;Hurricane Warning&amp;quot;))

bp +
  geom_path(data = ww_wlin,
            aes(x = long, y = lat, group = group, color = TCWW), size = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-33-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;See &lt;a href=&#34;https://ropensci.github.io/rrricanes/articles/articles/forecast_advisory.html&#34;&gt;Forecast/Adivsory GIS&lt;/a&gt; on the &lt;code&gt;rrricanes&lt;/code&gt; website for an example of putting all of this data together in one map.&lt;/p&gt;

&lt;h4 id=&#34;gis-prob-storm-surge&#34;&gt;gis_prob_storm_surge&lt;/h4&gt;

&lt;p&gt;We can also plot the probablistic storm surge for given locations. Again, you will need the storm &lt;code&gt;Key&lt;/code&gt; for this function. There are two additional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;products&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;datetime&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;products&lt;/code&gt; can be one or both of &amp;ldquo;esurge&amp;rdquo; and &amp;ldquo;psurge&amp;rdquo;. esurge shows the probability of the cyclone exceeding the given storm surge plus tide within a given forecast period. psurge shows the probability of a given storm surge within a specified forecast period.&lt;/p&gt;

&lt;p&gt;One or more products may not exist depending on the cyclone and advisory.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;products&lt;/code&gt; parameter expects a list of values for each product. For esurge products, valid values are 10, 20, 30, 40 or 50. For psurge products, valid values are 0, 1, 2, &amp;hellip;, 20.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see if any esurge products exist for Harvey.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(gis_prob_storm_surge(key = key,
                            products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10))))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 150
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And psurge:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(gis_prob_storm_surge(key = key, products = list(&amp;quot;psurge&amp;quot; = 0:20)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 630
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, we have access to a ton of data here. When discussing &lt;code&gt;gis_advisory&lt;/code&gt;, we were able to filter by advisory number. With &lt;code&gt;gis_prob_storm_surge&lt;/code&gt;, this is not an option; we have to use the &lt;code&gt;datetime&lt;/code&gt; parameter to filter. Let&amp;rsquo;s find the &lt;code&gt;Date&lt;/code&gt; for advisory 19.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(d &amp;lt;- ds$fstadv %&amp;gt;% filter(Adv == 19) %&amp;gt;% pull(Date))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2017-08-25 03:00:00 UTC&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;esurge&#34;&gt;esurge&lt;/h5&gt;

&lt;p&gt;Now, let&amp;rsquo;s view all esurge products for date only (exlude time).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_prob_storm_surge(key = key,
                     products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10)),
                     datetime = strftime(d, &amp;quot;%Y%m%d&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082500.zip&amp;quot;
##  [2] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082506.zip&amp;quot;
##  [3] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082512.zip&amp;quot;
##  [4] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082518.zip&amp;quot;
##  [5] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082500.zip&amp;quot;
##  [6] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082506.zip&amp;quot;
##  [7] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082512.zip&amp;quot;
##  [8] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082518.zip&amp;quot;
##  [9] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082500.zip&amp;quot;
## [10] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082506.zip&amp;quot;
## [11] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082512.zip&amp;quot;
## [12] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082518.zip&amp;quot;
## [13] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082500.zip&amp;quot;
## [14] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082506.zip&amp;quot;
## [15] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082512.zip&amp;quot;
## [16] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082518.zip&amp;quot;
## [17] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082500.zip&amp;quot;
## [18] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082506.zip&amp;quot;
## [19] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082512.zip&amp;quot;
## [20] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082518.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s still quite a bit. We can filter it to more by adding hour to the &lt;code&gt;datetime&lt;/code&gt; parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_prob_storm_surge(key = key,
                     products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10)),
                     datetime = strftime(d, &amp;quot;%Y%m%d%H&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This call will give you an error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: No data available for requested storm/advisory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But, this isn&amp;rsquo;t entirely correct. When an advisory package is issued it contains information for the release time. Some of the GIS datasets are based on the release time -3 hours. So, we need to subtract 3 hours from &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There is an additional value that, as of the latest release is not extracted, records the position of the cyclone three hours prior. (As I understand it from the NHC, this is due to the time it takes to collect and prepare the data.) Per &lt;a href=&#34;https://github.com/ropensci/rrricanes/issues/102&#34;&gt;Issue #102&lt;/a&gt;, these values will be added for release 0.2.1. Therefore, instead of subtracting three hours from the &lt;code&gt;Date&lt;/code&gt; variable, you can simply use the &lt;code&gt;PrevPosDate&lt;/code&gt; value for this function.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try it again with the math:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_prob_storm_surge(key = key,
                     products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10)),
                     datetime = strftime(d - 60 * 60 * 3, &amp;quot;%Y%m%d%H&amp;quot;,
                                         tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082500.zip&amp;quot;
## [2] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082500.zip&amp;quot;
## [3] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082500.zip&amp;quot;
## [4] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082500.zip&amp;quot;
## [5] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082500.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As I don&amp;rsquo;t want to get all of these datasets, I&amp;rsquo;ll limit my esurge to show surge values with at least a 50% chance of being exceeded:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_prob_storm_surge(key = key,
                            products = list(&amp;quot;esurge&amp;quot; = 50),
                            datetime = strftime(d - 60 * 60 * 3, &amp;quot;%Y%m%d%H&amp;quot;,
                                                tz = &amp;quot;UTC&amp;quot;)) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082500_e50&amp;quot;
## with 97 features
## It has 2 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will bring us a spatial polygon dataframe.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- shp_to_df(gis$al092017_2017082500_e50)
bb &amp;lt;- sp::bbox(gis$al092017_2017082500_e50)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	161313 obs. of  9 variables:
##  $ long   : num  -93.2 -93.2 -93.2 -93.2 -93.2 ...
##  $ lat    : num  29.9 29.9 29.9 29.9 29.9 ...
##  $ order  : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece  : Factor w/ 349 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group  : Factor w/ 7909 levels &amp;quot;0.1&amp;quot;,&amp;quot;0.2&amp;quot;,&amp;quot;0.3&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id     : chr  &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; ...
##  $ POINTID: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ TCSRG50: num  0 0 0 0 0 0 0 0 0 0 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
            aes(x = long, y = lat, group = group, color = TCSRG50)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-43-1.png&#34; title=&#34;plot of chunk unnamed-chunk-43&#34; alt=&#34;plot of chunk unnamed-chunk-43&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This plot tells us that, along the central Texas coast, the expected storm surge along with tides is greater than 7.5 feet and there is a 50% chance of this height being exceeded.&lt;/p&gt;

&lt;h4 id=&#34;psurge&#34;&gt;psurge&lt;/h4&gt;

&lt;p&gt;The psurge product gives us the probabilistic storm surge for a location within the given forecast period.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_prob_storm_surge(key = key,
                            products = list(&amp;quot;psurge&amp;quot; = 20),
                            datetime = strftime(d - 60 * 60 * 3, &amp;quot;%Y%m%d%H&amp;quot;,
                                                tz = &amp;quot;UTC&amp;quot;)) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082500_gt20&amp;quot;
## with 12 features
## It has 2 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will bring us a spatial polygon dataframe.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- shp_to_df(gis$al092017_2017082500_gt20)
bb &amp;lt;- sp::bbox(gis$al092017_2017082500_gt20)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	3293 obs. of  9 variables:
##  $ long     : num  -96.8 -96.8 -96.8 -96.8 -96.8 ...
##  $ lat      : num  28.5 28.5 28.4 28.4 28.4 ...
##  $ order    : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole     : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece    : Factor w/ 54 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group    : Factor w/ 227 levels &amp;quot;0.1&amp;quot;,&amp;quot;0.2&amp;quot;,&amp;quot;0.3&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id       : chr  &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; ...
##  $ POINTID  : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ PSurge20c: num  1 1 1 1 1 1 1 1 1 1 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
            aes(x = long, y = lat, group = group, color = PSurge20c)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-47-1.png&#34; title=&#34;plot of chunk unnamed-chunk-47&#34; alt=&#34;plot of chunk unnamed-chunk-47&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This map shows the cumulative probability that a storm surge of greater than 20 feet will be seen within the highlighted regions.&lt;/p&gt;

&lt;p&gt;This particular map doesn&amp;rsquo;t help much as we&amp;rsquo;ve zoomed in too far. What may provide use is a list of probability stations as obtained from the NHC. For this, you can use &lt;code&gt;al_prblty_stations&lt;/code&gt; (&lt;code&gt;ep_prblty_stations&lt;/code&gt; returns FALSE since, as of this writing, the format is invalid).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;stations &amp;lt;- al_prblty_stations()

al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
            aes(x = long, y = lat, group = group, color = PSurge20c)) +
  geom_label(data = stations, aes(x = Lon, y = Lat, label = Location)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-48-1.png&#34; title=&#34;plot of chunk unnamed-chunk-48&#34; alt=&#34;plot of chunk unnamed-chunk-48&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;gis-windfield&#34;&gt;gis_windfield&lt;/h4&gt;

&lt;p&gt;When possible, there may also be a cyclone wind radius dataset for the current and forecast positions. With this function we can resort back to &lt;code&gt;Key&lt;/code&gt; and an advisory number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_windfield(key = key, advisory = 19) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082503_forecastradii&amp;quot;
## with 15 features
## It has 13 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082503_initialradii&amp;quot;
## with 3 features
## It has 13 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(gis)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;al092017_2017082503_forecastradii&amp;quot; &amp;quot;al092017_2017082503_initialradii&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s get the bounding box and plot our initialradii dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$al092017_2017082503_initialradii)

al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = shp_to_df(gis$al092017_2017082503_initialradii),
            aes(x = long, y = lat, group = group, fill = factor(RADII)),
            alpha = 0.5) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-51-1.png&#34; title=&#34;plot of chunk unnamed-chunk-51&#34; alt=&#34;plot of chunk unnamed-chunk-51&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And add the forecast wind radii data onto the chart (modifying &lt;code&gt;bb&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$al092017_2017082503_forecastradii)

al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = shp_to_df(gis$al092017_2017082503_initialradii),
            aes(x = long, y = lat, group = group, fill = factor(RADII)),
            alpha = 0.5) +
  geom_polygon(data = shp_to_df(gis$al092017_2017082503_forecastradii),
               aes(x = long, y = lat, group = group, fill = factor(RADII)),
               alpha = 0.5) +
  geom_label(data = stations, aes(x = Lon, y = Lat, label = Location)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-52-1.png&#34; title=&#34;plot of chunk unnamed-chunk-52&#34; alt=&#34;plot of chunk unnamed-chunk-52&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;gis-wsp&#34;&gt;gis_wsp&lt;/h4&gt;

&lt;p&gt;Our last GIS dataset is wind speed probabilities. This dataset is not storm specific nor even basin-specific; you may get results for cyclones halfway across the world.&lt;/p&gt;

&lt;p&gt;The two parameters needed are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;datetime&lt;/code&gt; - again, using the %Y%m%d%H format (not all values are required)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;res&lt;/code&gt; - Resolution of the probabilities; 5 degrees, 0.5 degrees and 0.1 degrees.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Wind fields are for 34, 50 and 64 knots. Not all resolutions or windfields will be available at a given time.&lt;/p&gt;

&lt;p&gt;Sticking with our variable &lt;code&gt;d&lt;/code&gt;, let&amp;rsquo;s first make sure there is a dataset that exists for that time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_wsp(datetime = strftime(d - 60 * 60 * 3, format = &amp;quot;%Y%m%d%H&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this article, we&amp;rsquo;ll stick to the higher resolution plot.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;we need a temporarily fixed function to replace &lt;code&gt;gis_wsp()&lt;/code&gt;, which will be
fixed in package soon&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_wsp_2 &amp;lt;- function(datetime, res = c(5, 0.5, 0.1)) {
  res &amp;lt;- as.character(res)
  res &amp;lt;- stringr::str_replace(res, &amp;quot;^5$&amp;quot;, &amp;quot;5km&amp;quot;)
  res &amp;lt;- stringr::str_replace(res, &amp;quot;^0.5$&amp;quot;, &amp;quot;halfDeg&amp;quot;)
  res &amp;lt;- stringr::str_replace(res, &amp;quot;^0.1$&amp;quot;, &amp;quot;tenthDeg&amp;quot;)
  year &amp;lt;- stringr::str_sub(datetime, 0L, 4L)
  request &amp;lt;- httr::GET(&amp;quot;http://www.nhc.noaa.gov/gis/archive_wsp.php&amp;quot;,
                       query = list(year = year))
  contents &amp;lt;- httr::content(request, as = &amp;quot;parsed&amp;quot;, encoding = &amp;quot;UTF-8&amp;quot;)
  ds &amp;lt;- rvest::html_nodes(contents, xpath = &amp;quot;//a&amp;quot;) %&amp;gt;% rvest::html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
    stringr::str_extract(&amp;quot;.+\\.zip$&amp;quot;) %&amp;gt;% .[stats::complete.cases(.)]
  if (nchar(datetime) &amp;lt; 10) {
    ptn_datetime &amp;lt;- paste0(datetime, &amp;quot;[:digit:]+&amp;quot;)
  } else {
    ptn_datetime &amp;lt;- datetime
  }
  ptn_res &amp;lt;- paste(res, collapse = &amp;quot;|&amp;quot;)
  ptn &amp;lt;- sprintf(&amp;quot;%s_wsp_[:digit:]{1,3}hr(%s)&amp;quot;, ptn_datetime,
                 ptn_res)
  links &amp;lt;- ds[stringr::str_detect(ds, ptn)]
  links &amp;lt;- paste0(&amp;quot;http://www.nhc.noaa.gov/gis/&amp;quot;, links)
  return(links)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_wsp_2(
  datetime = strftime(d - 60 * 60 * 3, format = &amp;quot;%Y%m%d%H&amp;quot;, tz = &amp;quot;UTC&amp;quot;),
  res = 5) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;2017082500_wsp34knt120hr_5km&amp;quot;
## with 11 features
## It has 1 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;2017082500_wsp50knt120hr_5km&amp;quot;
## with 11 features
## It has 1 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;2017082500_wsp64knt120hr_5km&amp;quot;
## with 11 features
## It has 1 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All of these datasets are spatial polygon dataframes. Again, we will need to convert to dataframe using &lt;code&gt;shp_to_df&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$`2017082500_wsp34knt120hr_5km`)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Examine the structure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- shp_to_df(gis$`2017082500_wsp34knt120hr_5km`)
str(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	24182 obs. of  8 variables:
##  $ long      : num  -97.2 -97.2 -97.2 -97.3 -97.3 ...
##  $ lat       : num  20.3 20.3 20.3 20.3 20.4 ...
##  $ order     : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole      : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece     : Factor w/ 8 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group     : Factor w/ 52 levels &amp;quot;0.1&amp;quot;,&amp;quot;0.2&amp;quot;,&amp;quot;0.3&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id        : chr  &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; ...
##  $ PERCENTAGE: chr  &amp;quot;&amp;lt;5%&amp;quot; &amp;quot;&amp;lt;5%&amp;quot; &amp;quot;&amp;lt;5%&amp;quot; &amp;quot;&amp;lt;5%&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
               aes(x = long, y = lat, group = group, fill = PERCENTAGE),
               alpha = 0.25) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-58-1.png&#34; title=&#34;plot of chunk unnamed-chunk-58&#34; alt=&#34;plot of chunk unnamed-chunk-58&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There aren&amp;rsquo;t many ways we can narrow this down other than using arbitrary longitude values. The observations in the dataset do not have a variable identifying which storm each set of values belongs to. So, I&amp;rsquo;ll remove the &lt;code&gt;coord_equal&lt;/code&gt; call so we&amp;rsquo;re only focused on the Atlantic basin.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
               aes(x = long, y = lat, group = group, fill = PERCENTAGE),
               alpha = 0.25)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-59-1.png&#34; title=&#34;plot of chunk unnamed-chunk-59&#34; alt=&#34;plot of chunk unnamed-chunk-59&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, you can narrow it down further as you see fit.&lt;/p&gt;

&lt;p&gt;Do not confuse this GIS dataset with the &lt;code&gt;wndprb&lt;/code&gt; product or similar &lt;code&gt;prblty&lt;/code&gt; products; both of which only identify probabilities for given locations.&lt;/p&gt;

&lt;h4 id=&#34;gis-latest&#34;&gt;gis_latest&lt;/h4&gt;

&lt;p&gt;For active cyclones, you can retrieve all available GIS datasets using &lt;code&gt;gis_latest&lt;/code&gt;. Note that, unlike the previous GIS functions, this function will return a list of all GIS dataframes available.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_latest()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have a large list of GIS spatial dataframes. Two things to point out here; first, we now have a &amp;ldquo;windswath&amp;rdquo; GIS dataset. This dataset, to the best of my knowledge, does not exist on it&amp;rsquo;s own. Therefore, no archived &amp;ldquo;windswath&amp;rdquo; datasets are available.&lt;/p&gt;

&lt;p&gt;Second, I have found this data fluctuates even from minute to minute. Earlier this year when attempting to develop automated reporting, I found the return value of the call would vary almost with every call.&lt;/p&gt;

&lt;p&gt;Of course, that doesn&amp;rsquo;t mean it is not valuable, and why it has been included. You can easily perform checks for specific data you are looking for. If it doesn&amp;rsquo;t exist, bail and try again in a few minutes.&lt;/p&gt;

&lt;h3 id=&#34;potential-issues-using-rrricanes&#34;&gt;Potential Issues Using rrricanes&lt;/h3&gt;

&lt;p&gt;I cannot stress enough that &lt;code&gt;rrricanes&lt;/code&gt; &lt;strong&gt;is not intended for use during emergency situations&lt;/strong&gt;, as I myself learned &lt;a href=&#34;https://twitter.com/timtrice/status/901025869367586816&#34;&gt;during Hurricane Harvey&lt;/a&gt;. The package currently relies on the NHC website which, I truly believe, is curated by hand.&lt;/p&gt;

&lt;p&gt;The most common problems I&amp;rsquo;ve noticed are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The NHC website unable to load or slow to respond. This was a hassle in previous releases but seems to be ironed out as of release 0.2.0.6. Nonetheless, there may be instances where response time is slow.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Incorrect storm archive links. Another example would be during Harvey when the link to Harvey&amp;rsquo;s archive page was &lt;a href=&#34;https://twitter.com/PutmanSteve/status/900777826412105729&#34;&gt;listed incorrectly&lt;/a&gt;. If I manually typed the link as it should be, the storm&amp;rsquo;s correct archive page would load. However, the NHC website listed it incorrectly on the annual archives page.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As I become more aware of potential problems, I will look for workarounds. I would be greatly appreciative of any problems being posted to the &lt;a href=&#34;https://github.com/ropensci/rrricanes/issues&#34;&gt;rrricanes repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I will also post known issues beyond my control (such as NHC website issues) to Twitter using the &lt;a href=&#34;https://twitter.com/search?f=tweets&amp;amp;vertical=default&amp;amp;q=%23rrricanes&amp;amp;src=typd&#34;&gt;#rrricanes hashtag&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;future-plans&#34;&gt;Future Plans&lt;/h3&gt;

&lt;p&gt;The following data will be added to &lt;code&gt;rrricanes&lt;/code&gt; as time allows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Reconnaissance data (release 0.2.2)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Computer forecast model data (release 0.2.3)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Archived satellite images (tentative)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ship and buoy data (tentative)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reconnaissance data itself will be a massive project. There are numerous types of products. And, as advisory product formats have changed over the years, so have these. Any help in this task would be tremendously appreciated!&lt;/p&gt;

&lt;p&gt;Some computer forecast models are in the public domain and can certainly be of tremendous use. Some are difficult to find (especially archived). The caution in this area is that many websites post this data but may have limitations on how it can be accessed.&lt;/p&gt;

&lt;p&gt;Additionally, data may be added as deemed fitting.&lt;/p&gt;

&lt;h3 id=&#34;contribute&#34;&gt;Contribute&lt;/h3&gt;

&lt;p&gt;Anyone is more than welcome to contribute to the package. I would definitely appreciate any help. See &lt;a href=&#34;https://github.com/ropensci/rrricanes/blob/master/.github/CONTRIBUTING.md&#34;&gt;Contributions&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;I would ask that you follow the &lt;a href=&#34;http://style.tidyverse.org/&#34;&gt;Tidyverse style guide&lt;/a&gt;. Release 0.2.1 will fully incorporate these rules.&lt;/p&gt;

&lt;p&gt;You do not need to submit code in order to be listed as a contributor. If there is a data source (that can legally be scraped) that you feel should be added, please feel free to submit a request. Submitting bug reports and feature requests are all extremely valuable to the success of &lt;code&gt;rrricanes&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;I want to thank the &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; community for embracing &lt;code&gt;rrricanes&lt;/code&gt; and accepting the package into their vast portfolio. This is my first attempt and putting a project into part of a larger community and the lessons learned have been outstanding.&lt;/p&gt;

&lt;p&gt;I want to thank &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maelle Salmon&lt;/a&gt; who, in a sense, has been like a guiding angel from start to finish during the entire onboarding and review process.&lt;/p&gt;

&lt;p&gt;I want to give a very special thanks to &lt;a href=&#34;https://github.com/robinsones&#34;&gt;Emily Robinson&lt;/a&gt; and &lt;a href=&#34;https://github.com/jsta&#34;&gt;Joseph Stachelek&lt;/a&gt; for taking the time to put &lt;code&gt;rrricanes&lt;/code&gt; to the test, giving valuable insight and recommendations on improving it.&lt;/p&gt;

&lt;p&gt;And a thank-you also to &lt;a href=&#34;https://github.com/jimmylovestea&#34;&gt;James Molyneux&lt;/a&gt;, &lt;a href=&#34;https://github.com/mpadge&#34;&gt;Mark Padgham&lt;/a&gt;, and &lt;a href=&#34;https://github.com/hrbrmstr&#34;&gt;Bob Rudis&lt;/a&gt;, all of whom have offered guidance or input that has helped make &lt;code&gt;rrricanes&lt;/code&gt; far better than it would have been on my own.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Accessing patent data with the patentsview package</title>
      <link>https://ropensci.org/blog/2017/09/19/patentsview/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/19/patentsview/</guid>
      <description>
        
        

&lt;h3 id=&#34;why-care-about-patents&#34;&gt;Why care about patents?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Patents play a critical role in incentivizing innovation, without
which we wouldn&amp;rsquo;t have much of the technology we rely on everyday&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What does your iPhone, Google&amp;rsquo;s PageRank algorithm, and a butter
substitute called Smart Balance all have in common?&lt;/p&gt;

&lt;!-- These are open source images taken from: https://pixabay.com/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/iphone.png&#34; width=&#34;15%&#34;&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/google.jpg&#34; width=&#34;25%&#34;&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/butter.png&#34; width=&#34;25%&#34;&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;They all probably wouldn&amp;rsquo;t be here if not for patents. A patent
provides its owner with the ability to make money off of something that
they invented, without having to worry about someone else copying their
technology. Think Apple would spend millions of dollars developing the
iPhone if Samsung could just come along and &lt;a href=&#34;http://www.reuters.com/article/us-apple-samsung-elec-appeal-idUSKCN1271LF&#34;&gt;rip it
off&lt;/a&gt;?
Probably not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Patents offer a great opportunity for data analysis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are two primary reasons for this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Patent data is public&lt;/strong&gt;. In return for the exclusive right to
profit off an invention, an individual/company has to publicly
disclose the details of their invention to the rest of the world.
&lt;a href=&#34;http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;amp;Sect2=HITOFF&amp;amp;p=1&amp;amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;amp;r=11&amp;amp;f=G&amp;amp;l=50&amp;amp;co1=AND&amp;amp;d=PTXT&amp;amp;s1=dog&amp;amp;OS=dog&amp;amp;RS=dog&#34;&gt;Examples of those
details&lt;/a&gt;
include the patent&amp;rsquo;s title, abstract, technology classification,
assigned organizations, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patent data can answer questions that people care about&lt;/strong&gt;.
Companies (especially big ones like IBM and Google) have a vested
interest in extracting insights from patents, and spend a lot of
time/resources trying figure out how to best manage their
intellectual property (IP) rights. They&amp;rsquo;re plagued by questions like
&amp;ldquo;who should I sell my underperforming patents to,&amp;rdquo; &amp;ldquo;which technology
areas are open to new innovations,&amp;rdquo; &amp;ldquo;what&amp;rsquo;s going to be the next big
thing in the world of buttery spreads,&amp;rdquo; etc. Patents offer a way to
provide data-driven answers to these questions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Combined, these two things make patents a prime target for data
analysis. However, until recently it was hard to get at the data inside
these documents. One had to either collect it manually using the
official &lt;a href=&#34;https://en.wikipedia.org/wiki/United_States_Patent_and_Trademark_Office&#34;&gt;United States Patent and Trademark
Office&lt;/a&gt;
(USPTO) &lt;a href=&#34;http://patft.uspto.gov/netahtml/PTO/search-adv.htm&#34;&gt;search
engine&lt;/a&gt;, or figure
out a way to download, parse, and model huge XML data dumps. Enter
PatentsView.&lt;/p&gt;

&lt;h3 id=&#34;patentsview-and-the-patentsview-package&#34;&gt;PatentsView and the &lt;code&gt;patentsview&lt;/code&gt; package&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.patentsview.org/web/#viz/relationships&#34;&gt;PatentsView&lt;/a&gt; is one
of USPTO&amp;rsquo;s new initiatives intended to increase the usability and value
of patent data. One feature of this project is a publicly accessible API
that makes it easy to programmatically interact with the data. A few of
the reasons why I like the API (and PatentsView more generally):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The API is free (no credential required) and currently doesn&amp;rsquo;t
impose rate limits/bandwidth throttling.&lt;/li&gt;
&lt;li&gt;The project offers &lt;a href=&#34;http://www.patentsview.org/download/&#34;&gt;bulk downloads of patent
data&lt;/a&gt; on their website (in a
flat file format), for those who want to be closest to the data.&lt;/li&gt;
&lt;li&gt;Both the API and the bulk download data contain disambiguated
entities such as inventors, assignees, organizations, etc. In other
words, the API will tell you whether it thinks that John Smith on
patent X is the same person as John Smith on patent Y. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;patentsview&lt;/code&gt; R package is a wrapper around the PatentsView API. It
contains a function that acts as a client to the API (&lt;code&gt;search_pv()&lt;/code&gt;) as
well as several supporting functions. Full documentation of the package
can be found on its
&lt;a href=&#34;https://ropensci.github.io/patentsview/index.html&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;

&lt;p&gt;You can install the stable version of &lt;code&gt;patentsview&lt;/code&gt; from CRAN:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;patentsview&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version from GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (!require(devtools)) install.packages(&amp;quot;devtools&amp;quot;)

devtools::install_github(&amp;quot;ropensci/patentsview&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;getting-started&#34;&gt;Getting started&lt;/h3&gt;

&lt;p&gt;The package has one main function, &lt;code&gt;search_pv()&lt;/code&gt;, that makes it easy to
send requests to the API. There are two parameters to &lt;code&gt;search_pv()&lt;/code&gt; that
you&amp;rsquo;re going to want to think about just about every time you call it -
&lt;code&gt;query&lt;/code&gt; and &lt;code&gt;fields&lt;/code&gt;. You tell the API how you want to filter the patent
data with &lt;code&gt;query&lt;/code&gt;, and which fields you want to retrieve with
&lt;code&gt;fields&lt;/code&gt;. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h4 id=&#34;query&#34;&gt;&lt;code&gt;query&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Your query has to use the &lt;a href=&#34;http://www.patentsview.org/api/query-language.html&#34;&gt;PatentsView query
language&lt;/a&gt;, which is
a JSON-based syntax that is similar to the one used by Lucene. You can
write the query directly and pass it as a string to &lt;code&gt;search_pv()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(patentsview)

qry_1 &amp;lt;- &#39;{&amp;quot;_gt&amp;quot;:{&amp;quot;patent_year&amp;quot;:2007}}&#39;
search_pv(query = qry_1, fields = NULL) # This will retrieve a default set of fields
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_id    : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_number: chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_title : chr [1:25] &amp;quot;Sealing device for body suit and sealin&amp;quot;..
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;Or you can use the domain specific language (DSL) provided in the
&lt;code&gt;patentsview&lt;/code&gt; package to help you write the query:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;qry_2 &amp;lt;- qry_funs$gt(patent_year = 2007) # All DSL functions are in the qry_funs list
qry_2 # qry_2 is the same as qry_1
#&amp;gt; {&amp;quot;_gt&amp;quot;:{&amp;quot;patent_year&amp;quot;:2007}}

search_pv(query = qry_2)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_id    : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_number: chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_title : chr [1:25] &amp;quot;Sealing device for body suit and sealin&amp;quot;..
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;qry_1&lt;/code&gt; and &lt;code&gt;qry_2&lt;/code&gt; will result in the same HTTP call to the API. Both
queries search for patents in USPTO that were published after 2007.
There are three gotchas to look out for when writing a query:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Field is queryable.&lt;/strong&gt; The API has 7 endpoints (the default
endpoint is &amp;ldquo;patents&amp;rdquo;), and each endpoint has its own set of fields
that you can filter on. &lt;em&gt;The fields that you can filter on are not
necessarily the same as the ones that you can retrieve.&lt;/em&gt; In other
words, the fields that you can include in &lt;code&gt;query&lt;/code&gt; (e.g.,
&lt;code&gt;patent_year&lt;/code&gt;) are not necessarily the same as those that you can
include in &lt;code&gt;fields&lt;/code&gt;. To see which fields you can query on, look in
the &lt;code&gt;fieldsdf&lt;/code&gt; data frame (&lt;code&gt;View(patentsview::fieldsdf)&lt;/code&gt;) for fields
that have a &amp;ldquo;y&amp;rdquo; indicator in their &lt;code&gt;can_query&lt;/code&gt; column for your given
endpoint.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Correct data type for field.&lt;/strong&gt; If you&amp;rsquo;re filtering on a field in
your query, you have to make sure that the value you are filtering
on is consistent with the field&amp;rsquo;s data type. For example,
&lt;code&gt;patent_year&lt;/code&gt; has type &amp;ldquo;integer,&amp;rdquo; so if you pass 2007 as a string
then you&amp;rsquo;re going to get an error (&lt;code&gt;patent_year = 2007&lt;/code&gt; is good,
&lt;code&gt;patent_year = &amp;quot;2007&amp;quot;&lt;/code&gt; is no good). You can find a field&amp;rsquo;s data type
in the &lt;code&gt;fieldsdf&lt;/code&gt; data frame.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comparison function works with field&amp;rsquo;s data type.&lt;/strong&gt; The comparison
function(s) that you use (e.g., the greater-than function shown
above, &lt;code&gt;qry_funs$gt()&lt;/code&gt;) must be consistent with the field&amp;rsquo;s data
type. For example, you can&amp;rsquo;t use the &amp;ldquo;contains&amp;rdquo; function on fields
of type &amp;ldquo;integer&amp;rdquo; (&lt;code&gt;qry_funs$contains(patent_year = 2007)&lt;/code&gt; will
throw an error). See &lt;code&gt;?qry_funs&lt;/code&gt; for more details.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In short, use the &lt;code&gt;fieldsdf&lt;/code&gt; data frame when you write a query and you
should be fine. Check out the &lt;a href=&#34;https://ropensci.github.io/patentsview/articles/writing-queries.html&#34;&gt;writing queries
vignette&lt;/a&gt;
for more details.&lt;/p&gt;

&lt;h4 id=&#34;fields&#34;&gt;&lt;code&gt;fields&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Up until now we have been using the default value for &lt;code&gt;fields&lt;/code&gt;. This
results in the API giving us some small set of default fields. Let&amp;rsquo;s see
about retrieving some more fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;search_pv(
  query = qry_funs$gt(patent_year = 2007),
  fields = c(&amp;quot;patent_abstract&amp;quot;, &amp;quot;patent_average_processing_time&amp;quot;,
             &amp;quot;inventor_first_name&amp;quot;, &amp;quot;inventor_total_num_patents&amp;quot;)
)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame (with list column(s) inside) on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_abstract               : chr [1:25] &amp;quot;A sealing device for a&amp;quot;..
#&amp;gt;   ..$ patent_average_processing_time: chr [1:25] &amp;quot;1324&amp;quot; ...
#&amp;gt;   ..$ inventors                     :List of 25
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The fields that you can retrieve depends on the endpoint that you are
hitting. We&amp;rsquo;ve been using the &amp;ldquo;patents&amp;rdquo; endpoint thus far, so all of
these are retrievable:
&lt;code&gt;fieldsdf[fieldsdf$endpoint == &amp;quot;patents&amp;quot;, &amp;quot;field&amp;quot;]&lt;/code&gt;. You can also use
&lt;code&gt;get_fields()&lt;/code&gt; to list the retrievable fields for a given endpoint:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;search_pv(
  query = qry_funs$gt(patent_year = 2007),
  fields = get_fields(endpoint = &amp;quot;patents&amp;quot;, groups = c(&amp;quot;patents&amp;quot;, &amp;quot;inventors&amp;quot;))
)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame (with list column(s) inside) on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  31 variables:
#&amp;gt;   ..$ patent_abstract                       : chr [1:25] &amp;quot;A sealing devi&amp;quot;..
#&amp;gt;   ..$ patent_average_processing_time        : chr [1:25] &amp;quot;1324&amp;quot; ...
#&amp;gt;   ..$ patent_date                           : chr [1:25] &amp;quot;2008-01-01&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_city       : chr [1:25] &amp;quot;Cambridge&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_country    : chr [1:25] &amp;quot;US&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_id         : chr [1:25] &amp;quot;b9fc6599e3d60c&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_assignee_latitude   : chr [1:25] &amp;quot;42.3736&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_location_id: chr [1:25] &amp;quot;42.3736158|-71&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_assignee_longitude  : chr [1:25] &amp;quot;-71.1097&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_state      : chr [1:25] &amp;quot;MA&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_city       : chr [1:25] &amp;quot;Lucca&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_country    : chr [1:25] &amp;quot;IT&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_id         : chr [1:25] &amp;quot;6416028-3&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_latitude   : chr [1:25] &amp;quot;43.8376&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_location_id: chr [1:25] &amp;quot;43.8376211|10.&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_inventor_longitude  : chr [1:25] &amp;quot;10.4951&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_state      : chr [1:25] &amp;quot;Tuscany&amp;quot; ...
#&amp;gt;   ..$ patent_id                             : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_kind                           : chr [1:25] &amp;quot;B1&amp;quot; ...
#&amp;gt;   ..$ patent_number                         : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_num_cited_by_us_patents        : chr [1:25] &amp;quot;5&amp;quot; ...
#&amp;gt;   ..$ patent_num_claims                     : chr [1:25] &amp;quot;25&amp;quot; ...
#&amp;gt;   ..$ patent_num_combined_citations         : chr [1:25] &amp;quot;35&amp;quot; ...
#&amp;gt;   ..$ patent_num_foreign_citations          : chr [1:25] &amp;quot;0&amp;quot; ...
#&amp;gt;   ..$ patent_num_us_application_citations   : chr [1:25] &amp;quot;0&amp;quot; ...
#&amp;gt;   ..$ patent_num_us_patent_citations        : chr [1:25] &amp;quot;35&amp;quot; ...
#&amp;gt;   ..$ patent_processing_time                : chr [1:25] &amp;quot;792&amp;quot; ...
#&amp;gt;   ..$ patent_title                          : chr [1:25] &amp;quot;Sealing device&amp;quot;..
#&amp;gt;   ..$ patent_type                           : chr [1:25] &amp;quot;utility&amp;quot; ...
#&amp;gt;   ..$ patent_year                           : chr [1:25] &amp;quot;2008&amp;quot; ...
#&amp;gt;   ..$ inventors                             :List of 25
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s look at a quick example of pulling and analyzing patent data.
We&amp;rsquo;ll look at patents from the last ten years that are classified below
the &lt;a href=&#34;https://worldwide.espacenet.com/classification#!/CPC=H04L63/02&#34;&gt;H04L63/00 CPC
code&lt;/a&gt;.
Patents in this area relate to &amp;ldquo;network architectures or network
communication protocols for separating internal from external
traffic.&amp;rdquo; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; CPC codes offer a quick and dirty way to find patents of
interest, though getting a sense of their hierarchy can be tricky.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download the data&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(patentsview)

# Write a query:
query &amp;lt;- with_qfuns( # with_qfuns is basically just: with(qry_funs, ...)
  and(
    begins(cpc_subgroup_id = &#39;H04L63/02&#39;),
    gte(patent_year = 2007)
  )
)

# Create a list of fields:
fields &amp;lt;- c(
  c(&amp;quot;patent_number&amp;quot;, &amp;quot;patent_year&amp;quot;),
  get_fields(endpoint = &amp;quot;patents&amp;quot;, groups = c(&amp;quot;assignees&amp;quot;, &amp;quot;cpcs&amp;quot;))
)

# Send HTTP request to API&#39;s server:
pv_res &amp;lt;- search_pv(query = query, fields = fields, all_pages = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;See where the patents are coming from (geographically)&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(leaflet)
library(htmltools)
library(dplyr)
library(tidyr)

data &amp;lt;-
  pv_res$data$patents %&amp;gt;%
    unnest(assignees) %&amp;gt;%
    select(assignee_id, assignee_organization, patent_number,
           assignee_longitude, assignee_latitude) %&amp;gt;%
    group_by_at(vars(-matches(&amp;quot;pat&amp;quot;))) %&amp;gt;%
    mutate(num_pats = n()) %&amp;gt;%
    ungroup() %&amp;gt;%
    select(-patent_number) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(popup = paste0(&amp;quot;&amp;lt;font color=&#39;Black&#39;&amp;gt;&amp;quot;,
                          htmlEscape(assignee_organization), &amp;quot;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Patents:&amp;quot;,
                          num_pats, &amp;quot;&amp;lt;/font&amp;gt;&amp;quot;)) %&amp;gt;%
    mutate_at(vars(matches(&amp;quot;_l&amp;quot;)), as.numeric) %&amp;gt;%
    filter(!is.na(assignee_id))

leaflet(data) %&amp;gt;%
  addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %&amp;gt;%
  addCircleMarkers(lng = ~assignee_longitude, lat = ~assignee_latitude,
                   popup = ~popup, ~sqrt(num_pats), color = &amp;quot;yellow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Plot the growth of the field&amp;rsquo;s topics over time&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(ggplot2)
library(RColorBrewer)

data &amp;lt;-
  pv_res$data$patents %&amp;gt;%
    unnest(cpcs) %&amp;gt;%
    filter(cpc_subgroup_id != &amp;quot;H04L63/02&amp;quot;) %&amp;gt;% # remove patents categorized into only top-level category of H04L63/02
    mutate(
      title = case_when(
        grepl(&amp;quot;filtering&amp;quot;, .$cpc_subgroup_title, ignore.case = T) ~
          &amp;quot;Filtering policies&amp;quot;,
        .$cpc_subgroup_id %in% c(&amp;quot;H04L63/0209&amp;quot;, &amp;quot;H04L63/0218&amp;quot;) ~
          &amp;quot;Architectural arrangements&amp;quot;,
        grepl(&amp;quot;Firewall traversal&amp;quot;, .$cpc_subgroup_title, ignore.case = T) ~
          &amp;quot;Firewall traversal&amp;quot;,
        TRUE ~
          .$cpc_subgroup_title
      )
    ) %&amp;gt;%
    mutate(title = gsub(&amp;quot;.*(?=-)-&amp;quot;, &amp;quot;&amp;quot;, title, perl = TRUE)) %&amp;gt;%
    group_by(title, patent_year) %&amp;gt;%
    count() %&amp;gt;%
    ungroup() %&amp;gt;%
    mutate(patent_year = as.numeric(patent_year))

ggplot(data = data) +
  geom_smooth(aes(x = patent_year, y = n, colour = title), se = FALSE) +
  scale_x_continuous(&amp;quot;\nPublication year&amp;quot;, limits = c(2007, 2016),
                     breaks = 2007:2016) +
  scale_y_continuous(&amp;quot;Patents\n&amp;quot;, limits = c(0, 700)) +
  scale_colour_manual(&amp;quot;&amp;quot;, values = brewer.pal(5, &amp;quot;Set2&amp;quot;)) +
  theme_bw() + # theme inspired by https://hrbrmstr.github.io/hrbrthemes/
  theme(panel.border = element_blank(), axis.ticks = element_blank())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;learning-more&#34;&gt;Learning more&lt;/h3&gt;

&lt;p&gt;For analysis examples that go into a little more depth, check out the
&lt;a href=&#34;https://ropensci.github.io/patentsview/articles/citation-networks.html&#34;&gt;data applications
vignettes&lt;/a&gt;
on the package&amp;rsquo;s website. If you&amp;rsquo;re just interested in &lt;code&gt;search_pv()&lt;/code&gt;,
there are
&lt;a href=&#34;https://ropensci.github.io/patentsview/articles/examples.html&#34;&gt;examples&lt;/a&gt;
on the site for that as well. To contribute to the package or report an
issue, check out the &lt;a href=&#34;https://github.com/ropensci/patentsview/issues&#34;&gt;issues page on
GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;d like to thank the package&amp;rsquo;s two reviewers, &lt;a href=&#34;https://github.com/poldham&#34;&gt;Paul
Oldham&lt;/a&gt; and &lt;a href=&#34;http://blog.haunschmid.name/&#34;&gt;Verena
Haunschmid&lt;/a&gt;, for taking the time to review
the package and providing helpful feedback. I&amp;rsquo;d also like to thank
&lt;a href=&#34;http://www.masalmon.eu/&#34;&gt;Maëlle Salmon&lt;/a&gt; for shepherding the package
along the rOpenSci review process, as well &lt;a href=&#34;https://scottchamberlain.info/&#34;&gt;Scott
Chamberlain&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/stefaniebutland&#34;&gt;Stefanie
Butland&lt;/a&gt; for their miscellaneous
help.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;This is both good and bad, as there are errors in the disambiguation. The algorithm that is responsible for the disambiguation was created by the winner of the &lt;a href=&#34;http://www.patentsview.org/workshop/&#34;&gt;PatentsView Inventor Disambiguation Technical Workshop&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;These two parameters end up getting translated into a MySQL query by the API&amp;rsquo;s server, which then gets sent to a back-end database. &lt;code&gt;query&lt;/code&gt; and &lt;code&gt;fields&lt;/code&gt; are used to create the query&amp;rsquo;s &lt;code&gt;WHERE&lt;/code&gt; and &lt;code&gt;SELECT&lt;/code&gt; clauses, respectively.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;There is a slightly more in-depth definition that says that these are patents &amp;ldquo;related to the (logical) separation of traffic/(sub-) networks to achieve protection.&amp;rdquo;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>rOpenSci Software Review: Always Improving</title>
      <link>https://ropensci.org/blog/2017/09/11/software-review-update/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/11/software-review-update/</guid>
      <description>
        
        

&lt;p&gt;The R package ecosystem now contains more than 10K packages, and several flagship packages belong under the rOpenSci suite. Some of these are: &lt;a href=&#34;https://github.com/ropensci/magick&#34;&gt;magick&lt;/a&gt; for image manipulation, &lt;a href=&#34;https://github.com/ropensci/plotly&#34;&gt;plotly&lt;/a&gt; for interactive plots, and &lt;a href=&#34;https://github.com/ropensci/git2r&#34;&gt;git2r&lt;/a&gt; for interacting with &lt;code&gt;git&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;rOpenSci is a community of people making software to facilitate open and reproducible science/research. While the rOpenSci team continues to develop and maintain core infrastructure packages, an increasing number of packages in our suite are contributed by members of the extended R community.&lt;/p&gt;

&lt;p&gt;In the early days we accepted contributions to our suite without any formal process for submission or acceptance. When someone wanted to contribute software to our collection, and we could envision scientific applications, we just moved it aboard. But as our community and codebase grew, we began formalizing standards and processes to control quality. This is what became our peer review process.  You can read more about it in our recent &lt;a href=&#34;https://www.numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As our submissions have grown over the past couple of years, our standards around peer review have also changed and continue to evolve in response to changing community needs and updates to the R development infrastructure.&lt;/p&gt;

&lt;p&gt;Although a large number of packages submitted to CRAN could also be part of rOpenSci, our submissions are limited to packages that fit our mission and are able to pass a stringent and time intensive review process.&lt;/p&gt;

&lt;p&gt;Here, we summarize some of the more important changes to peer review at rOpenSci over the past year.  The most recent information can always be found at &lt;a href=&#34;https://onboarding.ropensci.org/&#34;&gt;https://onboarding.ropensci.org/&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;we-ve-expanded-our-scope&#34;&gt;We&amp;rsquo;ve Expanded Our Scope&lt;/h3&gt;

&lt;p&gt;Our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;Aims and Scope&lt;/a&gt; document what types of packages we accept from community contributors. The scope emerges from three main guidelines. First, we accept packages that fit our mission of enabling open and reproducible research. Second, we only accept packages that we feel our editors and community of reviewers are competent to review. Third, we accept packages for which we can reasonably endorse as improving on existing solutions.  In practice, we don&amp;rsquo;t accept  general packages. That&amp;rsquo;s why, for instance, our &amp;ldquo;data munging&amp;rdquo; category only applies to packages designed to work with specific scientific data types.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve refined our focal areas from&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data retrieval&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data visualization&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data deposition&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data munging&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reproducibility&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data retrieval&lt;/strong&gt; - packages for accessing and downloading data from online sources with scientific applications&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data deposition&lt;/strong&gt; - packages that support deposition of data into research repositories, including data formatting and metadata generation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data munging&lt;/strong&gt; - packages for processing data from formats mentioned above&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data extraction&lt;/strong&gt; - packages that aid in retrieving data from unstructured sources such as text, images and PDFs, as well as parsing scientific data types and outputs from scientific equipment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;database access&lt;/strong&gt; - bindings and wrappers for generic database APIs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reproducibility&lt;/strong&gt; - tools that facilitate reproducible research&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;geospatial data&lt;/strong&gt; - accessing geospatial data, manipulating geospatial data, and converting between geospatial data formats&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;text analysis&lt;/strong&gt; (pilot) - we are piloting a sub-specialty area for text analysis which includes implementation of statistical/ML methods for analyzing or extracting text data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You will note that we&amp;rsquo;ve removed data visualization. We&amp;rsquo;ve had some truly excellent data visualization packages come aboard, starting with &lt;a href=&#34;https://github.com/ropensci/plotly&#34;&gt;plotly&lt;/a&gt;.  But since then we&amp;rsquo;ve found data visualization is too general a field for us to confidently evaluate, and at this point have dropped it from our main categories.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve also added geospatial and text analysis as areas where we accept packages that might seem more general or methods-y than we would otherwise.  These are areas where we&amp;rsquo;ve built, among our staff and reviewers, topic-specific expertise.&lt;/p&gt;

&lt;p&gt;Given that we accept packages that improve on existing solutions, in practice we generally avoid accepting more than one package of similar scope. We&amp;rsquo;ve also added &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#package-overlap&#34;&gt;clarifying language&lt;/a&gt; about what this entails and how we define overlap with other packages.&lt;/p&gt;

&lt;p&gt;We now strongly encourage &lt;a href=&#34;https://github.com/ropensci/onboarding/issues?q=is%3Aissue+label%3A0%2Fpresubmission&#34;&gt;pre-submission inquiries&lt;/a&gt; to quickly assess whether the package falls into scope. Some of these lead to suggesting the person submit their package, while others are determined out-of-scope. This reduces effort on all sides for packages that be out-of-scope. Many authors do this prior to completing their package so they can decide whether to tailor their development process to rOpenSci.&lt;/p&gt;

&lt;p&gt;To see examples of what has recently been determined to be out-of-scope, see the &lt;a href=&#34;https://github.com/ropensci/onboarding/issues?q=is%3Aissue+is%3Aclosed+label%3Aout-of-scope&#34;&gt;out-of-scope label&lt;/a&gt; in the onboarding repository.&lt;/p&gt;

&lt;p&gt;As always, we&amp;rsquo;d like to emphasize that even when packages are out-of-scope, we&amp;rsquo;re very grateful that authors consider an rOpenSci submission!&lt;/p&gt;

&lt;h3 id=&#34;standards-changes&#34;&gt;Standards changes&lt;/h3&gt;

&lt;p&gt;Our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/packaging_guide.md&#34;&gt;packaging guide&lt;/a&gt; contains both recommended and required best practices. They evolve continually as our community reaches consensus on best practices that we want to encourage and standardize. Here are some of the changes we&amp;rsquo;ve incorporated in the past year.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We now encourage using a &lt;code&gt;object_verb()&lt;/code&gt; function naming scheme to avoid namespace conflicts.&lt;/li&gt;
&lt;li&gt;We now encourage functions to facilitate piping workflows if possible. We don&amp;rsquo;t have an official stance on using pipes in a package.&lt;/li&gt;
&lt;li&gt;We&amp;rsquo;ve clarified and filled out README recommendations&lt;/li&gt;
&lt;li&gt;Documentation: we now recommend inclusion of a package level manual file, and at least one vignette.&lt;/li&gt;
&lt;li&gt;Testing: we clarify that packages should pass &lt;code&gt;devtools::check()&lt;/code&gt; on all major platforms, that each package should have a test suite that covers all major functionality.&lt;/li&gt;
&lt;li&gt;Continuous integration (CI): we now require that packages with compiled code need to run continuous integration on all major platforms; integrate reporting of test coverage; include README badges of CI and coverage.&lt;/li&gt;
&lt;li&gt;We&amp;rsquo;ve clarified our recommended scaffolding suggestions around XML to be more nuanced. Briefly, we recommend the &lt;code&gt;xml2&lt;/code&gt; package in general, but &lt;code&gt;XML&lt;/code&gt; package may be needed in some cases.&lt;/li&gt;
&lt;li&gt;We added a section on CRAN gotchas to help package maintainers avoid common pitfalls encountered during CRAN submission.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Standards changes often take place because we find that both editors and reviewers are making the same recommendations on multiple packages.  Other requirements are added as good practices become accessible to the broader community. For instance, CI and code coverage reporting have gone from recommended to required as the tooling and documentation/tutorials for these have made them more accessible.&lt;/p&gt;

&lt;h3 id=&#34;process-changes&#34;&gt;Process changes&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Editors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the pace of package submissions increases, we&amp;rsquo;ve expanded our editorial team to keep up. &lt;a href=&#34;http://www.masalmon.eu/&#34;&gt;Maëlle Salmon&lt;/a&gt; joined us in February, bringing our &lt;a href=&#34;https://github.com/ropensci/onboarding#-editors-and-reviewers&#34;&gt;team to four&lt;/a&gt;. With four, we need to be more coordinated, so we&amp;rsquo;ve moved to a system of a rotating editor-in-chief, who makes decisions about scope, assigns handling editors, and brings up edge cases for discussion with the whole team.&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Welcome &lt;a href=&#34;https://twitter.com/ma_salmon&#34;&gt;@ma_salmon&lt;/a&gt; to our editorial team for open peer review of &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&#34;&gt;#rstats&lt;/a&gt; software &lt;a href=&#34;https://t.co/KsL0SF1b6K&#34;&gt;https://t.co/KsL0SF1b6K&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;&lt;/p&gt;&amp;mdash; rOpenSci (@rOpenSci) &lt;a href=&#34;https://twitter.com/rOpenSci/status/832228045587099649&#34;&gt;February 16, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The process our editors follow is summarized in our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/editors_guide.md&#34;&gt;editors&amp;rsquo; guide&lt;/a&gt;, which will help bring editors up to speed when we further expand our team.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As submissions increase, the entire process benefits more from automation. Right now most steps of the review system are manual - we aim to automate as much as possible. Here&amp;rsquo;s a few things we&amp;rsquo;re doing or planning on:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;With every package submission, we run &lt;a href=&#34;https://github.com/MangoTheCat/goodpractice&#34;&gt;goodpractice&lt;/a&gt; on the package to highlight common issues. We do this manually right now, but we&amp;rsquo;re working on an automated system (aka, bot) for automatically running &lt;code&gt;goodpractice&lt;/code&gt; on submissions and reporting back to the issue. Other rOpenSci specific checks, e.g., checking rOpenSci policies, are likely to be added in to this system.&lt;/li&gt;
&lt;li&gt;Reminders: Some readers that have reviewed for rOpenSci may remember the bot that would remind you to get your review in. We&amp;rsquo;ve disabled it for now - but will likely bring it back online soon. Right now, editors do these reminders manually.&lt;/li&gt;
&lt;li&gt;On approval, packages go through a number of housekeeping steps to ensure a smooth transfer. Eventually we&amp;rsquo;d like to automate this process.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Other changes&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://joss.theoj.org/&#34;&gt;JOSS&lt;/a&gt; harmonization/co-submission: For authors wishing to submit their software papers to the Journal of Open Source Software after acceptance, we have also begun streamlining the process. Editors check to make sure that the paper clearly states the scientific application, includes a separate &lt;code&gt;.bib&lt;/code&gt; file and that the accepted version of the software is deposited at Zenodo or Figshare with a DOI. Having these steps completed allows for a fast track acceptance at JOSS.&lt;/li&gt;
&lt;li&gt;Reviewer template and guide: We now have a &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/reviewer_template.md&#34;&gt;reviewer template&lt;/a&gt; - making reviews more standardized, and helping reviewers know what to look for. In addition, we have an updated reviewer guide that gives high level guidance, as well as specific things to look for, tools to use, and examples of good reviews. In addition, the guide gives guidance on how to submit reviews.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Badges: We now have badges for rOpenSci review. The badges show whether a package is under review or has been approved. Packages that are undergoing review or have been approved can put this badge in their README.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ropensci/onboarding/issues/86&#34;&gt;&lt;img src=&#34;http://badges.ropensci.org/86_status.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/ropensci/onboarding/issues/116&#34;&gt;&lt;img src=&#34;http://badges.ropensci.org/116_status.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Get in touch with us on Twitter (&lt;a href=&#34;https://twitter.com/ropensci&#34;&gt;@ropensci&lt;/a&gt;, or in the comments) if you have any questions or thoughts about our software review policies, scope, or processes.&lt;/p&gt;

&lt;p&gt;To find out more about our software review process join us on the next &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/31/comm-call-v14&#34;&gt;rOpenSci Community Call&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We hope to see you soon in the onboarding repository as a &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/new&#34;&gt;submitter&lt;/a&gt; or as a &lt;a href=&#34;https://ropensci.org/onboarding/&#34;&gt;reviewer&lt;/a&gt;!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Experiences as a first time rOpenSci package reviewer</title>
      <link>https://ropensci.org/blog/2017/09/08/first-review-experiences/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/08/first-review-experiences/</guid>
      <description>
        
        

&lt;p&gt;It all started January 26&lt;sup&gt;th&lt;/sup&gt; this year when I signed up to volunteer as
a reviewer for R packages submitted to rOpenSci. My main motivation for
wanting to volunteer was to learn something new and to
contribute to the R open source community. If you are wondering why the
people behind rOpenSci are doing this, you can read &lt;a href=&#34;https://www.numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science/&#34;&gt;How rOpenSci uses Code Review to Promote Reproducible Science&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Three months later I was contacted by &lt;a href=&#34;https://twitter.com/masalmon&#34;&gt;Maelle Salmon&lt;/a&gt; asking whether I was interested in
reviewing the R package &lt;a href=&#34;https://github.com/ropensci/patentsview&#34;&gt;&lt;code&gt;patentsview&lt;/code&gt;&lt;/a&gt; for rOpenSci. And yes, I
was! To be honest I was a little bit thrilled.&lt;/p&gt;

&lt;p&gt;The packages are submitted for review to rOpenSci via an issue to their
GitHub repository and also the reviews happen there. So you can check out
&lt;a href=&#34;https://github.com/ropensci/onboarding/issues&#34;&gt;all previous package submissions and reviews&lt;/a&gt;.
With all the information you
get from rOpenSci and also the help from the editor it is straightforward
to do the package review. Before I started I read the
reviewer guides (links below) and checked out a few of the existing
reviews. I installed the package &lt;code&gt;patentsview&lt;/code&gt; from GitHub and also
downloaded the source code so I could check out how it was implemented.&lt;/p&gt;

&lt;p&gt;I started by testing core functionality of the package by
running the examples that were mentioned in the README of the
package. I think this is a good
starting point because you get a feeling of what the author wants to
achieve with the package. Later on I came up with my
own queries (side note: this R package interacts with an API from which
you can query patents). During the process I used to switch between
writing queries like a normal user of the package
would do and checking the code. When I saw something in the code that
wasn&amp;rsquo;t quite clear to me or looked wrong I went back to writing new
queries to check whether the behavior of the methods was as expected.&lt;/p&gt;

&lt;p&gt;With this approach I was able to give feedback to the package author
which led to the inclusion of an additional unit test, a helper function
that makes the package easier to use, clarification of an error message
and an improved documentation. You can find the review I did &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/112&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are several R packages that helped me get started with my review,
e.g. &lt;a href=&#34;https://github.com/hadley/devtools&#34;&gt;&lt;code&gt;devtools&lt;/code&gt;&lt;/a&gt; and
&lt;a href=&#34;https://github.com/MangoTheCat/goodpractice&#34;&gt;&lt;code&gt;goodpractice&lt;/code&gt;&lt;/a&gt;. These
packages can also help you when you start writing your own packages. An
example for a very useful method is &lt;code&gt;devtools::spell_check()&lt;/code&gt;, which
performs a spell check on the package description and on manual pages.
At the beginning I had an issue with &lt;code&gt;goodpractice::gp()&lt;/code&gt; but Maelle Salmon
(the editor) helped me resolve it.&lt;/p&gt;

&lt;p&gt;In the rest of this article you can read what I gained personally from doing a
review.&lt;/p&gt;

&lt;h3 id=&#34;contributing-to-the-open-source-community&#34;&gt;Contributing to the open source community&lt;/h3&gt;

&lt;p&gt;When people think about contributing to the open source community, the
first thought is about creating a new R package or contributing to one
of the major packages out there. But not everyone has the resources
(e.g. time) to do so. You also don&amp;rsquo;t have awesome ideas every other day
which can immediately be implemented into new R packages to be used by
the community. Besides contributing with code there are also lots of
other things than can be useful for other R users, for example writing
blog posts about problems you solved, speaking at meetups or reviewing
code to help improve it. What I like much about reviewing code is that
people see things differently and have other experiences. As a reviewer,
you see a new package from the user&amp;rsquo;s perspective which can be hard for
the programmer themselves. Having someone else
review your code helps finding things that are missing because they seem
obvious to the package author or detect code pieces that require more
testing. I had a great feeling when I finished the review, since I had
helped improve an already amazing R package a little bit more.&lt;/p&gt;

&lt;h3 id=&#34;reviewing-helps-improve-your-own-coding-style&#34;&gt;Reviewing helps improve your own coding style&lt;/h3&gt;

&lt;p&gt;When I write R code I usually try to do it in the best way possible.
&lt;a href=&#34;https://google.github.io/styleguide/Rguide.xml&#34;&gt;Google&amp;rsquo;s R Style Guide&lt;/a&gt;
is a good start to get used to coding best practice in R and I also
enjoyed reading &lt;a href=&#34;https://github.com/timoxley/best-practices&#34;&gt;Programming Best Practices
Tidbits&lt;/a&gt;. So normally
when I think some piece of code can be improved (with respect to speed,
readability or memory usage) I check online whether I can find a
better solution. Often you just don&amp;rsquo;t think something can be
improved because you always did it in a certain way or the last time you
checked there was no better solution. This is when it helps to follow
other people&amp;rsquo;s code. I do this by reading their blogs, following many R
users on Twitter and checking their GitHub account. Reviewing an R
package also helped me a great deal with getting new ideas because I
checked each function a lot more carefully than when I read blog posts.
In my opinion, good code does not only use the best package for each
problem but also the small details are well implemented. One thing I
used to do wrong for a long time was filling of data.frames until I
found a better (much faster)
&lt;a href=&#34;https://stackoverflow.com/a/29419402&#34;&gt;solution on stackoverflow&lt;/a&gt;.
And with respect to this you
can learn a lot from someone else&amp;rsquo;s code. What I found really cool in
the package I reviewed was the usage of small helper functions (see
&lt;a href=&#34;https://github.com/ropensci/patentsview/blob/c03e1ab2537873d7a9b76025b0072953efb475c1/R/utils.R&#34;&gt;utils.R&lt;/a&gt;).
Functions like &lt;code&gt;paste0_stop&lt;/code&gt; and &lt;code&gt;paste0_message&lt;/code&gt; make the rest of the
code a lot easier to read.&lt;/p&gt;

&lt;h3 id=&#34;good-start-for-writing-your-own-packages&#34;&gt;Good start for writing your own packages&lt;/h3&gt;

&lt;p&gt;When reviewing an R package, you check the code like a really observant
user. I noticed many things that you usually don&amp;rsquo;t care about when using
an R package, like comments, how helpful the documentation and the
examples are, and also how well unit tests cover the code. I think that
reviewing a few good packages can prepare you very well for writing your
own packages.&lt;/p&gt;

&lt;h3 id=&#34;do-you-want-to-contribute-to-ropensci-yourself&#34;&gt;Do you want to contribute to rOpenSci yourself?&lt;/h3&gt;

&lt;p&gt;If I motivated you to become an rOpenSci reviewer, please sign up! Here
is a list of useful things if you want to become an rOpenSci reviewer
like me.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ropensci.org/onboarding/&#34;&gt;Form to sign up (just takes a minute)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://onboarding.ropensci.org/&#34;&gt;Information for reviewers&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://mozillascience.github.io/codeReview/review.html&#34;&gt;Mozilla reviewing guide&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;While writing this blog post I found a nice article about &lt;a href=&#34;http://www.tidyverse.org/articles/2017/08/contributing/&#34;&gt;contributing
to the tidyverse&lt;/a&gt; which is
mostly also applicable to other R packages in my opinion.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are generally interested in either submitting or reviewing an R package, I would like to invite you to the &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/31/comm-call-v14&#34;&gt; Community Call on rOpenSci software review and onboarding&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>How rOpenSci uses Code Review to Promote Reproducible Science</title>
      <link>https://ropensci.org/blog/2017/09/01/nf-softwarereview/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/01/nf-softwarereview/</guid>
      <description>
        
        

&lt;p&gt;At rOpenSci, we create and curate software to help scientists with the data life cycle. These tools access, download, manage, and archive scientific data in open, reproducible ways. Early on, we realized this could only be a community effort. The variety of scientific data and workflows could only be tackled by drawing on contributions of scientists with field-specific expertise.&lt;/p&gt;

&lt;p&gt;With the community approach came challenges. &lt;strong&gt;How could we ensure the quality of code written by scientists without formal training in software development practices? How could we drive adoption of best practices among our contributors? How could we create a community that would support each other in this work?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We have had great success addressing these challenges via the &lt;em&gt;peer review&lt;/em&gt;.&lt;/strong&gt; We draw elements from a process familiar to our target community – &lt;em&gt;academic peer review&lt;/em&gt; – and a practice from the software development world – &lt;em&gt;production code review&lt;/em&gt; – to create a system that fosters software quality, ongoing education, and community development.&lt;/p&gt;

&lt;h3 id=&#34;an-open-review-process&#34;&gt;An Open Review Process&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Production software review&lt;/strong&gt; occurs within software development teams, open source or not. Contributions to a software project are reviewed by one or more other team members before incorporation into project source code. Contributions are typically small patches, and review serves as a check on quality, as well as an opportunity for training in team standards.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In academic peer review&lt;/strong&gt;, external reviewers critique a complete product – usually a manuscript – with a very broad mandate to address any areas they see as deficient. Academic review is often anonymous and passing through it gives the product, and the author, a public mark of validation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We blend these approaches.&lt;/strong&gt; In our process, authors submit complete R packages to rOpenSci. Editors check that packages fit into our project’s scope, run a series of automated tests to ensure a baseline of code quality and completeness, and then assign two independent reviewers. Reviewers comment on usability, quality, and style of software code as well as documentation. Authors make changes in response, and once reviewers are satisfied with the updates, the package receives a badge of approval and joins our suite.&lt;/p&gt;

&lt;p&gt;This process is quite iterative. After reviewers post a first round of extensive reviews, authors and reviewers chat in an informal back-and-forth, only lightly moderated by an editor. This lets both reviewers and authors pose questions of each other and explain differences of opinion. It can proceed at a much faster pace than typical academic review correspondence. We use the GitHub issues system for this conversation, and responses take minutes to days, rather than weeks to months.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The exchange is also open and public&lt;/strong&gt;. Authors, reviewers, and editors all know each other’s identities. The broader community can view or even participate in the conversation as it happens. This provides an incentive to be thorough and provide non-adversarial, constructive reviews. Both authors and reviewers report that they enjoy and learn more from this open and direct exchange. It also has the benefit of building community. Participants have the opportunity to meaningfully network with new peers, and new collaborations have emerged via ideas spawned during the review process.&lt;/p&gt;

&lt;p&gt;We are aware that open systems can have drawbacks. For instance, in traditional academic review, double-blind peer review &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0169534707002704&#34;&gt;can increase representation of female authors&lt;/a&gt;, suggesting bias in non-blind reviews. It is also possible reviewers are less critical in open review. However, we posit that the openness of the review conversation provides a check on review quality and bias; it’s harder to inject unsupported or subjective comments in public and without the cover of anonymity. Ultimately, we believe the ability of authors and reviewers to have direct but public communication improves quality and fairness.&lt;/p&gt;

&lt;h3 id=&#34;guidance-and-standards&#34;&gt;Guidance and Standards&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;rOpenSci provides guidance on reviewing.&lt;/strong&gt; This falls into two main categories: &lt;strong&gt;high-level best practices&lt;/strong&gt; and &lt;strong&gt;low-level standards&lt;/strong&gt;. High-level best practices are general and broadly applicable across languages and applications. These are practices such as “Write re-usable functions rather than repeating the same code,” “Test edge cases,” or “Write documentation for all of your functions.” Because of their general nature, these can be drawn from other sources and not developed from scratch. Our best practices are based on guidance originally developed by &lt;a href=&#34;https://mozillascience.github.io/codeReview/intro.html&#34;&gt;Mozilla Science Lab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Low-level standards are specific to a language (in our case, R), applications (data interfaces) and user base (researchers). These include specific items such as naming conventions for functions, best choices of dependencies for certain tasks, and adherence to a code style guide. We have an extensive set of standards for our reviewers to check. These change over time as the R software ecosystem evolves, best practices change, and tooling and educational resources make new methods available to developers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Our standards also change based on feedback from reviewers.&lt;/strong&gt; We adopt into our standards suggestions that emerge in multiple reviewers across different packages. Many of these, we’ve found, have to do with with the ease-of-use and consistency of software APIs, and the type and location of information in documentation that make it easiest to find. This highlights one of the advantages of external reviewers – they can provide a fresh perspective on usability, as well as test software under different use-cases than imagined by the author.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As our standards have become more extensive, we have come to rely more on automated tools.&lt;/strong&gt; The R ecosystem, like most languages, has a suite of tools for code linting, function testing, static code analysis and continuous integration. We require authors to use these, and editors run submissions through a suite of tests prior to sending them for review. This frees reviewers from the burden of low-level tasks to focus on high-level critiques where they can add the most value.&lt;/p&gt;

&lt;h3 id=&#34;the-reviewer-community&#34;&gt;The Reviewer Community&lt;/h3&gt;

&lt;p&gt;One of the core challenges and rewards of our work has been developing a community of reviewers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reviewing is a high-skill activity.&lt;/strong&gt; Reviewers need expertise in the programming methods used in a software package and also the scientific field of its application. (“Find me someone who knows sensory ecology and sparse data structures!”) They need good communications skills and the time and willingness to volunteer. Thankfully, the open-science and open-source worlds are filled with generous, expert people. We have been able to expand our reviewer pool as the pace of submissions and the domains of their applications have grown.&lt;/p&gt;

&lt;p&gt;Developing the reviewer pool requires constant recruitment. Our editors actively and broadly engage with developer and research communities to find new reviewers. We recruit from authors of previous submissions, co-workers and colleagues, at conferences, through our other collaborative work and on social media. In the open-source software ecosystem, one can often identify people with particular expertise by looking at their published software or contribution to other projects, and we often will cold-email potential reviewers whose published work suggests they would be a good match for a submission.&lt;/p&gt;

&lt;p&gt;We cultivate our reviewer pool as well as expand it. We bring back reviewers so that they may develop reviewing as a skill, but not so often as to overburden them. We provide guidance and feedback to new recruits. When assigning reviewers to a submission, we aim to pair experienced reviewers with new ones, or reviewers with expertise on a package’s programming methods with those experienced in its field of application. &lt;strong&gt;These reviewers learn from each other, and diversity in perspectives is an advantage&lt;/strong&gt;; less experienced developers often provide insight that more experienced ones do not on software usability, API design, and documentation. More experienced developers will more often identify inefficiencies in code, pitfalls due to edge-cases, or suggest alternate implementation approaches.&lt;/p&gt;

&lt;h3 id=&#34;expanding-peer-review-for-code&#34;&gt;Expanding Peer Review for Code&lt;/h3&gt;

&lt;p&gt;Code review has been one of rOpenSci’s best initiatives. We build software, build skills, and build community, and the peer review process has been a major catalyst for all three. It has made both the software we develop internally and that we accept from outside contributors more reliable, usable, and maintainable. So &lt;strong&gt;we are working to promote open peer review of code by more organizations&lt;/strong&gt; working with scientific software. We helped launch &lt;a href=&#34;http://joss.theoj.org/&#34;&gt;The Journal of Open Source Software&lt;/a&gt;, which uses a version of our review process to provide a developer-friendly publication venue. JOSS’s success has led to a spin-off, the &lt;a href=&#34;http://jose.theoj.org/&#34;&gt;Journal of Open Source Education&lt;/a&gt;, which uses an open, code-review-like processes to provide feedback on curricula and educational materials. We are also developing a pilot program where software papers submitted to a partner academic journal receive a badge for going through rOpenSci review. We are encouraged by other review initiatives like &lt;a href=&#34;https://rescience.github.io/&#34;&gt;ReScience&lt;/a&gt; and &lt;a href=&#34;https://programminghistorian.org/&#34;&gt;The Programming Historian&lt;/a&gt;. &lt;a href=&#34;https://www.bioconductor.org/&#34;&gt;BioConductor&lt;/a&gt;’s code reviews, which predate ours by several years, recently switched to an open model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If your organization is developing or curating scientific code&lt;/strong&gt;, we believe code review, implemented well, can be a great benefit. It can take considerable effort to begin, but &lt;strong&gt;here are some of the key lessons we’ve learned that can help:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Develop standards and guidelines&lt;/strong&gt; for your authors and reviewers to use. Borrow these freely from other projects (feel free to use ours), and modify them iteratively as you go.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use automated tools&lt;/strong&gt; such as code linters, test suites, and test coverage measures to reduce burden on authors, reviewers, and editors as much as possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a clear scope.&lt;/strong&gt; Spell out to yourselves and potential contributors what your project will accept, and why. This will save a lot of confusion and effort in the future.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build a community&lt;/strong&gt; through incentives of networking, opportunities to learn, and kindness.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;rOpenSci is eager to work with other groups interested in developing similar review processes&lt;/strong&gt;, especially if you are interested in reviewing and curating scientific software in languages other than R or beyond our scope of supporting the data life cycle. Software that implements statistical algorithms, for instance, is an area ripe for open peer review of code. Please &lt;a href=&#34;https://ropensci.org/contact.html&#34;&gt;get in touch&lt;/a&gt; if you have questions or wish to co-pilot a review system for your project.&lt;/p&gt;

&lt;p&gt;And of course, if you want to review, we’re always looking for volunteers. Sign up at &lt;a href=&#34;https://ropensci.org/onboarding&#34;&gt;https://ropensci.org/onboarding&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;You can support rOpenSci by &lt;a href=&#34;https://www.numfocus.org/community/donate/&#34;&gt;becoming a NumFOCUS member&lt;/a&gt; or making a &lt;a href=&#34;https://www.numfocus.org/open-source-projects/&#34;&gt;donation to the project&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Community Call - rOpenSci Software Review and Onboarding</title>
      <link>https://ropensci.org/blog/2017/08/31/comm-call-v14/</link>
      <pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/08/31/comm-call-v14/</guid>
      <description>
        
        

&lt;p&gt;Are you thinking about submitting a package to rOpenSci&amp;rsquo;s open peer software review? Considering volunteering to review for the first time? Maybe you&amp;rsquo;re an experienced package author or reviewer and have ideas about how we can improve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Join our &lt;a href=&#34;https://github.com/ropensci/commcalls/issues/15&#34;&gt;Community Call on Wednesday, September 13th&lt;/a&gt;&lt;/strong&gt;. We want to get your feedback and we&amp;rsquo;d love to answer your questions!&lt;/p&gt;

&lt;h3 id=&#34;agenda&#34;&gt;Agenda&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Welcome (Stefanie Butland, rOpenSci Community Manager, 5 min)&lt;/li&gt;
&lt;li&gt;guest: Noam Ross, editor (15 min)
Noam will give an overview of the rOpenSci software review and onboarding, highlighting the role editors play and how decisions are made about policies and changes to the process.&lt;/li&gt;
&lt;li&gt;guest: Andee Kaplan, reviewer (15 min)
Andee will give her perspective as a package reviewer, sharing specifics about her workflow and her motivation for doing this.&lt;/li&gt;
&lt;li&gt;Q &amp;amp; A (25 min, moderated by Noam Ross)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;speaker-bios&#34;&gt;Speaker bios&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Andee Kaplan&lt;/strong&gt; is a Postdoctoral Fellow at Duke University. She is a recent PhD graduate from the Iowa State University Department of Statistics, where she learned a lot about R and reproducibility by developing a class on data stewardship for Agronomists. Andee has reviewed multiple (two!) packages for rOpenSci, &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/107&#34;&gt;&lt;code&gt;iheatmapr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/58&#34;&gt;&lt;code&gt;getlandsat&lt;/code&gt;&lt;/a&gt;, and hopes to one day be on the receiving end of the review process.&lt;/p&gt;

&lt;p&gt;Andee on &lt;a href=&#34;https://github.com/andeek&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/andeekaplan&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Noam Ross&lt;/strong&gt; is one of rOpenSci&amp;rsquo;s four editors for software peer review. Noam is a Senior Research Scientist at EcoHealth Alliance in New York, specializing in mathematical modeling of disease outbreaks, as well as training and standards for data science and reproducibility. Noam earned his Ph.D. in Ecology from the University of California-Davis, where he founded the Davis R Users&amp;rsquo; Group.&lt;/p&gt;

&lt;p&gt;Noam on &lt;a href=&#34;https://github.com/noamross&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/noamross&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;resources&#34;&gt;Resources&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science/&#34;&gt;How rOpenSci uses Code Review to Promote Reproducible Science&lt;/a&gt;; blog post Aug 11, 2017&lt;/li&gt;
&lt;li&gt;The what, why and how of &lt;a href=&#34;http://onboarding.ropensci.org/&#34;&gt;rOpenSci open peer review and onboarding&lt;/a&gt;; guidelines&lt;/li&gt;
&lt;li&gt;rOpenSci &lt;a href=&#34;https://github.com/ropensci/onboarding/issues&#34;&gt;software reviews in progress and completed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/packages/&#34;&gt;rOpenSci onboarded packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read on &lt;a href=&#34;https://ropensci.org/blog/&#34;&gt;our blog&lt;/a&gt; one of ten guest posts (to date) by authors of onboarded packages&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/22/first-package-review&#34;&gt;So you (don&amp;rsquo;t) think you can review a package&lt;/a&gt;; guest blog post by first-time reviewer Mara Averick, Aug 22, 2017&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/blog/blog/2016/03/28/software-review&#34;&gt;Onboarding at rOpenSci: A Year in Reviews&lt;/a&gt;; blog post Mar 28, 2016&lt;/li&gt;
&lt;li&gt;Soon after the Community Call, we&amp;rsquo;ll post the &lt;a href=&#34;https://vimeo.com/ropensci/videos&#34;&gt;video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>FedData - Getting assorted geospatial data into R</title>
      <link>https://ropensci.org/technotes/2017/08/24/feddata-release/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/08/24/feddata-release/</guid>
      <description>
        
        

&lt;p&gt;The package &lt;a href=&#34;https://github.com/ropensci/FedData&#34;&gt;&lt;code&gt;FedData&lt;/code&gt;&lt;/a&gt; has gone through software review and is now part of &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt;. &lt;code&gt;FedData&lt;/code&gt; includes functions to automate downloading geospatial data available from several federated data sources (mainly sources maintained by the US Federal government).&lt;/p&gt;

&lt;p&gt;Currently, the package enables extraction from six datasets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;http://ned.usgs.gov&#34;&gt;National Elevation Dataset (NED)&lt;/a&gt; digital elevation models (1 and &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; arc-second; USGS)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://nhd.usgs.gov&#34;&gt;National Hydrography Dataset (NHD)&lt;/a&gt; (USGS)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://websoilsurvey.sc.egov.usda.gov/&#34;&gt;Soil Survey Geographic (SSURGO) database&lt;/a&gt; from the National Cooperative Soil Survey (NCSS), which is led by the Natural Resources Conservation Service (NRCS) under the USDA,&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn&#34;&gt;Global Historical Climatology Network (GHCN)&lt;/a&gt;, coordinated by National Climatic Data Center at NOAA,&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://daymet.ornl.gov/&#34;&gt;Daymet&lt;/a&gt; gridded estimates of daily weather parameters for North America, version 3, available from the Oak Ridge National Laboratory&amp;rsquo;s Distributed Active Archive Center (DAAC), and&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://www.ncdc.noaa.gov/data-access/paleoclimatology-data/datasets/tree-ring&#34;&gt;International Tree Ring Data Bank (ITRDB)&lt;/a&gt;, coordinated by National Climatic Data Center at NOAA.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; is designed with the large-scale geographic information system (GIS) use-case in mind: cases where the use of dynamic web-services is impractical due to the scale (spatial and/or temporal) of analysis. It functions primarily as a means of downloading tiled or otherwise spatially-defined datasets; additionally, it can preprocess those datasets by extracting data within an area of interest (AoI), defined spatially. It relies heavily on the &lt;a href=&#34;https://cran.r-project.org/package=sp&#34;&gt;&lt;code&gt;sp&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/package=raster&#34;&gt;&lt;code&gt;raster&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://cran.r-project.org/package=rgdal&#34;&gt;&lt;code&gt;rgdal&lt;/code&gt;&lt;/a&gt; packages.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Load &lt;code&gt;FedData&lt;/code&gt; and define a study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# FedData Tester
library(FedData)
library(magrittr)

# Extract data for the Village Ecodynamics Project &amp;quot;VEPIIN&amp;quot; study area:
# http://veparchaeology.org
vepPolygon &amp;lt;- polygon_from_extent(raster::extent(672800, 740000, 4102000, 4170000),
                                  proj4string = &amp;quot;+proj=utm +datum=NAD83 +zone=12&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get and plot the National Elevation Dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NED (USA ONLY)
# Returns a raster
NED &amp;lt;- get_ned(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;)
# Plot with raster::plot
raster::plot(NED)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the Daymet dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the DAYMET (North America only)
# Returns a raster
DAYMET &amp;lt;- get_daymet(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;,
               elements = c(&amp;quot;prcp&amp;quot;,&amp;quot;tmax&amp;quot;),
               years = 1980:1985)
# Plot with raster::plot
raster::plot(DAYMET$tmax$X1985.10.23)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the daily GHCN precipitation data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the daily GHCN data (GLOBAL)
# Returns a list: the first element is the spatial locations of stations,
# and the second is a list of the stations and their daily data
GHCN.prcp &amp;lt;- get_ghcn_daily(template = vepPolygon,
                            label = &amp;quot;VEPIIN&amp;quot;,
                            elements = c(&#39;prcp&#39;))
# Plot the NED again
raster::plot(NED)
# Plot the spatial locations
sp::plot(GHCN.prcp$spatial,
         pch = 1,
         add = TRUE)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend=&amp;quot;GHCN Precipitation Records&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the daily GHCN temperature data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Elements for which you require the same data
# (i.e., minimum and maximum temperature for the same days)
# can be standardized using standardize==T
GHCN.temp &amp;lt;- get_ghcn_daily(template = vepPolygon,
                            label = &amp;quot;VEPIIN&amp;quot;,
                            elements = c(&#39;tmin&#39;,&#39;tmax&#39;),
                            years = 1980:1985,
                            standardize = TRUE)
# Plot the NED again
raster::plot(NED)
# Plot the spatial locations
sp::plot(GHCN.temp$spatial,
         add = TRUE,
         pch = 1)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend = &amp;quot;GHCN Temperature Records&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the National Hydrography Dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NHD (USA ONLY)
NHD &amp;lt;- get_nhd(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;)
# Plot the NED again
raster::plot(NED)
# Plot the NHD data
NHD %&amp;gt;%
  lapply(sp::plot,
         col = &#39;black&#39;,
         add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the NRCS SSURGO data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NRCS SSURGO data (USA ONLY)
SSURGO.VEPIIN &amp;lt;- get_ssurgo(template = vepPolygon,
                     label = &amp;quot;VEPIIN&amp;quot;)
#&amp;gt; Warning: 1 parsing failure.
#&amp;gt; row # A tibble: 1 x 5 col     row     col               expected actual expected   &amp;lt;int&amp;gt;   &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; actual 1  1276 slope.r no trailing characters     .5 file # ... with 1 more variables: file &amp;lt;chr&amp;gt;
# Plot the NED again
raster::plot(NED)
# Plot the SSURGO mapunit polygons
plot(SSURGO.VEPIIN$spatial,
     lwd = 0.1,
     add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the NRCS SSURGO data for particular soil survey areas&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Or, download by Soil Survey Area names
SSURGO.areas &amp;lt;- get_ssurgo(template = c(&amp;quot;CO670&amp;quot;,&amp;quot;CO075&amp;quot;),
                           label = &amp;quot;CO_TEST&amp;quot;)

# Let&#39;s just look at spatial data for CO675
SSURGO.areas.CO675 &amp;lt;- SSURGO.areas$spatial[SSURGO.areas$spatial$AREASYMBOL==&amp;quot;CO075&amp;quot;,]

# And get the NED data under them for pretty plotting
NED.CO675 &amp;lt;- get_ned(template = SSURGO.areas.CO675,
                            label = &amp;quot;SSURGO_CO675&amp;quot;)

# Plot the SSURGO mapunit polygons, but only for CO675
plot(NED.CO675)
plot(SSURGO.areas.CO675,
     lwd = 0.1,
     add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the ITRDB chronology locations in the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the ITRDB records
ITRDB &amp;lt;- get_itrdb(template = vepPolygon,
                        label = &amp;quot;VEPIIN&amp;quot;,
                        makeSpatial = TRUE)
# Plot the NED again
raster::plot(NED)
# Map the locations of the tree ring chronologies
plot(ITRDB$metadata,
     pch = 1,
     add = TRUE)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend = &amp;quot;ITRDB chronologies&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-13-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;todo&#34;&gt;TODO&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The current CRAN version of &lt;code&gt;FedData&lt;/code&gt;, v2.4.6, will (hopefully) be the final CRAN release of &lt;code&gt;FedData&lt;/code&gt; 2. &lt;code&gt;FedData&lt;/code&gt; 3 will be released in the coming months, but some code built on &lt;code&gt;FedData&lt;/code&gt; 2 will not be compatible with FedData 3.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; was initially developed prior to widespread use of modern web mapping services and RESTful APIs by many Federal data-holders. Future releases of &lt;code&gt;FedData&lt;/code&gt; will limit data transfer by utilizing server-side geospatial and data queries. We will also implement &lt;a href=&#34;https://github.com/hadley/dplyr&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/a&gt; verbs, tidy data structures, (&lt;a href=&#34;https://github.com/tidyverse/magrittr&#34;&gt;&lt;code&gt;magrittr&lt;/code&gt;&lt;/a&gt;) piping, functional programming using &lt;a href=&#34;https://github.com/hadley/purrr&#34;&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/a&gt;, simple features for spatial data from &lt;a href=&#34;https://github.com/edzer/sfr&#34;&gt;&lt;code&gt;sf&lt;/code&gt;&lt;/a&gt;, and local data storage in OGC-compliant data formats (probably GeoJSON and NetCDF). I am also aiming for 100% testing coverage.&lt;/p&gt;

&lt;p&gt;All that being said, much of the functionality of the &lt;code&gt;FedData&lt;/code&gt; package could be spun off into more domain-specific packages. For example, ITRDB download functions could be part of the &lt;a href=&#34;https://r-forge.r-project.org/projects/dplr/&#34;&gt;&lt;code&gt;dplR&lt;/code&gt;&lt;/a&gt; dendrochronology package; concepts/functions having to do with the GHCN data integrated into &lt;a href=&#34;https://github.com/ropensci/rnoaa&#34;&gt;&lt;code&gt;rnoaa&lt;/code&gt;&lt;/a&gt;; and Daymet concepts integrated into &lt;a href=&#34;https://github.com/khufkens/daymetr&#34;&gt;&lt;code&gt;daymetr&lt;/code&gt;&lt;/a&gt;. I welcome any and all suggestions about how to improve the utility of FedData; please &lt;a href=&#34;https://github.com/ropensci/FedData/issues&#34;&gt;submit an issue&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; is a product of SKOPE (&lt;a href=&#34;http://www.openskope.org&#34;&gt;Synthesizing Knowledge of Past Environments&lt;/a&gt;) and the &lt;a href=&#34;http://veparchaeology.org/&#34;&gt;Village Ecodynamics Project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; was reviewed for &lt;a href=&#34;https://ropensci.org&#34;&gt;rOpenSci&lt;/a&gt; by &lt;a href=&#34;https://github.com/jooolia&#34;&gt;@jooolia&lt;/a&gt;, with &lt;a href=&#34;https://github.com/sckott&#34;&gt;@sckott&lt;/a&gt; as onboarding editor, and was greatly improved as a result.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
