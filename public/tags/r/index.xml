<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/r/</link>
    <description>Recent content in R on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 12 Dec 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>fulltext v0.3: x, y, and z</title>
      <link>https://ropensci.org/technotes/2017/12/12/fulltext-update/</link>
      <pubDate>Tue, 12 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/12/12/fulltext-update/</guid>
      <description>
        
        

&lt;p&gt;Text-mining - the art of answering questions by extracting patterns, data, etc. out of the published literature - is not easy. It&amp;rsquo;s made incredibly difficult because of publishers. Novels, etc. by writers are one thing - but it&amp;rsquo;s hard to swallow the fact that the vast majority of publicly funded research across the globe is published in paywall journals. That is, taxpayers pay twice for research: once for the grant to fund the work, then again to be able to read it.&lt;/p&gt;

&lt;p&gt;Text-mining use cases run from determining the change in use of words through time [ref], to XYZ.&lt;/p&gt;

&lt;h2 id=&#34;the-fulltext-package&#34;&gt;the fulltext package&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;fulltext&lt;/code&gt; is a package to help R users get published literature from the web in it&amp;rsquo;s many forms, and across thousands upon thousands of publishers.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fulltext&lt;/code&gt; tries to make the following use cases as easy as possible:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Search for articles&lt;/li&gt;
&lt;li&gt;Fetch abstracts&lt;/li&gt;
&lt;li&gt;Fetch full text articles&lt;/li&gt;
&lt;li&gt;Get links for full text articles (xml, pdf)&lt;/li&gt;
&lt;li&gt;Extract text from articles / convert formats&lt;/li&gt;
&lt;li&gt;Collect sections of articles that you actually need (e.g., titles)&lt;/li&gt;
&lt;li&gt;Download supplementary materials&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;fulltext&lt;/code&gt; organizes funvctions around use cases, then provides flexiblity to query many data sources within that use case (i.e. function). For example &lt;code&gt;fulltext::ft_search&lt;/code&gt; searches for articles - you can choose among one or more of many data sources to search, passing options to each source as needed.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-does-a-workflow-with-fulltext-look-like&#34;&gt;What does a workflow with fulltext look like?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Search for articles with &lt;code&gt;ft_search()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fetch articles with &lt;code&gt;ft_get()&lt;/code&gt; using the output of the previous step&lt;/li&gt;
&lt;li&gt;Extract sections of articles needed with &lt;code&gt;chunks()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Do some further processing with &lt;code&gt;tm&lt;/code&gt; or &lt;code&gt;?????&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Install &lt;code&gt;fulltext&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;fulltext&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/fulltext&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(fulltext)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;search-ft-search&#34;&gt;Search: ft_search&lt;/h2&gt;

&lt;p&gt;With &lt;code&gt;ft_search&lt;/code&gt; you can search&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PLOS&lt;/li&gt;
&lt;li&gt;BMC&lt;/li&gt;
&lt;li&gt;Crossref&lt;/li&gt;
&lt;li&gt;Entrez&lt;/li&gt;
&lt;li&gt;arxiv&lt;/li&gt;
&lt;li&gt;biorxiv&lt;/li&gt;
&lt;li&gt;Euro&lt;/li&gt;
&lt;li&gt;Scopus&lt;/li&gt;
&lt;li&gt;Microsoft&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- ft_search(query=&#39;ecology&#39;, from=&#39;plos&#39;)
res
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query:
##   [ecology] 
## Found:
##   [PLoS: 40969; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] 
## Returned:
##   [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After running &lt;code&gt;ft_search&lt;/code&gt; you can index to each source that you
selected in the &lt;code&gt;from&lt;/code&gt; parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res$plos
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query: [ecology] 
## Records found, returned: [40969, 10] 
## License: [CC-BY] 
##                              id
## 1  10.1371/journal.pone.0001248
## 2  10.1371/journal.pone.0059813
## 3  10.1371/journal.pone.0155019
## 4  10.1371/journal.pone.0080763
## 5  10.1371/journal.pone.0150648
## 6  10.1371/journal.pcbi.1003594
## 7  10.1371/journal.pone.0102437
## 8  10.1371/journal.pone.0175014
## 9  10.1371/journal.pone.0166559
## 10 10.1371/journal.pone.0054689
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you give many values for the &lt;code&gt;from&lt;/code&gt; parameter you&amp;rsquo;ll get many results, for
example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- ft_search(query=&#39;ecology&#39;, from=c(&#39;plos&#39;, &#39;crossref&#39;))
res$plos
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query: [ecology] 
## Records found, returned: [40969, 10] 
## License: [CC-BY] 
##                              id
## 1  10.1371/journal.pone.0001248
## 2  10.1371/journal.pone.0059813
## 3  10.1371/journal.pone.0155019
## 4  10.1371/journal.pone.0080763
## 5  10.1371/journal.pone.0150648
## 6  10.1371/journal.pcbi.1003594
## 7  10.1371/journal.pone.0102437
## 8  10.1371/journal.pone.0175014
## 9  10.1371/journal.pone.0166559
## 10 10.1371/journal.pone.0054689
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res$crossref
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query: [ecology] 
## Records found, returned: [142207, 10] 
## License: [variable, see individual records] 
##                  container.title    created  deposited
## 1                        Ecology 2006-05-03 2017-04-14
## 2                        Ecology 2006-05-03 2016-03-07
## 3                        Ecology 2006-05-03 2016-03-07
## 4                        Ecology 2006-05-03 2016-03-07
## 5                        Ecology 2006-05-03 2017-04-15
## 6                        Ecology 2006-05-03 2017-04-15
## 7                        Ecology 2006-05-09 2016-03-07
## 8                        Ecology 2017-04-26 2017-07-11
## 9  Trends in Ecology &amp;amp; Evolution 2002-07-25 2017-06-14
## 10 Journal of Industrial Ecology 2017-10-05 2017-11-14
## Variables not shown: doi (chr), indexed (chr), issn (chr), issue (chr),
##      issued (chr), license_date (chr), license_url (chr),
##      license_delay.in.days (chr), license_content.version (chr), member
##      (chr), page (chr), prefix (chr), publisher (chr), reference.count
##      (chr), score (chr), source (chr), subject (chr), title (chr), type
##      (chr), url (chr), volume (chr), author (list), link (list), archive
##      (chr), alternative.id (chr), subtitle (chr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;fetch-abstracts-ft-abstract&#34;&gt;Fetch abstracts: ft_abstract&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;PLOS&lt;/li&gt;
&lt;li&gt;Scopus&lt;/li&gt;
&lt;li&gt;Microsoft&lt;/li&gt;
&lt;li&gt;Crossref&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(res &amp;lt;- ft_search(query = &#39;biology&#39;, from = &#39;plos&#39;, limit = 10, 
   plosopts = list(fq = list(&#39;doc_type:full&#39;, &#39;-article_type:correction&#39;,
                  &#39;-article_type:viewpoints&#39;))))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query:
##   [biology] 
## Found:
##   [PLoS: 170852; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] 
## Returned:
##   [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(out &amp;lt;- ft_abstract(x = res$plos$data$id, from = &amp;quot;plos&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &amp;lt;fulltext abstracts&amp;gt;
## Found:
##   [PLOS: 84; Scopus: 0; Microsoft: 0; Crossref: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;fetch-articles-ft-get&#34;&gt;Fetch articles: ft_get&lt;/h2&gt;

&lt;p&gt;The function &lt;code&gt;ft_get&lt;/code&gt; is the workhorse for getting the namesake of the package:
full text articles.&lt;/p&gt;

&lt;p&gt;Beware that using this function can be tricky depending on where you want to
get articles from. While searching (&lt;code&gt;ft_search&lt;/code&gt;) usually doesn&amp;rsquo;t present any
barriers or stumbling blocks, &lt;code&gt;ft_get&lt;/code&gt; can get frustrating because so many
publishers paywall their articles. The combination of paywalls and their
patchwork of who gets to get through them means that we can&amp;rsquo;t easily predict
who will run into problems with Elsevier, Wiley, etc.&lt;/p&gt;

&lt;p&gt;With this version we&amp;rsquo;ve tried to bulk up the documentation as much as possible
to make jumping over these barriers as easy as possible.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ft_get()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Error in stop(&amp;quot;no &#39;ft_get&#39; method for &amp;quot;, class(x), call. = FALSE): argument &amp;quot;x&amp;quot; is missing, with no default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;get-full-text-links-ft-links&#34;&gt;Get full text links: ft_links&lt;/h2&gt;

&lt;p&gt;In case you want to sort out full text links yourself, and have those links for whatever purpose, &lt;code&gt;ft_links&lt;/code&gt; is your friend. It grabs data from PLOS, Crossref, Entrez, and BMC.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(res2 &amp;lt;- ft_search(query=&#39;ecology&#39;, from=&#39;crossref&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query:
##   [ecology] 
## Found:
##   [PLoS: 0; BMC: 0; Crossref: 142207; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] 
## Returned:
##   [PLoS: 0; BMC: 0; Crossref: 10; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(out &amp;lt;- ft_links(res2))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &amp;lt;fulltext links&amp;gt;
## [Found] 10 
## [IDs] 10.2307/1929908 10.2307/1931096 10.2307/1934287 10.2307/1943146
##      10.2307/1930158 10.2307/1928969 10.2307/1935066 10.1002/ecy.1807
##      10.1016/s0169-5347(97)89918-1 10.1111/jiec.12669 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out$crossref
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $found
## [1] 10
## 
## $ids
##  [1] &amp;quot;10.2307/1929908&amp;quot;               &amp;quot;10.2307/1931096&amp;quot;              
##  [3] &amp;quot;10.2307/1934287&amp;quot;               &amp;quot;10.2307/1943146&amp;quot;              
##  [5] &amp;quot;10.2307/1930158&amp;quot;               &amp;quot;10.2307/1928969&amp;quot;              
##  [7] &amp;quot;10.2307/1935066&amp;quot;               &amp;quot;10.1002/ecy.1807&amp;quot;             
##  [9] &amp;quot;10.1016/s0169-5347(97)89918-1&amp;quot; &amp;quot;10.1111/jiec.12669&amp;quot;           
## 
## $data
## $data$`10.2307/1929908`
##                                                                     url
## 1 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1929908
##               doi        type member
## 1 10.2307/1929908 unspecified    311
## 
## $data$`10.2307/1931096`
##                                                                    url
## 1 http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1931096
##               doi        type member
## 1 10.2307/1931096 unspecified    311
## 
## $data$`10.2307/1934287`
##                                                                    url
## 1 http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1934287
##               doi        type member
## 1 10.2307/1934287 unspecified    311
## 
## $data$`10.2307/1943146`
##                                                                    url
## 1 http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1943146
##               doi        type member
## 1 10.2307/1943146 unspecified    311
## 
## $data$`10.2307/1930158`
##                                                                     url
## 1  http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1930158
## 2 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1930158
##               doi        type member
## 1 10.2307/1930158         pdf    311
## 2 10.2307/1930158 unspecified    311
## 
## $data$`10.2307/1928969`
##                                                                     url
## 1  http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1928969
## 2 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1928969
##               doi        type member
## 1 10.2307/1928969         pdf    311
## 2 10.2307/1928969 unspecified    311
## 
## $data$`10.2307/1935066`
##                                                                    url
## 1 http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1935066
##               doi        type member
## 1 10.2307/1935066 unspecified    311
## 
## $data$`10.1002/ecy.1807`
##                                                                      url
## 1 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fecy.1807
## 2       http://onlinelibrary.wiley.com/wol1/doi/10.1002/ecy.1807/fullpdf
##                doi        type member
## 1 10.1002/ecy.1807         pdf    311
## 2 10.1002/ecy.1807 unspecified    311
## 
## $data$`10.1016/s0169-5347(97)89918-1`
##                                                                                    url
## 1   https://api.elsevier.com/content/article/PII:S0169534797899181?httpAccept=text/xml
## 2 https://api.elsevier.com/content/article/PII:S0169534797899181?httpAccept=text/plain
##                             doi  type member
## 1 10.1016/s0169-5347(97)89918-1   xml     78
## 2 10.1016/s0169-5347(97)89918-1 plain     78
## 
## $data$`10.1111/jiec.12669`
##                                                                        url
## 1 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1111%2Fjiec.12669
## 2       http://onlinelibrary.wiley.com/wol1/doi/10.1111/jiec.12669/fullpdf
##                  doi        type member
## 1 10.1111/jiec.12669         pdf    311
## 2 10.1111/jiec.12669 unspecified    311
## 
## 
## $opts
## list()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out$crossref$data[[1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                                                                     url
## 1 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1929908
##               doi        type member
## 1 10.2307/1929908 unspecified    311
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;extract-text-ft-extract&#34;&gt;Extract text: ft_extract&lt;/h2&gt;

&lt;p&gt;This is a simple wrapper around the &lt;a href=&#34;https://github.com/ropensci/pdftools&#34;&gt;pdftools&lt;/a&gt; package - when dealing with
xml or plain text data, there&amp;rsquo;s no need to parse what you get from &lt;code&gt;ft_get&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;path &amp;lt;- system.file(&amp;quot;examples&amp;quot;, &amp;quot;example1.pdf&amp;quot;, package = &amp;quot;fulltext&amp;quot;)
(res &amp;lt;- ft_extract(path))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &amp;lt;document&amp;gt;/Library/Frameworks/R.framework/Versions/3.4/Resources/library/fulltext/examples/example1.pdf
##   Title: Suffering and mental health among older people living in nursing homes---a mixed-methods study
##   Producer: pdfTeX-1.40.10
##   Creation date: 2015-07-17
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that &lt;code&gt;ft_extract&lt;/code&gt; can now handle both a file path to a pdf file, or
raw bytes.&lt;/p&gt;

&lt;h2 id=&#34;extract-parts-of-documents-you-want-chunk&#34;&gt;Extract parts of documents you want: chunk&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;chunk()&lt;/code&gt; helps you quickly extract sections of articles you want aross many articles and many publishers. This only handles XML (there&amp;rsquo;s no structure in plain text and pdf text) We have internal scripts targeting specific publishers so that we can handle variation in how publishers structure their XML.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;rplos&amp;quot;)
(dois &amp;lt;- searchplos(q=&amp;quot;*:*&amp;quot;, fl=&#39;id&#39;,
   fq=list(&#39;doc_type:full&#39;,&amp;quot;article_type:\&amp;quot;research article\&amp;quot;&amp;quot;),
     limit=5)$data$id)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;10.1371/journal.pone.0074173&amp;quot; &amp;quot;10.1371/journal.pone.0060168&amp;quot;
## [3] &amp;quot;10.1371/journal.pone.0170933&amp;quot; &amp;quot;10.1371/journal.pone.0170932&amp;quot;
## [5] &amp;quot;10.1371/journal.pone.0187293&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- ft_get(dois, from = &amp;quot;plos&amp;quot;)
x %&amp;gt;% chunks(c(&amp;quot;doi&amp;quot;,&amp;quot;history&amp;quot;)) %&amp;gt;% tabularize()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $plos
##                            doi history.received history.accepted
## 1 10.1371/journal.pone.0074173       2013-04-04       2013-07-27
## 2 10.1371/journal.pone.0060168       2012-09-18       2013-02-25
## 3 10.1371/journal.pone.0170933       2016-10-05       2017-01-12
## 4 10.1371/journal.pone.0170932       2016-09-27       2017-01-12
## 5 10.1371/journal.pone.0187293       2016-12-16       2017-10-17
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;

&lt;h3 id=&#34;xxx&#34;&gt;xxx&lt;/h3&gt;

&lt;p&gt;xxx&lt;/p&gt;

&lt;h3 id=&#34;feedback&#34;&gt;Feedback!&lt;/h3&gt;

&lt;p&gt;Please do upgrade/install &lt;code&gt;fulltext&lt;/code&gt;  &lt;code&gt;v0.3&lt;/code&gt; and let us know what you think.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>.rprofile: Jenny Bryan</title>
      <link>https://ropensci.org/blog/2017/12/08/rprofile-jenny-bryan/</link>
      <pubDate>Fri, 08 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/12/08/rprofile-jenny-bryan/</guid>
      <description>
        
        &lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-08-rprofile-jenny-bryan/jenny_bryan_lowres.jpg&#34; alt=&#34;Jenny Bryan&#34; style=&#34;margin: 0px 20px; width: 250px;&#34; align=&#34;left&#34;&gt;
&lt;em&gt;Jenny Bryan @JennyBryan is a Software Engineer at RStudio and is on leave from being an Associate Professor at the University of British Columbia. Jenny serves in leadership positions with rOpenSci and &lt;a href=&#34;https://forwards.github.io/&#34;&gt;Forwards&lt;/a&gt; and as an Ordinary member of &lt;a href=&#34;https://www.r-project.org/foundation/&#34;&gt;The R Foundation&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;KO: What is your name, your title, and how many years have you worked in R?&lt;/p&gt;

&lt;p&gt;JB: I’m Jenny Bryan, I am a software engineer at RStudio (still getting used to that title)., And I am on leave from being an Associate Professor at the University of British Columbia. I’ve been working with R or it’s predecessors since 1996. I switched to R from S in the early 2000s.&lt;/p&gt;

&lt;p&gt;KO: Why did you make the switch to R from S?&lt;/p&gt;

&lt;p&gt;JB: It just seemed like the community was switching over to R and I didn’t have a specific reason to do otherwise, I was just following the communal path of least resistance.&lt;/p&gt;

&lt;p&gt;KO: You have a huge following from all the stuff you post about your course. Did you always want to be a teacher? How did you get into teaching?&lt;/p&gt;

&lt;p&gt;JB: No, I wouldn’t say that I always wanted to be a teacher, but I think I’ve enjoyed that above average compared to other professors. But it was more that I realized several years ago that I could have a bigger impact on what people did by improving data analysis workflows, thinking, and tooling instead of trying to make incremental progress on statistical methodology. It is a reflection of where I have a comparative advantage with respect to interest and aptitude, so it’s not really a knock on statistical methodology. But I feel we could use more people working on this side of the field &amp;ndash; working on knowledge translation.&lt;/p&gt;

&lt;p&gt;I was also reacting to what I saw in my collaborative work. I would work with people in genomics and if I’m completely honest with myself, often my biggest contribution to the paper would be getting all the datasets and analyses organized. I didn’t necessarily do some highly sophisticated statistical analysis. It would often boil down to just doing millions of t-tests or something. But the reason I had an impact on the project would be that I got everything organized so that we could re-run it and have more confidence in our results. And I was like, I have a PhD in stats, why is this my main contribution? Why do the postdocs, grad students, and bioinformaticians not know how to do these things? So then I started to make that more and more the focus of my course, instead of squeezing in more statistical methods. Then the teaching sort of changed who I was and what I allowed myself to think about and spend time on. I used to not let myself spend time on those things. Or if I did, I would feel guilty about it because I thought, I can’t get any professional credit for this! It’s not statistically profound, but it seems to be what the world needs me to do, and needs other people to be doing.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You don’t always have to be proving a theorem, you don’t always have to be writing a package, there’s still a lot of space for worthwhile activity in between all of those things.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Do you feel proud of what you’ve accomplished?&lt;/p&gt;

&lt;p&gt;JB: I finally in some sense gave myself permission to start teaching what I thought people actually needed to know. And then after spending lots of time on it in the classroom, you realize what gaps there are, you become increasingly familiar with the tooling that you’re teaching and you’re like, hey I could actually improve that. Or no one really talks about how you get the output of this step to flow nicely as the input into the following step, i.e. how to create workflows. It really helped open my mind to different forms of work that are still valuable. You don’t always have to be proving a theorem, you don’t always have to be writing a package, there’s still a lot of space for worthwhile activity in between all of those things. However because we don’t have names for all of it, it can be difficult from a career point of view. But so many people see it, use it, and are grateful for it.&lt;/p&gt;

&lt;p&gt;KO: Can you talk about your transition into working for RStudio and what that will look like on a day-to-day basis?&lt;/p&gt;

&lt;p&gt;JB: In many ways it looks a lot like my life already did because I had, especially in the last two to three years, decided if I want to work on R packages or on exposition, I’m going to do that. That’s what I think tenure is for! So I had decided to stop worrying about how to sell myself in a framework set up to reward traditional work in statistical methodology. That freed up a lot of mental energy, to pursue these other activities, unapologetically. Which lead to other opportunities, such as RStudio. I was already working mostly from home. The Statistics department is by no means a negative environment for me, but the internet helped me find virtual colleagues around the globe who really share my interests. The physical comfort of home is very appealing. RStudio is also very light on meetings, which is a beautiful thing.&lt;/p&gt;

&lt;p&gt;KO: What is your team like at RStudio? How many projects are you juggling at any given time? Do you have an idea of what you want to accomplish while you’re there?&lt;/p&gt;

&lt;p&gt;JB: The person I interact with most is &lt;a href=&#34;http://hadley.nz/&#34;&gt;Hadley Wickham&lt;/a&gt; and he now has a team of five. There’s a fair amount of back and forth with other team members. I might seek their advice on, e.g., development practices, or just put questions out there for everyone. This team is pretty new and the formalization of the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; is pretty new, so everyone has different packages that they’re working on, either from scratch or shifting some of the maintenance burden off of Hadley. There’s a concerted effort to figure out “what does it mean to be an ecosystem of packages that work together?&amp;ldquo;.&lt;/p&gt;

&lt;p&gt;KO: Do you have a well defined road map at this point on the team?&lt;/p&gt;

&lt;p&gt;JB: I’ve been on that team since January and before that we had queued up &lt;a href=&#34;http://readxl.tidyverse.org/&#34;&gt;readxl&lt;/a&gt; as a good project for me. It was also overdue for maintenance! I was already a “Spreadsheet Lady”, very familiar with the underlying objects, and with the problem space. It was a good opportunity for me to write compiled code which I hadn’t done in a really long time. I had never written C++ so it was a way to kill at least three birds with one stone. So that was an easy selection for the first thing to work on. And even before that was done, it was clear that going back and doing another project in the Google arena made sense. We knew we would do some work with interns. Wrapping the Google Drive API was going to be useful (in general and for a future update of &lt;a href=&#34;https://github.com/jennybc/googlesheets&#34;&gt;googlesheets&lt;/a&gt;) and I knew our intern &lt;a href=&#34;http://www.lucymcgowan.com/&#34;&gt;Lucy McGowan&lt;/a&gt; would be a great person to work with on it.&lt;/p&gt;

&lt;p&gt;So no, there’s not some detailed 18-month roadmap stretching ahead of me. I think it will cycle between doing something that’s mine or new and doing maintenance on something that already exists. I also continue to do a lot of exposition, training, and speaking.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It actually pisses me off when people criticize “when” people work - like that’s a signifier of a poor work-life balance … their heart is in the right place to encourage balance, but I have a certain amount of work I want to get done.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Day-to-day, do you have regular standups? How do you like your day to be structured?&lt;/p&gt;

&lt;p&gt;JB: Oh there&amp;rsquo;s how I wish my day was structured and how it&amp;rsquo;s actually structured. I wish I could get up and just work because that’s when I feel by far my most productive. Unfortunately, this coincides with the morning chaos of a household with three kids, who, despite the fact that we’re trying to get them more independent with lunches and getting to school, you cannot completely tune out through this part of the day. So I do not really get up and just work, I sort of work until everyone gets out the door. Then I usually go exercise at that point, get that taken care of. I get more work done in the afternoon until the children all arrive home. I do a lot of work between 9 or 10 at night and 1 in the morning. Not because I love working at that time, but that’s just what I have.&lt;/p&gt;

&lt;p&gt;Given that I have this platform, it actually pisses me off when people criticize “when” people work - like that’s a signifier of a poor work-life balance, though it is possible that I have a poor work-life balance, but I feel like it’s usually coming from people who don’t have the same constraints in their life. “You shouldn’t work on the weekends, You shouldn’t work in the evenings”. I’m like, when the heck else do you think I would work? I feel like sometimes people are - their heart is in the right place to encourage balance, but I have a certain amount of work I want to get done. And I have a family and it means that I work when my children are asleep.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;They’re happy years but the tension between all the things you want to do is unbelievable because you will not do all of them. You cannot do it all.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: This topic is very interesting and personal to me. As I get older I’ve been thinking (nebulously) about starting a family, and I don’t know what that looks like. It’s scary to me, to not want to give up this lifestyle and this career that I’ve started for myself.&lt;/p&gt;

&lt;p&gt;JB: My pivoting of thinking about myself as an applied statistician to more of a data scientist, coincided with me reemerging from having little kids. I had all of them pre-tenure and at some point we had “three under three”. I was trying to get tenure, just barely getting it all done and I was kind of in my own little world, just surviving. Then the tenure process completed successfully, the kids got older, they were all in school, and eventually they didn’t need any out of school care. So me being able to string multiple abstract thoughts together and carve out hours at a time to do thought work coincided with me also freeing myself to work on stuff that I found more interesting.&lt;/p&gt;

&lt;p&gt;I don’t know how this all would have worked out if the conventional academic statistical work had suited me better. The time where I was most conflicted between doing a decent job parenting and doing decent work was also when I was doing work I wasn’t passionate about. I can’t tell if having more enthusiasm about the work would have made that period harder or easier! I really thought about ditching it all more than a few times.&lt;/p&gt;

&lt;p&gt;The reinvigoration that coincided with switching emphasis also coincided with the reinvigoration that comes from the kids becoming more independent. It does eventually happen! There are some very tough years - they’re not dark years, they’re happy years but the tension between all the things you want to do is unbelievable because you will not do all of them. You cannot do it all.&lt;/p&gt;

&lt;p&gt;KO: What are your favorite tools for managing your workflow?&lt;/p&gt;

&lt;p&gt;JB: In terms of working with R I’ve completely standardized on working with RStudio. Before that I was an Emacs-ESS zealot and I still have more accumulated years in that sphere. But once RStudio really existed and was viable, I started teaching with it. I hate doing R one way when I’m in front of students and another when I’m alone. It got very confusing and mixing up the keyboard shortcuts would create chaos. So now I’ve fully embraced RStudio and have never looked back.&lt;/p&gt;

&lt;p&gt;I’m also a &lt;a href=&#34;http://happygitwithr.com/&#34;&gt;git evangelist&lt;/a&gt;. Everything I do is in git, everything is on &lt;a href=&#34;https://github.com/jennybc&#34;&gt;Github&lt;/a&gt; and at this point, almost everything is public because I’ve gotten unselfconscious enough to put it up there. Plus there’s enough volume now that no one could be looking at any particular one thing. It’s so much easier for me to find it again later. I just put everything in a public place rather than trying to have this granular access control; it simplifies things greatly. Working in the open has simplified a lot of decisions, that’s nice.&lt;/p&gt;

&lt;p&gt;Otherwise I feel like my workflow is very primitive. I have thousands of email in my inbox. I’ve completely given up on managing email and I’m mostly okay with that. It’s out of my control and I can’t commit to a system where I’m forced to get to inbox zero. I’ve just given up on it. And twitter and slack are important ways to feel connected when I’m sitting at home on my sofa.&lt;/p&gt;

&lt;p&gt;KO: Do you have any online blogs, personalities or podcasts that you particularly enjoy? It doesn’t have to be R related.&lt;/p&gt;

&lt;p&gt;JB: I do follow people on twitter and the &lt;a href=&#34;https://twitter.com/hashtag/rstats&#34;&gt;rstats hashtag&lt;/a&gt;, so that often results in serendipitous one-off links that I enjoy. I don’t follow certain blogs regularly, but there are certain places that I end up at regularly. I like the Not So Standard Deviations podcast. In the end I always listen to every episode, but it’s what I do on an airplane or car drive.&lt;/p&gt;

&lt;p&gt;KO: You build up a backlog?&lt;/p&gt;

&lt;p&gt;JB: Exactly. Then the next time I need to drive to Seattle in traffic, I’ll power through four episodes.&lt;/p&gt;

&lt;p&gt;KO: What are some of your favorite R packages - do you have some that you think are funny, or love?&lt;/p&gt;

&lt;p&gt;JB: I live entirely in the tidyverse. I’m not doing primary data analysis on projects anymore. It’s weird that the more involved you become in honing the tools, the less time you spend wielding them. So I’m increasingly focused on the data prep, data wrangling, data input part of the cycle and not on modeling. I did a lot more of that when I was a statistician and now it’s not where my comparative interest and advantage seems to lie. There’s plenty to do on the other end. And also not that many people who like it. I actually do enjoy it. I don’t have to force myself to enjoy it - this is really important, and it pleases me. Given how important I think the work is, it’s a relatively uncrowded field. Whereas machine learning, it seems like everyone wants to make a contribution there. I’m like, you go for it - I’m going to be over here getting data out of Excel spreadsheets.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Exploratory Data Analysis of Ancient Texts with rperseus</title>
      <link>https://ropensci.org/blog/2017/12/05/rperseus/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/12/05/rperseus/</guid>
      <description>
        
        

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;When I was in grad school at Emory, I had a favorite desk in the library. The desk wasn’t particularly cozy or private, but what it lacked in comfort it made up for in real estate. My books and I needed room to operate. Students of the ancient world require many tools, and when jumping between commentaries, lexicons, and interlinears, additional clutter is additional “friction”, i.e., lapses in thought due to frustration. Technical solutions to this clutter exist, but the best ones are proprietary and expensive. Furthermore, they are somewhat inflexible, and you may have to shoehorn your thoughts into their framework. More friction.&lt;/p&gt;

&lt;p&gt;Interfacing with &lt;a href=&#34;http://www.perseus.tufts.edu/hopper/&#34;&gt;the Perseus Digital Library&lt;/a&gt; was a popular online alternative. The library includes a catalog of classical texts, a Greek and Latin lexicon, and a word study tool for appearances and references in other literature. If the university library’s reference copies of BDAG&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;em&gt;Synopsis Quattuor Evangeliorum&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; were unavailable, Perseus was our next best thing.&lt;/p&gt;

&lt;p&gt;Fast forward several years, and I’ve abandoned my quest to become a biblical scholar. Much to my father’s dismay, I’ve learned writing code is more fun than writing exegesis papers. Still, I enjoy dabbling with dead languages, and it was the desire to wed my two loves, biblical studies and R, that birthed my latest package, &lt;code&gt;rperseus&lt;/code&gt;. The goal of this package is to furnish classicists with texts of the ancient world and a toolkit to unpack them.&lt;/p&gt;

&lt;h3 id=&#34;exploratory-data-analysis-in-biblical-studies&#34;&gt;Exploratory Data Analysis in Biblical Studies&lt;/h3&gt;

&lt;p&gt;Working with the Perseus Digital Library was already a trip down memory lane, but here’s an example of how I would have leveraged &lt;code&gt;rperseus&lt;/code&gt; many years ago.&lt;/p&gt;

&lt;p&gt;My best papers often sprung from the outer margins of my &lt;a href=&#34;https://en.wikipedia.org/wiki/Novum_Testamentum_Graece&#34;&gt;&lt;em&gt;Nestle-Aland Novum Testamentum Graece.&lt;/em&gt;&lt;/a&gt; Here the editors inserted cross references to parallel vocabulary, themes, and even grammatical constructions. Given the intertextuality of biblical literature, the margins are a rich source of questions: Where else does the author use similar vocabulary? How is the source material used differently? Does the literary context affect our interpretation of a particular word? This is exploratory data analysis in biblical studies.&lt;/p&gt;

&lt;p&gt;Unfortunately the excitement of your questions is incommensurate with the tedium of the process&amp;ndash;EDA continues by flipping back and forth between books, dog-earring pages, and avoiding paper cuts. &lt;code&gt;rperseus&lt;/code&gt; aims to streamline this process with two functions: &lt;code&gt;get_perseus_text&lt;/code&gt; and &lt;code&gt;perseus_parallel&lt;/code&gt;. The former returns a data frame containing the text from any work in the Perseus Digital Library, and the latter renders a parallel in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Suppose I am writing a paper on different expressions of love in Paul’s letters. Naturally, I start in 1 Corinthians 13, the famed “Love Chapter” often heard at weddings and seen on bumper stickers. I finish the chapter and turn to the margins. In the image below, I see references to Colossians 1:4, 1 Thessalonians 1:3, 5:8, Hebrews 10:22-24, and Romans 8:35-39.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/nantg.png&#34; alt=&#34;&#34; /&gt;
&lt;em&gt;1 Corinithians 13 in Nestle-Aland Novum Testamentum Graece&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ignoring that some scholars exclude Colossians from the “authentic” letters, let’s see the references alongside each other:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rperseus) #devtools::install_github(“ropensci/rperseus”)
library(tidyverse)

tribble(
  ~label, ~excerpt,
  &amp;quot;Colossians&amp;quot;, &amp;quot;1.4&amp;quot;,
  &amp;quot;1 Thessalonians&amp;quot;, &amp;quot;1.3&amp;quot;,
  &amp;quot;1 Thessalonians&amp;quot;, &amp;quot;5.8&amp;quot;,
  &amp;quot;Romans&amp;quot;, &amp;quot;8.35-8.39&amp;quot;
  ) %&amp;gt;% 
  left_join(perseus_catalog) %&amp;gt;%
  filter(language == &amp;quot;grc&amp;quot;) %&amp;gt;%
  select(urn, excerpt) %&amp;gt;%
  pmap_df(get_perseus_text) %&amp;gt;%
  perseus_parallel(words_per_row = 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/Parallel1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A brief explanation: First, I specify the labels and excerpts within a tibble. Second, I join the lazily loaded &lt;code&gt;perseus_catalog&lt;/code&gt; onto the data frame. Third, I filter for the Greek&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; and select the columns containing the arguments required for &lt;code&gt;get_perseus_text&lt;/code&gt;. Fourth, I map over each urn and excerpt, returning another data frame. Finally, I pipe the output into &lt;code&gt;perseus_parallel&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The key word shared by each passage is &lt;em&gt;agape&lt;/em&gt; (“love”). Without going into detail, it might be fruitful to consider the references alongside each other, pondering how the semantic range of &lt;em&gt;agape&lt;/em&gt; expands or contracts within the Pauline corpus. Paul had a penchant for appropriating and recasting old ideas&amp;ndash;often in slippery and unexpected ways&amp;ndash;and your Greek lexicon provides a mere approximation. In other words, how can we move from the dictionary definition of &lt;em&gt;agape&lt;/em&gt; towards Paul&amp;rsquo;s unique vision?&lt;/p&gt;

&lt;p&gt;If your Greek is rusty, you can parse each word with &lt;code&gt;parse_excerpt&lt;/code&gt; by locating the text&amp;rsquo;s urn within the &lt;code&gt;perseus_catalog&lt;/code&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parse_excerpt(urn = &amp;quot;urn:cts:greekLit:tlg0031.tlg012.perseus-grc2&amp;quot;, excerpt = &amp;quot;1.4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;form&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;verse&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;part_of_speech&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;person&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;number&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;tense&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;mood&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;voice&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;gender&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;case&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;degree&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ἀκούω&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ἀκούσαντες&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;verb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;plural&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aorist&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;participle&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;active&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;masculine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;nominative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ὁ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;τὴν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;article&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;singular&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;feminine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;accusative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;πίστις&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;πίστιν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;noun&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;singular&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;feminine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;accusative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ὑμός&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ὑμῶν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;pronoun&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;plural&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;masculine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;genative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If your Greek is &lt;em&gt;really&lt;/em&gt; rusty, you can also flip the &lt;code&gt;language&lt;/code&gt; filter to “eng” to view an older English translation.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; And if the margin references a text from the Old Testament, you can call the Septuagint as well as the original Hebrew.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tribble(
  ~label, ~excerpt,
  &amp;quot;Genesis&amp;quot;, &amp;quot;32.31&amp;quot;,
  &amp;quot;Genesis, pointed&amp;quot;, &amp;quot;32.31&amp;quot;,
  &amp;quot;Numeri&amp;quot;, &amp;quot;12.8&amp;quot;,
  &amp;quot;Numbers, pointed&amp;quot;, &amp;quot;12.8&amp;quot;
  ) %&amp;gt;% 
  left_join(perseus_catalog) %&amp;gt;%
  filter(language %in% c(&amp;quot;grc&amp;quot;, &amp;quot;hpt&amp;quot;)) %&amp;gt;%
  select(urn, excerpt) %&amp;gt;%
  pmap_df(get_perseus_text) %&amp;gt;%
  perseus_parallel()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/Parallel2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Admittedly, there is some “friction” here in joining the &lt;code&gt;perseus_catalog&lt;/code&gt; onto the initial tibble. There is a learning curve with getting acquainted with the idiosyncrasies of the catalog object. A later release will aim to streamline this workflow.&lt;/p&gt;

&lt;h3 id=&#34;future-work&#34;&gt;Future Work&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://ropensci.github.io/rperseus/articles/rperseus-vignette.html&#34;&gt;Check the vignette&lt;/a&gt; for a more general overview of &lt;code&gt;rperseus&lt;/code&gt;. In the meantime, I look forward to getting more intimately acquainted with the Perseus Digital Library. Tentative plans to extend &lt;code&gt;rperseus&lt;/code&gt; a Shiny interface to further reduce “friction” and a method of creating a “book” of custom parallels with &lt;code&gt;bookdown&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;I want to thank my two rOpenSci reviewers, &lt;a href=&#34;https://www.ildiczeller.com/&#34;&gt;Ildikó Czeller&lt;/a&gt; and &lt;a href=&#34;https://francoismichonneau.net/&#34;&gt;François Michonneau,&lt;/a&gt; for coaching me through the review process. They were the first two individuals to ever scrutinize my code, and I was lucky to hear their feedback. rOpenSci onboarding is truly a wonderful process.&lt;/p&gt;

&lt;!-- references --&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Bauer, Walter. &lt;em&gt;A Greek-English Lexicon of the New Testament and Other Early Christian Literature.&lt;/em&gt; Edited by Frederick W. Danker. 3rd ed. Chicago: University of Chicago Press, 2000.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Aland, Kurt. &lt;em&gt;Synopsis Quattuor Evangeliorum.&lt;/em&gt; Deutsche Bibelgesellschaft, 1997.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;The Greek text from the Perseus Digital Library is from 1885 standards. The advancement of textual criticism in the 20th century led to a more stable text you would find in current editions of the Greek New Testament.&lt;br /&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;The English translation is from Rainbow Missions, Inc. &lt;em&gt;World English Bible.&lt;/em&gt; Rainbow Missions, Inc.; revision of the American Standard Version of 1901. I’ve toyed with the idea of incorporating more modern translations, but that would require require resources beyond the Perseus Digital Library.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;&amp;ldquo;hpt&amp;rdquo; is the pointed Hebrew text from &lt;em&gt;Codex Leningradensis.&lt;/em&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>Magick 1.6: clipping, geometries, fonts, fuzz, and a bit of history</title>
      <link>https://ropensci.org/technotes/2017/12/05/magick-16/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/12/05/magick-16/</guid>
      <description>
        
        &lt;img src=&quot;https://i.imgur.com/tTFk7ig.jpg&quot; alt=&quot;cover image&quot;&gt;
        
        

&lt;p&gt;This week &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick&lt;/a&gt; 1.6 appeared on CRAN. This release is a big all-round maintenance update with lots of tweaks and improvements across the package.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/NEWS&#34;&gt;NEWS&lt;/a&gt; file gives an overview of changes in this version. In this post we highlight some changes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)
stopifnot(packageVersion(&#39;magick&#39;) &amp;gt;= 1.6)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you are new to magick, check out the &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;vignette&lt;/a&gt; for a quick introduction.&lt;/p&gt;

&lt;h2 id=&#34;perfect-graphics-rendering&#34;&gt;Perfect Graphics Rendering&lt;/h2&gt;

&lt;p&gt;I have fixed a few small rendering imperfections in the graphics device. The native magick graphics device &lt;code&gt;image_graph()&lt;/code&gt; now renders identical or better quality images as the R-base bitmap devices &lt;code&gt;png&lt;/code&gt;, &lt;code&gt;jpeg&lt;/code&gt;, etc.&lt;/p&gt;

&lt;p&gt;One issue was that sometimes magick graphics would show a 1px black border around the image. It turned out this is caused by rounding of clipping coordinates.&lt;/p&gt;

&lt;p&gt;When R calculates clipping area it often ends up at non-whole values. It is then up to the graphics device to decide what to do with the pixel that is partially clipped. Let&amp;rsquo;s show clipping in action:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;testplot &amp;lt;- function(title = &amp;quot;&amp;quot;){
  plot(1, main = title)
  abline(0, 1, col = &amp;quot;blue&amp;quot;, lwd = 2, lty = &amp;quot;solid&amp;quot;)
  abline(0.1, 1, col = &amp;quot;red&amp;quot;, lwd = 3, lty = &amp;quot;dotted&amp;quot;)
  abline(0.2, 1, col = &amp;quot;green&amp;quot;, lwd = 4, lty = &amp;quot;twodash&amp;quot;)
  abline(0.3, 1, col = &amp;quot;black&amp;quot;, lwd = 5, lty = &amp;quot;dotdash&amp;quot;)
  abline(0.4, 1, col = &amp;quot;purple&amp;quot;, lwd = 6, lty = &amp;quot;dashed&amp;quot;)
  abline(0.5, 1, col = &amp;quot;yellow&amp;quot;, lwd = 7, lty = &amp;quot;longdash&amp;quot;)
  abline(-0.1, 1, col = &amp;quot;blue&amp;quot;, lwd = 10, lend = &amp;quot;round&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
  abline(-0.2, 1, col = &amp;quot;blue&amp;quot;, lwd = 10, lend = &amp;quot;butt&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
  abline(-0.3, 1, col = &amp;quot;blue&amp;quot;, lwd = 10, lend = &amp;quot;square&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we run it with and without clipping:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img2 &amp;lt;- magick::image_graph(clip = FALSE)
testplot(&amp;quot;Without clipping&amp;quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/TtpjlLq.png&#34; alt=&#34;noclip.png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img1 &amp;lt;- magick::image_graph(clip = TRUE)
testplot(&amp;quot;With clipping&amp;quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/JbWMElL.png&#34; alt=&#34;clip.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see the latter image is now perfectly clipped. The colored lines are truncated exactly at the pixel where the axis starts. This is not always the case in base R ;)&lt;/p&gt;

&lt;h2 id=&#34;font-families&#34;&gt;Font Families&lt;/h2&gt;

&lt;p&gt;In magick there are two ways to render text on an image. You can either open the image or graphic in the magick graphics device and then use base R &lt;code&gt;text()&lt;/code&gt; function to print text. Alternatively there is &lt;code&gt;image_annotate()&lt;/code&gt; which is a simpler version to print some text on an image.&lt;/p&gt;

&lt;p&gt;Wherever text rendering is involved, two major headache arise: encoding and fonts. The latter is tricky because different operating systems have different fonts with different names. In addition a font can be specified as a name, or family name, or alias.&lt;/p&gt;

&lt;p&gt;Below is a simple test that I use to quickly inspect if fonts are working on different systems:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img &amp;lt;- image_graph(width = 800, height = 500, pointsize = 20, res = 96)
graphics::plot.new()
graphics::par(mar = c(0,0,3,0))
graphics::plot.window(xlim = c(0, 20), ylim = c(-.5, 8))
title(expression(Gamma %prop% sum(x[alpha], i==1, n) * sqrt(mu)), expression(hat(x)))

# Standard families as supported by other devices
text(0.95, 7, &amp;quot;abcdefg  - Helvetica&amp;quot;, pos = 4, family = &amp;quot;helvetica&amp;quot;)
text(0.95, 6, &amp;quot;abcdefg  - Sans (Arial)&amp;quot;, pos = 4, family = &amp;quot;sans&amp;quot;)
text(0.95, 5, &amp;quot;abcdefg - Serif (Times)&amp;quot;, pos = 4, family = &amp;quot;serif&amp;quot;)
text(0.95, 4, &amp;quot;abcdefg - Monospace (Courier New)&amp;quot;, pos = 4, family = &amp;quot;mono&amp;quot;)
text(0.95, 3, &amp;quot;abcdefg - Symbol Face&amp;quot;, pos = 4, font = 5)
text(0.95, 2, &amp;quot;abcdefg  - Comic Sans&amp;quot;, pos = 4, family = &amp;quot;Comic Sans&amp;quot;)
text(0.95, 1, &amp;quot;abcdefg - Georgia Serif&amp;quot;, pos = 4, family = &amp;quot;Georgia&amp;quot;)
text(0.95, 0, &amp;quot;abcdefg - Courier&amp;quot;, pos = 4, family = &amp;quot;Courier&amp;quot;)
dev.off()
img &amp;lt;- image_border(img, &#39;red&#39;, geometry = &#39;2x2&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/tzIktip.png&#34; alt=&#34;families&#34; /&gt;&lt;/p&gt;

&lt;p&gt;R requires that a graphics device supports at least 4 font types: &lt;code&gt;serif&lt;/code&gt;, &lt;code&gt;sans&lt;/code&gt;, &lt;code&gt;mono&lt;/code&gt; and &lt;code&gt;symbol&lt;/code&gt;. The latter is a special 8bit font with some Greek letters and other characters needed for rendering math. This set of fonts corresponds to the original &lt;strong&gt;13 base fonts&lt;/strong&gt; from the &lt;a href=&#34;https://en.wikipedia.org/wiki/PostScript_fonts#Core_Font_Set&#34;&gt;1984 postscript standard&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;4x Courier (Regular, Oblique, Bold, Bold Oblique)&lt;/li&gt;
&lt;li&gt;4x Helvetica (Regular, Oblique, Bold, Bold Oblique)&lt;/li&gt;
&lt;li&gt;4x Times (Roman, Italic, Bold, Bold Italic)&lt;/li&gt;
&lt;li&gt;Symbol&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below a photo of the 1985 &lt;a href=&#34;https://en.wikipedia.org/wiki/LaserWriter&#34;&gt;Apple Laser Writer&lt;/a&gt; which was &lt;a href=&#34;https://en.wikipedia.org/wiki/PostScript_fonts#History&#34;&gt;the first laser printer&lt;/a&gt; to use the PostScript language and support all these fonts! Not much later PostScript graphics devices were adopted by R&amp;rsquo;s predecessor &lt;a href=&#34;https://en.wikipedia.org/wiki/S_(programming_language)#.22New_S.22&#34;&gt;&amp;ldquo;The New S&amp;rdquo;&lt;/a&gt; (The New S Language, 1988).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://theappletimeline.com/images/color1000.jpg&#34; alt=&#34;printers&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;geometry-helpers&#34;&gt;Geometry Helpers&lt;/h2&gt;

&lt;p&gt;Another major improvement in this release is the introduction of helper functions for geometry and option strings. Many functions in magick require a special geometry syntax to specify a size, area, or point. For example to resize an image you need to specify a size:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;image_resize(img, &amp;quot;50%&amp;quot;)
image_resize(img, &amp;quot;300x300&amp;quot;)
image_resize(img, &amp;quot;300x300!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or to crop you need to specify an area which consists of a size and offset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;image_crop(img, &amp;quot;300x300+100+100&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We added a few handy &lt;code&gt;?geometry&lt;/code&gt; helper functions to generate proper geometry syntax&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/2jivLxi.png&#34; alt=&#34;geometries&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;magick-options&#34;&gt;Magick Options&lt;/h2&gt;

&lt;p&gt;A lot of the power in ImageMagick is contained in the hundreds of built-in filters, colorspaces, compose operators, disposal types, convolution kernels, noise types and what not. These are specified simply as a string in the function.&lt;/p&gt;

&lt;p&gt;For example in our previous &lt;a href=&#34;https://ropensci.org/technotes/2017/11/02/image-convolve/&#34;&gt;post about Image Convolution&lt;/a&gt; we discussed a few kernel types:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Gaussian Kernel
img %&amp;gt;% image_convolve(&#39;Gaussian:0x5&#39;, scaling = &#39;60,40%&#39;)

# Sobel Kernel
img %&amp;gt;% image_convolve(&#39;Sobel&#39;)

# Difference of Gaussians
img %&amp;gt;% image_convolve(&#39;DoG:0,0,2&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Supported values for each option are described in the online ImageMagick documentation. We now have added functions in the magick package that list all values for each option. This should make it a easier to see what is supported and harness the full power of built-in ImageMagick algorithms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/cid6JqU.png&#34; alt=&#34;options&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So we can now easily list e.g. supported kernel types:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; kernel_types()
 [1] &amp;quot;Undefined&amp;quot;     &amp;quot;Unity&amp;quot;         &amp;quot;Gaussian&amp;quot;      &amp;quot;DoG&amp;quot;          
 [5] &amp;quot;LoG&amp;quot;           &amp;quot;Blur&amp;quot;          &amp;quot;Comet&amp;quot;         &amp;quot;Binomial&amp;quot;     
 [9] &amp;quot;Laplacian&amp;quot;     &amp;quot;Sobel&amp;quot;         &amp;quot;FreiChen&amp;quot;      &amp;quot;Roberts&amp;quot;      
[13] &amp;quot;Prewitt&amp;quot;       &amp;quot;Compass&amp;quot;       &amp;quot;Kirsch&amp;quot;        &amp;quot;Diamond&amp;quot;      
[17] &amp;quot;Square&amp;quot;        &amp;quot;Rectangle&amp;quot;     &amp;quot;Disk&amp;quot;          &amp;quot;Octagon&amp;quot;      
[21] &amp;quot;Plus&amp;quot;          &amp;quot;Cross&amp;quot;         &amp;quot;Ring&amp;quot;          &amp;quot;Peaks&amp;quot;        
[25] &amp;quot;Edges&amp;quot;         &amp;quot;Corners&amp;quot;       &amp;quot;Diagonals&amp;quot;     &amp;quot;ThinDiagonals&amp;quot;
[29] &amp;quot;LineEnds&amp;quot;      &amp;quot;LineJunctions&amp;quot; &amp;quot;Ridges&amp;quot;        &amp;quot;ConvexHull&amp;quot;   
[33] &amp;quot;ThinSe&amp;quot;        &amp;quot;Skeleton&amp;quot;      &amp;quot;Chebyshev&amp;quot;     &amp;quot;Manhattan&amp;quot;    
[37] &amp;quot;Octagonal&amp;quot;     &amp;quot;Euclidean&amp;quot;     &amp;quot;User Defined&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a lot of kernels.&lt;/p&gt;

&lt;h2 id=&#34;fuzz-scaling&#34;&gt;Fuzz Scaling&lt;/h2&gt;

&lt;p&gt;Finally one more (breaking) change: several functions in magick use a &lt;code&gt;fuzz&lt;/code&gt; parameter to specify the max distance between two colors to be considered similar.&lt;/p&gt;

&lt;p&gt;For example the flood fill algorithm (the paint-bucket button in ms-paint) changes the color of a given starting pixel, and then recursively all adjacent pixels that have the same color. However sometimes neighboring pixels are not precisely the same color, but nearly the same. The &lt;code&gt;fuzz&lt;/code&gt; parameter allows the fill to continue when pixels are not the same but similar color.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Paint the shirt orange
frink &amp;lt;- image_read(&amp;quot;https://jeroen.github.io/images/frink.png&amp;quot;) %&amp;gt;%
  image_fill(&amp;quot;orange&amp;quot;, point = &amp;quot;+100+200&amp;quot;, fuzz = 25)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/VwlqYWy.png&#34; alt=&#34;frink&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What has changed in this version is that &lt;code&gt;fuzz&lt;/code&gt; parameter been rescaled to a percentage. Hence you should always provide a value between 0 and 100. Previously it was the absolute distance between colors, but this depends on the type and color depth of the image at hand, which was very confusing.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>The Value of Welcome, part 2: How to prepare 40 new community members for an unconference</title>
      <link>https://ropensci.org/blog/2017/12/01/unconf-welcome/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/12/01/unconf-welcome/</guid>
      <description>
        
        &lt;p&gt;I’ve raved about the value of &lt;a href=&#34;https://ropensci.org/blog/2017/07/18/value-of-welcome/&#34;&gt;extending a personalized welcome&lt;/a&gt; to new community members and I recently shared &lt;a href=&#34;https://ropensci.org/blog/2017/11/17/unconf-sixtips&#34;&gt;six tips for running a successful hackathon-flavoured unconference&lt;/a&gt;. Building on these, I’d like to share the specific approach and (free!) tools I used to help prepare new rOpenSci community members to be productive at our &lt;a href=&#34;http://unconf17.ropensci.org/&#34;&gt;unconference&lt;/a&gt;. My approach was inspired directly by my &lt;a href=&#34;https://blog.trelliscience.com/introducing-the-2017-community-engagement-fellows/&#34;&gt;AAAS Community Engagement Fellowship Program&lt;/a&gt; (AAAS-CEFP) training. Specifically, 1) one mentor said that the most successful conference they ever ran involved having one-to-one meetings with all participants prior to the event, and 2) prior to our in-person AAAS-CEFP training, we completed an intake questionnaire that forced us to consider things like “what do you hope to get out of this” and “what do you hope to contribute”.&lt;/p&gt;

&lt;p&gt;A challenge of this year’s unconference was the fact that we were inviting 70 people to participate. As a rule, one third of the crowd will have participated in one of our previous unconferences and two-thirds would be first-time participants. With only two days together, these people need to quickly self-sort into project groups and get working.&lt;/p&gt;

&lt;p&gt;So I sent this email to 45 first-time participants:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-01-unconf-welcome/pre-unconf-email.png&#34; alt=&#34;pre-unconf-email&#34;&gt;&lt;/p&gt;

&lt;p&gt;Arranging meetings is one of my least favorite activities, but the free &lt;a href=&#34;https://calendly.com/&#34;&gt;Calendly tool&lt;/a&gt; made this process relatively painless. When a person clicks on the calendar link in the email above, it reveals only times that I am available in my Google Calendar, the time slot they choose shows up in my calendar, and I receive a confirmation email indicating who booked a meeting with me. In my busiest week, I had 19 meetings, but that meant the bulk of them were done!&lt;/p&gt;

&lt;p&gt;To make the meeting time most effective I followed AAAS-CEFP program director &lt;a href=&#34;https://twitter.com/LouWoodley&#34;&gt;Lou Woodley’s&lt;/a&gt; model for onboarding our AAAS CEFP cohort by sending a set of questions to be answered in advance. I took the model to the next level by creating a free &lt;a href=&#34;https://www.google.ca/forms/about/&#34;&gt;Google Form&lt;/a&gt; questionnaire so that all answers were automatically collected and could be viewed per individual or collectively, and automatically exported to a spreadsheet.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-01-unconf-welcome/pre-unconf-google-form.png&#34; alt=&#34;pre-unconf-google-form&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Questions included:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;List three things you hope to get from the unconference&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;connect with people working in a similar domain&lt;/em&gt;, &lt;em&gt;learn about best practices in data science with R&lt;/em&gt;, or &lt;em&gt;develop a new package that does X&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List three things you hope to contribute to the unconference&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;expertise or experience in X&lt;/em&gt;, &lt;em&gt;mentoring skills&lt;/em&gt;, &lt;em&gt;write all the docs!&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Have you had any previous interactions with the rOpenSci community?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;I read the rOpenSci blog&lt;/em&gt;, or &lt;em&gt;I submitted software for rOpenSci open peer review&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Do you have any concerns about your readiness to participate?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;I’ve never developed an R package&lt;/em&gt; or &lt;em&gt;How do I decide what project to work on?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Would you be interested in writing a blog post about your unconference project or your unconference experience?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Do you  have a preferred working style or anything you would like to let us know about how you work best?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;I’m an introvert who likes to take my lunch alone sometimes to recover from group activities – it doesn’t mean I’m not having fun.&lt;/em&gt; or &lt;em&gt;I know I have a tendency to dominate in group discussions – it’s totally fine to ask me to step back and let others contribute. I won’t be offended.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These questions encouraged participants to reflect in advance. The example answer snippets we provided gave them ideas from which to seed their answers and in some cases gave them permission to show some vulnerability. Individual’s answers gave me cues for things to address in our chat and freed both of us to spend our time talking about the most important issues.&lt;/p&gt;

&lt;p&gt;The answers to the question “List three things you hope to get from the unconf” were so heartening:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-01-unconf-welcome/unconf-three-things.png&#34; alt=&#34;unconf-three-things&#34;&gt;&lt;/p&gt;

&lt;p&gt;Beautiful, but in a different way, were answers to the question, “Do you have any concerns about your readiness to participate?”. People expressed real concerns about impostor syndrome, their perceived ability to contribute &amp;ldquo;as much or as well&amp;rdquo; as others, and feeling &amp;ldquo;outclassed by all the geniuses present&amp;rdquo;. These responses prompted me to reassure people that they were 100% qualified to participate, and opened an opportunity to listen to and address specific concerns.&lt;/p&gt;

&lt;p&gt;To conduct the pre-unconference chats, I used video conferencing via &lt;a href=&#34;https://appear.in/&#34;&gt;appear.in&lt;/a&gt;, a free, browser-based application that does not require plugins or user accounts. Rather than being exhausted from these calls, I felt energized and optimistic and I experienced many direct positive outcomes. These conversations enabled me to prime people to connect on day-one of the unconference with others with similar interests or from related work sectors. Frequently, I noticed that immediately after our conversation, first-time participants would join the online discussion of existing project ideas, or they themselves proposed new ideas. My conversations with two first-time participants led directly to their proposing community-focussed projects - a &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/63&#34;&gt;group discussion&lt;/a&gt; and a new &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/64&#34;&gt;blog series of interviews&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;An unexpected benefit was that questions people asked me during the video chats led to actions I could take to improve the unconference. For example, when someone wanted to know what previous participants wished they knew beforehand, I asked for and &lt;a href=&#34;https://twitter.com/rOpenSci/status/855531572081991680&#34;&gt;shared example resources&lt;/a&gt;. One wise person asked me what my plan was for having project teams report out at the end of the unconference and this led directly to a streamlined plan (See &lt;a href=&#34;https://ropensci.org/blog/2017-11-17-unconf-sixtips&#34;&gt;Six tips for running a successful unconference&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Big thanks to AAAS CEFP training for giving me the confidence to try this community experiment! Arranging and carrying out these pre-unconference questionnaires and video chats took a big investment of my time and energy and yet, I consider this effort to be one of the biggest contributors to participants’ satisfaction with the unconference. Will 100% do this again!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Announcing a New rOpenSci Software Review Collaboration</title>
      <link>https://ropensci.org/blog/2017/11/29/review-collaboration-mee/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/29/review-collaboration-mee/</guid>
      <description>
        
        

&lt;p&gt;rOpenSci is pleased to announce a new collaboration with the &lt;a href=&#34;http://besjournals.onlinelibrary.wiley.com/hub/journal/10.1111/(ISSN)2041-210X/&#34;&gt;Methods in Ecology and Evolution (MEE)&lt;/a&gt;, a journal of the &lt;a href=&#34;http://www.britishecologicalsociety.org/&#34;&gt;British Ecological Society&lt;/a&gt;, published by Wiley press &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Publications destined for MEE that include the development of a scientific R package will now have the option of a joint review process whereby the R package is reviewed by rOpenSci, followed by fast-tracked review of the manuscript by MEE. Authors opting for this process will be recognized via a mark on both web and print versions of their paper.&lt;/p&gt;

&lt;p&gt;We are very excited for this partnership to improve the rigor of both scientific software and software publications and to provide greater recognition to developers in the fields of ecology and evolution.  It is a natural outgrowth of our interest in supporting scientists in developing and maintaining software, and of MEE&amp;rsquo;s mission of vetting and disseminating tools and methods for the research community. The collaboration formalizes and eases a path already pursued by researchers: The &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12593/abstract&#34;&gt;&lt;strong&gt;rotl&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12469/abstract&#34;&gt;&lt;strong&gt;RNexML&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00247.x/abstract&#34;&gt;&lt;strong&gt;treebase&lt;/strong&gt;&lt;/a&gt; packages were all developed or reviewed by rOpenSci and subsequently had associated manuscripts published in MEE.&lt;/p&gt;

&lt;h3 id=&#34;about-ropensci-software-review&#34;&gt;About rOpenSci software review&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; is a diverse community of researchers from academia, non-profit, government, and industry who collaborate to develop and maintain tools and practices around open data and reproducible research. The rOpenSci suite of tools is made of core infrastructure software developed and maintained by the &lt;a href=&#34;https://ropensci.org/about#team&#34;&gt;project staff&lt;/a&gt;. The suite also contains numerous packages that are contributed by members of the broader R community. The volume of community submissions has grown considerably over the years necessitating a formal system of review quite analogous to that of a peer reviewed academic journal.&lt;/p&gt;

&lt;p&gt;rOpenSci welcomes full software submissions that fit within our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;aims and scope&lt;/a&gt;, with the option of a pre-submission inquiry in cases when the scope of a submission is not immediately obvious. This software peer review framework, known as the rOpenSci Onboarding process, operates with three editors and one editor in chief who carefully vet all incoming submissions. After an editorial review, editors solicit detailed, public and signed reviews from two reviewers, and the path to acceptance from then on is similar to a standard journal review process. Details about the system are described in &lt;a href=&#34;https://ropensci.org/blog/2016/03/28/software-review/&#34;&gt;various&lt;/a&gt; &lt;a href=&#34;https://ropensci.org/blog/2017/09/01/nf-softwarereview/&#34;&gt;blog&lt;/a&gt; &lt;a href=&#34;https://ropensci.org/blog/2017/09/11/software-review-update/&#34;&gt;posts&lt;/a&gt; by the editorial team.&lt;/p&gt;

&lt;h3 id=&#34;collaboration-with-journals&#34;&gt;Collaboration with journals&lt;/h3&gt;

&lt;p&gt;This is our second collaboration with a journal. Since late 2015, rOpenSci has partnered with the &lt;a href=&#34;http://joss.theoj.org/&#34;&gt;Journal of Open Source software (JOSS)&lt;/a&gt;, an open access journal that publishes brief articles on research software. Packages accepted to rOpenSci can be submitted for fast-track publication at JOSS, in which JOSS editors may evaluate based on rOpenSci&amp;rsquo;s reviews alone. As rOpenSci&amp;rsquo;s review criteria is significantly more stringent and designed to be compatible with JOSS, these packages are generally accepted without additional review. We have had great success with this partnership providing rOpenSci authors with an additional venue to publicize and archive their work. Given this success, we are keen on expanding to other journals and fields where there is potential for software reviewed and created by rOpenSci to play a significant role in supporting scientific findings.&lt;/p&gt;

&lt;h3 id=&#34;the-details&#34;&gt;The details&lt;/h3&gt;

&lt;p&gt;Our new partnership with MEE broadly resembles that with JOSS, with the major difference that MEE, rather than rOpenSci, leads review of the manuscript component.  Authors with R packages and associated manuscripts that fit the Aims and Scope for both &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;rOpenSci&lt;/a&gt; and &lt;a href=&#34;http://www.methodsinecologyandevolution.org/view/0/aimsAndScope.html&#34;&gt;MEE&lt;/a&gt; are encouraged to first submit to rOpenSci. The &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12593/abstract&#34;&gt;&lt;strong&gt;rotl&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12469/abstract&#34;&gt;&lt;strong&gt;RNexML&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00247.x/abstract&#34;&gt;&lt;strong&gt;treebase&lt;/strong&gt;&lt;/a&gt; packages are all great examples of such packages. MEE editors may also refer authors to this option if authors submit an appropriate manuscript to MEE first.&lt;/p&gt;

&lt;p&gt;On submission to rOpenSci, authors can use our updated &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/issue_template.md&#34;&gt;submission template&lt;/a&gt; to choose MEE as a publication venue. Following acceptance by rOpenSci, the associated manuscript will be reviewed by an expedited process at MEE, with reviewers and editors having the knowledge that the software has already been reviewed and the public reviews available to them.&lt;/p&gt;

&lt;p&gt;Should the manuscript be accepted, a footnote will appear in the web version and the first page of the print version of the MEE article indicating that the software as well as the manuscript has been peer-reviewed, with a link to the rOpenSci open reviews.&lt;/p&gt;

&lt;p&gt;As with any collaboration, there may be a few hiccups early on and we welcome ideas to make the process more streamlined and efficient. We look forward to the community&amp;rsquo;s submissions and to your participation in this process.&lt;/p&gt;

&lt;p&gt;Many thanks to MEE&amp;rsquo;s Assistant Editor Chris Grieves and Senior Editor Bob O&amp;rsquo;Hara for working with us on this collaboration.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;See also MEE&amp;rsquo;s post from today at &lt;a href=&#34;https://methodsblog.wordpress.com/2017/11/29/software-review/&#34;&gt;https://methodsblog.wordpress.com/2017/11/29/software-review/&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>changes: easy Git-based version control from R</title>
      <link>https://ropensci.org/blog/2017/11/28/ropensci-changes/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/28/ropensci-changes/</guid>
      <description>
        
        

&lt;p&gt;Are you new to version control and always running into trouble with Git?
Or are you a seasoned user, haunted by the traumas of learning Git and reliving them whilst trying to teach it to others?
Yeah, us too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-28-ropensci-changes/monkeys.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Git is a version control tool designed for software development, and it is extraordinarily powerful. It didn’t actually dawn on me quite how amazing Git is until I spent a weekend in Melbourne with a group of Git whizzes using Git to write a package targeted toward Git beginners. Whew, talk about total Git immersion! I was taking part in the 2017 &lt;a href=&#34;https://ozunconf17.ropensci.org/&#34;&gt;rOpenSci ozunconf&lt;/a&gt;, in which forty-odd  developers, scientists, researchers, nerds, teachers, starving students, cat ladies, and R users of all descriptions form teams to create new R packages fulfilling some new and useful function. Many of the groups used Git for their collaborative workflows all weekend.&lt;/p&gt;

&lt;p&gt;Unfortunately, just like many a programming framework, Git can often be a teensy bit (read: extremely, prohibitively) intimidating, especially for beginners who don&amp;rsquo;t need all of Git&amp;rsquo;s numerous and baffling features.
It’s one of those platforms that makes your life a million times better once you know how to use it, but if you’re trying to teach yourself the basics using the internet, or—heaven forbid—trying to untangle yourself from some Git-branch tangle that you’ve unwittingly become snarled in… (definitely done that one…) well, let’s just say using your knuckles to break a brick wall can sometimes seem preferable.
Just ask the Git whizzes.
They laugh, because they’ve been there, done that.&lt;/p&gt;

&lt;p&gt;The funny thing is, doing basic version control in Git only requires a few commands.
After browsing through the available project ideas and settling into teams, a group of eight of us made a list of the commands that we use on a daily basis, and the list came to about a dozen.
We looked up our Git histories and compiled a Git vocabulary, which came out to less than 50 commands, including combination commands.&lt;/p&gt;

&lt;p&gt;As &lt;a href=&#34;https://github.com/goldingn&#34;&gt;Nick Golding&lt;/a&gt; so shrewdly recognized in the lead up to this year’s unconference, the real obstacle for new Git users is not the syntax, it&amp;rsquo;s actually (a) the scary, scary terminal window and (b) the fact that Git terminology was apparently chosen by randomly opening a verb dictionary and blindly pointing to a spot on the page.
(Ok, I’m exaggerating, but the point is that the terminology is pretty confusing).
We decided to address these two problems by making a package that uses the R console and reimagining the version control vocabulary and workflow for people who are new to version control and only need some of its many features.&lt;/p&gt;

&lt;p&gt;Somewhat ironically, nine people worked for two days on a dozen branches, using Git and GitHub to seamlessly merge our workflows.
It was wonderful to see how so many people’s various talents can be combined to make something that no group members could have done all on their own.&lt;/p&gt;

&lt;p&gt;Enter, &lt;code&gt;changes&lt;/code&gt; ( &lt;a href=&#34;https://github.com/ropenscilabs/ozrepro&#34;&gt;repo&lt;/a&gt;, &lt;a href=&#34;https://ropenscilabs.github.io/changes/&#34;&gt;website&lt;/a&gt; – made using &lt;a href=&#34;https://github.com/hadley/pkgdown&#34;&gt;pkgdown&lt;/a&gt;), our new R package to do version control with a few simple commands.
It uses Git and &lt;a href=&#34;https://cran.r-project.org/web/packages/git2r/index.html&#34;&gt;Git2r&lt;/a&gt; under the hood, but new users don’t need to know any Git to begin using version control with &lt;code&gt;changes&lt;/code&gt;.
Best of all, it works seamlessly with regular Git. So if a user thinks they&amp;rsquo;re ready to expand their horizons they can start using git commands via the &lt;a href=&#34;https://GitHub.com/jennybc/Githug&#34;&gt;Githug&lt;/a&gt; package, RStudio&amp;rsquo;s git interface, or on the command line.&lt;/p&gt;

&lt;p&gt;Here is an overview of some of the ways we’ve made simple version control easy with &lt;code&gt;changes&lt;/code&gt;:&lt;/p&gt;

&lt;h4 id=&#34;simple-terminology&#34;&gt;Simple terminology&lt;/h4&gt;

&lt;p&gt;It uses simple and deliberately un-git-like terminology:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You start a new version control project with &lt;code&gt;create_repo()&lt;/code&gt;, which is like &lt;code&gt;git init&lt;/code&gt; but it can set up a nice project directory structure for you, automatically ignoring things like output folders.&lt;/li&gt;
&lt;li&gt;All of the steps involved in commiting edits have been compressed into one function: &lt;code&gt;record()&lt;/code&gt;. All files that aren&amp;rsquo;t ignored will be committed, so users don&amp;rsquo;t need to know the difference between tracking, staging and committing files.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s easy to set which files to omit from version control with &lt;code&gt;ignore()&lt;/code&gt;, and to change your mind with &lt;code&gt;unignore()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;changes()&lt;/code&gt; lets you know which files have changed since the last record, like a hybrid of &lt;code&gt;git status&lt;/code&gt; and &lt;code&gt;git diff&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;You can look back in history with &lt;code&gt;timeline()&lt;/code&gt; (a simplified version of &lt;code&gt;git log&lt;/code&gt;), &lt;code&gt;go_to()&lt;/code&gt; a previous record (like &lt;code&gt;git checkout&lt;/code&gt;), and &lt;code&gt;scrub()&lt;/code&gt; any unwanted changes since the last record (like &lt;code&gt;git reset --hard&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;it-s-linear&#34;&gt;It&amp;rsquo;s linear&lt;/h4&gt;

&lt;p&gt;After a long discussion, we decided that changes won&amp;rsquo;t provide an interface to Git branches (at least not yet), as the merge conflicts it leads to are one of the scariest things about version control for beginners.
With linear version control, users can can easily &lt;code&gt;go_to()&lt;/code&gt; a past record with a version number, rather than unfamiliar &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Internals-Git-Objects&#34;&gt;SHA&amp;rsquo;s&lt;/a&gt;. These numbers appear in the a lovely visual representation of their &lt;code&gt;timeline()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      (1) initial commit
       |  2017-11-18 02:55
       |
      (2) set up project structure
       |  2017-11-18 02:55
       |
      (3) added stuff to readme
          2017-11-18 02:55
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to roll your project back to a previous record, you can &lt;code&gt;retrieve()&lt;/code&gt; it, and changes will simply append that record at the top of your timeline (storing all the later records, just in case).&lt;/p&gt;

&lt;h4 id=&#34;readable-messages-and-automatic-reminders&#34;&gt;Readable messages and automatic reminders&lt;/h4&gt;

&lt;p&gt;Some of Git&amp;rsquo;s messages and helpfiles are totally cryptic to all but the most hardened computer scientists.
Having been confronted with our fair share of &lt;a href=&#34;https://www.git-tower.com/learn/git/faq/detached-head-when-checkout-commit&#34;&gt;&lt;code&gt;detached HEAD&lt;/code&gt;&lt;/a&gt;s and offers to &lt;a href=&#34;https://git-scm.com/docs/git-push&#34;&gt;&lt;code&gt;update remote refs along with associated objects&lt;/code&gt;&lt;/a&gt;, we were keen to make sure all the error messages and helpfiles in changes are as intuitive and understandable as possible.&lt;/p&gt;

&lt;p&gt;It can also be hard to get into the swing of recording edits, so changes will give you reminders to encourage you to use &lt;code&gt;record()&lt;/code&gt; regularly. You can change the time interval for reminders, or switch them off, using &lt;code&gt;remind_me()&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;coming-soon&#34;&gt;Coming soon&lt;/h4&gt;

&lt;p&gt;We made a lot of progress in two days, but there&amp;rsquo;s plenty more we&amp;rsquo;re planning to add soon:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Simplified access to GitHub with a &lt;code&gt;sync()&lt;/code&gt; command to automagically handle most uses of &lt;code&gt;git fetch&lt;/code&gt;, &lt;code&gt;git merge&lt;/code&gt;, and &lt;code&gt;git push&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A Git training-wheels mode, so that people who want to move use Git can view the Git commands &lt;code&gt;changes&lt;/code&gt; is using under the hood.&lt;/li&gt;
&lt;li&gt;Added flexibility – we are working on adding functionality to handle simple deviations from the defaults, such as recording changes only to named files, or to all except some excluded files.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;rsquo;d be really keen to hear your suggestions too, so please let us know your ideas via the &lt;a href=&#34;https://github.com/ropenscilabs/changes/issues&#34;&gt;changes issue tracker&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;I have only recently started using Git and GitHub, and this year’s &lt;a href=&#34;https://ozunconf17.ropensci.org/&#34;&gt;rOpenSci ozunconf&lt;/a&gt; was a big eye-opener for me, in several ways.
Beyond finally understanding to power of proper version control, I met a group of wonderful people dedicated to participating in the R community.
Now as it turns out, R users take the word “community” very seriously.
Each and every person I met during the event was open and friendly.
Each person had ideas for attracting new users to R, making it easier to learn, making methods and data more readily available, and creating innovative new functionality.
Even before the workshop began, dozens of ideas for advancement circulated on &lt;a href=&#34;https://github.com/ropensci/ozunconf17/issues&#34;&gt;GitHub Issues&lt;/a&gt;.
Throughout the conference, it was a pleasure to be a part of the ongoing conversation and dialogue about growing and improving the R community.
That’s right, you can delete any lingering ‘introverted computer geek’ stereotypes you might still be harbouring in a cobwebbed attic of your mind.
In today’s day and age, programming is as much about helping each other, communicating, learning, and networking as it is about solving problems.
And building the community is a group effort.&lt;/p&gt;

&lt;p&gt;R users come from all sorts of backgrounds, but I was gratified to see scientists and researchers well-represented at the unconference.
Gone are the days when I need to feel like the ugly duckling for being the only R user in my biology lab!
If you still find yourself isolated, join the blooming online R users community, or any one of a number of meetups and clubs that are popping up everywhere.
I have dipped my toe in those waters, and boy am I glad I did!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>ochRe - Australia themed colour palettes</title>
      <link>https://ropensci.org/blog/2017/11/21/ochre/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/21/ochre/</guid>
      <description>
        
        

&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;The second rOpenSci &lt;a href=&#34;http://ozunconf17.ropensci.org/&#34;&gt;OzUnConf&lt;/a&gt; was held in Melbourne Australia a few weeks ago. A diverse range of &lt;a href=&#34;https://ropensci.org/blog/2017/10/31/ozunconf2017/&#34;&gt;scientists, developers and general good-eggs&lt;/a&gt; came together to make some R-magic happen and also learn a lot along the way. Before the conference began, a huge stack of projects were suggested on the unconf  &lt;a href=&#34;https://github.com/ropensci/ozunconf17/issues&#34;&gt;GitHub repo&lt;/a&gt;. For six data-visualisation enthusiasts, one issue in particular caught their eye, and the &lt;code&gt;ochRe&lt;/code&gt; package was born.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/AusElevationExamplePalettes.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/ropenscilabs/ochRe&#34;&gt;&lt;code&gt;ochRe&lt;/code&gt;&lt;/a&gt; package contains colour palettes influenced by the Australian landscape, iconic Australian artists and images. OchRe is originally the brain-child of &lt;a href=&#34;https://twitter.com/visnut&#34;&gt;Di Cook&lt;/a&gt;, who was inspired by Karthik Ram&amp;rsquo;s &lt;a href=&#34;https://github.com/karthik/wesanderson&#34;&gt;&lt;code&gt;wesanderson&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;h3 id=&#34;why-ochre&#34;&gt;Why &amp;ldquo;ochre&amp;rdquo;?&lt;/h3&gt;

&lt;p&gt;Naming our package was the &amp;ldquo;most important&amp;rdquo; task facing us after all jumping on board the project. Fuelled by a selection of pastries we opened the discussions, fully expecting this to take some time. Fortunately, we all agreed on the name in less than 5 minutes, which meant plenty of pastries were left for the serious business of package building.&lt;/p&gt;

&lt;p&gt;Ochre is naturally occuring, brownish-yellow &lt;a href=&#34;https://en.wikipedia.org/wiki/Ochre&#34;&gt;pigment&lt;/a&gt; found in many parts of Australia, so frequently in fact, that Australia is sometimes referred to as the &amp;ldquo;land of ochre soil&amp;rdquo;. Additionally, ochre pigment has been used for thousands of years by Aboriginal people in Australia, with many culturally important uses from artwork to the preservation of animal skins.&lt;/p&gt;

&lt;h3 id=&#34;building-ochre&#34;&gt;Building ochRe&lt;/h3&gt;

&lt;p&gt;We started our package building journey by each picking an iconic Australian artwork (this took longer than you might think). Once we had selected our images, we used the online &lt;a href=&#34;http://www.coolphptools.com/color_extract&#34;&gt;Image Color Extract PHP&lt;/a&gt; demo tool to extract the hex code for the main colours within each image. Some images required a more selective approach, so where needed the colour code extraction was done using the eyedropper tool (in macOS) or the Google Chrome colourPick extension.&lt;/p&gt;

&lt;p&gt;Once we were happy with the colours, codes and order for each palette we loaded this information into &lt;code&gt;ochRe&lt;/code&gt; as lists of hex codes associated with the palette name. We adopted &lt;code&gt;scales&lt;/code&gt; to improve the fucntionality of the packages when using &lt;code&gt;ggplot&lt;/code&gt;, in particular to allow manipulation of colour ramping and transparency. The package also contains a few simple functions for displaying the different palettes.&lt;/p&gt;

&lt;p&gt;Below are some examples of original art work and their associated palettes:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;namatjira_qual&lt;/code&gt; and &lt;code&gt;namatjira_div&lt;/code&gt; are both inspired by the watercolour painting &lt;a href=&#34;http://www.menziesartbrands.com/items/twin-ghosts&#34;&gt;&amp;ldquo;Twin Ghosts&amp;rdquo;&lt;/a&gt;, by Aboriginal artist Albert Namatjira. &lt;code&gt;namatjira_div&lt;/code&gt; is ordered for plotting divergent datasets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/TwinGhosts_AlbertNamatjira.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/namatjira_qual.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;nolan_ned&lt;/code&gt; palette is inspired by the famous paintings of the outlaw &lt;a href=&#34;https://cs.nga.gov.au/detail.cfm?irn=28926&#34;&gt;Ned Kelly&lt;/a&gt; by Sidney Nolan.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/nedKelly_sidneyNolan.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/nolan_ned.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;olsen_seq&lt;/code&gt; has been designed for plotting sequential data, such as a heat map or landscape layers. The colours come from the abstract piece &lt;a href=&#34;https://artsearch.nga.gov.au/Detail-LRG.cfm?IRN=26102&#34;&gt;&amp;ldquo;Sydney Sun&amp;rdquo;&lt;/a&gt;, 1965, by John Olsen.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/sydneySun_johnOlsen.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/olsen_seq.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There was a high proportion of ecologists at the #ozunconf, which inspired the somewhat pessimistic &lt;code&gt;healthy_reef&lt;/code&gt; and &lt;code&gt;dead_reef&lt;/code&gt; palettes, with the colours taken from recent underwater photgraphs of the Great Barrier Reef.&lt;/p&gt;

&lt;h3 id=&#34;introducing-ochre&#34;&gt;Introducing: &lt;code&gt;ochRe&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;Our package currently contains 16 colour palettes, each one inspired by either an Australian landscape, an artwork or image by an Australian artist, or an Australian animal. Some of the palettes are more suited to displaying continuous data (such as in the Australian elevation maps above). Other palettes will perform best plotting discrete data (as in the &lt;code&gt;parliament&lt;/code&gt; example below).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ochRe&lt;/code&gt; can be currently be installed from GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# You need to install the &#39;devtools&#39; package first
devtools::install_github(&amp;quot;ropenscilabs/ochRe&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can visualise all 16 palettes using the following code snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pal_names &amp;lt;- names(ochre_palettes)

par(mfrow=c(length(ochre_palettes)/2, 2), lheight = 2, mar=rep(1, 4), adj = 0)
for (i in 1:length(ochre_palettes)){
    viz_palette(ochre_palettes[[i]], pal_names[i])
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/ochrePalettes.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here are some worked examples, showing how to use the palettes for different types of data visualisation, including both &lt;code&gt;ggplot&lt;/code&gt; and base plotting in R.&lt;/p&gt;

&lt;p&gt;An example using base R and the &lt;code&gt;winmar&lt;/code&gt; palette, this is based on an iconic photograph by Wayne Ludbey, &amp;ldquo;Nicky Winmar St Kilda Footballer&amp;rdquo;, 1993. In the photo, Aboriginal AFL player Nicky Winmar is baring his skin in response to racial abuse during an AFL game.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## basic example code
pal &amp;lt;- colorRampPalette(ochre_palettes[[&amp;quot;winmar&amp;quot;]])
image(volcano, col = pal(20))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/VolcanoWithWinmar.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Paired scatter plot using the &lt;code&gt;emu_Woman_paired&lt;/code&gt; palette, inspired by &amp;ldquo;Emu Woman&amp;rdquo;, 1988-89, by Emily Kame Kngwarreye.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(ochRe)
library(naniar)

# Exploring missing values benefits from a paired palette, like the emu women
# Here missing status on air temperature is shown in a plot of the two wind variables
data(oceanbuoys)
oceanbuoys &amp;lt;- oceanbuoys %&amp;gt;% add_shadow(humidity, air_temp_c) 
ggplot(oceanbuoys, aes(x=wind_ew, y=wind_ns, colour=air_temp_c_NA)) + 
    geom_point(alpha=0.8) + 
    scale_colour_ochre(palette=&amp;quot;emu_woman_paired&amp;quot;) +
    theme_bw() + theme(aspect.ratio=1)

# Slightly more complicated, forcing the pairs
clrs &amp;lt;- ochre_palettes$emu_woman_paired[11:12]
ggplot(oceanbuoys, aes(x=wind_ew, y=wind_ns, colour=air_temp_c_NA)) + 
    geom_point(alpha=0.8) + 
    scale_colour_manual(values=clrs) +
    theme_bw() + theme(aspect.ratio=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/Ochre_Example1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A map of the Australian electoral boundaries, using the &lt;code&gt;galah&lt;/code&gt; palette. &lt;a href=&#34;https://en.wikipedia.org/wiki/Galah&#34;&gt;Galahs&lt;/a&gt; are a common species of cockatoo found throughout mainland Australia.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Map of the 2016 Australian electoral boundaries
# with the galah palette
library(eechidna)
library(ggthemes)
data(nat_map_2016)
data(nat_data_2016)
ggplot(aes(map_id=id), data=nat_data_2016) +
    geom_map(aes(fill=Area_SqKm), map=nat_map_2016) +
    expand_limits(x=nat_map$long, y=nat_map$lat) + 
    scale_fill_ochre(palette=&amp;quot;galah&amp;quot;, discrete=FALSE) +
    theme_map()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/Ochre_Example2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Results of the 2016 Australian election to the senate, coloured by political party using the &lt;code&gt;parliament&lt;/code&gt; palette. The colours for this palette were taken from the &lt;a href=&#34;https://www.aph.gov.au/~/media/06%20Visit%20Parliament/66%20Parl%20House%20Art%20Collection/661%20five%20treasures/five%20treasures%20detail%20pics/M19840057UntitledBOYDunframed.png?la=en&#34;&gt;tapestry&lt;/a&gt; by Arthur Boyd found in the Great Hall of Parliament House.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Election results
senate &amp;lt;- read_csv(&amp;quot;http://results.aec.gov.au/20499/Website/Downloads/SenateSenatorsElectedDownload-20499.csv&amp;quot;, 
                   skip = 1)
coalition &amp;lt;- c(&amp;quot;Country Liberals (NT)&amp;quot;, &amp;quot;Liberal&amp;quot;, &amp;quot;Liberal National Party of Queensland&amp;quot;, 
               &amp;quot;The Nationals&amp;quot;)
labor &amp;lt;- c(&amp;quot;Australian Labor Party&amp;quot;, &amp;quot;Australian Labor Party (Northern Territory) Branch&amp;quot;, 
           &amp;quot;Labor&amp;quot;)
greens &amp;lt;- c(&amp;quot;The Greens&amp;quot;, &amp;quot;Australian Greens&amp;quot;, &amp;quot;The Greens (WA)&amp;quot;)

senate &amp;lt;- senate %&amp;gt;% mutate(PartyNm = ifelse(as.character(PartyNm) %in% coalition, 
                                             &amp;quot;Liberal National Coalition&amp;quot;, PartyNm))

senate &amp;lt;- senate %&amp;gt;% mutate(PartyNm = ifelse(as.character(PartyNm) %in% labor, 
                                             &amp;quot;Australian Labor Party&amp;quot;, PartyNm))

senate &amp;lt;- senate %&amp;gt;% mutate(PartyNm = ifelse(as.character(PartyNm) %in% greens, 
                                             &amp;quot;Australian Greens&amp;quot;, PartyNm))

senate$PartyNm &amp;lt;- factor(senate$PartyNm, 
                         levels = names(sort(table(senate$PartyNm), 
                            decreasing = T)))

ggplot(data = senate, aes(x = PartyNm, fill = PartyNm)) + 
    geom_bar() + xlab(&amp;quot;&amp;quot;) + 
    ylab(&amp;quot;&amp;quot;) + scale_fill_ochre(palette=&amp;quot;parliament&amp;quot;) + coord_flip() + 
    theme_bw() + theme(legend.position = &amp;quot;None&amp;quot;) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/Ochre_Example3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For more information about the individual palettes available in &lt;code&gt;ochRe&lt;/code&gt; visit our &lt;a href=&#34;https://github.com/ropenscilabs/ochRe/tree/master/vignettes&#34;&gt;vignette&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All of the &lt;code&gt;ochRe&lt;/code&gt; team had a great time at #ozunconf, Thank you to the organisers for a brilliant event. Special Thanks to &lt;a href=&#34;https://github.com/mdsumner&#34;&gt;Michael Sumner&lt;/a&gt; for providing code to access the Australian elevation map you see at the start of this post.&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The Oz colour palette gang soakin it up outside &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; &lt;a href=&#34;https://t.co/XiLkhwZwTv&#34;&gt;pic.twitter.com/XiLkhwZwTv&lt;/a&gt;&lt;/p&gt;&amp;mdash; Miles McBain (@MilesMcBain) &lt;a href=&#34;https://twitter.com/MilesMcBain/status/923682409400250368?ref_src=twsrc%5Etfw&#34;&gt;October 26, 2017&lt;/a&gt;&lt;/blockquote&gt;

      </description>
    </item>
    
    <item>
      <title>Six tips for running a successful unconference</title>
      <link>https://ropensci.org/blog/2017/11/17/unconf-sixtips/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/17/unconf-sixtips/</guid>
      <description>
        
        

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-17-unconf-sixtips/ropensci-unconf17-community-nistara-randawa.jpg&#34; alt=&#34;Attendees at the May 2017 rOpenSci unconference. Photo credit: Nistara Randhawa&#34;&gt;
&lt;em&gt;Attendees at the May 2017 rOpenSci unconference. Photo credit: Nistara Randhawa&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In May 2017, I helped run a wildly successful “unconference” that had a huge positive impact on the community I serve. rOpenSci is a non-profit initiative enabling open and reproducible research by creating technical infrastructure in the form of staff- and community-contributed software tools in the R programming language that lower barriers to working with scientific data sources on the web, and creating social infrastructure through a welcoming and diverse community of software users and developers. Our 4th annual unconference brought together 70 people to hack on projects they dreamed up and to give them opportunities to meet and work together in person. One third of the participants had attended before, and two thirds were first-timers, selected from an open call for applications. We paid all costs up front for anyone who requested this in order to lower barriers to participation.&lt;/p&gt;

&lt;p&gt;It’s called an “unconference” because there is no schedule set before the event – participants discuss project ideas online in advance and projects are selected by participant-voting at the start. I’m sharing some tips here on how to do this well for this particular flavour of unconference.&lt;/p&gt;

&lt;h4 id=&#34;1-have-a-code-of-conduct&#34;&gt;1. Have a code of conduct&lt;/h4&gt;

&lt;p&gt;Having a &lt;a href=&#34;https://ropensci.org/blog/blog/2016/12/21/commcallv12-review-coc&#34;&gt;code of conduct&lt;/a&gt; that the organizers promote in the welcome goes a long way to creating a welcoming and safe environment and preventing violations in the first place.&lt;/p&gt;

&lt;h4 id=&#34;2-host-online-discussion-of-project-ideas-before-the-unconference&#34;&gt;2. Host online discussion of project ideas before the unconference&lt;/h4&gt;

&lt;p&gt;Our unconference centered on teams working on programming projects, rather than discussions, so prior to the unconference, we asked all participants to &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/&#34;&gt;suggest project ideas&lt;/a&gt; using an open online system, called GitHub, that allows everyone to see and comment on ideas or just share enthusiastic emoji to show support.&lt;/p&gt;

&lt;h4 id=&#34;3-have-a-pre-unconference-video-chat-with-first-time-participants&#34;&gt;3. Have a pre-unconference video-chat with first-time participants&lt;/h4&gt;

&lt;p&gt;Our AAAS CEFP training emphasizes the importance of extending a personalized welcome to community members, so I was inspired to make the bold move of talking with more than 40 first-time participants prior to the unconference. I asked each person to complete a short questionnaire to get them to consider the roles they anticipated playing prior to our chat. Frequently, within an hour of a video-chat, without prompting, I would see the person post a new project idea or share their thoughts about someone else’s. Other times, our conversation gave the person an opportunity to say “this idea maybe isn’t relevant but…” and I would help them talk through it, inevitably leading to “oh my gosh this is such a cool idea”. I got a huge return on my investment. People’s questions like “how do you plan to have 20 different projects present their work?” led to better planning on my part. Specific ideas for improvements came from me responding with “well…how would YOU do it?”&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Between the emails, slack channel, issues on GitHub, and personal video chats, I felt completely at ease going into the unconf (where I knew next to no one!).&lt;/p&gt;

&lt;p&gt;-rOpenSci unconf17 participant&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;4-run-an-effective-ice-breaker&#34;&gt;4. Run an effective ice breaker&lt;/h4&gt;

&lt;p&gt;I adapted the “&lt;a href=&#34;https://www.facinghistory.org/resource-library/teaching-strategies/barometer-taking-stand-controversial-issues&#34;&gt;Human Barometer&lt;/a&gt;” ice breaker to enable 70 people to share opinions across all perceived levels of who a participant is and introduce themselves to the entire group within a 1 hour period. Success depended on creating questions that were relevant to the unconference crowd, and on visually keeping track of who had spoken up in order to call on those who had not. Ice breakers and the rOpenSci version of the Human Barometer will be the subject of a future CEFP blog post.&lt;/p&gt;

&lt;h4 id=&#34;5-have-a-plan-to-capture-content&#34;&gt;5. Have a plan to capture content&lt;/h4&gt;

&lt;p&gt;So much work and money go into running a great unconference, you can’t afford to do it without a plan to capture and disseminate stories about the people and the products. Harness the brain work that went into the ideas! I used the concept of &lt;a href=&#34;http://www.socialfish.org/2016/11/you-have-more-content-than-you-realize/&#34;&gt;content pillars&lt;/a&gt; to come up with a plan. Every project group was given a public repository on GitHub to house their code and documentation. In a 2-day unconference with 70 people in 20 projects, how do people present their results?! We told everyone that they had just three minutes to present, and that the only presentation material they could use was their project README (the page of documentation in their code repository). No slides allowed! This kept their focus on great documentation for the project rather than an ephemeral set of pretty slides. Practically speaking, this meant that all presentations were accessible from a single laptop connected to the projector and that to access their presentation, a speaker had only to &lt;a href=&#34;https://ropenscilabs.github.io/runconf17-projects/&#34;&gt;click on the link to their repo&lt;/a&gt;. Where did the essence of this great idea come from? From a pre-unconference chat of course!&lt;/p&gt;

&lt;p&gt;In the week following the unconference, we used the projects’ README documentation to create a series of five posts released Monday through Friday, noting every one of the 20 projects with links to their code repositories. To get more in-depth stories about people and projects, I let participants know we were keen to host community-contributed blog posts and that accepted posts would be tweeted to rOpenSci’s &amp;gt;13,000 followers. Immediately after the unconference, we invited selected projects to contribute longer form narrative blog posts and posted these once a week. The series ended with &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/08/unconfroadsnottaken&#34;&gt;Unconf 2017: The Roads Not Taken&lt;/a&gt;, about all the great ideas that were not yet pursued and inviting people to contribute to these.&lt;/p&gt;

&lt;p&gt;All of this content is tied together in &lt;a href=&#34;https://ropensci.org/blog/blog/2017/06/02/unconf2017&#34;&gt;one blog post to summarize&lt;/a&gt; the unconference and link to all staff- and community-contributed posts in the unconference projects series as well as to posts of warm personal and career-focussed reflections by some participants.&lt;/p&gt;

&lt;h4 id=&#34;6-care-about-other-people-s-success&#34;&gt;6. Care about other people’s success&lt;/h4&gt;

&lt;p&gt;Community managers do a lot of “emotional work”. In all of this, my #1 key to running a successful unconference is to genuinely and deeply care that participants arrive feeling ready to hit the ground running and leave feeling like they got what they wanted out of the experience. Ultimately, the success of our unconference is more about the human legacy – building people’s capacity as software users and developers, and the in-person relationships they develop within our online community – than it is about the projects.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“I’ve steered clear of ‘hackathons’ because they feel intimidating and the ‘bad’ kind of competitive. But, this….this was something totally different.”&lt;/p&gt;

&lt;p&gt;– rOpenSci unconf17 participant&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Additional Resources&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.trelliscience.com/introducing-the-2017-community-engagement-fellows/&#34;&gt;AAAS Community Engagement Fellows Program&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://socialinsilico.wordpress.com/2014/11/07/all-together-now-event-formats-for-more-practical-sessions/&#34;&gt;All together now: Event formats for more practical sessions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Budd A, Dinkel H, Corpas M, Fuller JC, Rubinat L, Devos DP, et al. (2015) Ten Simple Rules for Organizing an Unconference. PLoS Comput Biol11(1): e1003905. &lt;a href=&#34;https://doi.org/10.1371/journal.pcbi.1003905&#34;&gt;https://doi.org/10.1371/journal.pcbi.1003905&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>2017 rOpenSci ozunconf :: Reflections and the realtime Package</title>
      <link>https://ropensci.org/blog/2017/11/14/realtime/</link>
      <pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/14/realtime/</guid>
      <description>
        
        &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This year&amp;rsquo;s &lt;a href=&#34;https://ozunconf17.ropensci.org/&#34;&gt;rOpenSci ozunconf&lt;/a&gt; was held in Melbourne, bringing together over 45 R enthusiasts from around the country and beyond. As is customary, ideas for projects were discussed in &lt;a href=&#34;https://github.com/ropensci/ozunconf17/issues&#34;&gt;GitHub Issues&lt;/a&gt; (41 of them by the time the unconf rolled around!) and there was no shortage of enthusiasm, interesting concepts, and varied experience.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been to a few unconfs now and I treasure the time I get to spend with new people, new ideas, new backgrounds, new approaches, and new insights. That&amp;rsquo;s not to take away from the time I get to spend with people I met at previous unconfs; I&amp;rsquo;ve gained great friendships and started collaborations on side projects with these wonderful people.&lt;/p&gt;

&lt;p&gt;When the call for nominations came around this year it was an easy decision. I don&amp;rsquo;t have employer support to attend these things so I take time off work and pay my own way. This is my networking time, my development time, and my skill-building time. I wasn&amp;rsquo;t sure what sort of project I&amp;rsquo;d be interested in but I had no doubts something would come up that sounded interesting.&lt;/p&gt;

&lt;p&gt;As it happened, I had been playing around with a bit of code, purely out of interest and hoping to learn how &lt;a href=&#34;https://www.htmlwidgets.org/&#34;&gt;&lt;code&gt;htmlwidgets&lt;/code&gt;&lt;/a&gt; work. The idea I had was to make a classic graphic equaliser visualisation like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/graphiceq.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;using R.&lt;/p&gt;

&lt;p&gt;This presents several challenges; how can I get live audio into R, and how fast can I plot the signal? I had doubts about both parts, partly because of the way that R calls tie up the session (&lt;a href=&#34;https://appsilondatascience.com/blog/rstats/2017/11/01/r-promises-hands-on.html&#34;&gt;for now&amp;hellip;&lt;/a&gt;) and partly because constructing a &lt;code&gt;ggplot2&lt;/code&gt; object is somewhat slow (in terms of raw audio speeds). I&amp;rsquo;d heard about &lt;code&gt;htmlwidgets&lt;/code&gt; and thought there must be a way to leverage that towards my goal.&lt;/p&gt;

&lt;p&gt;I searched for a graphic equaliser javascript library to work with and didn&amp;rsquo;t find much that aligned with what I had in my head. Eventually I stumbled on &lt;a href=&#34;https://p5js.org/&#34;&gt;&lt;code&gt;p5.js&lt;/code&gt;&lt;/a&gt; and its examples page which has an &lt;a href=&#34;https://p5js.org/examples/sound-frequency-spectrum.html&#34;&gt;audio-input plot with a live demo&lt;/a&gt;. It&amp;rsquo;s a frequency spectrum, but I figured that&amp;rsquo;s just a bit of binning away from what I need. Running the example there looks like&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/p5sound_optimised.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This seemed to be worth a go. I managed to follow enough of &lt;a href=&#34;https://www.htmlwidgets.org/develop_intro.html&#34;&gt;this tutorial&lt;/a&gt; to have the library called from R. I modified the javascript canvas code to look a little more familiar, and the first iteration of &lt;code&gt;geom_realtime()&lt;/code&gt; was born&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/3mdiCUbgxi0&#34; frameborder=&#34;0&#34; gesture=&#34;media&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;This seemed like enough of an idea that I proposed it in the GitHub Issues for the unconf. It got a bit of attention, which was worrying, because I had no idea what to do with this next. &lt;a href=&#34;https://github.com/petehaitch&#34;&gt;Peter Hickey&lt;/a&gt; pointed out that &lt;a href=&#34;https://github.com/seankross&#34;&gt;Sean Kross&lt;/a&gt; had &lt;a href=&#34;https://seankross.com/2017/08/11/Beyond-Axes-Simulating-Systems-with-Interactive-Graphics.html&#34;&gt;already wrapped some of the &lt;code&gt;p5.js&lt;/code&gt; calls into R calls&lt;/a&gt; with his &lt;code&gt;p5&lt;/code&gt; package, so this seemed like a great place to start. It&amp;rsquo;s quite a clever way of doing it too; it involves re-writing the &lt;code&gt;javascript&lt;/code&gt; which &lt;code&gt;htmlwidgets&lt;/code&gt; calls on each time you want to do something.&lt;/p&gt;

&lt;p&gt;Fast forward to the unconf and a decent number of people gathered around a little slip of paper with &lt;code&gt;geom_realtime()&lt;/code&gt; written on it. I had to admit to everyone that the &lt;code&gt;ggplot2&lt;/code&gt; aspect of my demo was a sham (it&amp;rsquo;s surprisingly easy to draw a canvas in just the right shade of grey with white gridlines), but people stayed, and we got to work seeing what else we could do with the idea. We came up with some suggestions for input sources, some different plot types we might like to support, and set about trying to understand what Sean&amp;rsquo;s package actually did.&lt;/p&gt;

&lt;p&gt;As it tends to work out, we had a great mix of people with different experience levels in different aspects of the project; some who knew how to make a package, some who knew how to work with &lt;code&gt;javascript&lt;/code&gt;, some who knew how to work with &lt;code&gt;websockets&lt;/code&gt;, some who knew about realtime data sources, and some who knew about nearly none of these things (✋ that would be me). If everyone knew every aspect about how to go about an unconf project I suspect the endeavor would be a bit boring. I love these events because I get to learn so much about so many different topics.&lt;/p&gt;

&lt;p&gt;I shared my demo script and we deconstructed the pieces. We dug into the inner workings of the &lt;code&gt;p5&lt;/code&gt; package and started determining which parts we could siphon off to meet our own needs. One of the aspects that we wanted to figure out was how to simulate realtime data. This could be useful both for testing, and also in the situation where one might want to &amp;rsquo;re-cast&amp;rsquo; some time-coded data. We were thankful that &lt;a href=&#34;https://github.com/kcf-jackson&#34;&gt;Jackson Kwok&lt;/a&gt; had gone deep-dive into &lt;code&gt;websockets&lt;/code&gt; and pretty soon (surprisingly soon, perhaps; within the first day) we had examples of (albeit, constructed) real-time (every 100ms) data streaming from a server and being plotted at-speed&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/realtime_runif_optimised.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Best of all, running the plot code didn&amp;rsquo;t tie up the session; it uses a listener written into the &lt;code&gt;javascript&lt;/code&gt; so it just waits for input on a particular port.&lt;/p&gt;

&lt;p&gt;With the core goal well underway, people started branching out into aspects they found most interesting. We had some people work on finding and connecting actual data sources, such as the bitcoin exchange rate&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/realtime_btc_optimised.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and a live-stream of binary-encoded data from the &lt;a href=&#34;http://qrng.anu.edu.au/index.php&#34;&gt;Australian National University (ANU) Quantum Random Numbers Server&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/realtime_bin_optimised.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Others formalised the code so that it can be piped into different &amp;lsquo;themes&amp;rsquo;, and retain the &lt;code&gt;p5&lt;/code&gt; structure for adding more components&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/realtime_bw.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These were still toy examples of course, but they highlight what&amp;rsquo;s possible. They were each constructed using an offshoot of the &lt;code&gt;p5&lt;/code&gt; package whereby the &lt;code&gt;javascript&lt;/code&gt; is re-written to include various features each time the plot is generated.&lt;/p&gt;

&lt;p&gt;Another route we took is to use the direct &lt;code&gt;javascript&lt;/code&gt; binding API with factory functions. This had less flexibility in terms of adding modular components, but meant that the &lt;code&gt;javascript&lt;/code&gt; could be modified without worrying about how it needed to interact with &lt;code&gt;p5&lt;/code&gt; so much. This resulted in some outstanding features such as side-scrolling and date-time stamps. We also managed to pipe the data off to another thread for additional processing (in R) before being sent to the plot.&lt;/p&gt;

&lt;p&gt;The example we ended up with reads the live-feed of Twitter posts under a given hashtag, computes a sentiment analysis on the words with R, and live-plots the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/auspol.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Overall I was amazed at the progress we made over just two days. Starting from a silly idea/demo, we built a package which can plot realtime data, and can even serve up some data to be plotted. I have no expectations that this will be the way of the future, but it&amp;rsquo;s been a fantastic learning experience for me (and hopefully others too). It&amp;rsquo;s highlighted that there&amp;rsquo;s ways to achieve realtime plots, even if we&amp;rsquo;ve used a library built for drawing rather than one built for plotting per se.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s even inspired offshoots in the form of some R packages;  &lt;a href=&#34;https://github.com/ropenscilabs/tRainspotting&#34;&gt;&lt;code&gt;tRainspotting&lt;/code&gt;&lt;/a&gt; which shows realtime data on New South Wales public transport using &lt;code&gt;leaflet&lt;/code&gt; as the canvas&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/tRainspotting.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and &lt;a href=&#34;https://github.com/kcf-jackson/jsReact/&#34;&gt;&lt;code&gt;jsReact&lt;/code&gt;&lt;/a&gt; which explores the interaction between R and Javascript&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/kcf-jackson/jsReact/raw/master/inst/example_5.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/kcf-jackson/jsReact/raw/master/inst/example_6.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The possibilities are truly astounding. My list of &amp;lsquo;things to learn&amp;rsquo; has grown significantly since the unconf, and projects are still starting up/continuing to develop. The &lt;a href=&#34;https://github.com/jonocarroll/ggeasy&#34;&gt;&lt;code&gt;ggeasy&lt;/code&gt;&lt;/a&gt; package isn&amp;rsquo;t related, but it was spawned from another unconf Github Issue idea. Again; ideas and collaborations starting and developing.&lt;/p&gt;

&lt;p&gt;I had a great time at the unconf, and I can&amp;rsquo;t wait until the next one. My hand will be going up to help out, attend, and help start something new.&lt;/p&gt;

&lt;p&gt;My thanks and congratulations go out to each of the &lt;a href=&#34;https://github.com/ropenscilabs/realtime&#34;&gt;&lt;code&gt;realtime&lt;/code&gt;&lt;/a&gt; developers: &lt;a href=&#34;https://github.com/richardbeare&#34;&gt;Richard Beare&lt;/a&gt;, &lt;a href=&#34;https://github.com/jonocarroll&#34;&gt;Jonathan Carroll&lt;/a&gt;, &lt;a href=&#34;https://github.com/kimnewzealand&#34;&gt;Kim Fitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/softloud&#34;&gt;Charles Gray&lt;/a&gt;, &lt;a href=&#34;https://github.com/jeffreyhanson&#34;&gt;Jeffrey O Hanson&lt;/a&gt;, &lt;a href=&#34;https://github.com/holtzy&#34;&gt;Yan Holtz&lt;/a&gt;, &lt;a href=&#34;https://github.com/kcf-jackson&#34;&gt;Jackson Kwok&lt;/a&gt;, &lt;a href=&#34;https://github.com/MilesMcBain&#34;&gt;Miles McBain&lt;/a&gt; and the entire cohort of &lt;a href=&#34;https://ozunconf17.ropensci.org&#34;&gt;2017 rOpenSci ozunconf attendees&lt;/a&gt;. In particular, my thanks go to the organisers of such a wonderful event; &lt;a href=&#34;https://github.com/njtierney&#34;&gt;Nick Tierney&lt;/a&gt;, &lt;a href=&#34;https://github.com/robjhyndman&#34;&gt;Rob Hyndman&lt;/a&gt;, &lt;a href=&#34;https://github.com/dicook&#34;&gt;Di Cook&lt;/a&gt;, and &lt;a href=&#34;https://github.com/MilesMcBain&#34;&gt;Miles McBain&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>.rprofile: Mara Averick</title>
      <link>https://ropensci.org/blog/2017/11/10/rprofile-mara-averick/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/10/rprofile-mara-averick/</guid>
      <description>
        
        &lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-10-rprofile-mara-averick/mara_averick.png&#34; alt=&#34;Mara Averick, Data Nerd At Large&#34; style=&#34;margin: 0px 20px; width: 250px;&#34; align=&#34;left&#34;&gt;
&lt;em&gt;Mara Averick is a non-profit data nerd, NBA stats junkie, and most recently, tidyverse developer advocate at RStudio. She is the voice behind two very popular Twitter accounts, @dataandme and @batpigandme. Mara and I discussed sports analytics, how attending a cool conference can change the approach to your career, and how she uses Twitter as a mechanism for self-imposed forced learning.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;KO: What is your name, job title, and how long have you been using R? [Note: This interview took place in May 2017. Mara joined RStudio as their tidyverse developer advocate in November 2017.]&lt;/p&gt;

&lt;p&gt;MA: My name is Mara Averick, I do consulting, data science, I just say “data nerd at large” because I’ve seen those Venn diagrams and I’m definitely not a data scientist. I used R in high school for fantasy basketball. I graduated from high school in 2003, and then in college used SPSS, and I didn’t use R for a long time. And then I was working with a company that does grant proposals for non-profits, doing all of the demand- and outcome-analysis and it all was in Excel and I thought, we could do better - R might also be helpful for this. It turns out there’s a package for American Community Survey data in R (&lt;a href=&#34;https://cran.r-project.org/web/packages/acs/index.html&#34;&gt;acs&lt;/a&gt;), so that was how I got back into R.&lt;/p&gt;

&lt;p&gt;KO: How did you find out about R when you first started using it in high school?&lt;/p&gt;

&lt;p&gt;MA: I honestly don’t remember. I didn’t even use RStudio until two years ago. I think it was probably from other fantasy nerds?&lt;/p&gt;

&lt;p&gt;KO: Is there an underground R fantasy basketball culture?&lt;/p&gt;

&lt;p&gt;MA: Well R for fantasy football is legit. &lt;a href=&#34;http://fantasyfootballanalytics.net/&#34;&gt;Fantasy Football Analytics&lt;/a&gt; is all R modeling.&lt;/p&gt;

&lt;p&gt;KO: That’s awesome - so now, do you work with sports analytics? Or is that your personal project/passion?&lt;/p&gt;

&lt;p&gt;MA: A little bit of both, I worked for this startup called Stattleship (&lt;a href=&#34;https://twitter.com/stattleship&#34;&gt;@stattleship&lt;/a&gt;). Because I’ll get involved with anything if there’s a good pun involved… and so we were doing sports analytics work that kind of ended up shifting more in a marketing direction. I still do consulting with the head data scientist [&lt;a href=&#34;https://twitter.com/tanyacash21/&#34;&gt;Tanya Cashorali&lt;/a&gt;] for that [at &lt;a href=&#34;http://tcbanalytics.com/&#34;&gt;TCB Analytics&lt;/a&gt;]. Some of the analysis/consulting will be with companies who are doing either consumer products for sports or data journalism stuff around sports analytics.&lt;/p&gt;

&lt;p&gt;KO: How often do you use R now?&lt;/p&gt;

&lt;p&gt;MA: Oh, I use R like every day. I use it… I don’t use Word any more. [Laughter] Yeah so one of the things about basketball is that there are times of the year where there are games every day. So that’s been my morning workflow for a while - scraping basketball data.&lt;/p&gt;

&lt;p&gt;KO: So you get up every morning and scrape what’s new in Basketball?&lt;/p&gt;

&lt;p&gt;MA: Yeah! So I end up in RStudio bright and early (often late, as well).&lt;/p&gt;

&lt;p&gt;KO: So is that literally what the first half hour of your day looks like?&lt;/p&gt;

&lt;p&gt;MA: No, so incidentally that’s kind of how this Twitter thing got started. My dog has long preceded me on Twitter and the internet at large, he’s kind of an internet famous dog &lt;a href=&#34;https://twitter.com/batpigandme&#34;&gt;@batpigandme&lt;/a&gt;. There’s an application called &lt;a href=&#34;https://buffer.com/&#34;&gt;Buffer&lt;/a&gt; which allows you to schedule tweets and facebook page posts, which was most of Batpig’s traffic - &lt;a href=&#34;https://www.facebook.com/batpigandme&#34;&gt;facebook page&lt;/a&gt; visits from Japan. And so I had this morning routine (started in the winter when I had one of those light things you sit in front of for a certain number of minutes) where I would wake up and schedule batpig posts while I’m sitting there and read emails. And that ended up being a nice morning workflow thing.&lt;/p&gt;

&lt;p&gt;I went to a &lt;a href=&#34;http://www.dogooddata.com/&#34;&gt;Do Good Data&lt;/a&gt; conference, which is a &lt;a href=&#34;http://www.dataanalystsforsocialgood.com/&#34;&gt;Data Analysts for Social Good&lt;/a&gt; (&lt;a href=&#34;https://twitter.com/DA4SG&#34;&gt;@DA4SG&lt;/a&gt;) event, just over two years ago, and everyone there was giving out their twitter handles, and I was like, oh - maybe people who aren’t dogs also use Twitter? [Laughter] So that was how I ended up creating my own account &lt;a href=&#34;https://twitter.com/dataandme&#34;&gt;@dataandme&lt;/a&gt; independent from Batpig.&lt;/p&gt;

&lt;p&gt;KO: What happened after you went to this conference? Was it awesome, did it inspire you?&lt;/p&gt;

&lt;p&gt;MA: Yeah so, I was the stats person at the company I was working at. And I didn’t realize there was all this really awesome work being done with really rigorous evaluation that wasn’t necessarily federal grant proposal stuff.  So I was really inspired by that and started learning more about what other people were doing, some of it in R, some of it not. I kept in touch with some of the people from that conference. And then NBA Twitter is also a thing it turns out, and NBA, R/Statistics is also a really big thing so that was kind of what pulled me in. And it was really fun. A lot of interesting projects and people that I work with were all through that [Twitter] which still surprises me - that I can read a book and tell the author something and they care? It’s weird.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I like to make arbitrary rules for myself, one of the things is I don’t tweet stuff that I haven’t read.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Everyone loves your twitter account. How do you find and curate the things you end up posting about?&lt;/p&gt;

&lt;p&gt;MA: I like to make arbitrary rules for myself, one of the things is I don’t tweet stuff that I haven’t read. I like to learn new things and/or I have to learn new things every day so I basically started scheduling [tweets] as a way to make myself read the things that I want to read and get back to.&lt;/p&gt;

&lt;p&gt;KO: Wait, so you schedule a tweet and then you’re like, okay well this is my deadline to read this thing - or I’ll be a liar.&lt;/p&gt;

&lt;p&gt;MA: Totally.&lt;/p&gt;

&lt;p&gt;KO: Whoa that’s awesome.&lt;/p&gt;

&lt;p&gt;MA: I’ve also never not finished a book in my life. It’s one of my rules, I’m really strict about it.&lt;/p&gt;

&lt;p&gt;KO: That’s a lot of pressure!&lt;/p&gt;

&lt;p&gt;MA: So that was kind of how it started out - especially because I didn’t even know all the stuff I didn’t know. Then, as I’ve used R more and more, there’s stuff that I’ve just happened to read because I don’t know what I’m doing.&lt;/p&gt;

&lt;p&gt;KO: The more you learn the more you can learn.&lt;/p&gt;

&lt;p&gt;MA: Yeah so now a lot of the stuff [tweets] is stuff I end up reading over the course of the day and then add it [to the queue]. Or it’s just stuff I’ve already read when I feel like being lazy.&lt;/p&gt;

&lt;p&gt;KO: Do you have side projects other than the basketball/sports stuff?&lt;/p&gt;

&lt;p&gt;MA: I actually majored in science and technology studies, which means I was randomly trained in ethical/legal/social implications of science. So I’m working on some data ethics projects which unfortunately I can’t talk about. And then my big side project for total amusement was this &lt;a href=&#34;https://archervisualization.herokuapp.com/&#34;&gt;D3.js in Action analysis of Archer&lt;/a&gt; which is a cartoon that I watch. But that’s also how I learned really how to use &lt;a href=&#34;https://github.com/juliasilge/tidytext&#34;&gt;tidytext&lt;/a&gt;. So then I ended up doing a technical review for David [Robinson] and Julia&amp;rsquo;s [Silge] book &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;Text Mining with R: A Tidy Approach&lt;/a&gt;. It was super fun. So yeah, I always have a bunch of random side projects going on.&lt;/p&gt;

&lt;p&gt;KO: How is your work-life balance?&lt;/p&gt;

&lt;p&gt;MA: It’s funny because I like what I do. So I don’t always know where that starts and ends. And I’m really bad at capitalism. It never occurs to me that I should be paid for doing some things. Especially if it involves open data and open source - surely you can’t charge for that? But I read a lot of stuff that’s not R too. I think I’m getting sort of a balance, but I’m not sure.&lt;/p&gt;

&lt;p&gt;KO: Switching back to your job-job now. Are you on a team, are you remote, are you in an office, what are the logistics like?&lt;/p&gt;

&lt;p&gt;MA: Kind of all of the above. In my old job I was on a team but I was the only person doing anything data related. And I developed some really lazy habits from that - really ugly code and committing terrible stuff to git. But with this NBA project I end up working with a lot of different people (who are also basketball-stat nerds).&lt;/p&gt;

&lt;p&gt;KO: Do you work with people who are employed by the actual NBA teams, or just people who are really interested in the subject?&lt;/p&gt;

&lt;p&gt;MA: No, so there is an unfortunate attrition of people whom I work with when they get hired by teams - which is not unfortunate, it’s awesome, but then they can no longer do anything with us. So that’s collaborative work but I don’t work on a team anymore.&lt;/p&gt;

&lt;p&gt;KO: So you don’t have daily stand-ups or anything.&lt;/p&gt;

&lt;p&gt;MA: No, no. I could probably benefit from that, but my goal is never to be 100% remote. After I went to that first data conference, I felt like being around all these people who are so much smarter than I am, and know so much more than I do is intimidating, but I also learned so much. And I learned so many things I was doing, not wrong, but inefficiently. I still learn about 80 things I’m doing inefficiently every day.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My goal right now - stop holding on to all of my projects that are not as done as I want them to be, and will never be done.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Do you have set beginnings and endings to projects? How many projects are you juggling at a given time?&lt;/p&gt;

&lt;p&gt;MA: After doing federal grant proposals, it doesn’t feel like anything is a deadline compared to that. They don’t care if your house burned down if it’s not in at the right time. So nothing feels as hard and fast as that. There are certain things like the NBA that —&lt;/p&gt;

&lt;p&gt;KO: There are timely things.&lt;/p&gt;

&lt;p&gt;MA: Yeah, and then sometimes we’ll just set arbitrary deadlines, just to kind of get out of a cycle of trying to perfect it, which I fall deeply into. Yeah so that’s kind of a little bit of my goal right now - stop holding on to all of my projects that are not as done as I want them to be, and will never be done. With the first iteration of this Archer thing I literally spent three days trying to get this faceted bar chart thing to sort in multiple ways and was super frustrated and then I tweeted something about it and immediately David Robinson responded with precisely what I needed and would have never figured out. So I’m working on doing that more. And also because it’s so helpful to me when other people do that.&lt;/p&gt;

&lt;p&gt;KO: How did you get hooked up with Julia and David, just through Twitter?&lt;/p&gt;

&lt;p&gt;MA: Yeah! So Julia I’d met at Open Vis Conf, David I’d read his &lt;a href=&#34;http://varianceexplained.org/programming/bad-code/&#34;&gt;blog about a million lines of bad code&lt;/a&gt; - it was open on my iPad for like years because I loved it so much, and still do. And yeah so again as this super random twitter-human that I feel like I am, I do end up meeting and doing things with cool people who are super smart and do really cool things.&lt;/p&gt;

&lt;p&gt;KO: It’s impressive how much you post and not just that, but it’s really evident that you care. People can tell that this isn’t just someone who reposts a million things day.&lt;/p&gt;

&lt;p&gt;MA: I mean it’s totally selfish, don’t get me wrong. But I’m super glad that it’s helpful to other people too. It gives me so much anxiety to think that people might think I know how to do all the things that I post, which I don’t, that’s why I had to read them - but even when I read them, sometimes I don’t know. The R community is pretty awesome, at least the parts of it that I know; which is not universally true of any community of any group of scientists. R Twitter is super-super helpful. And that was evident really quickly, at least to me.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My plea to everyone who has a blog is to put their Twitter handle somewhere on it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: What are some of your favorite things on the internet? Blogs, Twitter Accounts, Podcasts…&lt;/p&gt;

&lt;p&gt;MA: I have never skipped one of &lt;a href=&#34;https://juliasilge.com/&#34;&gt;Julia Silge’s blog&lt;/a&gt; posts. Her posts are always something that I know I should learn how to do. Both she and D-Rob [David Robinson] know their stuff and they write really well. So those are two blogs and follows that I love. &lt;a href=&#34;https://rud.is/b/&#34;&gt;Bob Rudis&lt;/a&gt; - almost daily, I can’t believe how quickly he churns stuff out. R-Bloggers is a great way to discover new stuff. Dr. Simon J [&lt;a href=&#34;https://drsimonj.svbtle.com/&#34;&gt;Simon Jackson&lt;/a&gt;] - I literally think of people by their twitter handles [&lt;a href=&#34;https://twitter.com/drsimonj&#34;&gt;@drsimonj&lt;/a&gt;], and there are so many others.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-10-rprofile-mara-averick/put-a-bird-on-it.png&#34; alt=&#34;PUT A BIRD ON IT!&#34; align=&#34;right&#34; style=&#34;margin: 0px 20px&#34;, width=&#34;350px&#34;&gt;&lt;/p&gt;

&lt;p&gt;Every day I’m amazed by all the stuff I didn’t know existed. And also there’s stuff that people wrote three or four years ago. A lot of the data vis stuff I end up finding from weird angles. So those are some of my favorites - I’m sure there are more. Oh! Thomas Lin Pedersen, &lt;a href=&#34;http://www.data-imaginist.com&#34;&gt;Data Imaginist is his blog&lt;/a&gt;. There are so many good blogs. My plea to everyone who has a blog is to put their twitter handle somewhere on it. I actually try really hard to find attribution stuff. Every now and then I get it really wrong and it’ll be someone who has nothing to do with it but who has the same name. There’s a bikini model who has the same name as someone who I said wrote a thing - which I vetted it too! I was like, well she’s multi-faceted, good for her! And then somebody was like, I don’t think that’s the right one. Oops! I have to say that that’s the one thing that Medium nailed - when you click share it gives you their twitter handle. If you have a blog, put your twitter handle there so I don’t end up attributing it to a bikini model.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>solrium 1.0: Working with Solr from R</title>
      <link>https://ropensci.org/technotes/2017/11/08/solrium-solr-r/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/11/08/solrium-solr-r/</guid>
      <description>
        
        

&lt;p&gt;Nearly 4 years ago I wrote on this blog about an R package &lt;a href=&#34;https://github.com/ropensci/solr&#34;&gt;solr&lt;/a&gt; for working with the database &lt;a href=&#34;https://lucene.apache.org/solr/&#34;&gt;Solr&lt;/a&gt;. Since then we&amp;rsquo;ve created a refresh of that package in the &lt;a href=&#34;https://github.com/ropensci/solrium&#34;&gt;solrium&lt;/a&gt; package. Since &lt;code&gt;solrium&lt;/code&gt; first hit CRAN about two years ago, users have raised a number of issues that required breaking changes. Thus, this blog post is about a major version bump in &lt;code&gt;solrium&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-is-solr&#34;&gt;What is Solr?&lt;/h2&gt;

&lt;p&gt;Solr is a &amp;ldquo;search platform&amp;rdquo; - a NoSQL database - data is organized by so called documents that are xml/json/etc blobs of text. Documents are nested within either collections or cores (depending on the mode you start Solr in). Solr makes it easy to search for documents, with a huge variety of parameters, and a number of different data formats (json/xml/csv). Solr is similar to &lt;a href=&#34;https://www.elastic.co/products/elasticsearch&#34;&gt;Elasticsearch&lt;/a&gt; (see our Elasticsearch client &lt;a href=&#34;https://github.com/ropensci/elastic&#34;&gt;elastic&lt;/a&gt;) - and was around before it. Solr in my opinion is harder to setup than Elasticsearch, but I don&amp;rsquo;t claim to be an expert on either.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;vignettes&#34;&gt;Vignettes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/solrium/vignettes/search.html&#34;&gt;Solr Search with solrium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/solrium/vignettes/local_setup.html&#34;&gt;Local Solr setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/solrium/vignettes/document_management.html&#34;&gt;Document management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/solrium/vignettes/cores_collections.html&#34;&gt;Cores/collections management&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;noteable-features&#34;&gt;Noteable features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Added in v1, you can now work with many connection objects to different Solr instances.&lt;/li&gt;
&lt;li&gt;Methods for the major search functionalities: search, highlight, stats, mlt, group, and facet. In addition, a catch all function &lt;code&gt;all&lt;/code&gt; to combine all of those.&lt;/li&gt;
&lt;li&gt;Comprehensive coverage of the Solr HTTP API&lt;/li&gt;
&lt;li&gt;Can coerce data from Solr API into data.frame&amp;rsquo;s when possible&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Install &lt;code&gt;solrium&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;solrium&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/solrium&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(solrium)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;initialize-a-client&#34;&gt;Initialize a client&lt;/h2&gt;

&lt;p&gt;A big change in &lt;code&gt;v1&lt;/code&gt; of &lt;code&gt;solrium&lt;/code&gt; is &lt;code&gt;solr_connect&lt;/code&gt; has been replaced by &lt;code&gt;SolrClient&lt;/code&gt;. Now you create an &lt;code&gt;R6&lt;/code&gt; connection object with &lt;code&gt;SolrClient&lt;/code&gt;, then you can call methods on that &lt;code&gt;R6&lt;/code&gt; object, &lt;strong&gt;OR&lt;/strong&gt; you can pass the connection object to functions.&lt;/p&gt;

&lt;p&gt;By default, &lt;code&gt;SolrClient$new()&lt;/code&gt; sets connections details for a Solr instance that&amp;rsquo;s running on &lt;code&gt;localhost&lt;/code&gt;, and on port &lt;code&gt;8983&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(conn &amp;lt;- SolrClient$new())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; &amp;lt;Solr Client&amp;gt;
#&amp;gt;   host: 127.0.0.1
#&amp;gt;   path: 
#&amp;gt;   port: 8983
#&amp;gt;   scheme: http
#&amp;gt;   errors: simple
#&amp;gt;   proxy:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On instantiation, it does not check that the Solr instance is up, but merely sets connection details. You can check if the instance is up by doing for example (assuming you have a collection named &lt;code&gt;gettingstarted&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;conn$ping(&amp;quot;gettingstarted&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; $responseHeader
#&amp;gt; $responseHeader$zkConnected
#&amp;gt; [1] TRUE
#&amp;gt; 
#&amp;gt; $responseHeader$status
#&amp;gt; [1] 0
#&amp;gt; 
#&amp;gt; $responseHeader$QTime
#&amp;gt; [1] 163
#&amp;gt; 
#&amp;gt; $responseHeader$params
#&amp;gt; $responseHeader$params$q
#&amp;gt; [1] &amp;quot;{!lucene}*:*&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$distrib
#&amp;gt; [1] &amp;quot;false&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$df
#&amp;gt; [1] &amp;quot;_text_&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$rows
#&amp;gt; [1] &amp;quot;10&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$wt
#&amp;gt; [1] &amp;quot;json&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$echoParams
#&amp;gt; [1] &amp;quot;all&amp;quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $status
#&amp;gt; [1] &amp;quot;OK&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A good hint when connecting to a publicly exposed Solr instance is that you likely don&amp;rsquo;t need to specify a port, so a pattern like this should work to connect to a URL like &lt;code&gt;http://foobar.com/search&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SolrClient$new(host = &amp;quot;foobar.com&amp;quot;, path = &amp;quot;search&amp;quot;, port = NULL)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the instance uses SSL, simply specify that like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SolrClient$new(host = &amp;quot;foobar.com&amp;quot;, path = &amp;quot;search&amp;quot;, port = NULL, scheme = &amp;quot;https&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;query-and-body-parameters&#34;&gt;Query and body parameters&lt;/h2&gt;

&lt;p&gt;Another big change in the package is that we wanted to make it easy to determine whether your Solr query gets passed as query parameters in a &lt;code&gt;GET&lt;/code&gt; request or as body in a &lt;code&gt;POST&lt;/code&gt; request. Solr clients in some other languages do this, and it made sense to port over that idea here. Now you pass your key-value pairs to either &lt;code&gt;params&lt;/code&gt; or &lt;code&gt;body&lt;/code&gt;. If nothing is passed to &lt;code&gt;body&lt;/code&gt;, we do a &lt;code&gt;GET&lt;/code&gt; request. If something is passed to &lt;code&gt;body&lt;/code&gt; we do a &lt;code&gt;POST&lt;/code&gt; request, even if there&amp;rsquo;s also key-value pairs passed to &lt;code&gt;params&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This change does break the interface we had in the old version, but we think it&amp;rsquo;s worth it.&lt;/p&gt;

&lt;p&gt;For example, to do a search you have to pass the collection name and a list of named parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;conn$search(name = &amp;quot;gettingstarted&amp;quot;, params = list(q = &amp;quot;*:*&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 5 x 5
#&amp;gt;      id   title title_str  `_version_` price
#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
#&amp;gt; 1    10 adfadsf   adfadsf 1.582913e+18    NA
#&amp;gt; 2    12  though    though 1.582913e+18    NA
#&amp;gt; 3    14 animals   animals 1.582913e+18    NA
#&amp;gt; 4     1    &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt; 1.582913e+18   100
#&amp;gt; 5     2    &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt; 1.582913e+18   500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can instead pass the connection object to &lt;code&gt;solr_search&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_search(conn, name = &amp;quot;gettingstarted&amp;quot;, params = list(q = &amp;quot;*:*&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 5 x 5
#&amp;gt;      id   title title_str  `_version_` price
#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
#&amp;gt; 1    10 adfadsf   adfadsf 1.582913e+18    NA
#&amp;gt; 2    12  though    though 1.582913e+18    NA
#&amp;gt; 3    14 animals   animals 1.582913e+18    NA
#&amp;gt; 4     1    &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt; 1.582913e+18   100
#&amp;gt; 5     2    &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt; 1.582913e+18   500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the same pattern applies for the other functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;solr_facet&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_group&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_mlt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_highlight&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_stats&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_all&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;new-functions-for-atomic-updates&#34;&gt;New functions for atomic updates&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ropensci/solrium/issues/97&#34;&gt;A user requested&lt;/a&gt; the ability to do &lt;a href=&#34;https://lucene.apache.org/solr/guide/7_0/updating-parts-of-documents.html&#34;&gt;atomic updates&lt;/a&gt; - partial updates to documents without having to re-index the entire document.&lt;/p&gt;

&lt;p&gt;Two functions were added: &lt;code&gt;update_atomic_json&lt;/code&gt; and &lt;code&gt;update_atomic_xml&lt;/code&gt; for JSON and XML based updates. Check out their help pages for usage.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;search-results-as-attributes&#34;&gt;Search results as attributes&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;solr_search&lt;/code&gt; and &lt;code&gt;solr_all&lt;/code&gt; in &lt;code&gt;v1&lt;/code&gt; gain attributes that include &lt;code&gt;numFound&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, and &lt;code&gt;maxScore&lt;/code&gt;. That is, you can get to these three values after data is returned. Note that some Solr instances may not return all three values.&lt;/p&gt;

&lt;p&gt;For example, let&amp;rsquo;s use the Public Library of Science Solr search instance at &lt;a href=&#34;http://api.plos.org/search&#34;&gt;http://api.plos.org/search&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plos &amp;lt;- SolrClient$new(host = &amp;quot;api.plos.org&amp;quot;, path = &amp;quot;search&amp;quot;, port = NULL)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Search&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- plos$search(params = list(q = &amp;quot;*:*&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get attributes&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;attr(res, &amp;quot;numFound&amp;quot;)
#&amp;gt; [1] 1902279
attr(res, &amp;quot;start&amp;quot;)
#&amp;gt; [1] 0
attr(res, &amp;quot;maxScore&amp;quot;)
#&amp;gt; [1] 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;automatically-adjust-rows-parameter&#34;&gt;Automatically adjust rows parameter&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ropensci/solrium/pull/102&#34;&gt;A user higlighted&lt;/a&gt; that &lt;a href=&#34;https://wiki.apache.org/solr/SolrPerformanceProblems#Asking_for_too_many_rows&#34;&gt;there&amp;rsquo;s a performance penalty when asking for too many rows&lt;/a&gt;. The resulting change in &lt;code&gt;solrium&lt;/code&gt; is that in some search functions we automatically adjust the &lt;code&gt;rows&lt;/code&gt; parameter to avoid the performance penalty.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;usage-in-other-packages&#34;&gt;Usage in other packages&lt;/h2&gt;

&lt;p&gt;I maintain 4 other packages that use &lt;code&gt;solrium&lt;/code&gt;: &lt;a href=&#34;https://github.com/ropensci/rplos&#34;&gt;rplos&lt;/a&gt;, &lt;a href=&#34;https://github.com/ropensci/ritis&#34;&gt;ritis&lt;/a&gt;, &lt;a href=&#34;https://github.com/ropensci/rdatacite&#34;&gt;rdatacite&lt;/a&gt;, and &lt;a href=&#34;https://github.com/ropensci/rdryad&#34;&gt;rdryad&lt;/a&gt;. If you are interested in using &lt;code&gt;solrium&lt;/code&gt; in your package, looking at any of those four packages will give a good sense of how to do it.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;

&lt;h3 id=&#34;solr-pkg&#34;&gt;solr pkg&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;solr&lt;/code&gt; package will soon be archived on CRAN. We&amp;rsquo;ve moved all packages depending on it to &lt;code&gt;solrium&lt;/code&gt;. Let me know ASAP if you have any complaints about archiving it on CRAN.&lt;/p&gt;

&lt;h3 id=&#34;feedback&#34;&gt;Feedback!&lt;/h3&gt;

&lt;p&gt;Please do upgrade/install &lt;code&gt;solrium&lt;/code&gt;  &lt;code&gt;v1&lt;/code&gt; and let us know what you think.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Using Magick with RMarkdown and Shiny</title>
      <link>https://ropensci.org/technotes/2017/11/07/magick-knitr/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/11/07/magick-knitr/</guid>
      <description>
        
        &lt;img src=&quot;https://i.imgur.com/tTFk7ig.jpg&quot; alt=&quot;cover image&quot;&gt;
        
        

&lt;p&gt;This week &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick&lt;/a&gt; 1.5 appeared on CRAN. The latest update adds support for using images in knitr documents and shiny apps. In this post we show how this nicely ties together a reproducible image workflow in R, from source image or plot directly into your report or application.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)
stopifnot(packageVersion(&#39;magick&#39;) &amp;gt;= 1.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also the magick &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;intro vignette&lt;/a&gt; has been updated in this version to cover the latest features available in the package.&lt;/p&gt;

&lt;h2 id=&#34;magick-in-knitr-rmarkdown-documents&#34;&gt;Magick in Knitr / RMarkdown Documents&lt;/h2&gt;

&lt;p&gt;Magick 1.5 is now fully compatible with knitr. To embed magick images in your rmarkdown report, simply use standard code chunk syntax in your &lt;code&gt;Rmd&lt;/code&gt; file. No special options or packages are required; the image automatically appears in your documents when printed!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Example from our post last week
image_read(&#39;logo:&#39;) %&amp;gt;%
  image_convolve(&#39;DoG:0,0,2&#39;) %&amp;gt;%
  image_negate() %&amp;gt;%
  image_resize(&amp;quot;400x400&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/PhwCJ4k.gif&#34; alt=&#34;fig1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can also combine this with the magick graphics device to post process or animate your plots and figures directly in knitr. Again no special packages or system dependencies are required.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Produce graphic
fig &amp;lt;- image_graph(width = 800, height = 600, res = 96)
ggplot2::qplot(factor(cyl), data = mtcars, fill = factor(gear))
invisible(dev.off())

print(fig)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/zFLcHws.png&#34; alt=&#34;fig2&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
# Some post-processing
frink &amp;lt;- image_read(&amp;quot;https://jeroen.github.io/images/frink.png&amp;quot;)

fig %&amp;gt;%
  image_rotate(10) %&amp;gt;%
  image_implode(.6) %&amp;gt;%
  image_composite(frink, offset = &amp;quot;+140+70&amp;quot;) %&amp;gt;%
  image_annotate(&amp;quot;Very usefull stuff&amp;quot;, size = 40, location = &amp;quot;+300+100&amp;quot;, color = &amp;quot;navy&amp;quot;, boxcolor = &amp;quot;pink&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/0E5cqaz.png&#34; alt=&#34;fig3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Same works for animation with &lt;code&gt;image_animate()&lt;/code&gt;; the figure shows automatically up in the report as a gif image:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;image_read(&amp;quot;https://jeroen.github.io/images/banana.gif&amp;quot;) %&amp;gt;%
  image_apply( function(banana){
    image_composite(fig, banana, offset = &amp;quot;+200+200&amp;quot;)
  }) %&amp;gt;%
  image_resize(&amp;quot;50%&amp;quot;) %&amp;gt;%
  image_animate()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/mi67gjt.gif&#34; alt=&#34;fig4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The magick vignette &lt;a href=&#34;https://raw.githubusercontent.com/ropensci/magick/master/vignettes/intro.Rmd&#34;&gt;source code&lt;/a&gt; is itself written in Rmarkdown, so it&amp;rsquo;s great example to see this in action. Try rendering it in RStudio to see how easy it is!&lt;/p&gt;

&lt;h2 id=&#34;magick-in-shiny-apps&#34;&gt;Magick in Shiny Apps&lt;/h2&gt;

&lt;p&gt;While we&amp;rsquo;re at it, several people had asked how to use magick images in shiny apps. The easiest way is to write the image to a &lt;code&gt;tempfile()&lt;/code&gt; within the &lt;code&gt;renderImage()&lt;/code&gt; callback function. For example the server part could look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;output$img &amp;lt;- renderImage({
    tmpfile &amp;lt;- image %&amp;gt;%
      image_resize(input$size) %&amp;gt;%
      image_implode(input$implode) %&amp;gt;%
      image_blur(input$blur, input$blur) %&amp;gt;%
      image_rotate(input$rotation) %&amp;gt;%
      image_write(tempfile(fileext=&#39;jpg&#39;), format = &#39;jpg&#39;)

  # Return a list
  list(src = tmpfile, contentType = &amp;quot;image/jpeg&amp;quot;)
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is a simple shiny app that demonstrates this. Have a look at the &lt;a href=&#34;https://github.com/jeroen/shinymagick/blob/master/app.R&#34;&gt;source code&lt;/a&gt; or just run it in R:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(shiny)
library(magick)
runGitHub(&amp;quot;jeroen/shinymagick&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://jeroen.shinyapps.io/shinymagick&#34;&gt;&lt;img src=&#34;https://i.imgur.com/tTFk7ig.jpg&#34; alt=&#34;tigrou&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Perhaps there&amp;rsquo;s an even better way to make this work by wrapping magick images into an htmlwidget but I have not figured this out yet.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Image Convolution in R using Magick</title>
      <link>https://ropensci.org/technotes/2017/11/02/image-convolve/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/11/02/image-convolve/</guid>
      <description>
        
        

&lt;p&gt;Release 1.4 of the &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick package&lt;/a&gt; introduces
a new feature called &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_(image_processing)#Convolution&#34;&gt;image convolution&lt;/a&gt; that
was requested by Thomas L. Pedersen. In this post we explain what this is all about.&lt;/p&gt;

&lt;h2 id=&#34;kernel-matrix&#34;&gt;Kernel Matrix&lt;/h2&gt;

&lt;p&gt;The new &lt;code&gt;image_convolve()&lt;/code&gt; function applies a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_(image_processing)&#34;&gt;kernel&lt;/a&gt; over the image. Kernel convolution means that each pixel value is recalculated using the &lt;em&gt;weighted neighborhood sum&lt;/em&gt; defined in the kernel matrix. For example lets look at this simple kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)

kern &amp;lt;- matrix(0, ncol = 3, nrow = 3)
kern[1, 2] &amp;lt;- 0.25
kern[2, c(1, 3)] &amp;lt;- 0.25
kern[3, 2] &amp;lt;- 0.25
kern
##      [,1] [,2] [,3]
## [1,] 0.00 0.25 0.00
## [2,] 0.25 0.00 0.25
## [3,] 0.00 0.25 0.00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This kernel changes each pixel to the mean of its horizontal and vertical neighboring pixels, which results in a slight blurring effect in the right-hand image below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img &amp;lt;- image_read(&#39;logo:&#39;)
img_blurred &amp;lt;- image_convolve(img, kern)
image_append(c(img, img_blurred))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Y6xByUL.gif&#34; alt=&#34;image_appended&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;standard-kernels&#34;&gt;Standard Kernels&lt;/h2&gt;

&lt;p&gt;Many operations in &lt;code&gt;magick&lt;/code&gt;  such as blurring, sharpening, and edge detection are
actually special cases of image convolution. The benefit of explicitly using
&lt;code&gt;image_convolve()&lt;/code&gt; is more control. For example, we can blur an image and then blend
it together with the original image in one step by mixing a blurring kernel with the
unit kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;Gaussian:0x5&#39;, scaling = &#39;60,40%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/6Vf6c2hl.gif&#34; alt=&#34;mixed&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The above requires a bit of explanation. ImageMagick defines several common
&lt;a href=&#34;http://www.imagemagick.org/Usage/convolve/&#34;&gt;standard kernels&lt;/a&gt; such as the
gaussian kernel. Most of the standard kernels take one or more parameters,
e.g. the example above used a gaussian kernel with 0 &lt;em&gt;radius&lt;/em&gt; and 5 &lt;em&gt;sigma&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In addition, &lt;code&gt;scaling&lt;/code&gt; argument defines the magnitude of the kernel, and possibly
how much of the original picture should be mixed in. Here we mix 60% of the
blurring with 40% of the original picture in order to get a diffused lightning effect.&lt;/p&gt;

&lt;h2 id=&#34;edge-detection&#34;&gt;Edge Detection&lt;/h2&gt;

&lt;p&gt;Another area where kernels are of use is in edge detection. A simple example of
a direction-aware edge detection kernel is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sobel_operator&#34;&gt;&lt;em&gt;Sobel&lt;/em&gt;&lt;/a&gt; kernel.
As can be seen below, vertical edges are detected while horizontals are not.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;Sobel&#39;) %&amp;gt;% image_negate()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/i8ndfCu.gif&#34; alt=&#34;edges&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Something less apparent is that the result of the edge detection is truncated.
Edge detection kernels can result in negative color values which get truncated to zero.
To combat this it is possible to add a &lt;code&gt;bias&lt;/code&gt; to the result. Often you&amp;rsquo;ll end up with
scaling the kernel to 50% and adding 50% bias to move the midpoint of the result to 50%
grey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;Sobel&#39;, scaling = &#39;50%&#39;, bias = &#39;50%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/llUawrg.gif&#34; alt=&#34;50pct&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sharpening&#34;&gt;Sharpening&lt;/h2&gt;

&lt;p&gt;ImageMagick has many more edge detection kernels, some of which are insensitive to
the direction of the edge. To emulate a classic high-pass filter from photoshop use
&lt;a href=&#34;https://en.wikipedia.org/wiki/Difference_of_Gaussians&#34;&gt;difference of gaussians&lt;/a&gt; kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;DoG:0,0,2&#39;) %&amp;gt;% image_negate()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/o5kODpc.gif&#34; alt=&#34;dog&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As with the blurring, the original image can be blended in with the transformed one, effectively sharpening the image along edges.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;DoG:0,0,2&#39;, scaling = &#39;100, 100%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/MtcMSn7.gif&#34; alt=&#34;combination&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://www.imagemagick.org/Usage/convolve/&#34;&gt;ImageMagick documentation&lt;/a&gt; has more examples of convolve with various avaiable kernels.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Building Communities Together at ozunconf, 2017</title>
      <link>https://ropensci.org/blog/2017/10/31/ozunconf2017/</link>
      <pubDate>Tue, 31 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/31/ozunconf2017/</guid>
      <description>
        
        

&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-hex-cookies.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Just last week we organised the 2nd rOpenSci &lt;a href=&#34;http://ozunconf17.ropensci.org&#34;&gt;ozunconference&lt;/a&gt;, the sibling rOpenSci unconference, held in Australia. Last year it was &lt;a href=&#34;http://auunconf.ropensci.org&#34;&gt;held in Brisbane&lt;/a&gt;, this time around, the ozunconf was hosted in Melbourne, from October 26-27, 2017.&lt;/p&gt;

&lt;p&gt;At the ozunconf, we brought together 45 R-software users and developers, scientists, and open data enthusiasts from academia, industry, government, and non-profits. Participants travelled from far and wide, with people coming from 6 cities around Australia, 2 cities in New Zealand, and one city in the USA. Before the ozunconf we discussed and dreamt up projects to work on for a few days, then met up and brought about a bakers dozen of them into reality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-womens-data-discuss.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;upskilling-participants&#34;&gt;Upskilling participants&lt;/h3&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Day 0.5 of &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; and an excellent intro to writing packages and using Git from &lt;a href=&#34;https://twitter.com/rdpeng?ref_src=twsrc%5Etfw&#34;&gt;@rdpeng&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/nj_tierney?ref_src=twsrc%5Etfw&#34;&gt;@nj_tierney&lt;/a&gt;. I enjoyed the post-it note system! &lt;a href=&#34;https://t.co/iKTbWkeCA0&#34;&gt;pic.twitter.com/iKTbWkeCA0&lt;/a&gt;&lt;/p&gt;&amp;mdash; Holly Kirk (@HollyKirk) &lt;a href=&#34;https://twitter.com/HollyKirk/status/923065587915415552?ref_src=twsrc%5Etfw&#34;&gt;October 25, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;On Day 0, one day before the ozunconf, &lt;a href=&#34;https://twitter.com/rdpeng&#34;&gt;Roger Peng&lt;/a&gt; and I ran a half day training session on how to develop R packages and share them on GitHub. The participants picked things up really quickly, and by the end of the session, everyone could make an R package, and push it to GitHub. We also introduced them to &lt;a href=&#34;https://www.youtube.com/watch?v=s3JldKoA0zw&amp;amp;feature=youtu.be&#34;&gt;the wonders of RMarkdown&lt;/a&gt;. The event then kicked on to the &lt;a href=&#34;https://www.meetup.com/R-Ladies-Melbourne/events/244102535/&#34;&gt;R-Ladies Melbourne special one year anniversary event&lt;/a&gt;, which featured a great talk and introduction to Random Forests by Elisabeth Vogel.&lt;/p&gt;

&lt;h3 id=&#34;bringing-people-together&#34;&gt;Bringing people together&lt;/h3&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Biscuit decorating at &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; ! &lt;a href=&#34;https://t.co/M8PxOyRUJI&#34;&gt;pic.twitter.com/M8PxOyRUJI&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nikeisha Caruana (@bluebirdi) &lt;a href=&#34;https://twitter.com/bluebirdi/status/923305923208036352?ref_src=twsrc%5Etfw&#34;&gt;October 25, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;Before the ozunconf, we discussed various ideas for projects in &lt;a href=&#34;https://github.com/ropensci/ozunconf17/issues&#34;&gt;the GitHub issues&lt;/a&gt;. Things really started to pick up in the last week and we ended up at 41 issues - almost as many issues as participants.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-oz-data-discuss.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Day one kicked off with decorating some hex cookies, baked by &lt;a href=&#34;https://twitter.com/visnut&#34;&gt;Di Cook&lt;/a&gt;. This uncovered a fun fact that &lt;a href=&#34;http://stefanbache.dk/&#34;&gt;Stefan Milton Bache&lt;/a&gt; - creator of the beloved pipe operator (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) from the &lt;a href=&#34;https://github.com/tidyverse/magrittr&#34;&gt;&lt;code&gt;magrittr&lt;/code&gt;&lt;/a&gt; package, apparently also created the first #rstats hex sticker.&lt;/p&gt;

&lt;p&gt;We then stuck the various projects that had been discussed throughout the week around the room and participants sticker voted on projects that they were interested in working on. Introductions were made, and quotes like these (from &lt;a href=&#34;https://twitter.com/stephdesilva&#34;&gt;Steph de Silva&lt;/a&gt;) led to entertaining discussions around data:&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;quot;I&amp;#39;m 50% data hazmat, 50% data grief counselling&amp;quot; best Intro ever at &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt;&lt;/p&gt;&amp;mdash; Steve Bennett (@stevage1) &lt;a href=&#34;https://twitter.com/stevage1/status/923314625428336641?ref_src=twsrc%5Etfw&#34;&gt;October 25, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;We were really lucky to be in the beautiful Monash City Campus, a place that almost seems to have been designed for an unconf, with some classroom style space, as well as plenty of nooks and crannies to sit in, including an outdoor astroturfed garden complete with bean bags and native flora.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-earo.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The Oz colour palette gang soakin it up outside &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; &lt;a href=&#34;https://t.co/XiLkhwZwTv&#34;&gt;pic.twitter.com/XiLkhwZwTv&lt;/a&gt;&lt;/p&gt;&amp;mdash; Miles McBain (@MilesMcBain) &lt;a href=&#34;https://twitter.com/MilesMcBain/status/923682409400250368?ref_src=twsrc%5Etfw&#34;&gt;October 26, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;The venue even seemed to reflect our love of hex stickers, providing a nice hex sticker themed carpet:&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Appropriately shaped carpet pattern for the &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; venue &lt;a href=&#34;https://twitter.com/hashtag/hex?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#hex&lt;/a&gt; &lt;a href=&#34;https://t.co/QsbVWhCQyb&#34;&gt;pic.twitter.com/QsbVWhCQyb&lt;/a&gt;&lt;/p&gt;&amp;mdash; Holly Kirk (@HollyKirk) &lt;a href=&#34;https://twitter.com/HollyKirk/status/923420699900997632?ref_src=twsrc%5Etfw&#34;&gt;October 26, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;We had some great sponsors for this event, including &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt;, &lt;a href=&#34;http://rstudio.com/&#34;&gt;RStudio&lt;/a&gt; &lt;a href=&#34;http://r-consortium.org/&#34;&gt;The RConsortium&lt;/a&gt;, &lt;a href=&#34;https://inghaminstitute.org.au/&#34;&gt;The Ingham Institute&lt;/a&gt;, and &lt;a href=&#34;http://www.monash.edu/business&#34;&gt;Monash Business School&lt;/a&gt;. The event was also organised by &lt;a href=&#34;https://twitter.com/nj_tierney&#34;&gt;myself&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/visnut&#34;&gt;Di Cook&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/robjhyndman&#34;&gt;Rob Hyndman&lt;/a&gt;, and also &lt;a href=&#34;https://twitter.com/milesmcbain&#34;&gt;Miles McBain&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-hex-mat.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We wrapped up at the end of day 2, giving each projects group three minutes to debrief on their projects, using the unconf style - only the README.md (mostly!). You can &lt;a href=&#34;https://ropenscilabs.github.io/ozunconf-projects/&#34;&gt;check out all the ozunconf projects here&lt;/a&gt;, thanks to a template from &lt;a href=&#34;http://seankross.com/&#34;&gt;Sean Kross&lt;/a&gt;. Soon we will publish a series of short posts covering some of these great fun projects.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a quick taster:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/realtime&#34;&gt;&lt;code&gt;realtime&lt;/code&gt;&lt;/a&gt;. Realtime streamingplots built on the p5.js library.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/ozrepro&#34;&gt;&lt;code&gt;stow&lt;/code&gt;&lt;/a&gt;. A simplified version control interface to git, from within R.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/icon&#34;&gt;&lt;code&gt;icon&lt;/code&gt;&lt;/a&gt;. Easily access and insert web icons into HTML and PDF documents.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/ochRe&#34;&gt;&lt;code&gt;ochRe&lt;/code&gt;&lt;/a&gt;. Provide Australia-themed Colour Palettes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;ll share a quick summary of all of the projects over the coming weeks.&lt;/p&gt;

&lt;h3 id=&#34;what-s-your-story&#34;&gt;What&amp;rsquo;s Your Story?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/StephdeSilva/status/923875737102200832&#34;&gt;Some key #ozunconf lessons from Steph de Silva&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The opportunity to try but not succeed is a luxury to be savoured&lt;/li&gt;
&lt;li&gt;Mistakes make you a better programmer&lt;/li&gt;
&lt;li&gt;The best thing about R isn&amp;rsquo;t the language, it&amp;rsquo;s the number of people around who want to help&lt;/li&gt;
&lt;li&gt;Your skills are valuable, so your productivity is too. Investing in the tools that maximise it is worthwhile.&lt;/li&gt;
&lt;li&gt;git really is out to get you.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A few people have already written about their unconf17 experience. Have you? Share the link in the comments below and we&amp;rsquo;ll add it here.&lt;/p&gt;

&lt;h4 id=&#34;projects-posts&#34;&gt;Projects posts&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.healthintersections.com.au/?p=2740&#34;&gt;FHIR and R: &lt;code&gt;ronfhir&lt;/code&gt;&lt;/a&gt;, by &lt;a href=&#34;http://www.healthintersections.com.au/&#34;&gt;Grahame Grieve&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/blog/2017/11/14/realtime/&#34;&gt;2017 rOpenSci ozunconf :: Reflections and the &lt;code&gt;realtime&lt;/code&gt; Package&lt;/a&gt;, by &lt;a href=&#34;https://jcarroll.com.au/&#34;&gt;Jonathan Carroll&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/blog/2017/11/21/ochre/&#34;&gt;&lt;code&gt;ochRe&lt;/code&gt; - Australia themed colour palettes&lt;/a&gt;, by &lt;a href=&#34;https://twitter.com/HollyKirk&#34;&gt;Holly Kirk&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/visnut&#34;&gt;Di Cook&lt;/a&gt;, &lt;a href=&#34;https://github.com/alicia-a&#34;&gt;Alicia Allan&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/ross_gayler&#34;&gt;Ross Gayler&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/rdpeng&#34;&gt;Roger Peng&lt;/a&gt;, &lt;a href=&#34;https://github.com/ellesaber&#34;&gt;Elle Saber&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;community-posts&#34;&gt;Community posts&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://simplystatistics.org/2017/10/30/how-do-you-convince-others-to-use-r/&#34;&gt;How do you convince others to use R?&lt;/a&gt;, by &lt;a href=&#34;http://www.biostat.jhsph.edu/~rpeng/&#34;&gt;Roger Peng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rex-analytics.com/failure-is-an-option/&#34;&gt;Failure Is An Option&lt;/a&gt;, by &lt;a href=&#34;https://twitter.com/StephdeSilva&#34;&gt;Steph da Silva&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;introducing-people-to-the-ropensci-community&#34;&gt;Introducing people to the rOpenSci community&lt;/h3&gt;

&lt;p&gt;rOpenSci has had a profound impact on me and my work. At the end of 2015 I got in touch with them to discuss arranging an unconference in Australia, and they welcomed me and my friends. Today, I am proud to be welcoming those from the ozunconf to this big, kind, wonderful community, and say, as Shannon Ellis summed up: &lt;a href=&#34;https://ropensci.org/blog/2017/06/23/community/&#34;&gt;&amp;ldquo;Hey! You there! You are welcome here&amp;rdquo;&lt;/a&gt;. It was also really great to have a diverse group of participants at the ozunconf, and in particular, that 40% of participants were women or other underrepresented genders.&lt;/p&gt;

&lt;h3 id=&#34;starts-not-ends&#34;&gt;Starts, not ends&lt;/h3&gt;

&lt;p&gt;One thing that I&amp;rsquo;ve realised in my involvement with organising and attending these events is that when the unconf ends, it feels a bit sad, sure, to say goodbye to the environment, the community, the friends, and the projects. At the last unconf in LA, we were sending out a stream of tweets, &amp;ldquo;&lt;a href=&#34;https://twitter.com/nj_tierney/status/868572134548713472&#34;&gt;it&amp;rsquo;s not&lt;/a&gt; &lt;a href=&#34;https://twitter.com/MilesMcBain/status/868590677843599360&#34;&gt;over&lt;/a&gt; &lt;a href=&#34;https://twitter.com/AmeliaMN/status/868605633435533312&#34;&gt;until&lt;/a&gt; &lt;a href=&#34;https://twitter.com/MilesMcBain/status/869044724086185985&#34;&gt;it&amp;rsquo;s&lt;/a&gt; &lt;a href=&#34;https://twitter.com/dataandme/status/869664700606406656&#34;&gt;over&lt;/a&gt;&amp;rdquo;. But, in reflection, standing back, taking it all in, the unconference doesn&amp;rsquo;t really end - it just starts. It starts many new things - projects, ideas, collaborations, and friendships.&lt;/p&gt;

&lt;p&gt;The ozunconf comes to an end. Now, let&amp;rsquo;s get started.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-group-photo.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Data from Public Bicycle Hire Systems</title>
      <link>https://ropensci.org/blog/2017/10/17/bikedata/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/17/bikedata/</guid>
      <description>
        
        

&lt;p&gt;A new rOpenSci package provides access to data to which users may already have directly contributed, and for which contribution is fun, keeps you fit, and &lt;a href=&#34;http://www.bmj.com/content/357/bmj.j1456&#34;&gt;helps make the world a better place&lt;/a&gt;. The data come from using public bicycle hire schemes, and the package is called &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt;. Public bicycle hire systems operate in many cities throughout the world, and most systems collect (generally anonymous) data, minimally consisting of the times and locations at which every single bicycle trip starts and ends. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package provides access to data from all cities which openly publish these data, currently including &lt;a href=&#34;https://tfl.gov.uk/modes/cycling/santander-cycles&#34;&gt;London, U.K.&lt;/a&gt;, and in the U.S.A., &lt;a href=&#34;https://www.citibikenyc.com&#34;&gt;New York&lt;/a&gt;, &lt;a href=&#34;https://bikeshare.metro.net&#34;&gt;Los Angeles&lt;/a&gt;, &lt;a href=&#34;https://www.rideindego.com&#34;&gt;Philadelphia&lt;/a&gt;, &lt;a href=&#34;https://www.divvybikes.com&#34;&gt;Chicago&lt;/a&gt;, &lt;a href=&#34;https://www.thehubway.com&#34;&gt;Boston&lt;/a&gt;, and &lt;a href=&#34;https://www.capitalbikeshare.com&#34;&gt;Washington DC&lt;/a&gt;. The package will expand as more cities openly publish their data (with the newly enormously expanded San Francisco system &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/2&#34;&gt;next on the list&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;why-bikedata&#34;&gt;Why bikedata?&lt;/h3&gt;

&lt;p&gt;The short answer to that question is that the package provides access to what is arguably one of the most spatially and temporally detailed databases of finely-scaled human movement throughout several of the world&amp;rsquo;s most important cities. Such data are likely to prove invaluable in the increasingly active and well-funded attempt to develop a science of cities. Such a science does not yet exist in any way comparable to most other well-established scientific disciplines, but the importance of developing a science of cities is indisputable, and reflected in such enterprises as the NYU-based &lt;a href=&#34;http://cusp.nyu.edu&#34;&gt;Center for Urban Science and Progress&lt;/a&gt;, or the UCL-based &lt;a href=&#34;https://www.ucl.ac.uk/bartlett/casa/&#34;&gt;Centre for Advanced Spatial Analysis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;People move through cities, yet at present anyone faced with the seemingly fundamental question of how, when, and where people do so would likely have to draw on some form of private data (typically operators of transport systems or mobile phone providers). There are very few open, public data providing insight into this question. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package aims to be one contribution towards filling this gap. The data accessed by the package are entirely open, and are constantly updated, typically on a monthly basis. The package thus provides ongoing insight into the dynamic changes and reconfigurations of these cities. Data currently available via the package amounts to several tens of Gigabytes, and will expand rapidly both with time, and with the inclusion of more cities.&lt;/p&gt;

&lt;h3 id=&#34;why-are-these-data-published&#34;&gt;Why are these data published?&lt;/h3&gt;

&lt;p&gt;In answer to that question, all credit must rightfully go to &lt;a href=&#34;http://www.theregister.co.uk/2011/01/11/transport_for_london_foi/&#34;&gt;Adrian Short&lt;/a&gt;, who submitted a Freedom of Information request in 2011 to Transport for London for usage statistics from the relatively new, and largely publicly-funded, bicycle scheme. This request from one individual ultimately resulted in the data being openly published on an ongoing basis. All U.S. systems included in &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; commenced operation subsequent to that point in time, and many of them have openly published their data from the very beginning. The majority of the world&amp;rsquo;s public bicycle hire systems (&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_bicycle-sharing_systems&#34;&gt;see list here&lt;/a&gt;) nevertheless do not openly publish data, notably including very large systems in China, France, and Spain. One important aspiration of the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package is to demonstrate the positive benefit for the cities themselves of openly and easily facilitating complex analyses of usage data, which brings us to &amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;what-s-important-about-these-data&#34;&gt;What&amp;rsquo;s important about these data?&lt;/h3&gt;

&lt;p&gt;As mentioned, the data really do provide uniquely valuable insights into the movement patterns and behaviour of people within some of the world&amp;rsquo;s major cities. While the more detailed explorations below demonstrate the kinds of things that can be done with the package, the variety of insights these data facilitate is best demonstrated through considering the work of other people, exemplified by &lt;a href=&#34;http://toddwschneider.com/posts/a-tale-of-twenty-two-million-citi-bikes-analyzing-the-nyc-bike-share-system/&#34;&gt;Todd Schneider&amp;rsquo;s high-profile blog piece&lt;/a&gt; on the New York City system. Todd&amp;rsquo;s analyses clearly demonstrate how these data can provide insight into where and when people move, into inter-relationships between various forms of transport, and into relationships with broader environmental factors such as weather. As cities evolve, and public bicycle hire schemes along with them, data from these systems can play a vital role in informing and guiding the ongoing processes of urban development. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package greatly facilitates analysing such processes, not only through making data access and aggregation enormously easier, but through enabling analyses from any one system to be immediately applied to, and compared with, any other systems.&lt;/p&gt;

&lt;h3 id=&#34;how-it-works&#34;&gt;How it works&lt;/h3&gt;

&lt;p&gt;The package currently focusses on the data alone, and provides functionality for downloading, storage, and aggregation. The data are stored in an &lt;code&gt;SQLite3&lt;/code&gt; database, enabling newly published data to be continually added, generally with one simple line of code. It&amp;rsquo;s as easy as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;store_bikedata (city = &amp;quot;chicago&amp;quot;, bikedb = &amp;quot;bikedb&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the nominated database (&lt;code&gt;bikedb&lt;/code&gt;) already holds data for Chicago, only new data will be added, otherwise all historical data will be downloaded and added. All bicycle hire systems accessed by &lt;code&gt;bikedata&lt;/code&gt; have fixed docking stations, and the primary means of aggregation is in terms of &amp;ldquo;trip matrices&amp;rdquo;, which are square matrices of numbers of trips between all pairs of stations, extracted with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chi&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that most parameters are highly flexible in terms of formatting, so pretty much anything starting with &lt;code&gt;&amp;quot;ch&amp;quot;&lt;/code&gt; will be recognised as Chicago. Of course, if the database only contains data for Chicago, the &lt;code&gt;city&lt;/code&gt; parameter may be omitted entirely. Trip matrices may be filtered by time, through combinations of year, month, day, hour, minute, or even second, as well as by demographic characteristics such as gender or date of birth for those systems which provide such data. (These latter data are freely provided by users of the systems, and there can be no guarantee of their accuracy.) These can all be combined in calls like the following, which further demonstrates the highly flexible ways of specifying the various parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (&amp;quot;bikedb&amp;quot;, city = &amp;quot;london, innit&amp;quot;,
                       start_date = 20160101, end_date = &amp;quot;16,02,28&amp;quot;,
                       start_time = 6, end_time = 24,
                       birth_year = 1980:1990, gender = &amp;quot;f&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The second mode of aggregation is as daily time series, via the &lt;code&gt;bike_daily_trips()&lt;/code&gt; function. See &lt;a href=&#34;https://ropensci.github.io/bikedata/articles/bikedata.html&#34;&gt;the vignette&lt;/a&gt; for further details.&lt;/p&gt;

&lt;h3 id=&#34;what-can-be-done-with-these-data&#34;&gt;What can be done with these data?&lt;/h3&gt;

&lt;p&gt;Lots of things. How about examining how far people ride. This requires getting the distances between all pairs of docking stations as routed through the street network, to yield a distance matrix corresponding to the trip matrix. The latest version of &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; has a brand new function to perform exactly that task, so it&amp;rsquo;s as easy as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;ropensci/bikedata&amp;quot;) # to install latest version
dists &amp;lt;- bike_distmat (bikedb = bikedb, city = &amp;quot;chicago&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are distances as routed through the underlying street network, with street types prioritised for bicycle travel. The network is extracted from OpenStreetMap using the &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;rOpenSci &lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;, and the distances are calculated using a brand new package called &lt;a href=&#34;https://cran.r-project.org/package=dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; (Distances on Directed Graphs). (Disclaimer: It&amp;rsquo;s my package, and this is a shameless plug for it - please use it!)&lt;/p&gt;

&lt;p&gt;The distance matrix extracted with &lt;code&gt;bike_distmat&lt;/code&gt; is between all stations listed for a given system, which &lt;code&gt;bike_tripmat&lt;/code&gt; will return trip matrices only between those stations in operation over a specified time period. Because systems expand over time, the two matrices will generally not be directly comparable, so it is necessary to submit both to the &lt;code&gt;bikedata&lt;/code&gt; function &lt;code&gt;match_matrices()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 636 636
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mats &amp;lt;- match_matrices (trips, dists)
trips &amp;lt;- mats$trip
dists &amp;lt;- mats$dist
dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 581 581
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;identical (rownames (trips), rownames (dists))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Distances can then be visually related to trip numbers to reveal their distributional form. These matrices contain too many values to plot directly, so the &lt;code&gt;hexbin&lt;/code&gt; package is used here to aggregate in a &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library (hexbin)
library (ggplot2)
dat &amp;lt;- data.frame (distance = as.vector (dmat),
                   number = as.vector (trips))
ggplot (dat, aes (x = distance, y = number)) +
    stat_binhex(aes(fill = log (..count..))) +
    scale_x_log10 (breaks = c (0.1, 0.5, 1, 2, 5, 10, 20),
                   labels = c (&amp;quot;0.1&amp;quot;, &amp;quot;0.5&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;20&amp;quot;)) +
    scale_y_log10 (breaks = c (10, 100, 1000)) +
    scale_fill_gradientn(colours = c(&amp;quot;seagreen&amp;quot;,&amp;quot;goldenrod1&amp;quot;),
                         name = &amp;quot;Frequency&amp;quot;, na.value = NA) +
    guides (fill = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/chicago.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The central region of the graph (yellow hexagons) reveals that numbers of trips generally decrease roughly exponentially with increasing distance (noting that scales are logarithmic), with most trip distances lying below 5km. What is the &amp;ldquo;average&amp;rdquo; distance travelled in Chicago? The easiest way to calculate this is as a weighted mean,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum (as.vector (dmat) * as.vector (trips) / sum (trips), na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 2.510285
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;giving a value of just over 2.5 kilometres. We could also compare differences in mean distances between cyclists who are registered with a system and causal users. These two categories may be loosely considered to reflect &amp;ldquo;residents&amp;rdquo; and &amp;ldquo;non-residents&amp;rdquo;. Let&amp;rsquo;s wrap this in a function so we can use it for even cooler stuff in a moment.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean &amp;lt;- function (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chicago&amp;quot;)
{
    tm &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
    tm_memb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = TRUE)
    tm_nomemb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = FALSE)
    stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
    dists &amp;lt;- bike_distmat (bikedb = bikedb, city = city)
    mats &amp;lt;- match_mats (dists, tm_memb)
    tm_memb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm_nomemb)
    tm_nomemb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm)
    tm &amp;lt;- mats$trip
    dists &amp;lt;- mats$dists

    d0 &amp;lt;- sum (as.vector (dists) * as.vector (tm) / sum (tm), na.rm = TRUE)
    dmemb &amp;lt;- sum (as.vector (dists) * as.vector (tmemb) / sum (t_memb), na.rm = TRUE)
    dnomemb &amp;lt;- sum (as.vector (dists) * as.vector (tm_nomemb) / sum (tm_nomemb), na.rm = TRUE)
    res &amp;lt;- c (d0, dmemb / dnomemb)
    names (res) &amp;lt;- c (&amp;quot;dmean&amp;quot;, &amp;quot;ratio_memb_non&amp;quot;)
    return (res)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Differences in distances ridden between &amp;ldquo;resident&amp;rdquo; and &amp;ldquo;non-resident&amp;rdquo; cyclists can then be calculated with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean (bikedb = bikedb, city = &amp;quot;ch&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##          dmean ratio_memb_non
##       2.510698       1.023225
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And system members cycle slightly longer distances than non-members. (Do not at this point ask about statistical tests - these comparisons are made between millions&amp;ndash;often tens of millions&amp;ndash;of points, and statistical significance may always be assumed to be negligibly small.) Whatever the reason for this difference between &amp;ldquo;residents&amp;rdquo; and others, we can use this exact same code to compare equivalent distances for all cities which record whether users are members or not (which is all cities except London and Washington DC).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cities &amp;lt;- c (&amp;quot;ny&amp;quot;, &amp;quot;ch&amp;quot;, &amp;quot;bo&amp;quot;, &amp;quot;la&amp;quot;, &amp;quot;ph&amp;quot;) # NYC, Chicago, Boston, LA, Philadelphia
sapply (cities, function (i) dmean (bikedb = bikedb, city = i))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                       ny       ch       bo       la       ph
## dmean          2.8519131 2.510285 2.153918 2.156919 1.702372
## ratio_memb_non 0.9833729 1.023385 1.000635 1.360099 1.130929
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we thus discover that Boston manifests the greatest equality in terms of distances cycled between residents and non-residents, while LA manifests the greatest difference. New York City is the only one of these five in which non-members of the system actually cycle further than members. (And note that these two measures can&amp;rsquo;t be statistically compared in any direct way, because mean distances are also affected by relative numbers of member to non-member trips.) These results likely reflect a host of (scientifically) interesting cultural and geo-spatial differences between these cities, and demonstrate how the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package (combined with &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt;&lt;/a&gt;) can provide unique insight into differences in human behaviour between some of the most important cities in the U.S.&lt;/p&gt;

&lt;h3 id=&#34;visualisation&#34;&gt;Visualisation&lt;/h3&gt;

&lt;p&gt;Many users are likely to want to visualise how people use a given bicycle system, and in particular are likely to want to produce maps. This is also readily done with the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt;, which can route and aggregate transit flows for a particular mode of transport throughout a street network. Let&amp;rsquo;s plot bicycle flows for the Indego System of Philadelphia PA. First get the trip matrix, along with the coordinates of all bicycle stations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;gmost/dodgr&amp;quot;) # to install latest version
city &amp;lt;- &amp;quot;ph&amp;quot;
# store_bikedata (bikedb = bikedb, city = city) # if not already done
trips &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
xy &amp;lt;- stns [, which (names (stns) %in% c (&amp;quot;longitude&amp;quot;, &amp;quot;latitude&amp;quot;))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows of cyclists are calculated between those &lt;code&gt;xy&lt;/code&gt;points, so the &lt;code&gt;trips&lt;/code&gt; table has to match the &lt;code&gt;stns&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;indx &amp;lt;- match (stns$stn_id, rownames (trips))
trips &amp;lt;- trips [indx, indx]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; can be used to extract the underlying street network surrounding those &lt;code&gt;xy&lt;/code&gt; points (expanded here by 50%):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;net &amp;lt;- dodgr_streetnet (pts = xy, expand = 0.5) %&amp;gt;%
    weight_streetnet (wt_profile = &amp;quot;bicycle&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then need to align the bicycle station coordinates in &lt;code&gt;xy&lt;/code&gt; to the nearest points (or &amp;ldquo;vertices&amp;rdquo;) in the street network:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;verts &amp;lt;- dodgr_vertices (net)
pts &amp;lt;- verts$id [match_pts_to_graph (verts, xy)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows between these points can then be mapped onto the underlying street network with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;flow &amp;lt;- dodgr_flows (net, from = pts, to = pts, flow = trips) %&amp;gt;%
    merge_directed_flows ()
net &amp;lt;- net [flow$edge_id, ]
net$flow &amp;lt;- flow$flow
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; documentation&lt;/a&gt; for further details of how this works. We&amp;rsquo;re now ready to plot those flows, but before we do, let&amp;rsquo;s overlay them on top of the rivers of Philadelphia, extracted with rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;q &amp;lt;- opq (&amp;quot;Philadelphia pa&amp;quot;)
rivers1 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;waterway&amp;quot;, value = &amp;quot;river&amp;quot;, value_exact = FALSE) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers2 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;natural&amp;quot;, value = &amp;quot;water&amp;quot;) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers &amp;lt;- c (rivers1, rivers2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally plot the map, using rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmplotr&#34;&gt;&lt;code&gt;osmplotr&lt;/code&gt; package&lt;/a&gt; to prepare a base map with the underlying rivers, and the &lt;code&gt;ggplot2::geom_segment()&lt;/code&gt; function to add the line segments with colours and widths weighted by bicycle flows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#gtlibrary (osmplotr)
require (ggplot2)
bb &amp;lt;- get_bbox (c (-75.22, 39.91, -75.10, 39.98))
cols &amp;lt;- colorRampPalette (c (&amp;quot;lawngreen&amp;quot;, &amp;quot;red&amp;quot;)) (30)
map &amp;lt;- osm_basemap (bb, bg = &amp;quot;gray10&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_multipolygons, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_lines, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_colourbar (zlims = range (net$flow / 1000), col = cols)
map &amp;lt;- map + geom_segment (data = net, size = net$flow / 50000,
                           aes (x = from_lon, y = from_lat, xend = to_lon, yend = to_lat,
                                colour = flow, size = flow)) +
    scale_colour_gradient (low = &amp;quot;lawngreen&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;none&amp;quot;)
print_osm_map (map)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/ph.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The colour bar on the right shows thousands of trips, with the map revealing the relatively enormous numbers crossing the South Street Bridge over the Schuylkill River, leaving most other flows coloured in the lower range of green or yellows. This map thus reveals that anyone wanting to see Philadelphia&amp;rsquo;s Indego bikes in action without braving the saddle themselves would be best advised to head straight for the South Street Bridge.&lt;/p&gt;

&lt;h3 id=&#34;future-plans&#34;&gt;Future plans&lt;/h3&gt;

&lt;p&gt;Although the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; greatly facilitates the production of such maps, the code is nevertheless rather protracted, and it would probably be very useful to convert much of the code in the preceding section to an internal &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; function to map trips between pairs of stations onto corresponding flows through the underlying street networks.&lt;/p&gt;

&lt;p&gt;Beyond that point, and the list of currently open issues awaiting development on the &lt;a href=&#34;https://github.com/ropensci/bikedata/issues&#34;&gt;github repository&lt;/a&gt;, future development is likely to depend very much on how users use the package, and on what extra features people might want. How can you help? A great place to start might be the official &lt;a href=&#34;https://ropensci.org/blog/blog/2017/10/02/hacktoberfest&#34;&gt;Hacktoberfest issue&lt;/a&gt;, helping to import the next lot of data from &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/34&#34;&gt;San Francisco&lt;/a&gt;. Or just use the package, and open up a new issue in response to any ideas that might pop up, no matter how minor they might seem. See the &lt;a href=&#34;https://github.com/ropensci/bikedata/blob/master/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for general advice.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Finally, this package wouldn&amp;rsquo;t be what it is without my co-author &lt;a href=&#34;https://github.com/richardellison&#34;&gt;Richard Ellison&lt;/a&gt;, who greatly accelerated development through encouraging C rather than C++ code for the SQL interfaces. &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maëlle Salmon&lt;/a&gt; majestically guided the entire review process, and made the transformation of the package to its current polished form a joy and a pleasure. I remain indebted to both &lt;a href=&#34;https://github.com/chucheria&#34;&gt;Bea Hernández&lt;/a&gt; and &lt;a href=&#34;https://github.com/eamcvey&#34;&gt;Elaine McVey&lt;/a&gt; for offering their time to extensively test and &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/116&#34;&gt;review the package&lt;/a&gt; as part of rOpenSci&amp;rsquo;s onboarding process. The review process has made the package what it is, and for that I am grateful to all involved!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>.rprofile: David Smith</title>
      <link>https://ropensci.org/blog/2017/10/13/rprofile-david-smith/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/13/rprofile-david-smith/</guid>
      <description>
        
        &lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-13-rprofile_david_smith/david-smith.jpg&#34; alt=&#34;David Smith, R Community Lead at Microsoft&#34; style=&#34;margin: 0px 20px; width: 250px;&#34; align=&#34;left&#34;&gt;
&lt;em&gt;David Smith is a Blogger and Community Lead at Microsoft. I had the chance to interview David last May at rOpenSci unconf17. We spoke about his career, the process of working remote within a team, community development/outreach and his personal methods for discovering great content to share and write about.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;KO: What is your name, job title, and how long have you been using R?&lt;/p&gt;

&lt;p&gt;DS: My name is David Smith. I work at Microsoft and my self-imposed title is ‘R Community Lead’. I’ve been working with R specifically for about 10 years, but I’d been working with S since the early 90s.&lt;/p&gt;

&lt;p&gt;KO: How did you transition into using R?&lt;/p&gt;

&lt;p&gt;DS: I was using S for a long, long time, and I worked for the company that commercialized S, where I was a project manager for S-PLUS. And I got out of that company, and then worked for a startup in a different industry for a couple of years. But while I was there, the people that founded Revolution Analytics approached me because they were setting up a company to build a commercial business around R, and reached out to me because of my connections with the S community.&lt;/p&gt;

&lt;p&gt;KO: So you came to Microsoft through Revolution?&lt;/p&gt;

&lt;p&gt;DS: That’s correct. I was with Revolution Analytics and then Microsoft bought that company, so I’ve been with Microsoft since then.&lt;/p&gt;

&lt;p&gt;KO: How has that transition gone, is there a Revolution team inside of Microsoft, or has it become more spread out?&lt;/p&gt;

&lt;p&gt;DS: It’s been more spread out. It got split up into the engineering people and the marketing people, sales people all got distributed around. When I first went to Microsoft I started off in the engineering group doing product management. But I was also doing the community role, and it just wasn’t a very good fit just time-wise, between doing community stuff and doing code or product management. So then I switched to a different group called the Ecosystem team, and so now I’m 100% focused on community within a group that’s focused on the ecosystem in general.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The one piece of advice I could give anyone starting out in their careers is - write what you do, write it in public, and make it so that other people can reproduce what you did.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: What does it mean to be 100% community focused, do you do a lot of training?&lt;/p&gt;

&lt;p&gt;DS: I don’t do a lot of training myself, but I work with a lot of other people on the team who do training. We’re focused mainly on building up the ecosystem of people that ultimately add value to the products that Microsoft has. And we’re specifically involved in the products that Microsoft has that now incorporate R by building up the value of the R ecosystem.&lt;/p&gt;

&lt;p&gt;KO: What does your day-to-day look like, are you in an office, do you work remote?&lt;/p&gt;

&lt;p&gt;DS: I work from home. I had moved from Seattle where Microsoft is headquartered to Chicago a couple of months before the acquisition happened, so I wasn’t about to move back to Seattle. But they let me work from home in Chicago, which has worked out great because most of my job is communicating with the community at large. So I do the &lt;a href=&#34;http://blog.revolutionanalytics.com/&#34;&gt;Revolutions Blog&lt;/a&gt;, which I’ve been writing for eight or nine years now, writing stories about people using R, and applications of R packages. All as a way of communicating to the wider world the cool stuff that people can do with R, and also to the R community occasionally, what kind of things you can do with R in the Microsoft environment.&lt;/p&gt;

&lt;p&gt;KO: Have you always been a writer or interested in writing and communications?&lt;/p&gt;

&lt;p&gt;DS: No, no. I have a mathematics and computer science degree. I’m not trained as a writer. But it’s actually been useful having the perspective of statistics and mathematics and programming, and to bring that to a broader audience through writing. I’ve learned a lot about the whole writing and blogging and journalism process through that, but I’m certainly not trained in that way.&lt;/p&gt;

&lt;p&gt;KO: How does your Ecosystems team at Microsoft function and collaborate?&lt;/p&gt;

&lt;p&gt;DS: Unlike many teams at Microsoft, our team is very distributed. We have people working remotely from Denver, I’m in Chicago, Seattle, we’re all kind of distributed all around the place. So we meet virtually through Skype, have video meetings once a week and communicate a lot online.&lt;/p&gt;

&lt;p&gt;KO: What kind of tools are you using?&lt;/p&gt;

&lt;p&gt;DS: Traditionally, as in Microsoft, mainly email and Skype for the meetings. I set up an internal team focused around community more broadly around Microsoft and we use Microsoft Teams for that,
which is a little bit like Slack. But a lot of the stuff that I do is more out in the open, so I use a lot of Twitter and Github for the code that I point to and stuff like that.&lt;/p&gt;

&lt;p&gt;KO: How do you manage your Twitter?&lt;/p&gt;

&lt;p&gt;DS: Twitter I do manually in real-time. I don’t do a lot of scheduling except for &lt;a href=&#34;https://twitter.com/rlangtip&#34;&gt;@RLangTip&lt;/a&gt;
which is a feed of daily R tips. And for that I do scheduling through Tweetdeck on the web.&lt;/p&gt;

&lt;p&gt;KO: How many Twitter accounts are you managing?&lt;/p&gt;

&lt;p&gt;DS: I run &lt;a href=&#34;https://twitter.com/revodavid&#34;&gt;@revodavid&lt;/a&gt; which is my personal twitter account, and &lt;a href=&#34;https://twitter.com/rlangtip&#34;&gt;@RLangTip&lt;/a&gt; which is R language tips. I tweet for &lt;a href=&#34;https://twitter.com/R_Forwards&#34;&gt;@R_Forwards&lt;/a&gt; which is the diversity community for R, &lt;a href=&#34;https://twitter.com/RConsortium&#34;&gt;@RConsortium&lt;/a&gt;, the R Consortium, so quite a few.&lt;/p&gt;

&lt;p&gt;KO: How long has this been a core part of your work day?&lt;/p&gt;

&lt;p&gt;DS: The community thing as a focus, maybe five or six years? My career path for a long time was in product management. So I managed S-PLUS as a product for a long time, I managed another product at a different startup, and then I came to Revolution and I did a combination of engineering and product management. But in the last 18 months I’ve been 100% in the community space.&lt;/p&gt;

&lt;p&gt;KO: How did you get into product management to begin with?&lt;/p&gt;

&lt;p&gt;DS: That’s a good question that I’m not sure I know the answer to. I started off my first job after university &amp;ndash; I actually left university specifically to become a support engineer for S-PLUS. When I took on that role, they didn’t really have product management yet at that company, and so when they were looking for somebody to basically steer S-PLUS as a product, it was a good fit for me and an opportunity to move to the States. I took that on and I kind of just learned product management as I did it. I went to a few sort of training/seminar type things, but I didn’t study it.&lt;/p&gt;

&lt;p&gt;KO: Sure. It seems like something that people just kind of get saddled with sometimes?&lt;/p&gt;

&lt;p&gt;DS: Exactly. It’s a discipline that doesn’t really have discipline. But for the various companies I’ve worked for, mostly startups, they all seem to have very different perspectives on what product management is and what the role of a product manager is.&lt;/p&gt;

&lt;p&gt;KO: Yeah, I know what you mean. Are you happy to have sort of moved away from that?&lt;/p&gt;

&lt;p&gt;DS: I am in the sense of &amp;ndash; it was different being in a startup where being a product manager was more like being the shepherd of an entire product ecosystem, whereas in a big company the product manager is a lot more focused and inherently so, a lot more narrow. I happen to prefer the bigger picture I guess.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Honestly, I kind of focus from the point of view of what interests me personally. Which doesn’t sound very community oriented at all… but it’s an exercise in empathy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: What’s your process for deciding what things you talk about and bring to the community?&lt;/p&gt;

&lt;p&gt;DS: Honestly, I kind of focus from the point of view of what interests me personally. Which doesn’t sound very community oriented at all… but it’s an exercise in empathy. If I can write about something, or find a topic that I might find is interesting or exciting and I can communicate that with other people, I’m motivated to write about it and I hope that people are then motivated to learn about it. Kind of the antithesis of this is when I worked in marketing for a while; a lot of that style of writing was the bane of my existence because you’re producing these documents that literally are designed for nobody to read, in this language that nobody engages with. I much prefer blogging and tweeting because it’s much more directly for people.&lt;/p&gt;

&lt;p&gt;KO: What have some of your most popular or successful engagements been about? Feel free to interpret ‘successful&amp;rsquo; in any way.&lt;/p&gt;

&lt;p&gt;DS: Well, from the point of view of what has been the most rewarding part of my job, is finding under-recognized or just these really cool things that people have done that just haven’t had a lot of exposure. And I’ve got a fairly big audience and a fairly wide reach, and it’s always fun for me to find things that people have done that maybe haven’t been seen. And it’s not my work, but I can &amp;ndash; you know &amp;ndash; take an eight page document that somebody’s written that has really cool things in it and just pull out various things. There is so much very cool stuff that people have done, half of the battle is getting it out there.&lt;/p&gt;

&lt;p&gt;KO: What are some of your favorite sources for discovering cool things on the internet?&lt;/p&gt;

&lt;p&gt;DS: There are channels on Reddit that I get a lot of material from, like &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful/&#34;&gt;/r/dataisbeautiful&lt;/a&gt; and things like that. It’s hard to say particular accounts on twitter, but I’ve spent a lot of time following people where I’ve read one of their blog posts and I find their twitter account, and they have just a few followers, I’ll follow them, and then over time it amounts to some good stuff. I have twitter open all day, every day. I don’t read everything on my feed every day, but I certainly keep it open.&lt;/p&gt;

&lt;p&gt;KO: How much of your day is just spent exploring?&lt;/p&gt;

&lt;p&gt;DS: A lot of it. I spend about half of any given day reading. It takes a long time, but every now and then you find this really cool stuff.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It’s one thing to be able to do really cool stuff in R or any other language, but until you can distill that down into something that other people consume, it’s going to be hard to sell yourself.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Do you have any last nuggets of wisdom for people starting out their careers in R?&lt;/p&gt;

&lt;p&gt;DS: For people starting out their careers, I think one of the most important skills to learn is that communication skill. It’s one thing to be able to do really cool stuff in R or any other language, but until you can distill that down into something that other people consume, it’s going to be hard to sell yourself. And it’s also going to be hard to be valuable. A lot of the people I’ve watched evolve in the community are people who have begun very early in their careers, blogging about what they do. The one piece of advice I could give anyone starting out in their careers is - write what you do, write it in public, and make it so that other people can reproduce what you did.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Changes to Internet Connectivity in R on Windows</title>
      <link>https://ropensci.org/technotes/2017/10/10/curl-30/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/10/10/curl-30/</guid>
      <description>
        
        

&lt;p&gt;This week we released version 3.0 of the &lt;a href=&#34;https://cran.r-project.org/web/packages/curl/vignettes/intro.html&#34;&gt;curl&lt;/a&gt; R package to CRAN. You may have never used this package directly, but &lt;code&gt;curl&lt;/code&gt; provides the foundation for most HTTP infrastructure in R, including &lt;code&gt;httr&lt;/code&gt;, &lt;code&gt;rvest&lt;/code&gt;, and all packages that build on it. If R packages need to go online, chances are traffic is going via curl.&lt;/p&gt;

&lt;p&gt;This release introduces an important change for Windows users: we are switching from OpenSSL to Secure Channel on Windows 7 / 2008-R2 and up. Let me explain this in a bit more detail.&lt;/p&gt;

&lt;h2 id=&#34;why-switching-ssl-backends&#34;&gt;Why Switching SSL backends&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://curl.haxx.se/libcurl/&#34;&gt;libcurl&lt;/a&gt; C library requires an external crypto library to provide the SSL layer (the S in HTTPS). On Linux / MacOS, libcurl is included with the OS so we don&amp;rsquo;t worry about this. However on Windows we ship our own build of libcurl so we can choose if we want to build against &lt;a href=&#34;https://www.openssl.org/&#34;&gt;OpenSSL&lt;/a&gt; or Windows native SSL api called &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/aa380123(v=vs.85).aspx&#34;&gt;Secure Channel&lt;/a&gt;, also referred to as just &amp;ldquo;WinSSL&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Thus far we have always used libcurl with OpenSSL, which works consistently on all versions of Windows. However OpenSSL requires that we provide our own CA bundle, which is not ideal. In particular users on corporate / government networks have reported difficulty connecting to the internet in R. The reason is often that their enterprise gateway / proxy uses custom certificates which are installed in the Windows certificate manager, but are not present in R&amp;rsquo;s bundle.&lt;/p&gt;

&lt;p&gt;Moreover shipping our own CA bundle can be a security risk. If a CA gets hacked, the corresponding certificate needs to be revoked immediately. Operating systems can quickly push a security update to all users, but we cannot do this in R.&lt;/p&gt;

&lt;h2 id=&#34;switching-to-winssl&#34;&gt;Switching to WinSSL&lt;/h2&gt;

&lt;p&gt;If we build libcurl against Windows native &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/aa380123(v=vs.85).aspx&#34;&gt;Secure Channel&lt;/a&gt;, it automatically uses the same SSL certificates as Internet Explorer. Hence we do not have to ship and maintain a custom CA bundle. Earlier this year I tried to switch the &lt;code&gt;curl&lt;/code&gt; package to WinSSL, and everything seemed to work great on my machine.&lt;/p&gt;

&lt;p&gt;However when we started checking reverse dependecies on CRAN WinBuilder, many packages depending on curl started to fail! It turned out Windows versions before Windows 7 do not natively support TLS 1.1 and 1.2 by default. Because TLS 1.2 is used by the majority of HTTPS servers today, WinSSL is basically useless on these machines. Unfortunately this also includes CRAN WinBuilder which runs Windows 2008 (the server edition of Vista).&lt;/p&gt;

&lt;p&gt;So we had no choice but to roll back to OpenSSL in order to keep everything working properly on CRAN. Bummer.&lt;/p&gt;

&lt;h2 id=&#34;towards-dual-ssl&#34;&gt;Towards Dual SSL&lt;/h2&gt;

&lt;p&gt;I had almost given up on this when a few weeks ago Daniel Stenberg posted the following &lt;a href=&#34;https://curl.haxx.se/mail/lib-2017-08/0118.html&#34;&gt;announcement&lt;/a&gt; on the libcurl mailing list:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hi friends!
As of minutes ago, libcurl has the ability to change SSL backend dynamically
at run-time - if built with the support enabled. That means that the choice
does no longer only have to happen at build-time.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This new feature gives us exactly the flexibility we need. We can take advantage of native Secure Channel on Windows 7 and up which are almost all users. However we can keep things working in legacy servers by falling back on OpenSSL on these machines, including the CRAN win builder.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cran/curl/blob/3.0/src/ssl.c#L11-L17&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/curl30.png&#34; alt=&#34;code&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So this is where we are. Version 3.0 of the curl R package uses the latest &lt;a href=&#34;https://github.com/rwinlib/libcurl/releases&#34;&gt;libcurl 7.56.0&lt;/a&gt; and automatically switches to native SSL on Windows 7 and up. If all goes well, nobody should not notice any changes, except those people on enterprise networks where things will, hopefully, magically start working.&lt;/p&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;Because each Windows network seems to have a different setup, testing and debugging these things is often difficult. We are interested to hear from Windows users if updating to curl 3.0 has improved the situation, or if any unexpected side effects arise. Please &lt;a href=&#34;https://github.com/jeroen/curl/issues&#34;&gt;open an issue&lt;/a&gt; on Github if you run into problems.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Governance, Engagement, and Resistance in the Open Science Movement: A Comparative Study</title>
      <link>https://ropensci.org/blog/2017/10/06/sholler-plan/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/06/sholler-plan/</guid>
      <description>
        
        

&lt;p&gt;A growing community of scientists from a variety of disciplines is moving the norms of scientific research toward open practices. Supporters of open science hope to increase the quality and efficiency of research by enabling the widespread sharing of datasets, research software source code, publications, and other processes and products of research. The speed at which the open science community seems to be growing mirrors the rapid development of technological capabilities, including robust open source scientific software, new services for data sharing and publication, and novel data science techniques for working with massive datasets. Organizations like rOpenSci harness such capabilities and deploy various combinations of these research tools, or what I refer to here as open science infrastructures, to facilitate open science.&lt;/p&gt;

&lt;p&gt;As studies of other digital infrastructures have pointed out, developing and deploying the technological capabilities that support innovative work within a community of practitioners constitutes just one part of making innovation happen. As quickly as the technical solutions to improving scientific research may be developing, a host of organizational and social issues are lagging behind and hampering the open science community’s ability to inscribe open practices in the culture of scientific research. Remedying organizational and social issues requires paying attention to open science infrastructures’ human components, such as designers, administrators, and users, as well as the policies, practices, and organizational structures that contribute to the smooth functioning of these systems.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; These elements of infrastructure development have, in the past, proven to be equal to or more important than technical capabilities in determining the trajectory the infrastructure takes (e.g., whether it “succeeds” or “fails”).&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;As a postdoc with rOpenSci, I have begun a qualitative, ethnographic project to explore the organizational and social processes involved in making open science the norm in two disciplines: astronomy and ecology. I focus on these two disciplines to narrow, isolate, and compare the set of contextual factors (e.g., disciplinary histories, research norms, and the like) that might influence perceptions of open science. Specifically, I aim to answer the following research questions (RQ):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;RQ1a: What are the primary motivations of scientists who actively engage with open science infrastructures?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RQ1b: What are the factors that influence resistance to open science among some scientists?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RQ2: What strategies do open science infrastructure leaders use to encourage participation, govern contributions, and overcome resistance to open science infrastructure use?&lt;/p&gt;

&lt;p&gt;a. To what extent do governance strategies balance standardization and flexibility, centralization and decentralization, and voluntary and mandatory contributions?&lt;/p&gt;

&lt;p&gt;b. By what mechanisms are open science policies and practices enforced?&lt;/p&gt;

&lt;p&gt;c. What are the commonalities and differences in the rationale behind choices of governance strategies?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below, I describe how I am systematically investigating these questions in two parts. In Part 1, I am identifying the issues raised by scientists who engage with or resist the open science movement. In Part 2, I am studying the governance strategies open science leaders and decision-makers use to elicit engagement with open science infrastructures in these disciplines.&lt;/p&gt;

&lt;h3 id=&#34;part-1-engagement-with-and-resistance-to-open-science&#34;&gt;Part 1: Engagement with and Resistance to Open Science&lt;/h3&gt;

&lt;p&gt;I am firmly rooted in a research tradition which emphasizes that studying the uptake of a new technology or technological approach, no matter the type of work or profession, begins with capturing how the people charged with changing their activities respond to the change “on the ground.” In this vein, Part 1 of the study aims to lend empirical support or opposition to arguments for and against open science that are commonly found in opinion pieces, on social media, and in organizational mission statements. A holistic reading of such documents reveals several commonalities in the positions for and against open science. Supporters of open science often cite increased transparency, reproducibility, and collaboration as the overwhelming benefits of making scientific research processes and products openly available. Detractors highlight concerns over “scooping,” ownership, and the time costs of curating and publishing code and data.&lt;/p&gt;

&lt;p&gt;I am seeking to verify and test these claims by interviewing and surveying astronomers and ecologists or, more broadly, earth and environmental scientists who fall on various parts of the open science engagement-to-resistance spectrum. I am conducting interviews using a semi-structured interview protocol&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; across all interviewees. I will then use a qualitative data analysis approach based on the grounded theory method&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; to extract themes from the responses, focusing on the factors that promote engagement (e.g., making data available, spending time developing research software, or making publications openly accessible) or resistance (e.g., unwillingness to share code used in a study or protecting access to research data). Similar questions will be asked at scale via a survey.&lt;/p&gt;

&lt;p&gt;Armed with themes from the responses, I will clarify and refine the claims often made in the public sphere about the benefits and drawbacks of open science. I hope to develop this part of the study into actionable recommendations for promoting open science, governing contributions to open science repositories, and addressing the concerns of scientists who are hesitant about engagement.&lt;/p&gt;

&lt;h3 id=&#34;part-2-focusing-on-governance&#34;&gt;Part 2: Focusing on Governance&lt;/h3&gt;

&lt;p&gt;Even with interviews and surveys of scientists on the ground, it is difficult to systematically trace and analyze the totality of social and political processes that support open science infrastructure development because the processes occur across geographic, disciplinary, and other boundaries.&lt;/p&gt;

&lt;p&gt;However, as others have pointed out,&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; the organizational and social elements of digital infrastructure development often become visible and amenable to study through infrastructure governance. Governance refers to the combination of “executive and management roles, program oversight functions organized into structures, and policies that define management principles and decision making.”&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; Effective governance provides projects with the direction and oversight necessary to achieve desired outcomes of infrastructure development while allowing room for creativity and innovation.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:9&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:10&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:10&#34;&gt;10&lt;/a&gt;&lt;/sup&gt; Studying a project’s governance surfaces the negotiation processes that occur among stakeholders—users, managers, organizations, policymakers, and the like—throughout the development process. Outcomes include agreements about the types of technologies used, standards defining the best practices for technology use, and other policies to ensure that a robust, sustainable infrastructure evolves.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:9&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:11&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:11&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Despite the scientific research community’s increasing reliance on open science infrastructures, few studies compare different infrastructure governance strategies&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and even fewer develop new or revised strategies for governing infrastructure development and use.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:12&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:12&#34;&gt;12&lt;/a&gt;&lt;/sup&gt; The primary goal of this part of the project is to address this gap in our understanding of the governance strategies used to create, maintain, and grow open science infrastructures.&lt;/p&gt;

&lt;p&gt;I am administering this part of the study by conducting in-depth, semi-structured interviews with leaders of various open science infrastructure projects supporting work in astronomy and ecology. I define “leaders” in this context as individuals or small groups of individuals who make decisions about the management of open science infrastructures and their component parts. This set of leaders includes founders and administrators of widely-used scientific software packages and collections of packages, of open data repositories, of open access publication and preprint services, and various combinations of open science tools. Furthermore, I intend to interview the leaders of organizations with which the open science community interacts—top publication editors, for example—to gauge how open science practices and processes are being governed outside of active open science organizations.&lt;/p&gt;

&lt;p&gt;I will conduct qualitative coding as described above to develop themes from the responses of open science leaders. I will then ground these themes in the literature on digital infrastructure governance—which emphasizes gradual, decentralized, and voluntary development—and look for avenues to improve governance strategies.&lt;/p&gt;

&lt;p&gt;Alongside the interview and survey methods, I am actively observing and retaining primary documents from the ongoing discourse around open science in popular scientific communication publications (e.g., &lt;i&gt;Nature&lt;/i&gt; and &lt;i&gt;Science&lt;/i&gt;), conferences and meetings (e.g., OpenCon and discipline-specific hackweeks), and in the popular media/social media (e.g., &lt;i&gt;The New York Times&lt;/i&gt; and Twitter threads).&lt;/p&gt;

&lt;h3 id=&#34;preliminary-themes&#34;&gt;Preliminary Themes&lt;/h3&gt;

&lt;p&gt;I entered this project with a very basic understanding of how open science “works”—the technical and social mechanisms by which scientists make processes and outputs publicly available. In learning about the open science movement, in general and in particular instantiations, I’ve begun to see the intricacies involved in efforts to change scientific research and its modes of communication: research data publication, citation, and access; journal publication availability; and research software development and software citation standards. Within the community trying to sustain these changes are participants and leaders who are facing and tackling several important issues head-on. I list some of the most common engagement, resistance, and governance challenges appearing in interview and observation transcripts below.&lt;/p&gt;

&lt;h4 id=&#34;participation-challenges&#34;&gt;Participation challenges&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Overcoming the fear of sharing code and data, specifically the fear of sharing “messy” code and the fear of being shamed for research errors.&lt;/li&gt;
&lt;li&gt;Defending the time and financial costs of participation in open science—particularly open source software development—to supervisors, collaborators, or tenure and promotion panels who are not engaged with open science.&lt;/li&gt;
&lt;li&gt;Finding time to make code and data usable for others (e.g., through good documentation or complete metadata) and, subsequently, finding a home where code and data can easily be searched and found.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;governance-challenges&#34;&gt;Governance challenges&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Navigating the issue of convincing researchers that software development and data publication/archiving “count” as research products, even though existing funding, publication, and tenure and promotion models may not yet value those contributions.&lt;/li&gt;
&lt;li&gt;Developing guidelines and processes for conducting peer review on research publication, software, and data contributions, especially the tensions involved in “open review.”&lt;/li&gt;
&lt;li&gt;Deciding whose responsibility it is to enforce code and data publication standards or policies, both within open science organizations and in traditional outlets like academic journals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The points raised in this post and the questions guiding my project might seem like discussions you’ve had too many times over coffee during a hackathon break or over beers after a conference session. If so, I’d love to hear from you, even if you are not an astronomer, an ecologist, or an active leader of an open science infrastructure (dsholler at berkeley dot edu). I am always looking for new ideas, both confirming and disconfirming, to refine my approach to this project.&lt;/p&gt;

&lt;!-- references --&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Braa, J., Hanseth, O., Heywood, A., Mohammed, W., Shaw, V. 2007. Developing health information systems in developing countries: The flexible standards strategy. &lt;i&gt;MIS Quarterly&lt;/i&gt;, 31(2), 381-402. &lt;a href=&#34;https://doi.org/10.2307/25148796&#34;&gt;https://doi.org/10.2307/25148796&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Tilson, D., Lyytinen, K., Sørensen, C. 2010 Digital infrastructures: The missing IS research agenda. &lt;i&gt;Information Systems Research&lt;/i&gt;, 21(4), 748-759. &lt;a href=&#34;https://doi.org/10.1287/isre.1100.0318&#34;&gt;https://doi.org/10.1287/isre.1100.0318&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Borgman, C. L. 2010. &lt;i&gt;Scholarship in the digital age: Information, infrastructure, and the Internet.&lt;/i&gt; MIT Press, Cambridge, MA. ISBN: 9780262250863
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Vaast, E., Walsham, G. 2009. Trans-situated learning: Supporting a network of practice with an information infrastructure. &lt;i&gt;Information Systems Research&lt;/i&gt;, 20(4), 547-564. &lt;a href=&#34;https://doi.org/10.1287/isre.1080.0228&#34;&gt;https://doi.org/10.1287/isre.1080.0228&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;Spradley, J. P. (2016). &lt;i&gt;The ethnographic interview&lt;/i&gt;. Longegrove, IL: Waveland Press. ISBN: 0030444969
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;Corbin, J., Strauss, A.,  1990. Grounded theory research: Procedures, canons, and evaluative criteria. &lt;i&gt;Qualitative Sociology&lt;/i&gt;, 13(1), 3-21. &lt;a href=&#34;https://doi.org/10.1007/BF00988593&#34;&gt;https://doi.org/10.1007/BF00988593&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;Barrett, M., Davidson, E., Prabhu, J., Vargo, S. L. 2015. Service innovation in the digital age: Key contributions and future directions. &lt;i&gt;MIS Quarterly&lt;/i&gt;, 39(1) 135-154. DOI: 10.25300/MISQ/2015/39:1.03
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;Hanford, M. 2005. Defining program governance and structure. &lt;i&gt;IBM developerWorks&lt;/i&gt;. Available at: &lt;a href=&#34;https://www.ibm.com/developerworks/rational/library/apr05/hanford/&#34;&gt;https://www.ibm.com/developerworks/rational/library/apr05/hanford/&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;Star, S. L., Ruhleder, K. 1996. Steps toward an ecology of infrastructure: Design and access for large information spaces. &lt;i&gt;Information Systems Research&lt;/i&gt;, 7(1), 111-134. &lt;a href=&#34;https://doi.org/10.1287/isre.7.1.111&#34;&gt;https://doi.org/10.1287/isre.7.1.111&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:9&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;Edwards, P. N., Jackson, S. J., Bowker, G. C., Knobel, C. P. 2007. &lt;i&gt;Understanding infrastructure: Dynamics, tensions, and design. Final report for Workshop on History and Theory of Infrastructure: Lessons for New Scientific Cyberinfrastructures&lt;/i&gt;. NSF Report. Available at: &lt;a href=&#34;https://deepblue.lib.umich.edu/handle/2027.42/49353&#34;&gt;https://deepblue.lib.umich.edu/handle/2027.42/49353&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:10&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;Hanseth, O., Jacucci, E., Grisot, M., Aanestad, M. 2006. Reflexive standardization: Side effects and complexity in standard making. &lt;i&gt;MIS Quarterly&lt;/i&gt;, 30(1), 563-581. &lt;a href=&#34;https://doi.org/10.2307/25148773&#34;&gt;https://doi.org/10.2307/25148773&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:11&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;Hanseth, O., &amp;amp; Lyytinen, K. (2010). Design theory for dynamic complexity in information infrastructures: the case of building internet. &lt;i&gt;Journal of Information Technology&lt;/i&gt;, 25(1), 1-19.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:12&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>googleLanguageR - Analysing language through the Google Cloud Machine Learning APIs</title>
      <link>https://ropensci.org/blog/2017/10/03/googlelanguager/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/03/googlelanguager/</guid>
      <description>
        
        

&lt;!-- open source image taken from: https://upload.wikimedia.org/wikipedia/commons/2/21/Bell_System_switchboard.jpg --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/switchboard.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the greatest assets human beings possess is the power of speech and language, from which almost all our other accomplishments flow. To be able to analyse communication offers us a chance to gain a greater understanding of one another.&lt;/p&gt;

&lt;p&gt;To help you with this, &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt;&lt;/a&gt; is an R package that allows you to perform speech-to-text transcription, neural net translation and natural language processing via the &lt;a href=&#34;https://cloud.google.com/products/machine-learning/&#34;&gt;Google Cloud machine learning services&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An introduction to the package is below, but you can find out more details at the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-s-bet&#34;&gt;Google&amp;rsquo;s bet&lt;/h3&gt;

&lt;p&gt;Google predicts that machine learning is to be a fundamental feature of business, and so they are looking to become the infrastructure that makes machine learning possible. Metaphorically speaking: If machine learning is electricity, then Google wants to be the pylons carrying it around the country.&lt;/p&gt;

&lt;!-- open source image taken from: https://pixabay.com/en/pylon-sky-electricity-tower-2515429/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/pylon.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Google may not be the only company with such ambitions, but one advantage Google has is the amount of data it possesses. Twenty years of web crawling has given it an unprecedented corpus to train its models.  In addition, its recent moves into voice and video gives it one of the biggest audio and speech datasets, all of which have been used to help create machine learning applications within its products such as search and Gmail. Further investment in machine learning is shown by Google&amp;rsquo;s purchase of &lt;a href=&#34;https://deepmind.com/&#34;&gt;Deepmind&lt;/a&gt;, a UK based A.I. research firm that recently was in the news for defeating the top Go champion with its neural network trained Go bot.  Google has also taken an open-source route with the creation and publication of &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt;, a leading machine learning framework.&lt;/p&gt;

&lt;p&gt;Whilst you can create your own machine learning models, for those users who haven&amp;rsquo;t the expertise, data or time to do so, Google also offers an increasing range of machine learning APIs that are pre-trained, such as image and video recognition or job search.  &lt;code&gt;googleLanguageR&lt;/code&gt; wraps the subset of those machine learning APIs that are language flavoured - Cloud Speech, Translation and Natural Language.&lt;/p&gt;

&lt;p&gt;Since they carry complementary outputs that can be used in each other&amp;rsquo;s input, all three of the APIs are included in one package. For example, you can transcribe a recording of someone speaking in Danish, translate that to English and then identify how positive or negative the writer felt about its content (sentiment analysis) then identify the most important concepts and objects within the content (entity analysis).&lt;/p&gt;

&lt;h3 id=&#34;motivations&#34;&gt;Motivations&lt;/h3&gt;

&lt;h4 id=&#34;fake-news&#34;&gt;Fake news&lt;/h4&gt;

&lt;p&gt;One reason why I started looking at this area was the growth of &amp;lsquo;fake news&amp;rsquo;, and its effect on political discourse on social media. I wondered if there was some way to put metrics on how much a news story fuelled one&amp;rsquo;s own bias within your own filter bubble.  The entity API provides a way to perform entity and sentiment analysis at scale on tweets, and by then comparing different users and news sources preferences the hope is to be able to judge how much they are in agreement with your own bias, views and trusted reputation sources.&lt;/p&gt;

&lt;h4 id=&#34;make-your-own-alexa&#34;&gt;Make your own Alexa&lt;/h4&gt;

&lt;p&gt;Another motivating application is the growth of voice commands that will become the primary way of user interface with technology.  Already, &lt;a href=&#34;https://www.thinkwithgoogle.com/data-gallery/detail/google-app-voice-search/&#34;&gt;Google reports up to 20% of search in its app&lt;/a&gt; is via voice search.  I&amp;rsquo;d like to be able to say &amp;ldquo;R, print me out that report for client X&amp;rdquo;.  A Shiny app that records your voice, uploads to the API then parses the return text into actions gives you a chance to create your very own Alexa-like infrastructure.&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;text-align:center&#34;&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/alexa.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The voice activated internet connected speaker, Amazon&amp;rsquo;s Alexa - image from www.amazon.co.uk&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;translate-everything&#34;&gt;Translate everything&lt;/h4&gt;

&lt;p&gt;Finally, I live and work in Denmark.  As Danish is only spoken by less than 6 million people, applications that work in English may not be available in Danish very quickly, if at all.  The API&amp;rsquo;s translation service is the one that made the news in 2016 for &lt;a href=&#34;https://research.googleblog.com/2016/09/a-neural-network-for-machine.html&#34;&gt;&amp;ldquo;inventing its own language&amp;rdquo;&lt;/a&gt;, and offers much better English to Danish translations that the free web version and may make services available in Denmark sooner.&lt;/p&gt;

&lt;h3 id=&#34;using-the-library&#34;&gt;Using the library&lt;/h3&gt;

&lt;p&gt;To use these APIs within R, you first need to do a one-time setup to create a Google Project, add a credit card and authenticate which is &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/#installation&#34;&gt;detailed on the package website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After that, you feed in the R objects you want to operate upon.  The &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/127&#34;&gt;rOpenSci review&lt;/a&gt; helped to ensure that this can scale up easily, so that you can feed in large character vectors which the library will parse and rate limit as required.  The functions also work within &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; pipe syntax.&lt;/p&gt;

&lt;h4 id=&#34;speech-to-text&#34;&gt;Speech-to-text&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/speech.html&#34;&gt;Cloud Speech API&lt;/a&gt; is exposed via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_speech.html&#34;&gt;&lt;code&gt;gl_speech&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It supports multiple audio formats and languages, and you can either feed a sub-60 second audio file directly, or perform asynchrnous requests for longer audio files.&lt;/p&gt;

&lt;p&gt;Example code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)

my_audio &amp;lt;- &amp;quot;my_audio_file.wav&amp;quot;
gl_speech(my_audio)
#  A tibble: 1 x 3
#  transcript confidence                 words
#* &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;                &amp;lt;list&amp;gt;
#1 Hello Mum  0.9227779 &amp;lt;data.frame [19 x 3]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;translation&#34;&gt;Translation&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/translation.html&#34;&gt;Cloud Translation API&lt;/a&gt; lets you translate text via &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_translate.html&#34;&gt;&lt;code&gt;gl_translate&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you are charged per character, one tip here if you are working with lots of different languages is to perform detection of language offline first using another rOpenSci package, &lt;a href=&#34;https://github.com/ropensci/cld2&#34;&gt;&lt;code&gt;cld2&lt;/code&gt;&lt;/a&gt;.  That way you can avoid charges for text that is already in your target language i.e. English.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
library(cld2)
library(purrr)

my_text &amp;lt;- c(&amp;quot;Katten sidder på måtten&amp;quot;, &amp;quot;The cat sat on the mat&amp;quot;)

## offline detect language via cld2
detected &amp;lt;- map_chr(my_text, detect_language)
# [1] &amp;quot;DANISH&amp;quot;  &amp;quot;ENGLISH&amp;quot;

## get non-English text
translate_me &amp;lt;- my_text[detected != &amp;quot;ENGLISH&amp;quot;]

## translate
gl_translate(translate_me)
## A tibble: 1 x 3
#                 translatedText detectedSourceLanguage                    text
#*                         &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;
#1 The cat is sitting on the mat                     da Katten sidder på måtten
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/nlp.html&#34;&gt;Natural Language API&lt;/a&gt; reveals the structure and meaning of text, accessible via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_nlp.html&#34;&gt;&lt;code&gt;gl_nlp&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It returns several analysis:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Entity analysis&lt;/em&gt; - finds named entities (currently proper names and common nouns) in the text along with entity types, salience, mentions for each entity, and other properties. If possible, will also return metadata about that entity such as a Wikipedia URL.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Syntax&lt;/em&gt; - analyzes the syntax of the text and provides sentence boundaries and tokenization along with part of speech tags, dependency trees, and other properties.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sentiment&lt;/em&gt; - the overall sentiment of the text, represented by a magnitude [0, +inf] and score between -1.0 (negative sentiment) and 1.0 (positive sentiment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all useful to get an understanding of the meaning of a sentence, and has potentially the greatest number of applications of the APIs featured.  With entity analysis, auto categorisation of text is possible; the syntax returns let you pull out nouns and verbs for parsing into other actions; and the sentiment analysis allows you to get a feeling for emotion within text.&lt;/p&gt;

&lt;p&gt;A demonstration is below which gives an idea of what output you can generate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
quote &amp;lt;- &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
nlp &amp;lt;- gl_nlp(quote)

str(nlp)
#List of 6
# $ sentences        :List of 1
#  ..$ :&#39;data.frame&#39;:	1 obs. of  4 variables:
#  .. ..$ content    : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
#  .. ..$ beginOffset: int 0
#  .. ..$ magnitude  : num 0.6
#  .. ..$ score      : num -0.6
# $ tokens           :List of 1
#  ..$ :&#39;data.frame&#39;:	20 obs. of  17 variables:
#  .. ..$ content       : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;things&amp;quot; &amp;quot;are&amp;quot; &amp;quot;infinite&amp;quot; ...
#  .. ..$ beginOffset   : int [1:20] 0 4 11 15 23 25 29 38 42 48 ...
#  .. ..$ tag           : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NOUN&amp;quot; &amp;quot;VERB&amp;quot; &amp;quot;ADJ&amp;quot; ...
#  .. ..$ aspect        : chr [1:20] &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; ...
#  .. ..$ case          : chr [1:20] &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; ...
#  .. ..$ form          : chr [1:20] &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; ...
#  .. ..$ gender        : chr [1:20] &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; ...
#  .. ..$ mood          : chr [1:20] &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;INDICATIVE&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; ...
#  .. ..$ number        : chr [1:20] &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;PLURAL&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; ...
#  .. ..$ person        : chr [1:20] &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; ...
#  .. ..$ proper        : chr [1:20] &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; ...
#  .. ..$ reciprocity   : chr [1:20] &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; ...
#  .. ..$ tense         : chr [1:20] &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;PRESENT&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; ...
#  .. ..$ voice         : chr [1:20] &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; ...
#  .. ..$ headTokenIndex: int [1:20] 1 2 2 2 2 6 2 6 9 6 ...
#  .. ..$ label         : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NSUBJ&amp;quot; &amp;quot;ROOT&amp;quot; &amp;quot;ACOMP&amp;quot; ...
#  .. ..$ value         : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;thing&amp;quot; &amp;quot;be&amp;quot; &amp;quot;infinite&amp;quot; ...
# $ entities         :List of 1
#  ..$ :Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	6 obs. of  9 variables:
#  .. ..$ name         : chr [1:6] &amp;quot;human stupidity&amp;quot; &amp;quot;things&amp;quot; &amp;quot;universe&amp;quot; &amp;quot;universe&amp;quot; ...
#  .. ..$ type         : chr [1:6] &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; ...
#  .. ..$ salience     : num [1:6] 0.1662 0.4771 0.2652 0.2652 0.0915 ...
#  .. ..$ mid          : Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ wikipedia_url: Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ magnitude    : num [1:6] NA NA NA NA NA NA
#  .. ..$ score        : num [1:6] NA NA NA NA NA NA
#  .. ..$ beginOffset  : int [1:6] 42 4 29 86 29 86
#  .. ..$ mention_type : chr [1:6] &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; ...
# $ language         : chr &amp;quot;en&amp;quot;
# $ text             : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
# $ documentSentiment:Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	1 obs. of  2 variables:
#  ..$ magnitude: num 0.6
#  ..$ score    : num -0.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;This package is 10 times better due to the efforts of the rOpenSci reviewers &lt;a href=&#34;http://enpiar.com/&#34;&gt;Neal Richardson&lt;/a&gt; and &lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt;, who have whipped the documentation, outputs and test cases into the form they are today in &lt;code&gt;0.1.0&lt;/code&gt;.  Many thanks to them.&lt;/p&gt;

&lt;p&gt;Hopefully, this is just the beginning and the package can be further improved by its users - if you do give the package a try and find a potential improvement, &lt;a href=&#34;https://github.com/ropensci/googleLanguageR/issues&#34;&gt;raise an issue on GitHub&lt;/a&gt; and we can try to implement it.  I&amp;rsquo;m excited to see what users can do with these powerful tools.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
