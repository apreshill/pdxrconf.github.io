<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rplos on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/rplos/</link>
    <description>Recent content in Rplos on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 22 Oct 2013 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/rplos/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>OA week - A simple use case for programmatic access to PLOS full text</title>
      <link>https://ropensci.org/blog/2013/10/22/oaweek-rplos/</link>
      <pubDate>Tue, 22 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2013/10/22/oaweek-rplos/</guid>
      <description>
        
        

&lt;p&gt;Open access week is here!  We love open access, and think it&amp;rsquo;s extremely important to publish in open access journals. One of the many benefits of open access literature is that we likely can use the text of articles in OA journals for many things, including text-mining.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s even more awesome is some OA publishers provide API (application programming interface) access to their full text articles. Public Library of Science (PLOS) is one of these. We have had an R package for a while now that makes it convenient to search PLOS full text programatically. You can search on specific parts of articles (e.g., just in titles, or just in results sections), and you can return specific parts of articles (e.g., just abstracts). There are additional options for more fine-grained control over searches like facetting.&lt;/p&gt;

&lt;p&gt;What if you want to find similar papers based on their text content?  This can be done using the PLOS search API, with help from the &lt;code&gt;tm&lt;/code&gt; R package. These are basic examples just to demonstrate that you can quickly go from a search of PLOS data to a visualization or analysis.&lt;/p&gt;

&lt;h3 id=&#34;install-rplos-and-other-packages-from-cran&#34;&gt;Install rplos and other packages from CRAN&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(c(&amp;quot;rplos&amp;quot;, &amp;quot;tm&amp;quot;, &amp;quot;wordcloud&amp;quot;, &amp;quot;RColorBrewer&amp;quot;, &amp;quot;proxy&amp;quot;, &amp;quot;plyr&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-some-text&#34;&gt;Get some text&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rplos)
out &amp;lt;- searchplos(&amp;quot;birds&amp;quot;, fields = &amp;quot;id,introduction&amp;quot;, limit = 20, toquery = list(&amp;quot;cross_published_journal_key:PLoSONE&amp;quot;,
    &amp;quot;doc_type:full&amp;quot;))
out$idshort &amp;lt;- sapply(out$id, function(x) strsplit(x, &amp;quot;\\.&amp;quot;)[[1]][length(strsplit(x,
    &amp;quot;\\.&amp;quot;)[[1]])], USE.NAMES = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result is a list of length &lt;code&gt;limit&lt;/code&gt; defined in the previous call.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nrow(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[1] 20
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;word-dictionaries&#34;&gt;Word dictionaries.&lt;/h3&gt;

&lt;p&gt;Next, we&amp;rsquo;ll use the tm package to create word dictionaries for each paper.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tm)
library(proxy)
corpus &amp;lt;- Corpus(DataframeSource(out[&amp;quot;introduction&amp;quot;]))

# Clean up corpus
corpus &amp;lt;- tm_map(corpus, function(x) removeWords(x, stopwords(&amp;quot;english&amp;quot;)))
corpus &amp;lt;- tm_map(corpus, function(x) removePunctuation(x))
tdm &amp;lt;- TermDocumentMatrix(corpus)
tdm$dimnames$Docs &amp;lt;- out$idshort

# Comparison among documents in a heatmap
dissmat &amp;lt;- dissimilarity(tdm, method = &amp;quot;Euclidean&amp;quot;)
get_dist_frame &amp;lt;- function(x) {
    temp &amp;lt;- data.frame(subset(data.frame(expand.grid(dimnames(as.matrix(x))),
        expand.grid(lower.tri(as.matrix(x)))), Var1.1 == &amp;quot;TRUE&amp;quot;)[, -3], as.vector(x))
    names(temp) &amp;lt;- c(&amp;quot;one&amp;quot;, &amp;quot;two&amp;quot;, &amp;quot;value&amp;quot;)
    tempout &amp;lt;- temp[!temp[, 1] == temp[, 2], ]
    tempout
}
dissmatdf &amp;lt;- get_dist_frame(dissmat)
ggplot(dissmatdf, aes(one, two)) + geom_tile(aes(fill = value), colour = &amp;quot;white&amp;quot;,
    binwidth = 3) + scale_fill_gradient(low = &amp;quot;white&amp;quot;, high = &amp;quot;steelblue&amp;quot;) +
    theme_grey(base_size = 16) + labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;) + scale_x_discrete(expand = c(0,
    0)) + scale_y_discrete(expand = c(0, 0)) + theme(axis.ticks = theme_blank(),
    axis.text.x = element_text(size = 12, hjust = 0.6, colour = &amp;quot;grey50&amp;quot;, angle = 90),
    panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), panel.border = theme_blank())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-10-22-oaweek-rplos-2/tmit.png&#34; alt=&#34;plot of chunk tmit&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Picking two with low values (=high similarity), dois 10.1371/journal.pone.0000184 and 10.1371/journal.pone.0004148, here&amp;rsquo;s some of the most common terms used (some overlap).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(plyr)
df1 &amp;lt;- sort(termFreq(corpus[[grep(&amp;quot;10.1371/journal.pone.0010997&amp;quot;, out$id)]]))
df1 &amp;lt;- data.frame(terms = names(df1[df1 &amp;gt; 2]), vals = df1[df1 &amp;gt; 2], row.names = NULL)
df2 &amp;lt;- sort(termFreq(corpus[[grep(&amp;quot;10.1371/journal.pone.0004148&amp;quot;, out$id)]]))
df2 &amp;lt;- data.frame(terms = names(df2[df2 &amp;gt; 1]), vals = df2[df2 &amp;gt; 1], row.names = NULL)
df1$terms &amp;lt;- reorder(df1$terms, df1$vals)
df2$terms &amp;lt;- reorder(df2$terms, df2$vals)
dfboth &amp;lt;- ldply(list(`0010997` = df1, `0004148` = df2))
ggplot(dfboth, aes(x = terms, y = vals)) + geom_histogram(stat = &amp;quot;identity&amp;quot;) +
    facet_grid(. ~ .id, scales = &amp;quot;free&amp;quot;) + theme(axis.text.x = element_text(angle = 90))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-10-22-oaweek-rplos-2/words.png&#34; alt=&#34;plot of chunk words&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;determine-similarity-among-papers&#34;&gt;Determine similarity among papers&lt;/h3&gt;

&lt;p&gt;Using a wordcloud&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(wordcloud)
library(RColorBrewer)

m &amp;lt;- as.matrix(tdm)
v &amp;lt;- sort(rowSums(m), decreasing = TRUE)
d &amp;lt;- data.frame(word = names(v), freq = v)
pal &amp;lt;- brewer.pal(9, &amp;quot;Blues&amp;quot;)
pal &amp;lt;- pal[-(1:2)]

# Plot the chart
wordcloud(d$word, d$freq, scale = c(3, 0.1), min.freq = 2, max.words = 250,
    random.order = FALSE, rot.per = 0.2, colors = pal)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-10-22-oaweek-rplos-2/wordcloud.png&#34; alt=&#34;plot of chunk wordcloud&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Revisiting our USGS app</title>
      <link>https://ropensci.org/blog/2013/06/19/usgs-app/</link>
      <pubDate>Wed, 19 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2013/06/19/usgs-app/</guid>
      <description>
        
        &lt;p&gt;R has a reputation of not playing nice on the web. At rOpenSci, we write R pacakages to bring data from around the web into R on your local machine - so we mostly don&amp;rsquo;t do any dev for the web. However, the United States Geological Survey (USGS) recenty held &lt;a href=&#34;http://applifyingusgsdata.challenge.gov/submissions/14242-taxaviewer&#34;&gt;an app competition&lt;/a&gt; - it was a good opportunity to play with R on the web. We won best overall app as described in &lt;a href=&#34;http://ropensci.org/blog/2013/04/22/usgs_app/&#34;&gt;an earlier post on this blog&lt;/a&gt;. Check out our app &lt;strong&gt;TaxaViewer&lt;/strong&gt; &lt;a href=&#34;http://glimmer.rstudio.com/ropensci/usgs_app/&#34;&gt;here&lt;/a&gt;. Last week we presented the app to the USGS - a video of the presentation will be coming soon. A screenshot:&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
{{ &amp;ldquo;assets/blog-images/usgsapp.png&amp;rdquo; | image_url }}
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;When we submitted the app we had static ggplot2 maps, but now the app has interactive maps that are a much better fit for a browser that isn&amp;rsquo;t restrained to static images. Ramnath Vaidyanathan helped us use &lt;a href=&#34;http://rcharts.github.io/&#34;&gt;rCharts&lt;/a&gt; to pass data from queries to the GBIF and BISON APIs to functions in rCharts to prepare the map page you see in the app. rCharts isn&amp;rsquo;t restricted to just maps - think interactive bar charts, scatter plots, etc.&lt;/p&gt;

&lt;p&gt;We didn&amp;rsquo;t originally have a tab for searching for mentions of taxonomic names in the literature - we added it in later (see the &lt;em&gt;Papers&lt;/em&gt; tab on the &lt;a href=&#34;http://glimmer.rstudio.com/ropensci/usgs_app/&#34;&gt;app&lt;/a&gt;), and people really liked it, suggesting that&amp;rsquo;s something we should explore more. How does it work? We take the list of taxonomic names input in the text box in the upper left, and query the &lt;a href=&#34;http://api.plos.org/&#34;&gt;PLOS search API&lt;/a&gt;, and return a table of papers listing the journal name and paper title. We provide a link to open an article from the search results in Macrodocs (thx &lt;a href=&#34;https://twitter.com/invisiblecomma&#34;&gt;@invisiblecomma&lt;/a&gt;!), like &lt;a href=&#34;http://macrodocs.org/?doi=10.1371/journal.pone.0017580&#34;&gt;this example&lt;/a&gt;. We chose PLOS as they have a relatively large set of articles, and they are all open access = immediate access to the text. Any interest in an app just for literature? Maybe litereature + metadata? And perhaps some text mining using the &lt;a href=&#34;http://cran.r-project.org/web/packages/tm/index.html&#34;&gt;tm package&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;We think of the USGS app as a visual interface to show what can be do in R on the command line, and give the code to do it. See the code (&amp;lt;/&amp;gt;) buttons that drop down in this screenshot below. This is a compelling use case for Shiny apps - show people what they can do visually - then give them the code to do it on their own machine.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
{{ &amp;ldquo;assets/blog-images/codemodal.png&amp;rdquo; | image_url }}
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
