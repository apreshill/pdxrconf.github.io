<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Infrastructure on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/infrastructure/</link>
    <description>Recent content in Infrastructure on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 25 May 2016 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/infrastructure/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Software sustainability research with rOpenSci</title>
      <link>https://ropensci.org/blog/2016/05/25/software-sustanability-ropensci/</link>
      <pubDate>Wed, 25 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2016/05/25/software-sustanability-ropensci/</guid>
      <description>
        
        

&lt;p&gt;I’m happy to announce that I’ve started a project with &lt;a href=&#34;https://ropensci.org&#34;&gt;rOpenSci&lt;/a&gt;
under &lt;a href=&#34;https://ropensci.org/blog/2015/11/19/helmsley-trust-funding/&#34;&gt;their recent award from the Helmsley Foundation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My work with rOpenSci will focus on sustainability of the project itself.  Sustainability can be defined as having the
resources to do the necessary work to continue and grow rOpenSci.  This is one of the most difficult challenges for
rOpenSci and for many other research software projects.&lt;/p&gt;

&lt;p&gt;rOpenSci has a very broad and very ambitious goal, as stated on their web site, &amp;ldquo;Transforming science through open data.&amp;rdquo;
In practice, the work being done by rOpenSci is &amp;ldquo;creating packages that allow access to data repositories through the
R statistical programming environment&amp;rdquo; with tools that “not only facilitate drawing data into an environment where it
can readily be manipulated, but also one in which those analyses and methods can be easily shared, replicated, and
extended by other researchers.”
An interesting question is how much rOpenSci will choose to move beyond R to meet its goal, which I would encourage as
much as possible.  I actually might rephrase the goal as &amp;ldquo;Transforming science through open data and open software&amp;rdquo;
better matching what is now happening in the project while not calling out R, since I would prefer to try to affect
the non-R science community as well.&lt;/p&gt;

&lt;h2 id=&#34;sustainability-resources-work&#34;&gt;Sustainability = Resources &amp;gt; Work&lt;/h2&gt;

&lt;p&gt;There are a variety of sustainability models that have worked for other open source projects
to bring in the resources needed to do the work,
such as institutional support, diversified grants, membership fees, donations and gifts, fees for services, volunteers,
etc.  With the rOpenSci team, I will test elements of these models for rOpenSci.  As part of this, I will also work
with the rOpenSci leadership team to identify new funding opportunities for rOpenSci from public and private funding
organizations.&lt;/p&gt;

&lt;p&gt;I will discuss and document how other projects have increased their scale of impact without an equivalent
increase in their staff, and how these models could work for rOpenSci.  I and the rOpenSci team will then test elements
of one or more of these activities.  I will also seek to find new partnership opportunities for rOpenSci with academic,
government, and private organizations.&lt;/p&gt;

&lt;p&gt;Sustainability is a key challenge for many academic open source projects, as I saw when I was leading NSF&amp;rsquo;s &lt;a href=&#34;http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=504817&#34;&gt;SI2 program&lt;/a&gt;.  One of the review criteria for proposals
to this program was to describe the proposed sustainability plan.  Few proposers gave fully convincing plans,
and the most common was probably to have a diverse set of funding sources.  Additionally, many projects
decided that they would leave sustainability until near the end of the project.  While we don&amp;rsquo;t know what
will lead to sustainability for academic open source software,
it seems clear that waiting to address it is not likely to make the effort
more successful.&lt;/p&gt;

&lt;p&gt;If we can solve this for rOpenSci, or even just make progress and
provide guidance for other projects about what might work and what
probably won&amp;rsquo;t, this will be a significant contribution.
The &lt;a href=&#34;http://wssspe.researchcomputing.org.uk/&#34;&gt;WSSSPE&lt;/a&gt; events will be a way to bring together
some of the academic open source community to understand how our lessons apply to other
projects.&lt;/p&gt;

&lt;h2 id=&#34;promoting-open-source&#34;&gt;Promoting Open Source&lt;/h2&gt;

&lt;p&gt;The last part of this work with rOpenSci, which overlaps some of my other work, involves ideas about how to develop the
overall scientific software community.
Working with the rOpenSci project and the wider scientific software community, I will examine incentives,
citation/credit models, and metrics, including understanding contributions of people in teams; funding models;
multi-disciplinary science; career paths for software developers and scientists who also develop some software;
software engineering; software communities and sociology; training and education; software dissemination, publication,
and peer review; catalogs, search, and review; portability; and reproducibility.  At least some of this will be done
under the &lt;a href=&#34;http://wssspe.researchcomputing.org.uk/&#34;&gt;WSSSPE&lt;/a&gt; umbrella.&lt;/p&gt;

&lt;p&gt;I will also work with rOpenSci to attempt to promote and encourage software
metrics. A number of projects, including &lt;a href=&#34;https://github.com/njsmith/sempervirens&#34;&gt;sempervirens&lt;/a&gt; and
&lt;a href=&#34;http://depsy.org&#34;&gt;Depsy&lt;/a&gt;, have begun to focus on particular parts of the problem space, but more general work and
tools are needed, in particular to impact those outside of just the R and python communities.  There’s also some
related work underway at NCSA and RENCI (that came out of WSSSPE3) to survey a set of developers on what impact
metrics they find useful, and I think the results of the survey will also help this overall community.&lt;/p&gt;

&lt;h2 id=&#34;the-culture-around-open-source-needs-improvement&#34;&gt;The Culture around Open Source Needs Improvement&lt;/h2&gt;

&lt;p&gt;Finally, I want to try to change our cultural understanding around the idea of open
source software.  The fact that open source software is commonly viewed as free software is a problem, since to many
people, free means of no value.  In particular, I believe we need to try to get to the point where companies that
make heavy use of open source software feel that they have an obligation to support either the software or its
developers in some way.  This is part of the larger sustainability discussion, since this is the way that open
source projects can gain more resources and reduce the amount of work that they have to do internally.&lt;/p&gt;

&lt;p&gt;Any thoughts, comments, or suggestions are welcome. This is also cross-posted on &lt;a href=&#34;https://danielskatzblog.wordpress.com/2016/05/25/software-sustainability-research-with-ropensci/&#34;&gt;Dan&amp;rsquo;s blog&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Onboarding at rOpenSci: A Year in Reviews</title>
      <link>https://ropensci.org/blog/2016/03/28/software-review/</link>
      <pubDate>Mon, 28 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2016/03/28/software-review/</guid>
      <description>
        
        

&lt;p&gt;Code review, in which peers manually inspect the source code of software
written by others, is widely recognized as one of the best tools for finding
bugs in software. Code review is relatively uncommon in &lt;em&gt;scientific&lt;/em&gt; software
development, though. Scientists, despite being familiar with the process of
peer review, often have little exposure to code review due to lack of training and
historically little incentive to share the source code from their research. So
scientific code, from one-off scripts to reusable R packages, is rarely subject
to review. Most R packages are subject only to the automated checks required by
  CRAN, which primarily ensure that packages can be installed on multiple systems.
As such, The burden is on software users to discern well-written and efficient
packages from poorly written ones.&lt;/p&gt;

&lt;p&gt;rOpenSci is a community of developer-scientists, creating R packages for other
scientists, and our package contributors have a mix of backgrounds. We aim to
serve our users with high-quality software, and also promote best practices
among our author base and in the scientific community in general. So for
the past year, rOpenSci has been piloting a system of peer code review for
submissions to &lt;a href=&#34;https://ropensci.org/packages/&#34;&gt;our suite of R packages&lt;/a&gt;. Here
we&amp;rsquo;ll outline how our system works, and what we&amp;rsquo;ve learned from our authors and
reviewers.&lt;/p&gt;

&lt;h2 id=&#34;our-system&#34;&gt;Our System&lt;/h2&gt;

&lt;p&gt;rOpenSci&amp;rsquo;s package review process owes much to the experiments of others
(such as &lt;a href=&#34;http://mcs.open.ac.uk/mp8/&#34;&gt;Marian Petre&lt;/a&gt; and the &lt;a href=&#34;https://mozillascience.org/code-review-for-science-what-we-learned&#34;&gt;Mozilla Science Lab&lt;/a&gt;),
as well as the &lt;a href=&#34;https://discuss.ropensci.org/t/code-review-onboarding-milestones/180&#34;&gt;active feedback&lt;/a&gt; &lt;a href=&#34;https://discuss.ropensci.org/t/how-could-the-onboarding-package-review-process-be-even-better/302&#34;&gt;from our
community&lt;/a&gt;. Here&amp;rsquo;s how it works: When an author submits
a package, our editors evaluate it for fit according to our &lt;a href=&#34;https://github.com/ropensci/policies#package-fit&#34;&gt;criteria&lt;/a&gt;, then assign reviewers who evaluate
the package for usability, quality, and style based on our &lt;a href=&#34;https://github.com/ropensci/packaging_guide#ropensci-packaging-guide&#34;&gt;guidelines&lt;/a&gt;. After the
reviewers evaluate and the author makes recommended changes, the package gets
the rOpenSci stamp in its README and is added to our collection.&lt;/p&gt;

&lt;p&gt;We work entirely through the GitHub issue system. To submit authors &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/new&#34;&gt;open
an issue&lt;/a&gt;. Reviewers post
reviews as comments on that issue. This means the entire process is open and
public from the start. Reviewers and authors are known to each other and free
to communicate directly in the issue thread. GitHub-based reviews have some
other nice features: reviewers can publicly consult others by &lt;strong&gt;\@tagging&lt;/strong&gt;
if outside expertise is wanted. Reviewers can also contribute to the package
directly via a pull request when this is more efficient than describing the
changes they suggest.&lt;/p&gt;

&lt;p&gt;This system deliberately combines elements of traditional academic peer review
(external peers), with practices from open-source software review. One design
goal was to keep reviews &lt;em&gt;non-adversarial&lt;/em&gt; - to focus on improving software
quality rather than judging the package or authors. We think the openness
of the process has something to do with this, as reviews are public and this
incentivizes reviewers to do good work and abide by our &lt;a href=&#34;https://github.com/ropensci/policies#code-of-conduct&#34;&gt;code of conduct&lt;/a&gt;. We also do not
explicitly reject packages, except for turning some away prior review when they
are out-of-scope. We do this because submitted packages are already public and
open-source, so &amp;ldquo;time to publication&amp;rdquo; has not been a concern. Packages that
require significant revisions can just remain on hold until authors incorporate
such changes and update the discussion thread.&lt;/p&gt;

&lt;h2 id=&#34;some-lessons-learned&#34;&gt;Some lessons learned&lt;/h2&gt;

&lt;p&gt;So far, we&amp;rsquo;ve received 16 packages. Of these, only 1 was rejected due to lack
of fit. 11 were reviewed, 6 of which were accepted, and 5 are awaiting changes
requested by reviewers. 4 are still awaiting at least one review.&lt;/p&gt;

&lt;p&gt;We also recently &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1zaE5MvqXyD0I7LWONh1HlQu98wTIZ6Uls4QVmKs2u-w/edit?usp=sharing&#34;&gt;surveyed&lt;/a&gt; our reviewers and
reviewees, asking them how long it took to review, their positive and negative
experiences with the system, and what they learned from the process.&lt;/p&gt;

&lt;h3 id=&#34;reviewers-and-reviewees-like-it&#34;&gt;Reviewers and reviewees like it!&lt;/h3&gt;

&lt;p&gt;Pretty much everyone who responded to the survey, which was most of our
reviewers, found value in the system.  While we didn&amp;rsquo;t ask anyone to rate the
system or quantify their satisfaction, the length of answers to &amp;ldquo;What was the
best thing about the software review process?&amp;rdquo; and the number of superlatives
and exclamation marks indicates a fair bit of enthusiasm.  Here are a couple of
choice quotes:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I don&amp;rsquo;t really see myself writing another serious package without having it go through code review.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;I learnt that code review is the best thing that can ever happen to your
package!&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Authors appreciated that their reviews were thorough, that they were able to
converse with (nice) reviewers, and that they picked up best practices from
other experienced authors. Reviewers also praised the ability to converse
directly with author, expand their community of colleagues and learn about new
and best practices from other authors.&lt;/p&gt;

&lt;p&gt;Interestingly, no one mentioned the credential of an rOpenSci &amp;ldquo;badge&amp;rdquo; as a
positive aspect of review.  While the badge may be a motivating factor,
it seems from the responses that authors primarily value the feedback itself.
There has been some &lt;a href=&#34;http://simplystatistics.org/2013/09/26/how-could-code-review-discourage-code-disclosure-reviewers-with-motivation/&#34;&gt;argument&lt;/a&gt; whether code
review will encourage or discourage scientists to publish their
code.  While our package authors represent a specific subset of scientists - those knowledgeable and motivated enough to create and disseminate packages - we think
our pilot shows that a well-designed review process can be encouraging.&lt;/p&gt;

&lt;h3 id=&#34;review-takes-a-lot-of-time&#34;&gt;Review takes a lot of time&lt;/h3&gt;

&lt;p&gt;We asked reviewers to estimate how much time each review took, and here&amp;rsquo;s what
they reported:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2016-03-28-software-review/reviewer-time-1.png&#34; alt=&#34;plot of chunk reviewer-time&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Answers varied from 1-10 hours with an average of 4. This is comparable
to &lt;a href=&#34;http://publishingresearchconsortium.com/index.php/112-prc-projects/research-reports/peer-review-in-scholarly-journals-research-report/142-peer-review-in-scholarly-journals-perspective-of-the-scholarly-community-an-international-study&#34;&gt;how long it takes researchers to review scholarly papers&lt;/a&gt;, but
it&amp;rsquo;s still a lot of time, and does not include further time corresponding with
the authors or re-reviewing an updated package.&lt;/p&gt;

&lt;p&gt;Package writing and reviewing are generally volunteer activities, and as one
respondent put it, the process &amp;ldquo;still feels more like community service than
a professional obligation.&amp;rdquo; 7 of 16 reviewers respondents mentioned the time to
it took to review and respond as a negative of the process. For this process to
be sustainable, we have to figure out how to limit the burden on our reviewers.&lt;/p&gt;

&lt;h3 id=&#34;we-can-be-clearer-about-the-beginning-and-end-of-the-process&#34;&gt;We can be clearer about the beginning and end of the process&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;It wasn&amp;rsquo;t immediately clear what to do&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A few respondents pointed out we could be better at explaining the review
process, both in how to get started and how it is supposed to wrap up. For the
former, we&amp;rsquo;ve recently updated our &lt;a href=&#34;https://github.com/ropensci/onboarding/wiki/For-Reviewers&#34;&gt;reviewer guide&lt;/a&gt;, including adding links to previous reviews. We
hope as our reviewer pool gets more experienced, and as software reviews become
more common, this gets easier. However, as our pool of editors and reviewers
grows, we&amp;rsquo;ll need to ensure that our communication is clear.&lt;/p&gt;

&lt;p&gt;As for the &lt;em&gt;end&lt;/em&gt; of the review, this can be an area of considerable ambiguity.
There&amp;rsquo;s a clear endpoint when a package is accepted, but with no &amp;ldquo;rejections&amp;rdquo;
some reviewers weren&amp;rsquo;t sure how to respond if authors didn&amp;rsquo;t follow up on their
comments. We realize it can be demotivating to reviewers if their suggestions
aren&amp;rsquo;t acted upon. (One reviewer pointed out that seeing her suggestions
implemented as a positive motivator.) It may be worthwhile to enforce a
deadline for package authors to respond.&lt;/p&gt;

&lt;h3 id=&#34;we-are-helping-drive-best-practices-with-our-author-base&#34;&gt;We are helping drive best practices with our author base&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I had never heard of continuous integration, and it is fantastic!&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We asked both reviewers and reviewees to tell us what they learned. While there
was a lot of variety in the responses, one common thread was learning and
appreciating best practices: continuous integration, documentation,
&amp;ldquo;the right way to do [X]&amp;ldquo;, were the common responses.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:RefC&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:RefC&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Importantly, a number of reviewers and reviewees commented that they &lt;em&gt;learned
the value of review&lt;/em&gt; through this process.&lt;/p&gt;

&lt;h2 id=&#34;questions-and-ideas-for-the-future&#34;&gt;Questions and ideas for the future&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Scaling and reviewer incentives.&lt;/em&gt; Like academic paper review or contributing
to free open-source projects, our package review is a volunteer activity.
How do we build an experienced reviewer base, maintain enthusiasm, and
avoid overburdening our reviewers? We will need to expand our reviewer pool in
order to spread the load. As such, we are moving to a system
of multiple &amp;ldquo;handling editors&amp;rdquo; to assign and keep up with reviews. Hopefully we
will be able to bring in more reviewers through their networks.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Author incentives.&lt;/em&gt; Our small pool of early adopters indicated that they
valued the review process itself, but will this be enough incentive to draw more
package authors to do the extra work it takes? An area to explore is finding
ways to help package authors gain greater visibility and credit for their work
after their packages pass review. This could take the form of &amp;ldquo;badges&amp;rdquo;, such
as those being developed by &lt;a href=&#34;https://osf.io/tvyxz/wiki/home/&#34;&gt;The Center for Open Science&lt;/a&gt;
and &lt;a href=&#34;https://www.mozillascience.org/projects/contributorship-badges&#34;&gt;Mozilla Science Lab&lt;/a&gt;, or providing an easier route to publishing software papers.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Automation.&lt;/em&gt; How can we automate more parts of the review process so as to
get more value out of reviewer and reviewees time? One suggestion has been to submit packages
&lt;a href=&#34;https://discuss.ropensci.org/t/how- could-the-onboarding-package-review-process-be-even-better/302/3&#34;&gt;&lt;em&gt;as&lt;/em&gt; pull requests&lt;/a&gt; to take more advantage of GitHub
review features such as in-line commenting. This may allow us to move the burden
of setting up continuous integration and testing away from the authors and onto
our own pipeline, and allow us to add rOpenSci-specific tests. We&amp;rsquo;ve also started using &lt;a href=&#34;https://github.com/ropenscilabs/heythere&#34;&gt;automated reminders&lt;/a&gt; to keep up with reviewers, which reduces the
burden on our editors to keep up with everyone.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have learned a ton from this experiment and look forward to making review
better! Many, many thanks to the authors who have contributed to the rOpenSci
package ecosystem and the reviewers who have lent their time to this project.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:RefC&#34;&gt;Also, &amp;ldquo;RefClasses are the devil&amp;rdquo;, said one reviewer.  Interpret that as you may.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:RefC&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
  </channel>
</rss>
