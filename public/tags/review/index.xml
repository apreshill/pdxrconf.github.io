<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Review on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/review/</link>
    <description>Recent content in Review on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 29 Nov 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/review/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Announcing a New rOpenSci Software Review Collaboration</title>
      <link>https://ropensci.org/blog/2017/11/29/review-collaboration-mee/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/29/review-collaboration-mee/</guid>
      <description>
        
        

&lt;p&gt;rOpenSci is pleased to announce a new collaboration with the &lt;a href=&#34;http://besjournals.onlinelibrary.wiley.com/hub/journal/10.1111/(ISSN)2041-210X/&#34;&gt;Methods in Ecology and Evolution (MEE)&lt;/a&gt;, a journal of the &lt;a href=&#34;http://www.britishecologicalsociety.org/&#34;&gt;British Ecological Society&lt;/a&gt;, published by Wiley press &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Publications destined for MEE that include the development of a scientific R package will now have the option of a joint review process whereby the R package is reviewed by rOpenSci, followed by fast-tracked review of the manuscript by MEE. Authors opting for this process will be recognized via a mark on both web and print versions of their paper.&lt;/p&gt;

&lt;p&gt;We are very excited for this partnership to improve the rigor of both scientific software and software publications and to provide greater recognition to developers in the fields of ecology and evolution.  It is a natural outgrowth of our interest in supporting scientists in developing and maintaining software, and of MEE&amp;rsquo;s mission of vetting and disseminating tools and methods for the research community. The collaboration formalizes and eases a path already pursued by researchers: The &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12593/abstract&#34;&gt;&lt;strong&gt;rotl&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12469/abstract&#34;&gt;&lt;strong&gt;RNexML&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00247.x/abstract&#34;&gt;&lt;strong&gt;treebase&lt;/strong&gt;&lt;/a&gt; packages were all developed or reviewed by rOpenSci and subsequently had associated manuscripts published in MEE.&lt;/p&gt;

&lt;h3 id=&#34;about-ropensci-software-review&#34;&gt;About rOpenSci software review&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; is a diverse community of researchers from academia, non-profit, government, and industry who collaborate to develop and maintain tools and practices around open data and reproducible research. The rOpenSci suite of tools is made of core infrastructure software developed and maintained by the &lt;a href=&#34;https://ropensci.org/about#team&#34;&gt;project staff&lt;/a&gt;. The suite also contains numerous packages that are contributed by members of the broader R community. The volume of community submissions has grown considerably over the years necessitating a formal system of review quite analogous to that of a peer reviewed academic journal.&lt;/p&gt;

&lt;p&gt;rOpenSci welcomes full software submissions that fit within our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;aims and scope&lt;/a&gt;, with the option of a pre-submission inquiry in cases when the scope of a submission is not immediately obvious. This software peer review framework, known as the rOpenSci Onboarding process, operates with three editors and one editor in chief who carefully vet all incoming submissions. After an editorial review, editors solicit detailed, public and signed reviews from two reviewers, and the path to acceptance from then on is similar to a standard journal review process. Details about the system are described in &lt;a href=&#34;https://ropensci.org/blog/2016/03/28/software-review/&#34;&gt;various&lt;/a&gt; &lt;a href=&#34;https://ropensci.org/blog/2017/09/01/nf-softwarereview/&#34;&gt;blog&lt;/a&gt; &lt;a href=&#34;https://ropensci.org/blog/2017/09/11/software-review-update/&#34;&gt;posts&lt;/a&gt; by the editorial team.&lt;/p&gt;

&lt;h3 id=&#34;collaboration-with-journals&#34;&gt;Collaboration with journals&lt;/h3&gt;

&lt;p&gt;This is our second collaboration with a journal. Since late 2015, rOpenSci has partnered with the &lt;a href=&#34;http://joss.theoj.org/&#34;&gt;Journal of Open Source software (JOSS)&lt;/a&gt;, an open access journal that publishes brief articles on research software. Packages accepted to rOpenSci can be submitted for fast-track publication at JOSS, in which JOSS editors may evaluate based on rOpenSci&amp;rsquo;s reviews alone. As rOpenSci&amp;rsquo;s review criteria is significantly more stringent and designed to be compatible with JOSS, these packages are generally accepted without additional review. We have had great success with this partnership providing rOpenSci authors with an additional venue to publicize and archive their work. Given this success, we are keen on expanding to other journals and fields where there is potential for software reviewed and created by rOpenSci to play a significant role in supporting scientific findings.&lt;/p&gt;

&lt;h3 id=&#34;the-details&#34;&gt;The details&lt;/h3&gt;

&lt;p&gt;Our new partnership with MEE broadly resembles that with JOSS, with the major difference that MEE, rather than rOpenSci, leads review of the manuscript component.  Authors with R packages and associated manuscripts that fit the Aims and Scope for both &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;rOpenSci&lt;/a&gt; and &lt;a href=&#34;http://www.methodsinecologyandevolution.org/view/0/aimsAndScope.html&#34;&gt;MEE&lt;/a&gt; are encouraged to first submit to rOpenSci. The &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12593/abstract&#34;&gt;&lt;strong&gt;rotl&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12469/abstract&#34;&gt;&lt;strong&gt;RNexML&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00247.x/abstract&#34;&gt;&lt;strong&gt;treebase&lt;/strong&gt;&lt;/a&gt; packages are all great examples of such packages. MEE editors may also refer authors to this option if authors submit an appropriate manuscript to MEE first.&lt;/p&gt;

&lt;p&gt;On submission to rOpenSci, authors can use our updated &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/issue_template.md&#34;&gt;submission template&lt;/a&gt; to choose MEE as a publication venue. Following acceptance by rOpenSci, the associated manuscript will be reviewed by an expedited process at MEE, with reviewers and editors having the knowledge that the software has already been reviewed and the public reviews available to them.&lt;/p&gt;

&lt;p&gt;Should the manuscript be accepted, a footnote will appear in the web version and the first page of the print version of the MEE article indicating that the software as well as the manuscript has been peer-reviewed, with a link to the rOpenSci open reviews.&lt;/p&gt;

&lt;p&gt;As with any collaboration, there may be a few hiccups early on and we welcome ideas to make the process more streamlined and efficient. We look forward to the community&amp;rsquo;s submissions and to your participation in this process.&lt;/p&gt;

&lt;p&gt;Many thanks to MEE&amp;rsquo;s Assistant Editor Chris Grieves and Senior Editor Bob O&amp;rsquo;Hara for working with us on this collaboration.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;See also MEE&amp;rsquo;s post from today at &lt;a href=&#34;https://methodsblog.wordpress.com/2017/11/29/software-review/&#34;&gt;https://methodsblog.wordpress.com/2017/11/29/software-review/&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>Data from Public Bicycle Hire Systems</title>
      <link>https://ropensci.org/blog/2017/10/17/bikedata/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/17/bikedata/</guid>
      <description>
        
        

&lt;p&gt;A new rOpenSci package provides access to data to which users may already have directly contributed, and for which contribution is fun, keeps you fit, and &lt;a href=&#34;http://www.bmj.com/content/357/bmj.j1456&#34;&gt;helps make the world a better place&lt;/a&gt;. The data come from using public bicycle hire schemes, and the package is called &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt;. Public bicycle hire systems operate in many cities throughout the world, and most systems collect (generally anonymous) data, minimally consisting of the times and locations at which every single bicycle trip starts and ends. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package provides access to data from all cities which openly publish these data, currently including &lt;a href=&#34;https://tfl.gov.uk/modes/cycling/santander-cycles&#34;&gt;London, U.K.&lt;/a&gt;, and in the U.S.A., &lt;a href=&#34;https://www.citibikenyc.com&#34;&gt;New York&lt;/a&gt;, &lt;a href=&#34;https://bikeshare.metro.net&#34;&gt;Los Angeles&lt;/a&gt;, &lt;a href=&#34;https://www.rideindego.com&#34;&gt;Philadelphia&lt;/a&gt;, &lt;a href=&#34;https://www.divvybikes.com&#34;&gt;Chicago&lt;/a&gt;, &lt;a href=&#34;https://www.thehubway.com&#34;&gt;Boston&lt;/a&gt;, and &lt;a href=&#34;https://www.capitalbikeshare.com&#34;&gt;Washington DC&lt;/a&gt;. The package will expand as more cities openly publish their data (with the newly enormously expanded San Francisco system &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/2&#34;&gt;next on the list&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;why-bikedata&#34;&gt;Why bikedata?&lt;/h3&gt;

&lt;p&gt;The short answer to that question is that the package provides access to what is arguably one of the most spatially and temporally detailed databases of finely-scaled human movement throughout several of the world&amp;rsquo;s most important cities. Such data are likely to prove invaluable in the increasingly active and well-funded attempt to develop a science of cities. Such a science does not yet exist in any way comparable to most other well-established scientific disciplines, but the importance of developing a science of cities is indisputable, and reflected in such enterprises as the NYU-based &lt;a href=&#34;http://cusp.nyu.edu&#34;&gt;Center for Urban Science and Progress&lt;/a&gt;, or the UCL-based &lt;a href=&#34;https://www.ucl.ac.uk/bartlett/casa/&#34;&gt;Centre for Advanced Spatial Analysis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;People move through cities, yet at present anyone faced with the seemingly fundamental question of how, when, and where people do so would likely have to draw on some form of private data (typically operators of transport systems or mobile phone providers). There are very few open, public data providing insight into this question. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package aims to be one contribution towards filling this gap. The data accessed by the package are entirely open, and are constantly updated, typically on a monthly basis. The package thus provides ongoing insight into the dynamic changes and reconfigurations of these cities. Data currently available via the package amounts to several tens of Gigabytes, and will expand rapidly both with time, and with the inclusion of more cities.&lt;/p&gt;

&lt;h3 id=&#34;why-are-these-data-published&#34;&gt;Why are these data published?&lt;/h3&gt;

&lt;p&gt;In answer to that question, all credit must rightfully go to &lt;a href=&#34;http://www.theregister.co.uk/2011/01/11/transport_for_london_foi/&#34;&gt;Adrian Short&lt;/a&gt;, who submitted a Freedom of Information request in 2011 to Transport for London for usage statistics from the relatively new, and largely publicly-funded, bicycle scheme. This request from one individual ultimately resulted in the data being openly published on an ongoing basis. All U.S. systems included in &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; commenced operation subsequent to that point in time, and many of them have openly published their data from the very beginning. The majority of the world&amp;rsquo;s public bicycle hire systems (&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_bicycle-sharing_systems&#34;&gt;see list here&lt;/a&gt;) nevertheless do not openly publish data, notably including very large systems in China, France, and Spain. One important aspiration of the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package is to demonstrate the positive benefit for the cities themselves of openly and easily facilitating complex analyses of usage data, which brings us to &amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;what-s-important-about-these-data&#34;&gt;What&amp;rsquo;s important about these data?&lt;/h3&gt;

&lt;p&gt;As mentioned, the data really do provide uniquely valuable insights into the movement patterns and behaviour of people within some of the world&amp;rsquo;s major cities. While the more detailed explorations below demonstrate the kinds of things that can be done with the package, the variety of insights these data facilitate is best demonstrated through considering the work of other people, exemplified by &lt;a href=&#34;http://toddwschneider.com/posts/a-tale-of-twenty-two-million-citi-bikes-analyzing-the-nyc-bike-share-system/&#34;&gt;Todd Schneider&amp;rsquo;s high-profile blog piece&lt;/a&gt; on the New York City system. Todd&amp;rsquo;s analyses clearly demonstrate how these data can provide insight into where and when people move, into inter-relationships between various forms of transport, and into relationships with broader environmental factors such as weather. As cities evolve, and public bicycle hire schemes along with them, data from these systems can play a vital role in informing and guiding the ongoing processes of urban development. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package greatly facilitates analysing such processes, not only through making data access and aggregation enormously easier, but through enabling analyses from any one system to be immediately applied to, and compared with, any other systems.&lt;/p&gt;

&lt;h3 id=&#34;how-it-works&#34;&gt;How it works&lt;/h3&gt;

&lt;p&gt;The package currently focusses on the data alone, and provides functionality for downloading, storage, and aggregation. The data are stored in an &lt;code&gt;SQLite3&lt;/code&gt; database, enabling newly published data to be continually added, generally with one simple line of code. It&amp;rsquo;s as easy as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;store_bikedata (city = &amp;quot;chicago&amp;quot;, bikedb = &amp;quot;bikedb&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the nominated database (&lt;code&gt;bikedb&lt;/code&gt;) already holds data for Chicago, only new data will be added, otherwise all historical data will be downloaded and added. All bicycle hire systems accessed by &lt;code&gt;bikedata&lt;/code&gt; have fixed docking stations, and the primary means of aggregation is in terms of &amp;ldquo;trip matrices&amp;rdquo;, which are square matrices of numbers of trips between all pairs of stations, extracted with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chi&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that most parameters are highly flexible in terms of formatting, so pretty much anything starting with &lt;code&gt;&amp;quot;ch&amp;quot;&lt;/code&gt; will be recognised as Chicago. Of course, if the database only contains data for Chicago, the &lt;code&gt;city&lt;/code&gt; parameter may be omitted entirely. Trip matrices may be filtered by time, through combinations of year, month, day, hour, minute, or even second, as well as by demographic characteristics such as gender or date of birth for those systems which provide such data. (These latter data are freely provided by users of the systems, and there can be no guarantee of their accuracy.) These can all be combined in calls like the following, which further demonstrates the highly flexible ways of specifying the various parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (&amp;quot;bikedb&amp;quot;, city = &amp;quot;london, innit&amp;quot;,
                       start_date = 20160101, end_date = &amp;quot;16,02,28&amp;quot;,
                       start_time = 6, end_time = 24,
                       birth_year = 1980:1990, gender = &amp;quot;f&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The second mode of aggregation is as daily time series, via the &lt;code&gt;bike_daily_trips()&lt;/code&gt; function. See &lt;a href=&#34;https://ropensci.github.io/bikedata/articles/bikedata.html&#34;&gt;the vignette&lt;/a&gt; for further details.&lt;/p&gt;

&lt;h3 id=&#34;what-can-be-done-with-these-data&#34;&gt;What can be done with these data?&lt;/h3&gt;

&lt;p&gt;Lots of things. How about examining how far people ride. This requires getting the distances between all pairs of docking stations as routed through the street network, to yield a distance matrix corresponding to the trip matrix. The latest version of &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; has a brand new function to perform exactly that task, so it&amp;rsquo;s as easy as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;ropensci/bikedata&amp;quot;) # to install latest version
dists &amp;lt;- bike_distmat (bikedb = bikedb, city = &amp;quot;chicago&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are distances as routed through the underlying street network, with street types prioritised for bicycle travel. The network is extracted from OpenStreetMap using the &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;rOpenSci &lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;, and the distances are calculated using a brand new package called &lt;a href=&#34;https://cran.r-project.org/package=dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; (Distances on Directed Graphs). (Disclaimer: It&amp;rsquo;s my package, and this is a shameless plug for it - please use it!)&lt;/p&gt;

&lt;p&gt;The distance matrix extracted with &lt;code&gt;bike_distmat&lt;/code&gt; is between all stations listed for a given system, which &lt;code&gt;bike_tripmat&lt;/code&gt; will return trip matrices only between those stations in operation over a specified time period. Because systems expand over time, the two matrices will generally not be directly comparable, so it is necessary to submit both to the &lt;code&gt;bikedata&lt;/code&gt; function &lt;code&gt;match_matrices()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 636 636
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mats &amp;lt;- match_matrices (trips, dists)
trips &amp;lt;- mats$trip
dists &amp;lt;- mats$dist
dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 581 581
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;identical (rownames (trips), rownames (dists))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Distances can then be visually related to trip numbers to reveal their distributional form. These matrices contain too many values to plot directly, so the &lt;code&gt;hexbin&lt;/code&gt; package is used here to aggregate in a &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library (hexbin)
library (ggplot2)
dat &amp;lt;- data.frame (distance = as.vector (dmat),
                   number = as.vector (trips))
ggplot (dat, aes (x = distance, y = number)) +
    stat_binhex(aes(fill = log (..count..))) +
    scale_x_log10 (breaks = c (0.1, 0.5, 1, 2, 5, 10, 20),
                   labels = c (&amp;quot;0.1&amp;quot;, &amp;quot;0.5&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;20&amp;quot;)) +
    scale_y_log10 (breaks = c (10, 100, 1000)) +
    scale_fill_gradientn(colours = c(&amp;quot;seagreen&amp;quot;,&amp;quot;goldenrod1&amp;quot;),
                         name = &amp;quot;Frequency&amp;quot;, na.value = NA) +
    guides (fill = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/chicago.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The central region of the graph (yellow hexagons) reveals that numbers of trips generally decrease roughly exponentially with increasing distance (noting that scales are logarithmic), with most trip distances lying below 5km. What is the &amp;ldquo;average&amp;rdquo; distance travelled in Chicago? The easiest way to calculate this is as a weighted mean,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum (as.vector (dmat) * as.vector (trips) / sum (trips), na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 2.510285
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;giving a value of just over 2.5 kilometres. We could also compare differences in mean distances between cyclists who are registered with a system and causal users. These two categories may be loosely considered to reflect &amp;ldquo;residents&amp;rdquo; and &amp;ldquo;non-residents&amp;rdquo;. Let&amp;rsquo;s wrap this in a function so we can use it for even cooler stuff in a moment.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean &amp;lt;- function (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chicago&amp;quot;)
{
    tm &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
    tm_memb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = TRUE)
    tm_nomemb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = FALSE)
    stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
    dists &amp;lt;- bike_distmat (bikedb = bikedb, city = city)
    mats &amp;lt;- match_mats (dists, tm_memb)
    tm_memb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm_nomemb)
    tm_nomemb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm)
    tm &amp;lt;- mats$trip
    dists &amp;lt;- mats$dists

    d0 &amp;lt;- sum (as.vector (dists) * as.vector (tm) / sum (tm), na.rm = TRUE)
    dmemb &amp;lt;- sum (as.vector (dists) * as.vector (tmemb) / sum (t_memb), na.rm = TRUE)
    dnomemb &amp;lt;- sum (as.vector (dists) * as.vector (tm_nomemb) / sum (tm_nomemb), na.rm = TRUE)
    res &amp;lt;- c (d0, dmemb / dnomemb)
    names (res) &amp;lt;- c (&amp;quot;dmean&amp;quot;, &amp;quot;ratio_memb_non&amp;quot;)
    return (res)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Differences in distances ridden between &amp;ldquo;resident&amp;rdquo; and &amp;ldquo;non-resident&amp;rdquo; cyclists can then be calculated with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean (bikedb = bikedb, city = &amp;quot;ch&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##          dmean ratio_memb_non
##       2.510698       1.023225
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And system members cycle slightly longer distances than non-members. (Do not at this point ask about statistical tests - these comparisons are made between millions&amp;ndash;often tens of millions&amp;ndash;of points, and statistical significance may always be assumed to be negligibly small.) Whatever the reason for this difference between &amp;ldquo;residents&amp;rdquo; and others, we can use this exact same code to compare equivalent distances for all cities which record whether users are members or not (which is all cities except London and Washington DC).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cities &amp;lt;- c (&amp;quot;ny&amp;quot;, &amp;quot;ch&amp;quot;, &amp;quot;bo&amp;quot;, &amp;quot;la&amp;quot;, &amp;quot;ph&amp;quot;) # NYC, Chicago, Boston, LA, Philadelphia
sapply (cities, function (i) dmean (bikedb = bikedb, city = i))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                       ny       ch       bo       la       ph
## dmean          2.8519131 2.510285 2.153918 2.156919 1.702372
## ratio_memb_non 0.9833729 1.023385 1.000635 1.360099 1.130929
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we thus discover that Boston manifests the greatest equality in terms of distances cycled between residents and non-residents, while LA manifests the greatest difference. New York City is the only one of these five in which non-members of the system actually cycle further than members. (And note that these two measures can&amp;rsquo;t be statistically compared in any direct way, because mean distances are also affected by relative numbers of member to non-member trips.) These results likely reflect a host of (scientifically) interesting cultural and geo-spatial differences between these cities, and demonstrate how the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package (combined with &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt;&lt;/a&gt;) can provide unique insight into differences in human behaviour between some of the most important cities in the U.S.&lt;/p&gt;

&lt;h3 id=&#34;visualisation&#34;&gt;Visualisation&lt;/h3&gt;

&lt;p&gt;Many users are likely to want to visualise how people use a given bicycle system, and in particular are likely to want to produce maps. This is also readily done with the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt;, which can route and aggregate transit flows for a particular mode of transport throughout a street network. Let&amp;rsquo;s plot bicycle flows for the Indego System of Philadelphia PA. First get the trip matrix, along with the coordinates of all bicycle stations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;gmost/dodgr&amp;quot;) # to install latest version
city &amp;lt;- &amp;quot;ph&amp;quot;
# store_bikedata (bikedb = bikedb, city = city) # if not already done
trips &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
xy &amp;lt;- stns [, which (names (stns) %in% c (&amp;quot;longitude&amp;quot;, &amp;quot;latitude&amp;quot;))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows of cyclists are calculated between those &lt;code&gt;xy&lt;/code&gt;points, so the &lt;code&gt;trips&lt;/code&gt; table has to match the &lt;code&gt;stns&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;indx &amp;lt;- match (stns$stn_id, rownames (trips))
trips &amp;lt;- trips [indx, indx]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; can be used to extract the underlying street network surrounding those &lt;code&gt;xy&lt;/code&gt; points (expanded here by 50%):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;net &amp;lt;- dodgr_streetnet (pts = xy, expand = 0.5) %&amp;gt;%
    weight_streetnet (wt_profile = &amp;quot;bicycle&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then need to align the bicycle station coordinates in &lt;code&gt;xy&lt;/code&gt; to the nearest points (or &amp;ldquo;vertices&amp;rdquo;) in the street network:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;verts &amp;lt;- dodgr_vertices (net)
pts &amp;lt;- verts$id [match_pts_to_graph (verts, xy)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows between these points can then be mapped onto the underlying street network with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;flow &amp;lt;- dodgr_flows (net, from = pts, to = pts, flow = trips) %&amp;gt;%
    merge_directed_flows ()
net &amp;lt;- net [flow$edge_id, ]
net$flow &amp;lt;- flow$flow
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; documentation&lt;/a&gt; for further details of how this works. We&amp;rsquo;re now ready to plot those flows, but before we do, let&amp;rsquo;s overlay them on top of the rivers of Philadelphia, extracted with rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;q &amp;lt;- opq (&amp;quot;Philadelphia pa&amp;quot;)
rivers1 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;waterway&amp;quot;, value = &amp;quot;river&amp;quot;, value_exact = FALSE) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers2 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;natural&amp;quot;, value = &amp;quot;water&amp;quot;) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers &amp;lt;- c (rivers1, rivers2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally plot the map, using rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmplotr&#34;&gt;&lt;code&gt;osmplotr&lt;/code&gt; package&lt;/a&gt; to prepare a base map with the underlying rivers, and the &lt;code&gt;ggplot2::geom_segment()&lt;/code&gt; function to add the line segments with colours and widths weighted by bicycle flows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#gtlibrary (osmplotr)
require (ggplot2)
bb &amp;lt;- get_bbox (c (-75.22, 39.91, -75.10, 39.98))
cols &amp;lt;- colorRampPalette (c (&amp;quot;lawngreen&amp;quot;, &amp;quot;red&amp;quot;)) (30)
map &amp;lt;- osm_basemap (bb, bg = &amp;quot;gray10&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_multipolygons, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_lines, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_colourbar (zlims = range (net$flow / 1000), col = cols)
map &amp;lt;- map + geom_segment (data = net, size = net$flow / 50000,
                           aes (x = from_lon, y = from_lat, xend = to_lon, yend = to_lat,
                                colour = flow, size = flow)) +
    scale_colour_gradient (low = &amp;quot;lawngreen&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;none&amp;quot;)
print_osm_map (map)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/ph.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The colour bar on the right shows thousands of trips, with the map revealing the relatively enormous numbers crossing the South Street Bridge over the Schuylkill River, leaving most other flows coloured in the lower range of green or yellows. This map thus reveals that anyone wanting to see Philadelphia&amp;rsquo;s Indego bikes in action without braving the saddle themselves would be best advised to head straight for the South Street Bridge.&lt;/p&gt;

&lt;h3 id=&#34;future-plans&#34;&gt;Future plans&lt;/h3&gt;

&lt;p&gt;Although the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; greatly facilitates the production of such maps, the code is nevertheless rather protracted, and it would probably be very useful to convert much of the code in the preceding section to an internal &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; function to map trips between pairs of stations onto corresponding flows through the underlying street networks.&lt;/p&gt;

&lt;p&gt;Beyond that point, and the list of currently open issues awaiting development on the &lt;a href=&#34;https://github.com/ropensci/bikedata/issues&#34;&gt;github repository&lt;/a&gt;, future development is likely to depend very much on how users use the package, and on what extra features people might want. How can you help? A great place to start might be the official &lt;a href=&#34;https://ropensci.org/blog/blog/2017/10/02/hacktoberfest&#34;&gt;Hacktoberfest issue&lt;/a&gt;, helping to import the next lot of data from &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/34&#34;&gt;San Francisco&lt;/a&gt;. Or just use the package, and open up a new issue in response to any ideas that might pop up, no matter how minor they might seem. See the &lt;a href=&#34;https://github.com/ropensci/bikedata/blob/master/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for general advice.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Finally, this package wouldn&amp;rsquo;t be what it is without my co-author &lt;a href=&#34;https://github.com/richardellison&#34;&gt;Richard Ellison&lt;/a&gt;, who greatly accelerated development through encouraging C rather than C++ code for the SQL interfaces. &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maëlle Salmon&lt;/a&gt; majestically guided the entire review process, and made the transformation of the package to its current polished form a joy and a pleasure. I remain indebted to both &lt;a href=&#34;https://github.com/chucheria&#34;&gt;Bea Hernández&lt;/a&gt; and &lt;a href=&#34;https://github.com/eamcvey&#34;&gt;Elaine McVey&lt;/a&gt; for offering their time to extensively test and &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/116&#34;&gt;review the package&lt;/a&gt; as part of rOpenSci&amp;rsquo;s onboarding process. The review process has made the package what it is, and for that I am grateful to all involved!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>googleLanguageR - Analysing language through the Google Cloud Machine Learning APIs</title>
      <link>https://ropensci.org/blog/2017/10/03/googlelanguager/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/03/googlelanguager/</guid>
      <description>
        
        

&lt;!-- open source image taken from: https://upload.wikimedia.org/wikipedia/commons/2/21/Bell_System_switchboard.jpg --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/switchboard.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the greatest assets human beings possess is the power of speech and language, from which almost all our other accomplishments flow. To be able to analyse communication offers us a chance to gain a greater understanding of one another.&lt;/p&gt;

&lt;p&gt;To help you with this, &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt;&lt;/a&gt; is an R package that allows you to perform speech-to-text transcription, neural net translation and natural language processing via the &lt;a href=&#34;https://cloud.google.com/products/machine-learning/&#34;&gt;Google Cloud machine learning services&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An introduction to the package is below, but you can find out more details at the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-s-bet&#34;&gt;Google&amp;rsquo;s bet&lt;/h3&gt;

&lt;p&gt;Google predicts that machine learning is to be a fundamental feature of business, and so they are looking to become the infrastructure that makes machine learning possible. Metaphorically speaking: If machine learning is electricity, then Google wants to be the pylons carrying it around the country.&lt;/p&gt;

&lt;!-- open source image taken from: https://pixabay.com/en/pylon-sky-electricity-tower-2515429/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/pylon.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Google may not be the only company with such ambitions, but one advantage Google has is the amount of data it possesses. Twenty years of web crawling has given it an unprecedented corpus to train its models.  In addition, its recent moves into voice and video gives it one of the biggest audio and speech datasets, all of which have been used to help create machine learning applications within its products such as search and Gmail. Further investment in machine learning is shown by Google&amp;rsquo;s purchase of &lt;a href=&#34;https://deepmind.com/&#34;&gt;Deepmind&lt;/a&gt;, a UK based A.I. research firm that recently was in the news for defeating the top Go champion with its neural network trained Go bot.  Google has also taken an open-source route with the creation and publication of &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt;, a leading machine learning framework.&lt;/p&gt;

&lt;p&gt;Whilst you can create your own machine learning models, for those users who haven&amp;rsquo;t the expertise, data or time to do so, Google also offers an increasing range of machine learning APIs that are pre-trained, such as image and video recognition or job search.  &lt;code&gt;googleLanguageR&lt;/code&gt; wraps the subset of those machine learning APIs that are language flavoured - Cloud Speech, Translation and Natural Language.&lt;/p&gt;

&lt;p&gt;Since they carry complementary outputs that can be used in each other&amp;rsquo;s input, all three of the APIs are included in one package. For example, you can transcribe a recording of someone speaking in Danish, translate that to English and then identify how positive or negative the writer felt about its content (sentiment analysis) then identify the most important concepts and objects within the content (entity analysis).&lt;/p&gt;

&lt;h3 id=&#34;motivations&#34;&gt;Motivations&lt;/h3&gt;

&lt;h4 id=&#34;fake-news&#34;&gt;Fake news&lt;/h4&gt;

&lt;p&gt;One reason why I started looking at this area was the growth of &amp;lsquo;fake news&amp;rsquo;, and its effect on political discourse on social media. I wondered if there was some way to put metrics on how much a news story fuelled one&amp;rsquo;s own bias within your own filter bubble.  The entity API provides a way to perform entity and sentiment analysis at scale on tweets, and by then comparing different users and news sources preferences the hope is to be able to judge how much they are in agreement with your own bias, views and trusted reputation sources.&lt;/p&gt;

&lt;h4 id=&#34;make-your-own-alexa&#34;&gt;Make your own Alexa&lt;/h4&gt;

&lt;p&gt;Another motivating application is the growth of voice commands that will become the primary way of user interface with technology.  Already, &lt;a href=&#34;https://www.thinkwithgoogle.com/data-gallery/detail/google-app-voice-search/&#34;&gt;Google reports up to 20% of search in its app&lt;/a&gt; is via voice search.  I&amp;rsquo;d like to be able to say &amp;ldquo;R, print me out that report for client X&amp;rdquo;.  A Shiny app that records your voice, uploads to the API then parses the return text into actions gives you a chance to create your very own Alexa-like infrastructure.&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;text-align:center&#34;&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/alexa.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The voice activated internet connected speaker, Amazon&amp;rsquo;s Alexa - image from www.amazon.co.uk&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;translate-everything&#34;&gt;Translate everything&lt;/h4&gt;

&lt;p&gt;Finally, I live and work in Denmark.  As Danish is only spoken by less than 6 million people, applications that work in English may not be available in Danish very quickly, if at all.  The API&amp;rsquo;s translation service is the one that made the news in 2016 for &lt;a href=&#34;https://research.googleblog.com/2016/09/a-neural-network-for-machine.html&#34;&gt;&amp;ldquo;inventing its own language&amp;rdquo;&lt;/a&gt;, and offers much better English to Danish translations that the free web version and may make services available in Denmark sooner.&lt;/p&gt;

&lt;h3 id=&#34;using-the-library&#34;&gt;Using the library&lt;/h3&gt;

&lt;p&gt;To use these APIs within R, you first need to do a one-time setup to create a Google Project, add a credit card and authenticate which is &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/#installation&#34;&gt;detailed on the package website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After that, you feed in the R objects you want to operate upon.  The &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/127&#34;&gt;rOpenSci review&lt;/a&gt; helped to ensure that this can scale up easily, so that you can feed in large character vectors which the library will parse and rate limit as required.  The functions also work within &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; pipe syntax.&lt;/p&gt;

&lt;h4 id=&#34;speech-to-text&#34;&gt;Speech-to-text&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/speech.html&#34;&gt;Cloud Speech API&lt;/a&gt; is exposed via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_speech.html&#34;&gt;&lt;code&gt;gl_speech&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It supports multiple audio formats and languages, and you can either feed a sub-60 second audio file directly, or perform asynchrnous requests for longer audio files.&lt;/p&gt;

&lt;p&gt;Example code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)

my_audio &amp;lt;- &amp;quot;my_audio_file.wav&amp;quot;
gl_speech(my_audio)
#  A tibble: 1 x 3
#  transcript confidence                 words
#* &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;                &amp;lt;list&amp;gt;
#1 Hello Mum  0.9227779 &amp;lt;data.frame [19 x 3]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;translation&#34;&gt;Translation&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/translation.html&#34;&gt;Cloud Translation API&lt;/a&gt; lets you translate text via &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_translate.html&#34;&gt;&lt;code&gt;gl_translate&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you are charged per character, one tip here if you are working with lots of different languages is to perform detection of language offline first using another rOpenSci package, &lt;a href=&#34;https://github.com/ropensci/cld2&#34;&gt;&lt;code&gt;cld2&lt;/code&gt;&lt;/a&gt;.  That way you can avoid charges for text that is already in your target language i.e. English.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
library(cld2)
library(purrr)

my_text &amp;lt;- c(&amp;quot;Katten sidder på måtten&amp;quot;, &amp;quot;The cat sat on the mat&amp;quot;)

## offline detect language via cld2
detected &amp;lt;- map_chr(my_text, detect_language)
# [1] &amp;quot;DANISH&amp;quot;  &amp;quot;ENGLISH&amp;quot;

## get non-English text
translate_me &amp;lt;- my_text[detected != &amp;quot;ENGLISH&amp;quot;]

## translate
gl_translate(translate_me)
## A tibble: 1 x 3
#                 translatedText detectedSourceLanguage                    text
#*                         &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;
#1 The cat is sitting on the mat                     da Katten sidder på måtten
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/nlp.html&#34;&gt;Natural Language API&lt;/a&gt; reveals the structure and meaning of text, accessible via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_nlp.html&#34;&gt;&lt;code&gt;gl_nlp&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It returns several analysis:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Entity analysis&lt;/em&gt; - finds named entities (currently proper names and common nouns) in the text along with entity types, salience, mentions for each entity, and other properties. If possible, will also return metadata about that entity such as a Wikipedia URL.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Syntax&lt;/em&gt; - analyzes the syntax of the text and provides sentence boundaries and tokenization along with part of speech tags, dependency trees, and other properties.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sentiment&lt;/em&gt; - the overall sentiment of the text, represented by a magnitude [0, +inf] and score between -1.0 (negative sentiment) and 1.0 (positive sentiment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all useful to get an understanding of the meaning of a sentence, and has potentially the greatest number of applications of the APIs featured.  With entity analysis, auto categorisation of text is possible; the syntax returns let you pull out nouns and verbs for parsing into other actions; and the sentiment analysis allows you to get a feeling for emotion within text.&lt;/p&gt;

&lt;p&gt;A demonstration is below which gives an idea of what output you can generate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
quote &amp;lt;- &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
nlp &amp;lt;- gl_nlp(quote)

str(nlp)
#List of 6
# $ sentences        :List of 1
#  ..$ :&#39;data.frame&#39;:	1 obs. of  4 variables:
#  .. ..$ content    : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
#  .. ..$ beginOffset: int 0
#  .. ..$ magnitude  : num 0.6
#  .. ..$ score      : num -0.6
# $ tokens           :List of 1
#  ..$ :&#39;data.frame&#39;:	20 obs. of  17 variables:
#  .. ..$ content       : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;things&amp;quot; &amp;quot;are&amp;quot; &amp;quot;infinite&amp;quot; ...
#  .. ..$ beginOffset   : int [1:20] 0 4 11 15 23 25 29 38 42 48 ...
#  .. ..$ tag           : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NOUN&amp;quot; &amp;quot;VERB&amp;quot; &amp;quot;ADJ&amp;quot; ...
#  .. ..$ aspect        : chr [1:20] &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; ...
#  .. ..$ case          : chr [1:20] &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; ...
#  .. ..$ form          : chr [1:20] &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; ...
#  .. ..$ gender        : chr [1:20] &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; ...
#  .. ..$ mood          : chr [1:20] &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;INDICATIVE&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; ...
#  .. ..$ number        : chr [1:20] &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;PLURAL&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; ...
#  .. ..$ person        : chr [1:20] &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; ...
#  .. ..$ proper        : chr [1:20] &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; ...
#  .. ..$ reciprocity   : chr [1:20] &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; ...
#  .. ..$ tense         : chr [1:20] &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;PRESENT&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; ...
#  .. ..$ voice         : chr [1:20] &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; ...
#  .. ..$ headTokenIndex: int [1:20] 1 2 2 2 2 6 2 6 9 6 ...
#  .. ..$ label         : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NSUBJ&amp;quot; &amp;quot;ROOT&amp;quot; &amp;quot;ACOMP&amp;quot; ...
#  .. ..$ value         : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;thing&amp;quot; &amp;quot;be&amp;quot; &amp;quot;infinite&amp;quot; ...
# $ entities         :List of 1
#  ..$ :Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	6 obs. of  9 variables:
#  .. ..$ name         : chr [1:6] &amp;quot;human stupidity&amp;quot; &amp;quot;things&amp;quot; &amp;quot;universe&amp;quot; &amp;quot;universe&amp;quot; ...
#  .. ..$ type         : chr [1:6] &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; ...
#  .. ..$ salience     : num [1:6] 0.1662 0.4771 0.2652 0.2652 0.0915 ...
#  .. ..$ mid          : Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ wikipedia_url: Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ magnitude    : num [1:6] NA NA NA NA NA NA
#  .. ..$ score        : num [1:6] NA NA NA NA NA NA
#  .. ..$ beginOffset  : int [1:6] 42 4 29 86 29 86
#  .. ..$ mention_type : chr [1:6] &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; ...
# $ language         : chr &amp;quot;en&amp;quot;
# $ text             : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
# $ documentSentiment:Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	1 obs. of  2 variables:
#  ..$ magnitude: num 0.6
#  ..$ score    : num -0.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;This package is 10 times better due to the efforts of the rOpenSci reviewers &lt;a href=&#34;http://enpiar.com/&#34;&gt;Neal Richardson&lt;/a&gt; and &lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt;, who have whipped the documentation, outputs and test cases into the form they are today in &lt;code&gt;0.1.0&lt;/code&gt;.  Many thanks to them.&lt;/p&gt;

&lt;p&gt;Hopefully, this is just the beginning and the package can be further improved by its users - if you do give the package a try and find a potential improvement, &lt;a href=&#34;https://github.com/ropensci/googleLanguageR/issues&#34;&gt;raise an issue on GitHub&lt;/a&gt; and we can try to implement it.  I&amp;rsquo;m excited to see what users can do with these powerful tools.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>rrricanes to Access Tropical Cyclone Data</title>
      <link>https://ropensci.org/blog/2017/09/27/rrricanes/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/27/rrricanes/</guid>
      <description>
        
        

&lt;h3 id=&#34;what-is-rrricanes&#34;&gt;What is rrricanes&lt;/h3&gt;

&lt;h4 id=&#34;why-write-rrricanes&#34;&gt;Why Write rrricanes?&lt;/h4&gt;

&lt;p&gt;There is a tremendous amount of weather data available on the internet. Much of it is in raw format and not very easy to obtain. Hurricane data is no different. When one thinks of this data they may be inclined to think it is a bunch of map coordinates with some wind values and not much else. A deeper look will reveal structural and forecast data. An even deeper look will find millions of data points from hurricane reconnaissance, computer forecast models, ship and buoy observations, satellite and radar imagery, &amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; is an attempt to bring this data together in a way that doesn&amp;rsquo;t just benefit R users, but other languages as well.&lt;/p&gt;

&lt;p&gt;I began learning R in 2015 and immediately had wished I had a hurricane-specific dataset when Hurricane Patricia became a harmless, but historic hurricane roaming the Pacific waters. I found this idea revisited again as Hurricane Matthew took aim at Florida and the southeast in 2016. Unable to use R to study and consolidate Matthew&amp;rsquo;s data in R led me to begin learning package development. Thus, the birth of &lt;code&gt;rrricanes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In this article, I will take you on a lengthy tour of the most important features of &lt;code&gt;rrricanes&lt;/code&gt; and what the data means. If you have a background working with hurricane data, most of this will be redundant. My aim here is to cover the big ideas behind the package and explain them under the assumption you, the reader, are unfamiliar with the data offered.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; &lt;strong&gt;is not intended to be used in emergency situations&lt;/strong&gt;. I write this article as areas I have lived or currently live are under the gun from Hurricane Harvey and &lt;code&gt;rrricanes&lt;/code&gt; is unable to obtain data due to external issues (I will describe these later). It is designed with the intent of answering questions and exploring ideas outside of a time-sensitive environment.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; will not be available in CRAN for quite some time. The current schedule is May 15, 2018 (the &amp;ldquo;start&amp;rdquo; of the East Pacific hurricane season). This year is soley for testing under real-time conditions.&lt;/p&gt;

&lt;h4 id=&#34;and-rrricanesdata&#34;&gt;And rrricanesdata&lt;/h4&gt;

&lt;p&gt;The NHC archives text products dating back to at least 1998 (some earlier years exist but yet to be implemented in this package). Accessing this data is a time-consuming process on any computer. A limit of 4 requests per second is put in place to avoid being banned (or restricted) from the archives. So, if a hurricane has 20 text products you wish to pull and parse, this will take 5 seconds. Most cyclones have more and some, far more.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; is a compliment package to &lt;code&gt;rrricanes&lt;/code&gt;. &lt;code&gt;rrricanesdata&lt;/code&gt; contains post-scraped datasets of the archives for all available storms with the exception of advisories issued in the current month.This means you can explore the various datasets without the wait.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; will be updated monthly if an advisory has been issued the previous month. There will be regular monthly updates approximately from May through November - the typical hurricane season. In some cases, a cyclone may develop in the off-season. &lt;code&gt;rrricanesdata&lt;/code&gt; will be updated on the same schedule.&lt;/p&gt;

&lt;h4 id=&#34;eli5-the-data&#34;&gt;ELI5 the Data&lt;/h4&gt;

&lt;p&gt;This package covers tropical cyclones that have developed in the Atlantic basin (north Atlantic ocean) or East Pacific basin (northeast Pacific east of 140#&amp;deg;W). Central Pacific (140#&amp;deg;W - 180#&amp;deg;W) may be mixed in if listed in the NHC archives.&lt;/p&gt;

&lt;p&gt;While traditionally the hurricane season for each basin runs from mid-May or June through November, some cyclones have developed outside of this time frame.&lt;/p&gt;

&lt;p&gt;Every tropical cylone (any tropical low whether classified as a tropical depression, tropical storm or hurricane) contains a core set of text products officially issued from the National Hurricane Center. These products are issued every six hours.&lt;/p&gt;

&lt;p&gt;Much of this data has changed in format over the years. Some products have been discontinued and replaced by new products or wrapped into existing products. Some of these products are returned in raw text format; it is not cleaned and may contain HTML characters. Other products are parsed with every piece of data extracted and cleaned.&lt;/p&gt;

&lt;p&gt;I have done my best to ensure data is high quality. But, I cannot guarantee it is perfect. If you do believe you have found an error, please &lt;a href=&#34;https://github.com/ropensci/rrricanes/issues&#34;&gt;let me know&lt;/a&gt;; even if it seems small. I would rather be notified of a false error than ignore a true one.&lt;/p&gt;

&lt;h4 id=&#34;the-products&#34;&gt;The Products&lt;/h4&gt;

&lt;p&gt;Each advisory product is listed below with an abbreviation in parentheses. Unless otherwise noted, these products are issued every six hours. Generally, the times issued are 03:00, 09:00, 15:00 and 21:00 UTC. Some products may be issued in three-hour increments and, sometimes, two-hour increments. &lt;code&gt;update&lt;/code&gt; can be issued at any time.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Storm Discussion (&lt;code&gt;discus&lt;/code&gt;) - These are technical discussions centered on the current structure of the cyclone, satellite presentation, computer forecast model tendencies and more. These products are not parsed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Forecast/Adivsory (&lt;code&gt;fstadv&lt;/code&gt;) - This data-rich product lists the current location of the cyclone, its wind structure, forecast and forecast wind structure.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Public Advisory (&lt;code&gt;public&lt;/code&gt;) - These are general text statements issued for the public-at-large. Information in these products is a summary of the Forecast/Advisory product along with any watches and warnings issued, changed, or cancelled. Public Advisory products are the only regularly-scheduled product that may be issued intermittently (every three hours and, occasionally, every two hours) when watches and warnings are in effect. These products are not parsed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Wind Speed Probabilities (&lt;code&gt;wndprb&lt;/code&gt;) - These products list the probability of a minimum sustained wind speed expected in a given forecast window. This product replaces the Strike Probabilities product beginning in 2006 (see below).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Updates (&lt;code&gt;update&lt;/code&gt;) - Tropical Cyclone Updates may be issued at any time if a storm is an immediate threat to land or if the cyclone undergoes a significant change of strength or structure. The information in this product is general. These products are not parsed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;discontinued-products&#34;&gt;Discontinued Products&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Strike Probabilities (&lt;code&gt;prblty&lt;/code&gt;) - List the probability of a tropical cyclone passing within 65 nautical miles of a location within a forecast window. Replaced in 2006 by the Wind Speed Probabilities product.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Position Estimates (&lt;code&gt;posest&lt;/code&gt;) - Typically issued as a storm is threatening land but generally rare (see Hurricane Ike 2008, Key AL092008). It is generally just an update of the current location of the cyclone. After the 2011 hurricane season, this product was discontinued; Updates are now issued in their place. These products are not parsed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;primary-key&#34;&gt;Primary Key&lt;/h4&gt;

&lt;p&gt;Every cyclone has a &lt;code&gt;Key&lt;/code&gt;. However, not all products contain this value (&lt;code&gt;prblty&lt;/code&gt;, for example). Products issued during and after the 2005 hurricane season contain this variable.&lt;/p&gt;

&lt;p&gt;Use &lt;code&gt;Key&lt;/code&gt; to tie datasets together. If &lt;code&gt;Key&lt;/code&gt; does not exist, you will need to use a combination of &lt;code&gt;Name&lt;/code&gt; and &lt;code&gt;Date&lt;/code&gt;, depending on your requirements. Keep in mind that, unless a name is retired, names are recycled every seven years. For example, there are multiple cyclones named Katrina but you may want to isolate on Katrina, 2005.&lt;/p&gt;

&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; will not be submitted to CRAN until prior to the hurricane season, 2018. It can be installed via github using &lt;code&gt;devtools&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/rrricanes&amp;quot;, build_vignettes = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;optional-supporting-packages&#34;&gt;Optional Supporting Packages&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; uses a drat repository to host the large, pre-processed datasets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;rrricanesdata&amp;quot;,
                 repos = &amp;quot;https://timtrice.github.io/drat/&amp;quot;,
                 type = &amp;quot;source&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To use high resolution tracking charts, you may also wish to install the `rnaturalearthhires&amp;rsquo; package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;rnaturalearthhires&amp;quot;,
                 repos = &amp;quot;http://packages.ropensci.org&amp;quot;,
                 type = &amp;quot;source&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Linux users may also need to install:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;libgdal-dev&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;libproj-dev&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;libxml2-dev&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;get-a-list-of-storms&#34;&gt;Get a List of Storms&lt;/h3&gt;

&lt;p&gt;We start exploring &lt;code&gt;rrricanes&lt;/code&gt; by finding a storm (or storms) we wish to analyze. For this, we use &lt;code&gt;get_storms&lt;/code&gt;. There are two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;years&lt;/code&gt; Between 1998 and current year&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;basins&lt;/code&gt; One or both &amp;ldquo;AL&amp;rdquo; and &amp;ldquo;EP&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An empty call to the function will return storms for both the Atlantic and East Pacific basin for the current year.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
library(rrricanes)
get_storms() %&amp;gt;% print(n = nrow(.))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 33 x 4
##     Year                           Name Basin
##    &amp;lt;dbl&amp;gt;                          &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
##  1  2017          Tropical Storm Arlene    AL
##  2  2017            Tropical Storm Bret    AL
##  3  2017           Tropical Storm Cindy    AL
##  4  2017       Tropical Depression Four    AL
##  5  2017             Tropical Storm Don    AL
##  6  2017           Tropical Storm Emily    AL
##  7  2017             Hurricane Franklin    AL
##  8  2017                 Hurricane Gert    AL
##  9  2017               Hurricane Harvey    AL
## 10  2017 Potential Tropical Cyclone Ten    AL
## 11  2017                 Hurricane Irma    AL
## 12  2017                 Hurricane Jose    AL
## 13  2017                Hurricane Katia    AL
## 14  2017                  Hurricane Lee    AL
## 15  2017                Hurricane Maria    AL
## 16  2017          Tropical Storm Adrian    EP
## 17  2017         Tropical Storm Beatriz    EP
## 18  2017          Tropical Storm Calvin    EP
## 19  2017                 Hurricane Dora    EP
## 20  2017               Hurricane Eugene    EP
## 21  2017             Hurricane Fernanda    EP
## 22  2017            Tropical Storm Greg    EP
## 23  2017    Tropical Depression Eight-E    EP
## 24  2017               Hurricane Hilary    EP
## 25  2017                Hurricane Irwin    EP
## 26  2017   Tropical Depression Eleven-E    EP
## 27  2017            Tropical Storm Jova    EP
## 28  2017              Hurricane Kenneth    EP
## 29  2017           Tropical Storm Lidia    EP
## 30  2017                 Hurricane Otis    EP
## 31  2017                  Hurricane Max    EP
## 32  2017                Hurricane Norma    EP
## 33  2017           Tropical Storm Pilar    EP
## # ... with 1 more variables: Link &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Function &lt;code&gt;get_storms&lt;/code&gt; returns four variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Year - year of the cyclone.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Name - name of the cyclone.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Basin - basin the cyclone developed (AL for Atlantic, EP for east Pacific).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Link - URL to the cyclone&amp;rsquo;s archive page.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The variables &lt;code&gt;Name&lt;/code&gt; and &lt;code&gt;Link&lt;/code&gt; are the only variables that could potentially change. For example, you&amp;rsquo;ll notice a &lt;code&gt;Name&lt;/code&gt; value of &lt;u&gt;Potential Tropical Cyclone Ten&lt;/u&gt;. If this storm became a tropical storm then it would receive a new name and the link to the archive page would change as well.&lt;/p&gt;

&lt;p&gt;For this example we will explore &lt;u&gt;Hurricane Harvey&lt;/u&gt;.&lt;/p&gt;

&lt;h3 id=&#34;text-products&#34;&gt;Text Products&lt;/h3&gt;

&lt;h4 id=&#34;current-data&#34;&gt;Current Data&lt;/h4&gt;

&lt;p&gt;Once we have identified the storms we want to retrieve we can begin working on getting the products. In the earlier discussion of the available products, recall I used abbreviations such as &lt;code&gt;discus&lt;/code&gt;, &lt;code&gt;fstadv&lt;/code&gt;, etc. These are the terms we will use when obtaining data.&lt;/p&gt;

&lt;p&gt;The easiest method to getting storm data is the function &lt;code&gt;get_storm_data&lt;/code&gt;. This function can take multiple storm archive URLs and return multiple datasets within a list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ds &amp;lt;- get_storms() %&amp;gt;%
  filter(Name == &amp;quot;Hurricane Harvey&amp;quot;) %&amp;gt;%
  pull(Link) %&amp;gt;%
  get_storm_data(products = c(&amp;quot;discus&amp;quot;, &amp;quot;fstadv&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This process may take some time (particularly, &lt;code&gt;fstadv&lt;/code&gt; products). This is because the NHC website allows no more than 80 connections every 10 seconds. &lt;code&gt;rrricanes&lt;/code&gt; processes four links every half second.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; uses the &lt;code&gt;dplyr&lt;/code&gt; progress bar to keep you informed of the status. You can turn this off by setting option &lt;code&gt;dplyr.show_progress&lt;/code&gt; to FALSE.&lt;/p&gt;

&lt;p&gt;An additional option is &lt;code&gt;rrricanes.working_msg&lt;/code&gt;; FALSE by default. This option will show a message for each advisory currently being worked. I primarily added it to help find products causing problems but you may find it useful at some point.&lt;/p&gt;

&lt;p&gt;At this point, we have a list - &lt;code&gt;ds&lt;/code&gt; - of dataframes. Each dataframe is named after the product.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(ds)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;discus&amp;quot; &amp;quot;fstadv&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;discus&lt;/code&gt; is one of the products that isn&amp;rsquo;t parsed; the full text of the product is returned.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(ds$discus)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	43 obs. of  6 variables:
##  $ Status  : chr  &amp;quot;Potential Tropical Cyclone&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; ...
##  $ Name    : chr  &amp;quot;Nine&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##  $ Adv     : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Date    : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 21:00:00&amp;quot; ...
##  $ Key     : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Contents: chr  &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nPotential Tropical Cyclone Nine Discussion Number   1\nNWS National&amp;quot;| __truncated__ &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nTropical Storm Harvey Discussion Number   2\nNWS National Hurricane&amp;quot;| __truncated__ &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nTropical Storm Harvey Discussion Number   3\nNWS National Hurricane&amp;quot;| __truncated__ &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nTropical Storm Harvey Discussion Number   4\nNWS National Hurricane&amp;quot;| __truncated__ ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;fstadv&lt;/code&gt; dataframes, however, are parsed and contain the bulk of the information for the storm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(ds$fstadv)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	43 obs. of  117 variables:
##  $ Status       : chr  &amp;quot;Potential Tropical Cyclone&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; ...
##  $ Name         : chr  &amp;quot;Nine&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##  $ Adv          : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Date         : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 21:00:00&amp;quot; ...
##  $ Key          : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Lat          : num  13.1 13 13 13.1 13.1 13.4 13.7 13.8 13.9 14.1 ...
##  $ Lon          : num  -54.1 -55.8 -57.4 -59.1 -61.3 -62.9 -64.1 -65.9 -68.1 -70 ...
##  $ Wind         : num  30 35 35 35 35 35 35 35 35 30 ...
##  $ Gust         : num  40 45 45 45 45 45 45 45 45 40 ...
##  $ Pressure     : num  1008 1004 1005 1004 1005 ...
##  $ PosAcc       : num  30 30 30 30 30 40 40 30 30 30 ...
##  $ FwdDir       : num  270 270 270 270 270 275 275 275 275 275 ...
##  $ FwdSpeed     : num  15 16 16 16 18 18 16 18 19 19 ...
##  $ Eye          : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SeasNE       : num  NA 60 60 60 75 150 60 60 45 NA ...
##  $ SeasSE       : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasSW       : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasNW       : num  NA 60 60 60 60 60 45 60 45 NA ...
##  $ NE64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SE64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SW64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NW64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NE50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SE50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SW50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NW50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NE34         : num  NA 30 50 50 60 60 60 60 0 NA ...
##  $ SE34         : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SW34         : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ NW34         : num  NA 30 50 50 60 60 60 60 60 NA ...
##  $ Hr12FcstDate : POSIXct, format: &amp;quot;2017-08-18 00:00:00&amp;quot; &amp;quot;2017-08-18 06:00:00&amp;quot; ...
##  $ Hr12Lat      : num  13.1 13.1 13.2 13.2 13.3 13.6 14 14 14.1 14.3 ...
##  $ Hr12Lon      : num  -56.4 -58.3 -59.9 -61.7 -63.8 -65.7 -66.8 -68.7 -70.9 -73 ...
##  $ Hr12Wind     : num  30 35 35 35 35 35 35 35 35 30 ...
##  $ Hr12Gust     : num  40 45 45 45 45 45 45 45 45 40 ...
##  $ Hr12NE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NE34     : num  NA 30 50 50 60 60 60 60 60 NA ...
##  $ Hr12SE34     : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ Hr12SW34     : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ Hr12NW34     : num  NA 30 50 50 60 60 60 60 60 NA ...
##  $ Hr24FcstDate : POSIXct, format: &amp;quot;2017-08-18 12:00:00&amp;quot; &amp;quot;2017-08-18 18:00:00&amp;quot; ...
##  $ Hr24Lat      : num  13.2 13.4 13.6 13.5 13.6 13.9 14.3 14.3 14.4 14.6 ...
##  $ Hr24Lon      : num  -59.8 -61.6 -63.3 -65.2 -67.3 -69.3 -70.4 -72.7 -74.9 -77 ...
##  $ Hr24Wind     : num  35 40 40 40 40 40 40 40 40 35 ...
##  $ Hr24Gust     : num  45 50 50 50 50 50 50 50 50 45 ...
##  $ Hr24NE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NE34     : num  50 40 50 50 60 60 60 60 60 60 ...
##  $ Hr24SE34     : num  0 0 0 0 30 0 0 0 0 0 ...
##  $ Hr24SW34     : num  0 0 0 0 30 0 0 0 0 0 ...
##  $ Hr24NW34     : num  50 40 50 50 60 60 60 60 60 60 ...
##  $ Hr36FcstDate : POSIXct, format: &amp;quot;2017-08-19 00:00:00&amp;quot; &amp;quot;2017-08-19 06:00:00&amp;quot; ...
##  $ Hr36Lat      : num  13.5 13.7 13.9 13.9 14 14.2 14.5 14.6 14.9 15.2 ...
##  $ Hr36Lon      : num  -63.2 -65.1 -67 -68.8 -71.1 -73 -74.3 -76.7 -78.7 -80.5 ...
##  $ Hr36Wind     : num  40 45 45 40 40 40 40 45 45 40 ...
##  $ Hr36Gust     : num  50 55 55 50 50 50 50 55 55 50 ...
##  $ Hr36NE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NE34     : num  60 60 60 60 60 60 60 70 70 60 ...
##  $ Hr36SE34     : num  0 30 30 30 30 0 0 0 0 0 ...
##  $ Hr36SW34     : num  0 30 30 30 30 0 0 0 0 0 ...
##  $ Hr36NW34     : num  60 60 60 60 60 60 60 70 70 60 ...
##  $ Hr48FcstDate : POSIXct, format: &amp;quot;2017-08-19 12:00:00&amp;quot; &amp;quot;2017-08-19 18:00:00&amp;quot; ...
##  $ Hr48Lat      : num  13.9 14 14.1 14.1 14.3 14.5 14.8 15.2 15.7 16 ...
##  $ Hr48Lon      : num  -66.7 -68.8 -70.9 -72.7 -75 -76.7 -78.1 -80.1 -82.4 -83.8 ...
##  $ Hr48Wind     : num  45 45 50 50 50 45 45 50 50 45 ...
##  $ Hr48Gust     : num  55 55 60 60 60 55 55 60 60 55 ...
##  $ Hr48NE50     : num  NA NA 30 30 30 NA NA 30 30 NA ...
##  $ Hr48SE50     : num  NA NA 0 0 0 NA NA 0 0 NA ...
##  $ Hr48SW50     : num  NA NA 0 0 0 NA NA 0 0 NA ...
##  $ Hr48NW50     : num  NA NA 30 30 30 NA NA 30 30 NA ...
##  $ Hr48NE34     : num  60 60 60 60 70 60 60 90 90 70 ...
##  $ Hr48SE34     : num  30 30 30 30 30 30 0 50 50 0 ...
##  $ Hr48SW34     : num  30 30 30 30 30 30 0 40 40 0 ...
##  $ Hr48NW34     : num  60 60 60 60 70 60 60 70 70 70 ...
##  $ Hr72FcstDate : POSIXct, format: &amp;quot;2017-08-20 12:00:00&amp;quot; &amp;quot;2017-08-20 18:00:00&amp;quot; ...
##  $ Hr72Lat      : num  14.5 14.5 14.8 15 15 15.5 16.5 17 17.5 18 ...
##  $ Hr72Lon      : num  -74.5 -76.5 -78.6 -80.2 -82 -83.5 -84.7 -86.5 -88 -89 ...
##  $ Hr72Wind     : num  55 55 60 60 60 60 55 55 60 45 ...
##  $ Hr72Gust     : num  65 65 75 75 75 75 65 65 75 55 ...
##   [list output truncated]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each product can also be accessed on its own. For example, if you only wish to view &lt;code&gt;discus&lt;/code&gt; products, use the &lt;code&gt;get_discus&lt;/code&gt; function. &lt;code&gt;fstadv&lt;/code&gt; products can be accessed with &lt;code&gt;get_fstadv&lt;/code&gt;. Every products specific function is preceeded by &lt;code&gt;get_&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To understand the variable definitions, access the help file for each of these functions (i.e., &lt;code&gt;?get_fstadv&lt;/code&gt;). They contain full definitions on the variables and their purpose.&lt;/p&gt;

&lt;p&gt;As you can see, the &lt;code&gt;fstadv&lt;/code&gt; dataframe is very wide. There may be instances you only want to focus on specific pieces of the product. I&amp;rsquo;ve developed tidy functions to help trim these datasets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_fcst&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_fcst_wr&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_fstadv&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_wr&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These datasets exist in &lt;code&gt;rrricanesdata&lt;/code&gt; as &lt;code&gt;fcst&lt;/code&gt;, &lt;code&gt;fcst_wr&lt;/code&gt;, &lt;code&gt;adv&lt;/code&gt;, and &lt;code&gt;wr&lt;/code&gt;, respectively (see below).&lt;/p&gt;

&lt;p&gt;Most tropical cyclone forecast/advisory products will contain multiple forecast points. Initially, only three-day forecasts were issued. Beginning the with the 2003 season, 96 hour (five-day) forecasts were issued.&lt;/p&gt;

&lt;p&gt;If a storm is not expected to survive the full forecast period, then only relevant forecasts will be issued.&lt;/p&gt;

&lt;p&gt;We use &lt;code&gt;tidy_fcst&lt;/code&gt; to return these forecast points in a tidy fashion from &lt;code&gt;fstadv&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_fcst(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	283 obs. of  8 variables:
##  $ Key     : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv     : num  1 1 1 1 1 1 1 2 2 2 ...
##  $ Date    : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 15:00:00&amp;quot; ...
##  $ FcstDate: POSIXct, format: &amp;quot;2017-08-18 00:00:00&amp;quot; &amp;quot;2017-08-18 12:00:00&amp;quot; ...
##  $ Lat     : num  13.1 13.2 13.5 13.9 14.5 15.5 17 13.1 13.4 13.7 ...
##  $ Lon     : num  -56.4 -59.8 -63.2 -66.7 -74.5 -82 -87.5 -58.3 -61.6 -65.1 ...
##  $ Wind    : num  30 35 40 45 55 65 65 35 40 45 ...
##  $ Gust    : num  40 45 50 55 65 80 80 45 50 55 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wind radius values are issued with parameters of 34, 50 and 64. These values are the radius to which minimum one-minute sustained winds can be expected or exist.&lt;/p&gt;

&lt;p&gt;A tropical depression will not have associated wind radius values since the maximum winds of a depression are 30 knots. If a tropical storm has winds less than 50 knots, then it will only have wind radius values for the 34-knot wind field. If winds are greater than 50 knots, then it will have wind radius values for 34 and 50 knot winds. A hurricane will have all wind radius fields.&lt;/p&gt;

&lt;p&gt;Wind radius values are further seperated by quadrant; NE (northeast), SE, SW and NW. Not all quadrants will have values; particularly if the cyclone is struggling to organize. For example, you will often find a minimal hurricane only has hurricane-force winds (64 knots) in the northeast quadrant.&lt;/p&gt;

&lt;p&gt;When appropriate, a forecast/advisory product will contain these values for the current position and for each forecast position. Use &lt;code&gt;tidy_wr&lt;/code&gt; and &lt;code&gt;tidy_fcst_wr&lt;/code&gt;, respectively, for these variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_wr(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	56 obs. of  8 variables:
##  $ Key      : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv      : num  2 3 4 5 6 7 8 9 15 16 ...
##  $ Date     : POSIXct, format: &amp;quot;2017-08-17 21:00:00&amp;quot; &amp;quot;2017-08-18 03:00:00&amp;quot; ...
##  $ WindField: num  34 34 34 34 34 34 34 34 34 34 ...
##  $ NE       : num  30 50 50 60 60 60 60 0 100 80 ...
##  $ SE       : num  0 0 0 0 0 0 0 0 0 30 ...
##  $ SW       : num  0 0 0 0 0 0 0 0 0 20 ...
##  $ NW       : num  30 50 50 60 60 60 60 60 50 50 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_fcst_wr(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	246 obs. of  9 variables:
##  $ Key      : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv      : num  1 1 1 1 1 2 2 2 2 2 ...
##  $ Date     : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 15:00:00&amp;quot; ...
##  $ FcstDate : POSIXct, format: &amp;quot;2017-08-18 12:00:00&amp;quot; &amp;quot;2017-08-19 00:00:00&amp;quot; ...
##  $ WindField: num  34 34 34 34 50 34 34 34 34 34 ...
##  $ NE       : num  50 60 60 80 30 30 40 60 60 80 ...
##  $ SE       : num  0 0 30 40 0 0 0 30 30 40 ...
##  $ SW       : num  0 0 30 40 0 0 0 30 30 40 ...
##  $ NW       : num  50 60 60 80 30 30 40 60 60 80 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, you may only want to focus on current storm details. For this, we use &lt;code&gt;tidy_fstadv&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_fstadv(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	43 obs. of  18 variables:
##  $ Key     : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv     : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Date    : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 21:00:00&amp;quot; ...
##  $ Status  : chr  &amp;quot;Potential Tropical Cyclone&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; ...
##  $ Name    : chr  &amp;quot;Nine&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##  $ Lat     : num  13.1 13 13 13.1 13.1 13.4 13.7 13.8 13.9 14.1 ...
##  $ Lon     : num  -54.1 -55.8 -57.4 -59.1 -61.3 -62.9 -64.1 -65.9 -68.1 -70 ...
##  $ Wind    : num  30 35 35 35 35 35 35 35 35 30 ...
##  $ Gust    : num  40 45 45 45 45 45 45 45 45 40 ...
##  $ Pressure: num  1008 1004 1005 1004 1005 ...
##  $ PosAcc  : num  30 30 30 30 30 40 40 30 30 30 ...
##  $ FwdDir  : num  270 270 270 270 270 275 275 275 275 275 ...
##  $ FwdSpeed: num  15 16 16 16 18 18 16 18 19 19 ...
##  $ Eye     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SeasNE  : num  NA 60 60 60 75 150 60 60 45 NA ...
##  $ SeasSE  : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasSW  : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasNW  : num  NA 60 60 60 60 60 45 60 45 NA ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In release 0.2.1, &lt;code&gt;tidy_fstadv&lt;/code&gt; will be renamed to &lt;code&gt;tidy_adv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One final note on the data: all speed variables are measured in knots, distance variables in nautical miles, and pressure variables in millibars. Functions &lt;code&gt;knots_to_mph&lt;/code&gt; and &lt;code&gt;mb_to_in&lt;/code&gt; are available for speed/pressure conversions. Function &lt;code&gt;nm_to_sm&lt;/code&gt; to convert nautical miles to survey miles will be included in release 0.2.1.&lt;/p&gt;

&lt;h4 id=&#34;archived-data&#34;&gt;Archived Data&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; was built to make it easier to get pre-processed datasets. As mentioned earlier, &lt;code&gt;rrricanesdata&lt;/code&gt; will be updated the first of every month if any advisory was issued for the previous month. (As I am now writing this portion in September, all of Hurricane Harvey&amp;rsquo;s advisories - the last one issued the morning of August 31 - exist in &lt;code&gt;rrricanesdata&lt;/code&gt; release 0.0.1.4.)&lt;/p&gt;

&lt;p&gt;As with &lt;code&gt;rrricanes&lt;/code&gt;, &lt;code&gt;rrricanesdata&lt;/code&gt; is not available in CRAN (nor will be due to size limitations).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll load all datasets with the call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rrricanesdata)
data(list = data(package = &amp;quot;rrricanesdata&amp;quot;)$results[,3])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All core product datasets are available. The dataframes &lt;code&gt;adv&lt;/code&gt;, &lt;code&gt;fcst&lt;/code&gt;, &lt;code&gt;fcst_wr&lt;/code&gt; and &lt;code&gt;wr&lt;/code&gt; are the dataframes created by &lt;code&gt;tidy_fstadv&lt;/code&gt;, &lt;code&gt;tidy_fcst&lt;/code&gt;, &lt;code&gt;tidy_fcst_wr&lt;/code&gt; and &lt;code&gt;tidy_wr&lt;/code&gt;, respectively.&lt;/p&gt;

&lt;h3 id=&#34;tracking-charts&#34;&gt;Tracking Charts&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; also comes with helper functions to quickly generate tracking charts. These charts use &lt;code&gt;rnaturalearthdata&lt;/code&gt; (for high resolution maps, use package &lt;code&gt;rnaturalearthhires&lt;/code&gt;). These charts are not required - &lt;a href=&#34;https://twitter.com/hrbrmstr/status/900762714477350913&#34;&gt;Bob Rudis demonstrates&lt;/a&gt; demonstrates succintly - so feel free to experiment.&lt;/p&gt;

&lt;p&gt;You can generate a default plot for the entire globe with &lt;code&gt;tracking_chart&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tracking_chart()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-11-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You may find this handy when examining cyclones that cross basins (from the Atlantic to east Pacific such as Hurricane Otto, 2016).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tracking_chart&lt;/code&gt; takes three parameters (in addition to dots for other &lt;code&gt;ggplot&lt;/code&gt; calls):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;countries&lt;/code&gt; - By default, show country borders&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;states&lt;/code&gt; - By default, show state borders&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;res&lt;/code&gt; - resolution; default is 110nm.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We do not see countries and states in the map above because of the ggplot defaults. Let&amp;rsquo;s try it again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-12-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can &amp;ldquo;zoom in&amp;rdquo; on each basin with helper functions &lt;code&gt;al_tracking_chart&lt;/code&gt; and &lt;code&gt;ep_tracking_chart&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-13-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ep_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-14-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;gis-data&#34;&gt;GIS Data&lt;/h3&gt;

&lt;p&gt;GIS data exists for some cyclones and varies by year. This is a relatively new archive by the NHC and is inconsistent from storm to storm.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;gis&amp;rdquo; functions are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_advisory&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_latest&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_prob_storm_surge&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_windfield&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_wsp&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another area of inconsistency with these products is how they are organized. For example, &lt;code&gt;gis_advisory&lt;/code&gt;, &lt;code&gt;gis_prob_storm_surge&lt;/code&gt; and &lt;code&gt;gis_windfield&lt;/code&gt; can be retrieved with a storm &lt;code&gt;Key&lt;/code&gt; (unique identifier for every cyclone; see &lt;code&gt;fstadv$Key&lt;/code&gt;). Except for &lt;code&gt;gis_prob_storm_surge&lt;/code&gt;, you can even pass an advisory number (see &lt;code&gt;fstadv$Adv&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;gis_wsp&lt;/code&gt; requires a datetime value; to access a specific GIS package for a storm&amp;rsquo;s advisory you would need to use a variable such as &lt;code&gt;fstadv$Date&lt;/code&gt;, subtract three hours and convert to &amp;ldquo;%Y%m%d%H&amp;rdquo; format (&amp;ldquo;%m&amp;rdquo;, &amp;ldquo;%d&amp;rdquo;, and &amp;ldquo;%H&amp;rdquo; are optional).&lt;/p&gt;

&lt;p&gt;All above functions only return URL&amp;rsquo;s to their respective datasets. This was done to allow you to validate the quantity of datasets you wish to retrieve as, in some cases, the dataset may not exist at all or there may be several available. Use &lt;code&gt;gis_download&lt;/code&gt; with the requested URL&amp;rsquo;s to retrieve your datasets.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go through each of these. First, let&amp;rsquo;s get the &lt;code&gt;Key&lt;/code&gt; of Hurricane Harvey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Remember that ds already and only contains data for Hurricane Harvey
key &amp;lt;- ds$fstadv %&amp;gt;% pull(Key) %&amp;gt;% first()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;gis-advisory&#34;&gt;gis_advisory&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;gis_advisory&lt;/code&gt; returns a dataset package containing past and forecast plot points and lines, a forecast cone (area representing where the cyclone could track), wind radius data and current watches and warnings.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;gis_advisory&lt;/code&gt; takes two parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Key&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;advisory&lt;/code&gt; (optional)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we leave out advisory we get all related datasets for Hurricane Harvey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- gis_advisory(key = key)
length(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 77
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(x, n = 5L)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_001.zip&amp;quot;
## [2] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_001A.zip&amp;quot;
## [3] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_002.zip&amp;quot;
## [4] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_002A.zip&amp;quot;
## [5] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_003.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, there is quite a bit (and why the core gis functions only return URLs rather than the actual datasets). Let&amp;rsquo;s trim this down a bit. Sneaking a peek (&lt;a href=&#34;http://www.nhc.noaa.gov/archive/2017/HARVEY_graphics.php?product=5day_cone_with_line_and_wind&#34;&gt;cheating&lt;/a&gt;) I find advisory 19 seems a good choice.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_advisory(key = key, advisory = 19)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_019.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Good; there is a data package available for this advisory. Once you have confirmed the package you want to retrieve, use &lt;code&gt;gis_download&lt;/code&gt; to get the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_advisory(key = key, advisory = 19) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_5day_lin&amp;quot;
## with 1 features
## It has 7 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_5day_pgn&amp;quot;
## with 1 features
## It has 7 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_5day_pts&amp;quot;
## with 8 features
## It has 23 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_ww_wwlin&amp;quot;
## with 5 features
## It has 8 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see what we have.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(gis)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## List of 4
##  $ al092017_019_5day_lin:Formal class &#39;SpatialLinesDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 4 slots
##   .. ..@ data       :&#39;data.frame&#39;:	1 obs. of  7 variables:
##   .. .. ..$ STORMNAME: chr &amp;quot;Harvey&amp;quot;
##   .. .. ..$ STORMTYPE: chr &amp;quot;HU&amp;quot;
##   .. .. ..$ ADVDATE  : chr &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot;
##   .. .. ..$ ADVISNUM : chr &amp;quot;19&amp;quot;
##   .. .. ..$ STORMNUM : num 9
##   .. .. ..$ FCSTPRD  : num 120
##   .. .. ..$ BASIN    : chr &amp;quot;AL&amp;quot;
##   .. ..@ lines      :List of 1
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:8, 1:2] -94.6 -95.6 -96.5 -97.1 -97.3 -97.5 -97 -95 25.2 26.1 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;0&amp;quot;
##   .. ..@ bbox       : num [1:2, 1:2] -97.5 25.2 -94.6 29.5
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
##  $ al092017_019_5day_pgn:Formal class &#39;SpatialPolygonsDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. ..@ data       :&#39;data.frame&#39;:	1 obs. of  7 variables:
##   .. .. ..$ STORMNAME: chr &amp;quot;Harvey&amp;quot;
##   .. .. ..$ STORMTYPE: chr &amp;quot;HU&amp;quot;
##   .. .. ..$ ADVDATE  : chr &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot;
##   .. .. ..$ ADVISNUM : chr &amp;quot;19&amp;quot;
##   .. .. ..$ STORMNUM : num 9
##   .. .. ..$ FCSTPRD  : num 120
##   .. .. ..$ BASIN    : chr &amp;quot;AL&amp;quot;
##   .. ..@ polygons   :List of 1
##   .. .. ..$ :Formal class &#39;Polygons&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. .. .. .. ..@ Polygons :List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Polygon&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. .. .. .. .. .. .. ..@ labpt  : num [1:2] -95.4 29.2
##   .. .. .. .. .. .. .. ..@ area   : num 51.7
##   .. .. .. .. .. .. .. ..@ hole   : logi FALSE
##   .. .. .. .. .. .. .. ..@ ringDir: int 1
##   .. .. .. .. .. .. .. ..@ coords : num [1:1482, 1:2] -94.6 -94.7 -94.7 -94.7 -94.7 ...
##   .. .. .. .. ..@ plotOrder: int 1
##   .. .. .. .. ..@ labpt    : num [1:2] -95.4 29.2
##   .. .. .. .. ..@ ID       : chr &amp;quot;0&amp;quot;
##   .. .. .. .. ..@ area     : num 51.7
##   .. ..@ plotOrder  : int 1
##   .. ..@ bbox       : num [1:2, 1:2] -100 24.9 -91 33
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
##  $ al092017_019_5day_pts:Formal class &#39;SpatialPointsDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. ..@ data       :&#39;data.frame&#39;:	8 obs. of  23 variables:
##   .. .. ..$ ADVDATE  : chr [1:8] &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; ...
##   .. .. ..$ ADVISNUM : chr [1:8] &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; ...
##   .. .. ..$ BASIN    : chr [1:8] &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; ...
##   .. .. ..$ DATELBL  : chr [1:8] &amp;quot;10:00 PM Thu&amp;quot; &amp;quot;7:00 AM Fri&amp;quot; &amp;quot;7:00 PM Fri&amp;quot; &amp;quot;7:00 AM Sat&amp;quot; ...
##   .. .. ..$ DVLBL    : chr [1:8] &amp;quot;H&amp;quot; &amp;quot;H&amp;quot; &amp;quot;M&amp;quot; &amp;quot;M&amp;quot; ...
##   .. .. ..$ FCSTPRD  : num [1:8] 120 120 120 120 120 120 120 120
##   .. .. ..$ FLDATELBL: chr [1:8] &amp;quot;2017-08-24 7:00 PM Thu CDT&amp;quot; &amp;quot;2017-08-25 7:00 AM Fri CDT&amp;quot; &amp;quot;2017-08-25 7:00 PM Fri CDT&amp;quot; &amp;quot;2017-08-26 7:00 AM Sat CDT&amp;quot; ...
##   .. .. ..$ GUST     : num [1:8] 90 115 135 120 85 45 45 45
##   .. .. ..$ LAT      : num [1:8] 25.2 26.1 27.2 28.1 28.6 28.5 28.5 29.5
##   .. .. ..$ LON      : num [1:8] -94.6 -95.6 -96.5 -97.1 -97.3 -97.5 -97 -95
##   .. .. ..$ MAXWIND  : num [1:8] 75 95 110 100 70 35 35 35
##   .. .. ..$ MSLP     : num [1:8] 973 9999 9999 9999 9999 ...
##   .. .. ..$ SSNUM    : num [1:8] 1 2 3 3 1 0 0 0
##   .. .. ..$ STORMNAME: chr [1:8] &amp;quot;Hurricane Harvey&amp;quot; &amp;quot;Hurricane Harvey&amp;quot; &amp;quot;Hurricane Harvey&amp;quot; &amp;quot;Hurricane Harvey&amp;quot; ...
##   .. .. ..$ STORMNUM : num [1:8] 9 9 9 9 9 9 9 9
##   .. .. ..$ STORMSRC : chr [1:8] &amp;quot;Tropical Cyclone&amp;quot; &amp;quot;Tropical Cyclone&amp;quot; &amp;quot;Tropical Cyclone&amp;quot; &amp;quot;Tropical Cyclone&amp;quot; ...
##   .. .. ..$ STORMTYPE: chr [1:8] &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; &amp;quot;MH&amp;quot; &amp;quot;MH&amp;quot; ...
##   .. .. ..$ TCDVLP   : chr [1:8] &amp;quot;Hurricane&amp;quot; &amp;quot;Hurricane&amp;quot; &amp;quot;Major Hurricane&amp;quot; &amp;quot;Major Hurricane&amp;quot; ...
##   .. .. ..$ TAU      : num [1:8] 0 12 24 36 48 72 96 120
##   .. .. ..$ TCDIR    : num [1:8] 315 9999 9999 9999 9999 ...
##   .. .. ..$ TCSPD    : num [1:8] 9 9999 9999 9999 9999 ...
##   .. .. ..$ TIMEZONE : chr [1:8] &amp;quot;CDT&amp;quot; &amp;quot;CDT&amp;quot; &amp;quot;CDT&amp;quot; &amp;quot;CDT&amp;quot; ...
##   .. .. ..$ VALIDTIME: chr [1:8] &amp;quot;25/0000&amp;quot; &amp;quot;25/1200&amp;quot; &amp;quot;26/0000&amp;quot; &amp;quot;26/1200&amp;quot; ...
##   .. ..@ coords.nrs : num(0)
##   .. ..@ coords     : num [1:8, 1:2] -94.6 -95.6 -96.5 -97.1 -97.3 -97.5 -97 -95 25.2 26.1 ...
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : NULL
##   .. .. .. ..$ : chr [1:2] &amp;quot;coords.x1&amp;quot; &amp;quot;coords.x2&amp;quot;
##   .. ..@ bbox       : num [1:2, 1:2] -97.5 25.2 -94.6 29.5
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;coords.x1&amp;quot; &amp;quot;coords.x2&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
##  $ al092017_019_ww_wwlin:Formal class &#39;SpatialLinesDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 4 slots
##   .. ..@ data       :&#39;data.frame&#39;:	5 obs. of  8 variables:
##   .. .. ..$ STORMNAME: chr [1:5] &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##   .. .. ..$ STORMTYPE: chr [1:5] &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; ...
##   .. .. ..$ ADVDATE  : chr [1:5] &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; ...
##   .. .. ..$ ADVISNUM : chr [1:5] &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; ...
##   .. .. ..$ STORMNUM : num [1:5] 9 9 9 9 9
##   .. .. ..$ FCSTPRD  : num [1:5] 120 120 120 120 120
##   .. .. ..$ BASIN    : chr [1:5] &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; ...
##   .. .. ..$ TCWW     : chr [1:5] &amp;quot;TWA&amp;quot; &amp;quot;HWA&amp;quot; &amp;quot;TWR&amp;quot; &amp;quot;TWR&amp;quot; ...
##   .. ..@ lines      :List of 5
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] -97.7 -97.4 -97.2 24.3 25.2 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;0&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] -97.2 -97.2 -97.3 26 26.1 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;1&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] -97.2 -97.2 -97.3 26 26.1 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;2&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:10, 1:2] -95.6 -95.3 -95.1 -95.1 -94.8 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;3&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:16, 1:2] -97.3 -97.3 -97.3 -97.4 -97.4 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;4&amp;quot;
##   .. ..@ bbox       : num [1:2, 1:2] -97.7 24.3 -94.4 29.8
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We get four spatial dataframes - points, polygons and lines.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(gis)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;al092017_019_5day_lin&amp;quot; &amp;quot;al092017_019_5day_pgn&amp;quot; &amp;quot;al092017_019_5day_pts&amp;quot;
## [4] &amp;quot;al092017_019_ww_wwlin&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the expection of point spatial dataframes (which can be converted to dataframe using &lt;code&gt;tibble::as_data_frame&lt;/code&gt;, use helper function &lt;code&gt;shp_to_df&lt;/code&gt; to convert the spatial dataframes to dataframes.&lt;/p&gt;

&lt;h4 id=&#34;forecast-track&#34;&gt;Forecast Track&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)
al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_path(data = shp_to_df(gis$al092017_019_5day_lin), aes(x = long, y = lat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-21-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Use &lt;code&gt;geom_path&lt;/code&gt; instead of &lt;code&gt;geom_line&lt;/code&gt; to keep the positions in order.&lt;/p&gt;

&lt;p&gt;You can &amp;ldquo;zoom in&amp;rdquo; even further using &lt;code&gt;ggplot2::coord_equal&lt;/code&gt;. For that, we need to know the limits of our objects (minimum and maximum latitude and longitude) or bounding box. Thankfully, the &lt;code&gt;sp&lt;/code&gt; package can get us this information with the &lt;code&gt;bbox&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;But, we don&amp;rsquo;t want to use the &amp;ldquo;al092017_019_5day_lin&amp;rdquo; dataset. Our &lt;code&gt;gis&lt;/code&gt; dataset contains a forecast cone which expands well beyond the lines dataset.  Take a look:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp::bbox(gis$al092017_019_5day_lin)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##     min   max
## x -97.5 -94.6
## y  25.2  29.5
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp::bbox(gis$al092017_019_5day_pgn)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##          min       max
## x -100.01842 -90.96327
## y   24.86433  33.01644
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, let&amp;rsquo;s get the bounding box of our forecast cone dataset and zoom in on our map.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$al092017_019_5day_pgn)
al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_path(data = shp_to_df(gis$al092017_019_5day_lin),
            aes(x = long, y = lat)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-24-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s much better. For simplicity I&amp;rsquo;m going to save the base map, &lt;code&gt;bp&lt;/code&gt;, without the line plot.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp &amp;lt;- al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;forecast-points&#34;&gt;Forecast Points&lt;/h4&gt;

&lt;p&gt;Forecast points identify each forecast position along with forecast winds and date. Remember that for point spatial dataframes you use &lt;code&gt;tibble::as_data_frame&lt;/code&gt; rather than &lt;code&gt;sp_to_df&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_point(data = tibble::as_data_frame(gis$al092017_019_5day_pts),
             aes(x = long, y = lat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you ran the code above you would get an error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error in FUN(X[[i]], ...) : object &#39;long&#39; not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Why? The variable &lt;code&gt;long&lt;/code&gt; does not exist as it does in other GIS datasets; it is &lt;code&gt;lon&lt;/code&gt;. This is one of the inconsistencies I was referring to previously. Additionally, the variables are all uppercase.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(gis$al092017_019_5day_pts)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;ADVDATE&amp;quot;   &amp;quot;ADVISNUM&amp;quot;  &amp;quot;BASIN&amp;quot;     &amp;quot;DATELBL&amp;quot;   &amp;quot;DVLBL&amp;quot;
##  [6] &amp;quot;FCSTPRD&amp;quot;   &amp;quot;FLDATELBL&amp;quot; &amp;quot;GUST&amp;quot;      &amp;quot;LAT&amp;quot;       &amp;quot;LON&amp;quot;
## [11] &amp;quot;MAXWIND&amp;quot;   &amp;quot;MSLP&amp;quot;      &amp;quot;SSNUM&amp;quot;     &amp;quot;STORMNAME&amp;quot; &amp;quot;STORMNUM&amp;quot;
## [16] &amp;quot;STORMSRC&amp;quot;  &amp;quot;STORMTYPE&amp;quot; &amp;quot;TCDVLP&amp;quot;    &amp;quot;TAU&amp;quot;       &amp;quot;TCDIR&amp;quot;
## [21] &amp;quot;TCSPD&amp;quot;     &amp;quot;TIMEZONE&amp;quot;  &amp;quot;VALIDTIME&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s try it again.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_point(data = tibble::as_data_frame(gis$al092017_019_5day_pts),
             aes(x = LON, y = LAT))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-28-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Better.&lt;/p&gt;

&lt;h4 id=&#34;forecast-cone&#34;&gt;Forecast Cone&lt;/h4&gt;

&lt;p&gt;A forecast cone identifies the probability of error in a forecast. Forecasting tropical cyclones is tricky business - errors increase the further out a forecast is issued. Theoretically, any area within a forecast cone is at risk of seeing cyclone conditions within the given period of time.&lt;/p&gt;

&lt;p&gt;Generally, a forecast cone package contains two subsets: 72-hour forecast cone and 120-hour forecast cone. This is identified in the dataset under the variable &lt;code&gt;FCSTPRD&lt;/code&gt;. Let&amp;rsquo;s take a look at the 72-hour forecast period:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_polygon(data = shp_to_df(gis$al092017_019_5day_pgn) %&amp;gt;%
                 filter(FCSTPRD == 72),
               aes(x = long, y = lat, color = FCSTPRD))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-29-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nothing there!&lt;/p&gt;

&lt;p&gt;As mentioned earlier, these are experimental products issued by the NHC and they do contain inconsistencies. To demonstrate, I&amp;rsquo;ll use Hurricane Ike advisory 42.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- gis_advisory(key = &amp;quot;AL092008&amp;quot;, advisory = 42) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_5day_lin&amp;quot;
## with 2 features
## It has 9 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_5day_pgn&amp;quot;
## with 2 features
## It has 9 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_5day_pts&amp;quot;
## with 13 features
## It has 20 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_ww_wwlin&amp;quot;
## with 5 features
## It has 10 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = shp_to_df(df$al092008_042_5day_pgn) %&amp;gt;%
                 filter(FCSTPRD == 72),
                  aes(x = long, y = lat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-30-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We do, however, have a 120-hour forecast cone for Hurricane Harvey. Let&amp;rsquo;s go ahead and plot that.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_polygon(data = gis$al092017_019_5day_pgn,
               aes(x = long, y = lat), alpha = 0.15)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-31-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s an odd-looking forecast cone, for sure. But this demonstrates the entire area that Harvey could have potentially traveled.&lt;/p&gt;

&lt;h4 id=&#34;watches-and-warnings&#34;&gt;Watches and Warnings&lt;/h4&gt;

&lt;p&gt;Our last dataset in this package is &amp;ldquo;al092017_09_ww_wlin&amp;rdquo;. These are the current watches and warnings in effect. This is a spatial lines dataframe that needs &lt;code&gt;shp_to_df&lt;/code&gt;. Again, we use &lt;code&gt;geom_path&lt;/code&gt; instead of &lt;code&gt;geom_line&lt;/code&gt;. And we want to group our paths by the variable &lt;code&gt;TCWW&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_path(data = shp_to_df(gis$al092017_019_ww_wwlin),
            aes(x = long, y = lat, group = group, color = TCWW), size = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-32-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The paths won&amp;rsquo;t follow our coastlines exactly but you get the idea. The abbreviations don&amp;rsquo;t really give much information, either. Convert &lt;code&gt;TCWW&lt;/code&gt; to factor and provide better labels for your legend.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ww_wlin &amp;lt;- shp_to_df(gis$al092017_019_ww_wwlin)
ww_wlin$TCWW &amp;lt;- factor(ww_wlin$TCWW,
                              levels = c(&amp;quot;TWA&amp;quot;, &amp;quot;TWR&amp;quot;, &amp;quot;HWA&amp;quot;, &amp;quot;HWR&amp;quot;),
                              labels = c(&amp;quot;Tropical Storm Watch&amp;quot;,
                                         &amp;quot;Tropical Storm Warning&amp;quot;,
                                         &amp;quot;Hurricane Watch&amp;quot;,
                                         &amp;quot;Hurricane Warning&amp;quot;))

bp +
  geom_path(data = ww_wlin,
            aes(x = long, y = lat, group = group, color = TCWW), size = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-33-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;See &lt;a href=&#34;https://ropensci.github.io/rrricanes/articles/articles/forecast_advisory.html&#34;&gt;Forecast/Adivsory GIS&lt;/a&gt; on the &lt;code&gt;rrricanes&lt;/code&gt; website for an example of putting all of this data together in one map.&lt;/p&gt;

&lt;h4 id=&#34;gis-prob-storm-surge&#34;&gt;gis_prob_storm_surge&lt;/h4&gt;

&lt;p&gt;We can also plot the probablistic storm surge for given locations. Again, you will need the storm &lt;code&gt;Key&lt;/code&gt; for this function. There are two additional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;products&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;datetime&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;products&lt;/code&gt; can be one or both of &amp;ldquo;esurge&amp;rdquo; and &amp;ldquo;psurge&amp;rdquo;. esurge shows the probability of the cyclone exceeding the given storm surge plus tide within a given forecast period. psurge shows the probability of a given storm surge within a specified forecast period.&lt;/p&gt;

&lt;p&gt;One or more products may not exist depending on the cyclone and advisory.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;products&lt;/code&gt; parameter expects a list of values for each product. For esurge products, valid values are 10, 20, 30, 40 or 50. For psurge products, valid values are 0, 1, 2, &amp;hellip;, 20.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see if any esurge products exist for Harvey.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(gis_prob_storm_surge(key = key,
                            products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10))))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 150
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And psurge:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(gis_prob_storm_surge(key = key, products = list(&amp;quot;psurge&amp;quot; = 0:20)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 630
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, we have access to a ton of data here. When discussing &lt;code&gt;gis_advisory&lt;/code&gt;, we were able to filter by advisory number. With &lt;code&gt;gis_prob_storm_surge&lt;/code&gt;, this is not an option; we have to use the &lt;code&gt;datetime&lt;/code&gt; parameter to filter. Let&amp;rsquo;s find the &lt;code&gt;Date&lt;/code&gt; for advisory 19.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(d &amp;lt;- ds$fstadv %&amp;gt;% filter(Adv == 19) %&amp;gt;% pull(Date))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2017-08-25 03:00:00 UTC&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;esurge&#34;&gt;esurge&lt;/h5&gt;

&lt;p&gt;Now, let&amp;rsquo;s view all esurge products for date only (exlude time).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_prob_storm_surge(key = key,
                     products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10)),
                     datetime = strftime(d, &amp;quot;%Y%m%d&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082500.zip&amp;quot;
##  [2] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082506.zip&amp;quot;
##  [3] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082512.zip&amp;quot;
##  [4] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082518.zip&amp;quot;
##  [5] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082500.zip&amp;quot;
##  [6] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082506.zip&amp;quot;
##  [7] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082512.zip&amp;quot;
##  [8] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082518.zip&amp;quot;
##  [9] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082500.zip&amp;quot;
## [10] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082506.zip&amp;quot;
## [11] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082512.zip&amp;quot;
## [12] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082518.zip&amp;quot;
## [13] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082500.zip&amp;quot;
## [14] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082506.zip&amp;quot;
## [15] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082512.zip&amp;quot;
## [16] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082518.zip&amp;quot;
## [17] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082500.zip&amp;quot;
## [18] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082506.zip&amp;quot;
## [19] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082512.zip&amp;quot;
## [20] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082518.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s still quite a bit. We can filter it to more by adding hour to the &lt;code&gt;datetime&lt;/code&gt; parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_prob_storm_surge(key = key,
                     products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10)),
                     datetime = strftime(d, &amp;quot;%Y%m%d%H&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This call will give you an error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: No data available for requested storm/advisory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But, this isn&amp;rsquo;t entirely correct. When an advisory package is issued it contains information for the release time. Some of the GIS datasets are based on the release time -3 hours. So, we need to subtract 3 hours from &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There is an additional value that, as of the latest release is not extracted, records the position of the cyclone three hours prior. (As I understand it from the NHC, this is due to the time it takes to collect and prepare the data.) Per &lt;a href=&#34;https://github.com/ropensci/rrricanes/issues/102&#34;&gt;Issue #102&lt;/a&gt;, these values will be added for release 0.2.1. Therefore, instead of subtracting three hours from the &lt;code&gt;Date&lt;/code&gt; variable, you can simply use the &lt;code&gt;PrevPosDate&lt;/code&gt; value for this function.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try it again with the math:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_prob_storm_surge(key = key,
                     products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10)),
                     datetime = strftime(d - 60 * 60 * 3, &amp;quot;%Y%m%d%H&amp;quot;,
                                         tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082500.zip&amp;quot;
## [2] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082500.zip&amp;quot;
## [3] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082500.zip&amp;quot;
## [4] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082500.zip&amp;quot;
## [5] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082500.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As I don&amp;rsquo;t want to get all of these datasets, I&amp;rsquo;ll limit my esurge to show surge values with at least a 50% chance of being exceeded:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_prob_storm_surge(key = key,
                            products = list(&amp;quot;esurge&amp;quot; = 50),
                            datetime = strftime(d - 60 * 60 * 3, &amp;quot;%Y%m%d%H&amp;quot;,
                                                tz = &amp;quot;UTC&amp;quot;)) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082500_e50&amp;quot;
## with 97 features
## It has 2 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will bring us a spatial polygon dataframe.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- shp_to_df(gis$al092017_2017082500_e50)
bb &amp;lt;- sp::bbox(gis$al092017_2017082500_e50)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	161313 obs. of  9 variables:
##  $ long   : num  -93.2 -93.2 -93.2 -93.2 -93.2 ...
##  $ lat    : num  29.9 29.9 29.9 29.9 29.9 ...
##  $ order  : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece  : Factor w/ 349 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group  : Factor w/ 7909 levels &amp;quot;0.1&amp;quot;,&amp;quot;0.2&amp;quot;,&amp;quot;0.3&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id     : chr  &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; ...
##  $ POINTID: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ TCSRG50: num  0 0 0 0 0 0 0 0 0 0 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
            aes(x = long, y = lat, group = group, color = TCSRG50)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-43-1.png&#34; title=&#34;plot of chunk unnamed-chunk-43&#34; alt=&#34;plot of chunk unnamed-chunk-43&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This plot tells us that, along the central Texas coast, the expected storm surge along with tides is greater than 7.5 feet and there is a 50% chance of this height being exceeded.&lt;/p&gt;

&lt;h4 id=&#34;psurge&#34;&gt;psurge&lt;/h4&gt;

&lt;p&gt;The psurge product gives us the probabilistic storm surge for a location within the given forecast period.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_prob_storm_surge(key = key,
                            products = list(&amp;quot;psurge&amp;quot; = 20),
                            datetime = strftime(d - 60 * 60 * 3, &amp;quot;%Y%m%d%H&amp;quot;,
                                                tz = &amp;quot;UTC&amp;quot;)) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082500_gt20&amp;quot;
## with 12 features
## It has 2 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will bring us a spatial polygon dataframe.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- shp_to_df(gis$al092017_2017082500_gt20)
bb &amp;lt;- sp::bbox(gis$al092017_2017082500_gt20)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	3293 obs. of  9 variables:
##  $ long     : num  -96.8 -96.8 -96.8 -96.8 -96.8 ...
##  $ lat      : num  28.5 28.5 28.4 28.4 28.4 ...
##  $ order    : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole     : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece    : Factor w/ 54 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group    : Factor w/ 227 levels &amp;quot;0.1&amp;quot;,&amp;quot;0.2&amp;quot;,&amp;quot;0.3&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id       : chr  &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; ...
##  $ POINTID  : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ PSurge20c: num  1 1 1 1 1 1 1 1 1 1 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
            aes(x = long, y = lat, group = group, color = PSurge20c)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-47-1.png&#34; title=&#34;plot of chunk unnamed-chunk-47&#34; alt=&#34;plot of chunk unnamed-chunk-47&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This map shows the cumulative probability that a storm surge of greater than 20 feet will be seen within the highlighted regions.&lt;/p&gt;

&lt;p&gt;This particular map doesn&amp;rsquo;t help much as we&amp;rsquo;ve zoomed in too far. What may provide use is a list of probability stations as obtained from the NHC. For this, you can use &lt;code&gt;al_prblty_stations&lt;/code&gt; (&lt;code&gt;ep_prblty_stations&lt;/code&gt; returns FALSE since, as of this writing, the format is invalid).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;stations &amp;lt;- al_prblty_stations()

al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
            aes(x = long, y = lat, group = group, color = PSurge20c)) +
  geom_label(data = stations, aes(x = Lon, y = Lat, label = Location)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-48-1.png&#34; title=&#34;plot of chunk unnamed-chunk-48&#34; alt=&#34;plot of chunk unnamed-chunk-48&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;gis-windfield&#34;&gt;gis_windfield&lt;/h4&gt;

&lt;p&gt;When possible, there may also be a cyclone wind radius dataset for the current and forecast positions. With this function we can resort back to &lt;code&gt;Key&lt;/code&gt; and an advisory number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_windfield(key = key, advisory = 19) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082503_forecastradii&amp;quot;
## with 15 features
## It has 13 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082503_initialradii&amp;quot;
## with 3 features
## It has 13 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(gis)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;al092017_2017082503_forecastradii&amp;quot; &amp;quot;al092017_2017082503_initialradii&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s get the bounding box and plot our initialradii dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$al092017_2017082503_initialradii)

al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = shp_to_df(gis$al092017_2017082503_initialradii),
            aes(x = long, y = lat, group = group, fill = factor(RADII)),
            alpha = 0.5) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-51-1.png&#34; title=&#34;plot of chunk unnamed-chunk-51&#34; alt=&#34;plot of chunk unnamed-chunk-51&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And add the forecast wind radii data onto the chart (modifying &lt;code&gt;bb&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$al092017_2017082503_forecastradii)

al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = shp_to_df(gis$al092017_2017082503_initialradii),
            aes(x = long, y = lat, group = group, fill = factor(RADII)),
            alpha = 0.5) +
  geom_polygon(data = shp_to_df(gis$al092017_2017082503_forecastradii),
               aes(x = long, y = lat, group = group, fill = factor(RADII)),
               alpha = 0.5) +
  geom_label(data = stations, aes(x = Lon, y = Lat, label = Location)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-52-1.png&#34; title=&#34;plot of chunk unnamed-chunk-52&#34; alt=&#34;plot of chunk unnamed-chunk-52&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;gis-wsp&#34;&gt;gis_wsp&lt;/h4&gt;

&lt;p&gt;Our last GIS dataset is wind speed probabilities. This dataset is not storm specific nor even basin-specific; you may get results for cyclones halfway across the world.&lt;/p&gt;

&lt;p&gt;The two parameters needed are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;datetime&lt;/code&gt; - again, using the %Y%m%d%H format (not all values are required)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;res&lt;/code&gt; - Resolution of the probabilities; 5 degrees, 0.5 degrees and 0.1 degrees.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Wind fields are for 34, 50 and 64 knots. Not all resolutions or windfields will be available at a given time.&lt;/p&gt;

&lt;p&gt;Sticking with our variable &lt;code&gt;d&lt;/code&gt;, let&amp;rsquo;s first make sure there is a dataset that exists for that time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_wsp(datetime = strftime(d - 60 * 60 * 3, format = &amp;quot;%Y%m%d%H&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this article, we&amp;rsquo;ll stick to the higher resolution plot.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;we need a temporarily fixed function to replace &lt;code&gt;gis_wsp()&lt;/code&gt;, which will be
fixed in package soon&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_wsp_2 &amp;lt;- function(datetime, res = c(5, 0.5, 0.1)) {
  res &amp;lt;- as.character(res)
  res &amp;lt;- stringr::str_replace(res, &amp;quot;^5$&amp;quot;, &amp;quot;5km&amp;quot;)
  res &amp;lt;- stringr::str_replace(res, &amp;quot;^0.5$&amp;quot;, &amp;quot;halfDeg&amp;quot;)
  res &amp;lt;- stringr::str_replace(res, &amp;quot;^0.1$&amp;quot;, &amp;quot;tenthDeg&amp;quot;)
  year &amp;lt;- stringr::str_sub(datetime, 0L, 4L)
  request &amp;lt;- httr::GET(&amp;quot;http://www.nhc.noaa.gov/gis/archive_wsp.php&amp;quot;,
                       query = list(year = year))
  contents &amp;lt;- httr::content(request, as = &amp;quot;parsed&amp;quot;, encoding = &amp;quot;UTF-8&amp;quot;)
  ds &amp;lt;- rvest::html_nodes(contents, xpath = &amp;quot;//a&amp;quot;) %&amp;gt;% rvest::html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
    stringr::str_extract(&amp;quot;.+\\.zip$&amp;quot;) %&amp;gt;% .[stats::complete.cases(.)]
  if (nchar(datetime) &amp;lt; 10) {
    ptn_datetime &amp;lt;- paste0(datetime, &amp;quot;[:digit:]+&amp;quot;)
  } else {
    ptn_datetime &amp;lt;- datetime
  }
  ptn_res &amp;lt;- paste(res, collapse = &amp;quot;|&amp;quot;)
  ptn &amp;lt;- sprintf(&amp;quot;%s_wsp_[:digit:]{1,3}hr(%s)&amp;quot;, ptn_datetime,
                 ptn_res)
  links &amp;lt;- ds[stringr::str_detect(ds, ptn)]
  links &amp;lt;- paste0(&amp;quot;http://www.nhc.noaa.gov/gis/&amp;quot;, links)
  return(links)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_wsp_2(
  datetime = strftime(d - 60 * 60 * 3, format = &amp;quot;%Y%m%d%H&amp;quot;, tz = &amp;quot;UTC&amp;quot;),
  res = 5) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;2017082500_wsp34knt120hr_5km&amp;quot;
## with 11 features
## It has 1 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;2017082500_wsp50knt120hr_5km&amp;quot;
## with 11 features
## It has 1 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;2017082500_wsp64knt120hr_5km&amp;quot;
## with 11 features
## It has 1 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All of these datasets are spatial polygon dataframes. Again, we will need to convert to dataframe using &lt;code&gt;shp_to_df&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$`2017082500_wsp34knt120hr_5km`)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Examine the structure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- shp_to_df(gis$`2017082500_wsp34knt120hr_5km`)
str(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	24182 obs. of  8 variables:
##  $ long      : num  -97.2 -97.2 -97.2 -97.3 -97.3 ...
##  $ lat       : num  20.3 20.3 20.3 20.3 20.4 ...
##  $ order     : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole      : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece     : Factor w/ 8 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group     : Factor w/ 52 levels &amp;quot;0.1&amp;quot;,&amp;quot;0.2&amp;quot;,&amp;quot;0.3&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id        : chr  &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; ...
##  $ PERCENTAGE: chr  &amp;quot;&amp;lt;5%&amp;quot; &amp;quot;&amp;lt;5%&amp;quot; &amp;quot;&amp;lt;5%&amp;quot; &amp;quot;&amp;lt;5%&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
               aes(x = long, y = lat, group = group, fill = PERCENTAGE),
               alpha = 0.25) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-58-1.png&#34; title=&#34;plot of chunk unnamed-chunk-58&#34; alt=&#34;plot of chunk unnamed-chunk-58&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There aren&amp;rsquo;t many ways we can narrow this down other than using arbitrary longitude values. The observations in the dataset do not have a variable identifying which storm each set of values belongs to. So, I&amp;rsquo;ll remove the &lt;code&gt;coord_equal&lt;/code&gt; call so we&amp;rsquo;re only focused on the Atlantic basin.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
               aes(x = long, y = lat, group = group, fill = PERCENTAGE),
               alpha = 0.25)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-59-1.png&#34; title=&#34;plot of chunk unnamed-chunk-59&#34; alt=&#34;plot of chunk unnamed-chunk-59&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, you can narrow it down further as you see fit.&lt;/p&gt;

&lt;p&gt;Do not confuse this GIS dataset with the &lt;code&gt;wndprb&lt;/code&gt; product or similar &lt;code&gt;prblty&lt;/code&gt; products; both of which only identify probabilities for given locations.&lt;/p&gt;

&lt;h4 id=&#34;gis-latest&#34;&gt;gis_latest&lt;/h4&gt;

&lt;p&gt;For active cyclones, you can retrieve all available GIS datasets using &lt;code&gt;gis_latest&lt;/code&gt;. Note that, unlike the previous GIS functions, this function will return a list of all GIS dataframes available.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_latest()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have a large list of GIS spatial dataframes. Two things to point out here; first, we now have a &amp;ldquo;windswath&amp;rdquo; GIS dataset. This dataset, to the best of my knowledge, does not exist on it&amp;rsquo;s own. Therefore, no archived &amp;ldquo;windswath&amp;rdquo; datasets are available.&lt;/p&gt;

&lt;p&gt;Second, I have found this data fluctuates even from minute to minute. Earlier this year when attempting to develop automated reporting, I found the return value of the call would vary almost with every call.&lt;/p&gt;

&lt;p&gt;Of course, that doesn&amp;rsquo;t mean it is not valuable, and why it has been included. You can easily perform checks for specific data you are looking for. If it doesn&amp;rsquo;t exist, bail and try again in a few minutes.&lt;/p&gt;

&lt;h3 id=&#34;potential-issues-using-rrricanes&#34;&gt;Potential Issues Using rrricanes&lt;/h3&gt;

&lt;p&gt;I cannot stress enough that &lt;code&gt;rrricanes&lt;/code&gt; &lt;strong&gt;is not intended for use during emergency situations&lt;/strong&gt;, as I myself learned &lt;a href=&#34;https://twitter.com/timtrice/status/901025869367586816&#34;&gt;during Hurricane Harvey&lt;/a&gt;. The package currently relies on the NHC website which, I truly believe, is curated by hand.&lt;/p&gt;

&lt;p&gt;The most common problems I&amp;rsquo;ve noticed are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The NHC website unable to load or slow to respond. This was a hassle in previous releases but seems to be ironed out as of release 0.2.0.6. Nonetheless, there may be instances where response time is slow.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Incorrect storm archive links. Another example would be during Harvey when the link to Harvey&amp;rsquo;s archive page was &lt;a href=&#34;https://twitter.com/PutmanSteve/status/900777826412105729&#34;&gt;listed incorrectly&lt;/a&gt;. If I manually typed the link as it should be, the storm&amp;rsquo;s correct archive page would load. However, the NHC website listed it incorrectly on the annual archives page.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As I become more aware of potential problems, I will look for workarounds. I would be greatly appreciative of any problems being posted to the &lt;a href=&#34;https://github.com/ropensci/rrricanes/issues&#34;&gt;rrricanes repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I will also post known issues beyond my control (such as NHC website issues) to Twitter using the &lt;a href=&#34;https://twitter.com/search?f=tweets&amp;amp;vertical=default&amp;amp;q=%23rrricanes&amp;amp;src=typd&#34;&gt;#rrricanes hashtag&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;future-plans&#34;&gt;Future Plans&lt;/h3&gt;

&lt;p&gt;The following data will be added to &lt;code&gt;rrricanes&lt;/code&gt; as time allows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Reconnaissance data (release 0.2.2)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Computer forecast model data (release 0.2.3)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Archived satellite images (tentative)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ship and buoy data (tentative)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reconnaissance data itself will be a massive project. There are numerous types of products. And, as advisory product formats have changed over the years, so have these. Any help in this task would be tremendously appreciated!&lt;/p&gt;

&lt;p&gt;Some computer forecast models are in the public domain and can certainly be of tremendous use. Some are difficult to find (especially archived). The caution in this area is that many websites post this data but may have limitations on how it can be accessed.&lt;/p&gt;

&lt;p&gt;Additionally, data may be added as deemed fitting.&lt;/p&gt;

&lt;h3 id=&#34;contribute&#34;&gt;Contribute&lt;/h3&gt;

&lt;p&gt;Anyone is more than welcome to contribute to the package. I would definitely appreciate any help. See &lt;a href=&#34;https://github.com/ropensci/rrricanes/blob/master/.github/CONTRIBUTING.md&#34;&gt;Contributions&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;I would ask that you follow the &lt;a href=&#34;http://style.tidyverse.org/&#34;&gt;Tidyverse style guide&lt;/a&gt;. Release 0.2.1 will fully incorporate these rules.&lt;/p&gt;

&lt;p&gt;You do not need to submit code in order to be listed as a contributor. If there is a data source (that can legally be scraped) that you feel should be added, please feel free to submit a request. Submitting bug reports and feature requests are all extremely valuable to the success of &lt;code&gt;rrricanes&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;I want to thank the &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; community for embracing &lt;code&gt;rrricanes&lt;/code&gt; and accepting the package into their vast portfolio. This is my first attempt and putting a project into part of a larger community and the lessons learned have been outstanding.&lt;/p&gt;

&lt;p&gt;I want to thank &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maelle Salmon&lt;/a&gt; who, in a sense, has been like a guiding angel from start to finish during the entire onboarding and review process.&lt;/p&gt;

&lt;p&gt;I want to give a very special thanks to &lt;a href=&#34;https://github.com/robinsones&#34;&gt;Emily Robinson&lt;/a&gt; and &lt;a href=&#34;https://github.com/jsta&#34;&gt;Joseph Stachelek&lt;/a&gt; for taking the time to put &lt;code&gt;rrricanes&lt;/code&gt; to the test, giving valuable insight and recommendations on improving it.&lt;/p&gt;

&lt;p&gt;And a thank-you also to &lt;a href=&#34;https://github.com/jimmylovestea&#34;&gt;James Molyneux&lt;/a&gt;, &lt;a href=&#34;https://github.com/mpadge&#34;&gt;Mark Padgham&lt;/a&gt;, and &lt;a href=&#34;https://github.com/hrbrmstr&#34;&gt;Bob Rudis&lt;/a&gt;, all of whom have offered guidance or input that has helped make &lt;code&gt;rrricanes&lt;/code&gt; far better than it would have been on my own.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Accessing patent data with the patentsview package</title>
      <link>https://ropensci.org/blog/2017/09/19/patentsview/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/19/patentsview/</guid>
      <description>
        
        

&lt;h3 id=&#34;why-care-about-patents&#34;&gt;Why care about patents?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Patents play a critical role in incentivizing innovation, without
which we wouldn&amp;rsquo;t have much of the technology we rely on everyday&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What does your iPhone, Google&amp;rsquo;s PageRank algorithm, and a butter
substitute called Smart Balance all have in common?&lt;/p&gt;

&lt;!-- These are open source images taken from: https://pixabay.com/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/iphone.png&#34; width=&#34;15%&#34;&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/google.jpg&#34; width=&#34;25%&#34;&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/butter.png&#34; width=&#34;25%&#34;&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;They all probably wouldn&amp;rsquo;t be here if not for patents. A patent
provides its owner with the ability to make money off of something that
they invented, without having to worry about someone else copying their
technology. Think Apple would spend millions of dollars developing the
iPhone if Samsung could just come along and &lt;a href=&#34;http://www.reuters.com/article/us-apple-samsung-elec-appeal-idUSKCN1271LF&#34;&gt;rip it
off&lt;/a&gt;?
Probably not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Patents offer a great opportunity for data analysis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are two primary reasons for this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Patent data is public&lt;/strong&gt;. In return for the exclusive right to
profit off an invention, an individual/company has to publicly
disclose the details of their invention to the rest of the world.
&lt;a href=&#34;http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;amp;Sect2=HITOFF&amp;amp;p=1&amp;amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;amp;r=11&amp;amp;f=G&amp;amp;l=50&amp;amp;co1=AND&amp;amp;d=PTXT&amp;amp;s1=dog&amp;amp;OS=dog&amp;amp;RS=dog&#34;&gt;Examples of those
details&lt;/a&gt;
include the patent&amp;rsquo;s title, abstract, technology classification,
assigned organizations, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patent data can answer questions that people care about&lt;/strong&gt;.
Companies (especially big ones like IBM and Google) have a vested
interest in extracting insights from patents, and spend a lot of
time/resources trying figure out how to best manage their
intellectual property (IP) rights. They&amp;rsquo;re plagued by questions like
&amp;ldquo;who should I sell my underperforming patents to,&amp;rdquo; &amp;ldquo;which technology
areas are open to new innovations,&amp;rdquo; &amp;ldquo;what&amp;rsquo;s going to be the next big
thing in the world of buttery spreads,&amp;rdquo; etc. Patents offer a way to
provide data-driven answers to these questions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Combined, these two things make patents a prime target for data
analysis. However, until recently it was hard to get at the data inside
these documents. One had to either collect it manually using the
official &lt;a href=&#34;https://en.wikipedia.org/wiki/United_States_Patent_and_Trademark_Office&#34;&gt;United States Patent and Trademark
Office&lt;/a&gt;
(USPTO) &lt;a href=&#34;http://patft.uspto.gov/netahtml/PTO/search-adv.htm&#34;&gt;search
engine&lt;/a&gt;, or figure
out a way to download, parse, and model huge XML data dumps. Enter
PatentsView.&lt;/p&gt;

&lt;h3 id=&#34;patentsview-and-the-patentsview-package&#34;&gt;PatentsView and the &lt;code&gt;patentsview&lt;/code&gt; package&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.patentsview.org/web/#viz/relationships&#34;&gt;PatentsView&lt;/a&gt; is one
of USPTO&amp;rsquo;s new initiatives intended to increase the usability and value
of patent data. One feature of this project is a publicly accessible API
that makes it easy to programmatically interact with the data. A few of
the reasons why I like the API (and PatentsView more generally):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The API is free (no credential required) and currently doesn&amp;rsquo;t
impose rate limits/bandwidth throttling.&lt;/li&gt;
&lt;li&gt;The project offers &lt;a href=&#34;http://www.patentsview.org/download/&#34;&gt;bulk downloads of patent
data&lt;/a&gt; on their website (in a
flat file format), for those who want to be closest to the data.&lt;/li&gt;
&lt;li&gt;Both the API and the bulk download data contain disambiguated
entities such as inventors, assignees, organizations, etc. In other
words, the API will tell you whether it thinks that John Smith on
patent X is the same person as John Smith on patent Y. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;patentsview&lt;/code&gt; R package is a wrapper around the PatentsView API. It
contains a function that acts as a client to the API (&lt;code&gt;search_pv()&lt;/code&gt;) as
well as several supporting functions. Full documentation of the package
can be found on its
&lt;a href=&#34;https://ropensci.github.io/patentsview/index.html&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;

&lt;p&gt;You can install the stable version of &lt;code&gt;patentsview&lt;/code&gt; from CRAN:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;patentsview&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version from GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (!require(devtools)) install.packages(&amp;quot;devtools&amp;quot;)

devtools::install_github(&amp;quot;ropensci/patentsview&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;getting-started&#34;&gt;Getting started&lt;/h3&gt;

&lt;p&gt;The package has one main function, &lt;code&gt;search_pv()&lt;/code&gt;, that makes it easy to
send requests to the API. There are two parameters to &lt;code&gt;search_pv()&lt;/code&gt; that
you&amp;rsquo;re going to want to think about just about every time you call it -
&lt;code&gt;query&lt;/code&gt; and &lt;code&gt;fields&lt;/code&gt;. You tell the API how you want to filter the patent
data with &lt;code&gt;query&lt;/code&gt;, and which fields you want to retrieve with
&lt;code&gt;fields&lt;/code&gt;. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h4 id=&#34;query&#34;&gt;&lt;code&gt;query&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Your query has to use the &lt;a href=&#34;http://www.patentsview.org/api/query-language.html&#34;&gt;PatentsView query
language&lt;/a&gt;, which is
a JSON-based syntax that is similar to the one used by Lucene. You can
write the query directly and pass it as a string to &lt;code&gt;search_pv()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(patentsview)

qry_1 &amp;lt;- &#39;{&amp;quot;_gt&amp;quot;:{&amp;quot;patent_year&amp;quot;:2007}}&#39;
search_pv(query = qry_1, fields = NULL) # This will retrieve a default set of fields
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_id    : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_number: chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_title : chr [1:25] &amp;quot;Sealing device for body suit and sealin&amp;quot;..
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;Or you can use the domain specific language (DSL) provided in the
&lt;code&gt;patentsview&lt;/code&gt; package to help you write the query:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;qry_2 &amp;lt;- qry_funs$gt(patent_year = 2007) # All DSL functions are in the qry_funs list
qry_2 # qry_2 is the same as qry_1
#&amp;gt; {&amp;quot;_gt&amp;quot;:{&amp;quot;patent_year&amp;quot;:2007}}

search_pv(query = qry_2)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_id    : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_number: chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_title : chr [1:25] &amp;quot;Sealing device for body suit and sealin&amp;quot;..
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;qry_1&lt;/code&gt; and &lt;code&gt;qry_2&lt;/code&gt; will result in the same HTTP call to the API. Both
queries search for patents in USPTO that were published after 2007.
There are three gotchas to look out for when writing a query:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Field is queryable.&lt;/strong&gt; The API has 7 endpoints (the default
endpoint is &amp;ldquo;patents&amp;rdquo;), and each endpoint has its own set of fields
that you can filter on. &lt;em&gt;The fields that you can filter on are not
necessarily the same as the ones that you can retrieve.&lt;/em&gt; In other
words, the fields that you can include in &lt;code&gt;query&lt;/code&gt; (e.g.,
&lt;code&gt;patent_year&lt;/code&gt;) are not necessarily the same as those that you can
include in &lt;code&gt;fields&lt;/code&gt;. To see which fields you can query on, look in
the &lt;code&gt;fieldsdf&lt;/code&gt; data frame (&lt;code&gt;View(patentsview::fieldsdf)&lt;/code&gt;) for fields
that have a &amp;ldquo;y&amp;rdquo; indicator in their &lt;code&gt;can_query&lt;/code&gt; column for your given
endpoint.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Correct data type for field.&lt;/strong&gt; If you&amp;rsquo;re filtering on a field in
your query, you have to make sure that the value you are filtering
on is consistent with the field&amp;rsquo;s data type. For example,
&lt;code&gt;patent_year&lt;/code&gt; has type &amp;ldquo;integer,&amp;rdquo; so if you pass 2007 as a string
then you&amp;rsquo;re going to get an error (&lt;code&gt;patent_year = 2007&lt;/code&gt; is good,
&lt;code&gt;patent_year = &amp;quot;2007&amp;quot;&lt;/code&gt; is no good). You can find a field&amp;rsquo;s data type
in the &lt;code&gt;fieldsdf&lt;/code&gt; data frame.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comparison function works with field&amp;rsquo;s data type.&lt;/strong&gt; The comparison
function(s) that you use (e.g., the greater-than function shown
above, &lt;code&gt;qry_funs$gt()&lt;/code&gt;) must be consistent with the field&amp;rsquo;s data
type. For example, you can&amp;rsquo;t use the &amp;ldquo;contains&amp;rdquo; function on fields
of type &amp;ldquo;integer&amp;rdquo; (&lt;code&gt;qry_funs$contains(patent_year = 2007)&lt;/code&gt; will
throw an error). See &lt;code&gt;?qry_funs&lt;/code&gt; for more details.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In short, use the &lt;code&gt;fieldsdf&lt;/code&gt; data frame when you write a query and you
should be fine. Check out the &lt;a href=&#34;https://ropensci.github.io/patentsview/articles/writing-queries.html&#34;&gt;writing queries
vignette&lt;/a&gt;
for more details.&lt;/p&gt;

&lt;h4 id=&#34;fields&#34;&gt;&lt;code&gt;fields&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Up until now we have been using the default value for &lt;code&gt;fields&lt;/code&gt;. This
results in the API giving us some small set of default fields. Let&amp;rsquo;s see
about retrieving some more fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;search_pv(
  query = qry_funs$gt(patent_year = 2007),
  fields = c(&amp;quot;patent_abstract&amp;quot;, &amp;quot;patent_average_processing_time&amp;quot;,
             &amp;quot;inventor_first_name&amp;quot;, &amp;quot;inventor_total_num_patents&amp;quot;)
)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame (with list column(s) inside) on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_abstract               : chr [1:25] &amp;quot;A sealing device for a&amp;quot;..
#&amp;gt;   ..$ patent_average_processing_time: chr [1:25] &amp;quot;1324&amp;quot; ...
#&amp;gt;   ..$ inventors                     :List of 25
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The fields that you can retrieve depends on the endpoint that you are
hitting. We&amp;rsquo;ve been using the &amp;ldquo;patents&amp;rdquo; endpoint thus far, so all of
these are retrievable:
&lt;code&gt;fieldsdf[fieldsdf$endpoint == &amp;quot;patents&amp;quot;, &amp;quot;field&amp;quot;]&lt;/code&gt;. You can also use
&lt;code&gt;get_fields()&lt;/code&gt; to list the retrievable fields for a given endpoint:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;search_pv(
  query = qry_funs$gt(patent_year = 2007),
  fields = get_fields(endpoint = &amp;quot;patents&amp;quot;, groups = c(&amp;quot;patents&amp;quot;, &amp;quot;inventors&amp;quot;))
)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame (with list column(s) inside) on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  31 variables:
#&amp;gt;   ..$ patent_abstract                       : chr [1:25] &amp;quot;A sealing devi&amp;quot;..
#&amp;gt;   ..$ patent_average_processing_time        : chr [1:25] &amp;quot;1324&amp;quot; ...
#&amp;gt;   ..$ patent_date                           : chr [1:25] &amp;quot;2008-01-01&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_city       : chr [1:25] &amp;quot;Cambridge&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_country    : chr [1:25] &amp;quot;US&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_id         : chr [1:25] &amp;quot;b9fc6599e3d60c&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_assignee_latitude   : chr [1:25] &amp;quot;42.3736&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_location_id: chr [1:25] &amp;quot;42.3736158|-71&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_assignee_longitude  : chr [1:25] &amp;quot;-71.1097&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_state      : chr [1:25] &amp;quot;MA&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_city       : chr [1:25] &amp;quot;Lucca&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_country    : chr [1:25] &amp;quot;IT&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_id         : chr [1:25] &amp;quot;6416028-3&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_latitude   : chr [1:25] &amp;quot;43.8376&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_location_id: chr [1:25] &amp;quot;43.8376211|10.&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_inventor_longitude  : chr [1:25] &amp;quot;10.4951&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_state      : chr [1:25] &amp;quot;Tuscany&amp;quot; ...
#&amp;gt;   ..$ patent_id                             : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_kind                           : chr [1:25] &amp;quot;B1&amp;quot; ...
#&amp;gt;   ..$ patent_number                         : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_num_cited_by_us_patents        : chr [1:25] &amp;quot;5&amp;quot; ...
#&amp;gt;   ..$ patent_num_claims                     : chr [1:25] &amp;quot;25&amp;quot; ...
#&amp;gt;   ..$ patent_num_combined_citations         : chr [1:25] &amp;quot;35&amp;quot; ...
#&amp;gt;   ..$ patent_num_foreign_citations          : chr [1:25] &amp;quot;0&amp;quot; ...
#&amp;gt;   ..$ patent_num_us_application_citations   : chr [1:25] &amp;quot;0&amp;quot; ...
#&amp;gt;   ..$ patent_num_us_patent_citations        : chr [1:25] &amp;quot;35&amp;quot; ...
#&amp;gt;   ..$ patent_processing_time                : chr [1:25] &amp;quot;792&amp;quot; ...
#&amp;gt;   ..$ patent_title                          : chr [1:25] &amp;quot;Sealing device&amp;quot;..
#&amp;gt;   ..$ patent_type                           : chr [1:25] &amp;quot;utility&amp;quot; ...
#&amp;gt;   ..$ patent_year                           : chr [1:25] &amp;quot;2008&amp;quot; ...
#&amp;gt;   ..$ inventors                             :List of 25
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s look at a quick example of pulling and analyzing patent data.
We&amp;rsquo;ll look at patents from the last ten years that are classified below
the &lt;a href=&#34;https://worldwide.espacenet.com/classification#!/CPC=H04L63/02&#34;&gt;H04L63/00 CPC
code&lt;/a&gt;.
Patents in this area relate to &amp;ldquo;network architectures or network
communication protocols for separating internal from external
traffic.&amp;rdquo; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; CPC codes offer a quick and dirty way to find patents of
interest, though getting a sense of their hierarchy can be tricky.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download the data&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(patentsview)

# Write a query:
query &amp;lt;- with_qfuns( # with_qfuns is basically just: with(qry_funs, ...)
  and(
    begins(cpc_subgroup_id = &#39;H04L63/02&#39;),
    gte(patent_year = 2007)
  )
)

# Create a list of fields:
fields &amp;lt;- c(
  c(&amp;quot;patent_number&amp;quot;, &amp;quot;patent_year&amp;quot;),
  get_fields(endpoint = &amp;quot;patents&amp;quot;, groups = c(&amp;quot;assignees&amp;quot;, &amp;quot;cpcs&amp;quot;))
)

# Send HTTP request to API&#39;s server:
pv_res &amp;lt;- search_pv(query = query, fields = fields, all_pages = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;See where the patents are coming from (geographically)&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(leaflet)
library(htmltools)
library(dplyr)
library(tidyr)

data &amp;lt;-
  pv_res$data$patents %&amp;gt;%
    unnest(assignees) %&amp;gt;%
    select(assignee_id, assignee_organization, patent_number,
           assignee_longitude, assignee_latitude) %&amp;gt;%
    group_by_at(vars(-matches(&amp;quot;pat&amp;quot;))) %&amp;gt;%
    mutate(num_pats = n()) %&amp;gt;%
    ungroup() %&amp;gt;%
    select(-patent_number) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(popup = paste0(&amp;quot;&amp;lt;font color=&#39;Black&#39;&amp;gt;&amp;quot;,
                          htmlEscape(assignee_organization), &amp;quot;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Patents:&amp;quot;,
                          num_pats, &amp;quot;&amp;lt;/font&amp;gt;&amp;quot;)) %&amp;gt;%
    mutate_at(vars(matches(&amp;quot;_l&amp;quot;)), as.numeric) %&amp;gt;%
    filter(!is.na(assignee_id))

leaflet(data) %&amp;gt;%
  addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %&amp;gt;%
  addCircleMarkers(lng = ~assignee_longitude, lat = ~assignee_latitude,
                   popup = ~popup, ~sqrt(num_pats), color = &amp;quot;yellow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Plot the growth of the field&amp;rsquo;s topics over time&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(ggplot2)
library(RColorBrewer)

data &amp;lt;-
  pv_res$data$patents %&amp;gt;%
    unnest(cpcs) %&amp;gt;%
    filter(cpc_subgroup_id != &amp;quot;H04L63/02&amp;quot;) %&amp;gt;% # remove patents categorized into only top-level category of H04L63/02
    mutate(
      title = case_when(
        grepl(&amp;quot;filtering&amp;quot;, .$cpc_subgroup_title, ignore.case = T) ~
          &amp;quot;Filtering policies&amp;quot;,
        .$cpc_subgroup_id %in% c(&amp;quot;H04L63/0209&amp;quot;, &amp;quot;H04L63/0218&amp;quot;) ~
          &amp;quot;Architectural arrangements&amp;quot;,
        grepl(&amp;quot;Firewall traversal&amp;quot;, .$cpc_subgroup_title, ignore.case = T) ~
          &amp;quot;Firewall traversal&amp;quot;,
        TRUE ~
          .$cpc_subgroup_title
      )
    ) %&amp;gt;%
    mutate(title = gsub(&amp;quot;.*(?=-)-&amp;quot;, &amp;quot;&amp;quot;, title, perl = TRUE)) %&amp;gt;%
    group_by(title, patent_year) %&amp;gt;%
    count() %&amp;gt;%
    ungroup() %&amp;gt;%
    mutate(patent_year = as.numeric(patent_year))

ggplot(data = data) +
  geom_smooth(aes(x = patent_year, y = n, colour = title), se = FALSE) +
  scale_x_continuous(&amp;quot;\nPublication year&amp;quot;, limits = c(2007, 2016),
                     breaks = 2007:2016) +
  scale_y_continuous(&amp;quot;Patents\n&amp;quot;, limits = c(0, 700)) +
  scale_colour_manual(&amp;quot;&amp;quot;, values = brewer.pal(5, &amp;quot;Set2&amp;quot;)) +
  theme_bw() + # theme inspired by https://hrbrmstr.github.io/hrbrthemes/
  theme(panel.border = element_blank(), axis.ticks = element_blank())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;learning-more&#34;&gt;Learning more&lt;/h3&gt;

&lt;p&gt;For analysis examples that go into a little more depth, check out the
&lt;a href=&#34;https://ropensci.github.io/patentsview/articles/citation-networks.html&#34;&gt;data applications
vignettes&lt;/a&gt;
on the package&amp;rsquo;s website. If you&amp;rsquo;re just interested in &lt;code&gt;search_pv()&lt;/code&gt;,
there are
&lt;a href=&#34;https://ropensci.github.io/patentsview/articles/examples.html&#34;&gt;examples&lt;/a&gt;
on the site for that as well. To contribute to the package or report an
issue, check out the &lt;a href=&#34;https://github.com/ropensci/patentsview/issues&#34;&gt;issues page on
GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;d like to thank the package&amp;rsquo;s two reviewers, &lt;a href=&#34;https://github.com/poldham&#34;&gt;Paul
Oldham&lt;/a&gt; and &lt;a href=&#34;http://blog.haunschmid.name/&#34;&gt;Verena
Haunschmid&lt;/a&gt;, for taking the time to review
the package and providing helpful feedback. I&amp;rsquo;d also like to thank
&lt;a href=&#34;http://www.masalmon.eu/&#34;&gt;Maëlle Salmon&lt;/a&gt; for shepherding the package
along the rOpenSci review process, as well &lt;a href=&#34;https://scottchamberlain.info/&#34;&gt;Scott
Chamberlain&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/stefaniebutland&#34;&gt;Stefanie
Butland&lt;/a&gt; for their miscellaneous
help.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;This is both good and bad, as there are errors in the disambiguation. The algorithm that is responsible for the disambiguation was created by the winner of the &lt;a href=&#34;http://www.patentsview.org/workshop/&#34;&gt;PatentsView Inventor Disambiguation Technical Workshop&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;These two parameters end up getting translated into a MySQL query by the API&amp;rsquo;s server, which then gets sent to a back-end database. &lt;code&gt;query&lt;/code&gt; and &lt;code&gt;fields&lt;/code&gt; are used to create the query&amp;rsquo;s &lt;code&gt;WHERE&lt;/code&gt; and &lt;code&gt;SELECT&lt;/code&gt; clauses, respectively.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;There is a slightly more in-depth definition that says that these are patents &amp;ldquo;related to the (logical) separation of traffic/(sub-) networks to achieve protection.&amp;rdquo;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>rOpenSci Software Review: Always Improving</title>
      <link>https://ropensci.org/blog/2017/09/11/software-review-update/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/11/software-review-update/</guid>
      <description>
        
        

&lt;p&gt;The R package ecosystem now contains more than 10K packages, and several flagship packages belong under the rOpenSci suite. Some of these are: &lt;a href=&#34;https://github.com/ropensci/magick&#34;&gt;magick&lt;/a&gt; for image manipulation, &lt;a href=&#34;https://github.com/ropensci/plotly&#34;&gt;plotly&lt;/a&gt; for interactive plots, and &lt;a href=&#34;https://github.com/ropensci/git2r&#34;&gt;git2r&lt;/a&gt; for interacting with &lt;code&gt;git&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;rOpenSci is a community of people making software to facilitate open and reproducible science/research. While the rOpenSci team continues to develop and maintain core infrastructure packages, an increasing number of packages in our suite are contributed by members of the extended R community.&lt;/p&gt;

&lt;p&gt;In the early days we accepted contributions to our suite without any formal process for submission or acceptance. When someone wanted to contribute software to our collection, and we could envision scientific applications, we just moved it aboard. But as our community and codebase grew, we began formalizing standards and processes to control quality. This is what became our peer review process.  You can read more about it in our recent &lt;a href=&#34;https://www.numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As our submissions have grown over the past couple of years, our standards around peer review have also changed and continue to evolve in response to changing community needs and updates to the R development infrastructure.&lt;/p&gt;

&lt;p&gt;Although a large number of packages submitted to CRAN could also be part of rOpenSci, our submissions are limited to packages that fit our mission and are able to pass a stringent and time intensive review process.&lt;/p&gt;

&lt;p&gt;Here, we summarize some of the more important changes to peer review at rOpenSci over the past year.  The most recent information can always be found at &lt;a href=&#34;https://onboarding.ropensci.org/&#34;&gt;https://onboarding.ropensci.org/&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;we-ve-expanded-our-scope&#34;&gt;We&amp;rsquo;ve Expanded Our Scope&lt;/h3&gt;

&lt;p&gt;Our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;Aims and Scope&lt;/a&gt; document what types of packages we accept from community contributors. The scope emerges from three main guidelines. First, we accept packages that fit our mission of enabling open and reproducible research. Second, we only accept packages that we feel our editors and community of reviewers are competent to review. Third, we accept packages for which we can reasonably endorse as improving on existing solutions.  In practice, we don&amp;rsquo;t accept  general packages. That&amp;rsquo;s why, for instance, our &amp;ldquo;data munging&amp;rdquo; category only applies to packages designed to work with specific scientific data types.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve refined our focal areas from&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data retrieval&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data visualization&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data deposition&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data munging&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reproducibility&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data retrieval&lt;/strong&gt; - packages for accessing and downloading data from online sources with scientific applications&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data deposition&lt;/strong&gt; - packages that support deposition of data into research repositories, including data formatting and metadata generation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data munging&lt;/strong&gt; - packages for processing data from formats mentioned above&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data extraction&lt;/strong&gt; - packages that aid in retrieving data from unstructured sources such as text, images and PDFs, as well as parsing scientific data types and outputs from scientific equipment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;database access&lt;/strong&gt; - bindings and wrappers for generic database APIs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reproducibility&lt;/strong&gt; - tools that facilitate reproducible research&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;geospatial data&lt;/strong&gt; - accessing geospatial data, manipulating geospatial data, and converting between geospatial data formats&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;text analysis&lt;/strong&gt; (pilot) - we are piloting a sub-specialty area for text analysis which includes implementation of statistical/ML methods for analyzing or extracting text data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You will note that we&amp;rsquo;ve removed data visualization. We&amp;rsquo;ve had some truly excellent data visualization packages come aboard, starting with &lt;a href=&#34;https://github.com/ropensci/plotly&#34;&gt;plotly&lt;/a&gt;.  But since then we&amp;rsquo;ve found data visualization is too general a field for us to confidently evaluate, and at this point have dropped it from our main categories.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve also added geospatial and text analysis as areas where we accept packages that might seem more general or methods-y than we would otherwise.  These are areas where we&amp;rsquo;ve built, among our staff and reviewers, topic-specific expertise.&lt;/p&gt;

&lt;p&gt;Given that we accept packages that improve on existing solutions, in practice we generally avoid accepting more than one package of similar scope. We&amp;rsquo;ve also added &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#package-overlap&#34;&gt;clarifying language&lt;/a&gt; about what this entails and how we define overlap with other packages.&lt;/p&gt;

&lt;p&gt;We now strongly encourage &lt;a href=&#34;https://github.com/ropensci/onboarding/issues?q=is%3Aissue+label%3A0%2Fpresubmission&#34;&gt;pre-submission inquiries&lt;/a&gt; to quickly assess whether the package falls into scope. Some of these lead to suggesting the person submit their package, while others are determined out-of-scope. This reduces effort on all sides for packages that be out-of-scope. Many authors do this prior to completing their package so they can decide whether to tailor their development process to rOpenSci.&lt;/p&gt;

&lt;p&gt;To see examples of what has recently been determined to be out-of-scope, see the &lt;a href=&#34;https://github.com/ropensci/onboarding/issues?q=is%3Aissue+is%3Aclosed+label%3Aout-of-scope&#34;&gt;out-of-scope label&lt;/a&gt; in the onboarding repository.&lt;/p&gt;

&lt;p&gt;As always, we&amp;rsquo;d like to emphasize that even when packages are out-of-scope, we&amp;rsquo;re very grateful that authors consider an rOpenSci submission!&lt;/p&gt;

&lt;h3 id=&#34;standards-changes&#34;&gt;Standards changes&lt;/h3&gt;

&lt;p&gt;Our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/packaging_guide.md&#34;&gt;packaging guide&lt;/a&gt; contains both recommended and required best practices. They evolve continually as our community reaches consensus on best practices that we want to encourage and standardize. Here are some of the changes we&amp;rsquo;ve incorporated in the past year.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We now encourage using a &lt;code&gt;object_verb()&lt;/code&gt; function naming scheme to avoid namespace conflicts.&lt;/li&gt;
&lt;li&gt;We now encourage functions to facilitate piping workflows if possible. We don&amp;rsquo;t have an official stance on using pipes in a package.&lt;/li&gt;
&lt;li&gt;We&amp;rsquo;ve clarified and filled out README recommendations&lt;/li&gt;
&lt;li&gt;Documentation: we now recommend inclusion of a package level manual file, and at least one vignette.&lt;/li&gt;
&lt;li&gt;Testing: we clarify that packages should pass &lt;code&gt;devtools::check()&lt;/code&gt; on all major platforms, that each package should have a test suite that covers all major functionality.&lt;/li&gt;
&lt;li&gt;Continuous integration (CI): we now require that packages with compiled code need to run continuous integration on all major platforms; integrate reporting of test coverage; include README badges of CI and coverage.&lt;/li&gt;
&lt;li&gt;We&amp;rsquo;ve clarified our recommended scaffolding suggestions around XML to be more nuanced. Briefly, we recommend the &lt;code&gt;xml2&lt;/code&gt; package in general, but &lt;code&gt;XML&lt;/code&gt; package may be needed in some cases.&lt;/li&gt;
&lt;li&gt;We added a section on CRAN gotchas to help package maintainers avoid common pitfalls encountered during CRAN submission.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Standards changes often take place because we find that both editors and reviewers are making the same recommendations on multiple packages.  Other requirements are added as good practices become accessible to the broader community. For instance, CI and code coverage reporting have gone from recommended to required as the tooling and documentation/tutorials for these have made them more accessible.&lt;/p&gt;

&lt;h3 id=&#34;process-changes&#34;&gt;Process changes&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Editors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the pace of package submissions increases, we&amp;rsquo;ve expanded our editorial team to keep up. &lt;a href=&#34;http://www.masalmon.eu/&#34;&gt;Maëlle Salmon&lt;/a&gt; joined us in February, bringing our &lt;a href=&#34;https://github.com/ropensci/onboarding#-editors-and-reviewers&#34;&gt;team to four&lt;/a&gt;. With four, we need to be more coordinated, so we&amp;rsquo;ve moved to a system of a rotating editor-in-chief, who makes decisions about scope, assigns handling editors, and brings up edge cases for discussion with the whole team.&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Welcome &lt;a href=&#34;https://twitter.com/ma_salmon&#34;&gt;@ma_salmon&lt;/a&gt; to our editorial team for open peer review of &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&#34;&gt;#rstats&lt;/a&gt; software &lt;a href=&#34;https://t.co/KsL0SF1b6K&#34;&gt;https://t.co/KsL0SF1b6K&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;&lt;/p&gt;&amp;mdash; rOpenSci (@rOpenSci) &lt;a href=&#34;https://twitter.com/rOpenSci/status/832228045587099649&#34;&gt;February 16, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The process our editors follow is summarized in our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/editors_guide.md&#34;&gt;editors&amp;rsquo; guide&lt;/a&gt;, which will help bring editors up to speed when we further expand our team.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As submissions increase, the entire process benefits more from automation. Right now most steps of the review system are manual - we aim to automate as much as possible. Here&amp;rsquo;s a few things we&amp;rsquo;re doing or planning on:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;With every package submission, we run &lt;a href=&#34;https://github.com/MangoTheCat/goodpractice&#34;&gt;goodpractice&lt;/a&gt; on the package to highlight common issues. We do this manually right now, but we&amp;rsquo;re working on an automated system (aka, bot) for automatically running &lt;code&gt;goodpractice&lt;/code&gt; on submissions and reporting back to the issue. Other rOpenSci specific checks, e.g., checking rOpenSci policies, are likely to be added in to this system.&lt;/li&gt;
&lt;li&gt;Reminders: Some readers that have reviewed for rOpenSci may remember the bot that would remind you to get your review in. We&amp;rsquo;ve disabled it for now - but will likely bring it back online soon. Right now, editors do these reminders manually.&lt;/li&gt;
&lt;li&gt;On approval, packages go through a number of housekeeping steps to ensure a smooth transfer. Eventually we&amp;rsquo;d like to automate this process.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Other changes&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://joss.theoj.org/&#34;&gt;JOSS&lt;/a&gt; harmonization/co-submission: For authors wishing to submit their software papers to the Journal of Open Source Software after acceptance, we have also begun streamlining the process. Editors check to make sure that the paper clearly states the scientific application, includes a separate &lt;code&gt;.bib&lt;/code&gt; file and that the accepted version of the software is deposited at Zenodo or Figshare with a DOI. Having these steps completed allows for a fast track acceptance at JOSS.&lt;/li&gt;
&lt;li&gt;Reviewer template and guide: We now have a &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/reviewer_template.md&#34;&gt;reviewer template&lt;/a&gt; - making reviews more standardized, and helping reviewers know what to look for. In addition, we have an updated reviewer guide that gives high level guidance, as well as specific things to look for, tools to use, and examples of good reviews. In addition, the guide gives guidance on how to submit reviews.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Badges: We now have badges for rOpenSci review. The badges show whether a package is under review or has been approved. Packages that are undergoing review or have been approved can put this badge in their README.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ropensci/onboarding/issues/86&#34;&gt;&lt;img src=&#34;http://badges.ropensci.org/86_status.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/ropensci/onboarding/issues/116&#34;&gt;&lt;img src=&#34;http://badges.ropensci.org/116_status.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Get in touch with us on Twitter (&lt;a href=&#34;https://twitter.com/ropensci&#34;&gt;@ropensci&lt;/a&gt;, or in the comments) if you have any questions or thoughts about our software review policies, scope, or processes.&lt;/p&gt;

&lt;p&gt;To find out more about our software review process join us on the next &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/31/comm-call-v14&#34;&gt;rOpenSci Community Call&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We hope to see you soon in the onboarding repository as a &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/new&#34;&gt;submitter&lt;/a&gt; or as a &lt;a href=&#34;https://ropensci.org/onboarding/&#34;&gt;reviewer&lt;/a&gt;!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Experiences as a first time rOpenSci package reviewer</title>
      <link>https://ropensci.org/blog/2017/09/08/first-review-experiences/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/08/first-review-experiences/</guid>
      <description>
        
        

&lt;p&gt;It all started January 26&lt;sup&gt;th&lt;/sup&gt; this year when I signed up to volunteer as
a reviewer for R packages submitted to rOpenSci. My main motivation for
wanting to volunteer was to learn something new and to
contribute to the R open source community. If you are wondering why the
people behind rOpenSci are doing this, you can read &lt;a href=&#34;https://www.numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science/&#34;&gt;How rOpenSci uses Code Review to Promote Reproducible Science&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Three months later I was contacted by &lt;a href=&#34;https://twitter.com/masalmon&#34;&gt;Maelle Salmon&lt;/a&gt; asking whether I was interested in
reviewing the R package &lt;a href=&#34;https://github.com/ropensci/patentsview&#34;&gt;&lt;code&gt;patentsview&lt;/code&gt;&lt;/a&gt; for rOpenSci. And yes, I
was! To be honest I was a little bit thrilled.&lt;/p&gt;

&lt;p&gt;The packages are submitted for review to rOpenSci via an issue to their
GitHub repository and also the reviews happen there. So you can check out
&lt;a href=&#34;https://github.com/ropensci/onboarding/issues&#34;&gt;all previous package submissions and reviews&lt;/a&gt;.
With all the information you
get from rOpenSci and also the help from the editor it is straightforward
to do the package review. Before I started I read the
reviewer guides (links below) and checked out a few of the existing
reviews. I installed the package &lt;code&gt;patentsview&lt;/code&gt; from GitHub and also
downloaded the source code so I could check out how it was implemented.&lt;/p&gt;

&lt;p&gt;I started by testing core functionality of the package by
running the examples that were mentioned in the README of the
package. I think this is a good
starting point because you get a feeling of what the author wants to
achieve with the package. Later on I came up with my
own queries (side note: this R package interacts with an API from which
you can query patents). During the process I used to switch between
writing queries like a normal user of the package
would do and checking the code. When I saw something in the code that
wasn&amp;rsquo;t quite clear to me or looked wrong I went back to writing new
queries to check whether the behavior of the methods was as expected.&lt;/p&gt;

&lt;p&gt;With this approach I was able to give feedback to the package author
which led to the inclusion of an additional unit test, a helper function
that makes the package easier to use, clarification of an error message
and an improved documentation. You can find the review I did &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/112&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are several R packages that helped me get started with my review,
e.g. &lt;a href=&#34;https://github.com/hadley/devtools&#34;&gt;&lt;code&gt;devtools&lt;/code&gt;&lt;/a&gt; and
&lt;a href=&#34;https://github.com/MangoTheCat/goodpractice&#34;&gt;&lt;code&gt;goodpractice&lt;/code&gt;&lt;/a&gt;. These
packages can also help you when you start writing your own packages. An
example for a very useful method is &lt;code&gt;devtools::spell_check()&lt;/code&gt;, which
performs a spell check on the package description and on manual pages.
At the beginning I had an issue with &lt;code&gt;goodpractice::gp()&lt;/code&gt; but Maelle Salmon
(the editor) helped me resolve it.&lt;/p&gt;

&lt;p&gt;In the rest of this article you can read what I gained personally from doing a
review.&lt;/p&gt;

&lt;h3 id=&#34;contributing-to-the-open-source-community&#34;&gt;Contributing to the open source community&lt;/h3&gt;

&lt;p&gt;When people think about contributing to the open source community, the
first thought is about creating a new R package or contributing to one
of the major packages out there. But not everyone has the resources
(e.g. time) to do so. You also don&amp;rsquo;t have awesome ideas every other day
which can immediately be implemented into new R packages to be used by
the community. Besides contributing with code there are also lots of
other things than can be useful for other R users, for example writing
blog posts about problems you solved, speaking at meetups or reviewing
code to help improve it. What I like much about reviewing code is that
people see things differently and have other experiences. As a reviewer,
you see a new package from the user&amp;rsquo;s perspective which can be hard for
the programmer themselves. Having someone else
review your code helps finding things that are missing because they seem
obvious to the package author or detect code pieces that require more
testing. I had a great feeling when I finished the review, since I had
helped improve an already amazing R package a little bit more.&lt;/p&gt;

&lt;h3 id=&#34;reviewing-helps-improve-your-own-coding-style&#34;&gt;Reviewing helps improve your own coding style&lt;/h3&gt;

&lt;p&gt;When I write R code I usually try to do it in the best way possible.
&lt;a href=&#34;https://google.github.io/styleguide/Rguide.xml&#34;&gt;Google&amp;rsquo;s R Style Guide&lt;/a&gt;
is a good start to get used to coding best practice in R and I also
enjoyed reading &lt;a href=&#34;https://github.com/timoxley/best-practices&#34;&gt;Programming Best Practices
Tidbits&lt;/a&gt;. So normally
when I think some piece of code can be improved (with respect to speed,
readability or memory usage) I check online whether I can find a
better solution. Often you just don&amp;rsquo;t think something can be
improved because you always did it in a certain way or the last time you
checked there was no better solution. This is when it helps to follow
other people&amp;rsquo;s code. I do this by reading their blogs, following many R
users on Twitter and checking their GitHub account. Reviewing an R
package also helped me a great deal with getting new ideas because I
checked each function a lot more carefully than when I read blog posts.
In my opinion, good code does not only use the best package for each
problem but also the small details are well implemented. One thing I
used to do wrong for a long time was filling of data.frames until I
found a better (much faster)
&lt;a href=&#34;https://stackoverflow.com/a/29419402&#34;&gt;solution on stackoverflow&lt;/a&gt;.
And with respect to this you
can learn a lot from someone else&amp;rsquo;s code. What I found really cool in
the package I reviewed was the usage of small helper functions (see
&lt;a href=&#34;https://github.com/ropensci/patentsview/blob/c03e1ab2537873d7a9b76025b0072953efb475c1/R/utils.R&#34;&gt;utils.R&lt;/a&gt;).
Functions like &lt;code&gt;paste0_stop&lt;/code&gt; and &lt;code&gt;paste0_message&lt;/code&gt; make the rest of the
code a lot easier to read.&lt;/p&gt;

&lt;h3 id=&#34;good-start-for-writing-your-own-packages&#34;&gt;Good start for writing your own packages&lt;/h3&gt;

&lt;p&gt;When reviewing an R package, you check the code like a really observant
user. I noticed many things that you usually don&amp;rsquo;t care about when using
an R package, like comments, how helpful the documentation and the
examples are, and also how well unit tests cover the code. I think that
reviewing a few good packages can prepare you very well for writing your
own packages.&lt;/p&gt;

&lt;h3 id=&#34;do-you-want-to-contribute-to-ropensci-yourself&#34;&gt;Do you want to contribute to rOpenSci yourself?&lt;/h3&gt;

&lt;p&gt;If I motivated you to become an rOpenSci reviewer, please sign up! Here
is a list of useful things if you want to become an rOpenSci reviewer
like me.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ropensci.org/onboarding/&#34;&gt;Form to sign up (just takes a minute)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://onboarding.ropensci.org/&#34;&gt;Information for reviewers&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://mozillascience.github.io/codeReview/review.html&#34;&gt;Mozilla reviewing guide&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;While writing this blog post I found a nice article about &lt;a href=&#34;http://www.tidyverse.org/articles/2017/08/contributing/&#34;&gt;contributing
to the tidyverse&lt;/a&gt; which is
mostly also applicable to other R packages in my opinion.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are generally interested in either submitting or reviewing an R package, I would like to invite you to the &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/31/comm-call-v14&#34;&gt; Community Call on rOpenSci software review and onboarding&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>How rOpenSci uses Code Review to Promote Reproducible Science</title>
      <link>https://ropensci.org/blog/2017/09/01/nf-softwarereview/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/01/nf-softwarereview/</guid>
      <description>
        
        

&lt;p&gt;At rOpenSci, we create and curate software to help scientists with the data life cycle. These tools access, download, manage, and archive scientific data in open, reproducible ways. Early on, we realized this could only be a community effort. The variety of scientific data and workflows could only be tackled by drawing on contributions of scientists with field-specific expertise.&lt;/p&gt;

&lt;p&gt;With the community approach came challenges. &lt;strong&gt;How could we ensure the quality of code written by scientists without formal training in software development practices? How could we drive adoption of best practices among our contributors? How could we create a community that would support each other in this work?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We have had great success addressing these challenges via the &lt;em&gt;peer review&lt;/em&gt;.&lt;/strong&gt; We draw elements from a process familiar to our target community – &lt;em&gt;academic peer review&lt;/em&gt; – and a practice from the software development world – &lt;em&gt;production code review&lt;/em&gt; – to create a system that fosters software quality, ongoing education, and community development.&lt;/p&gt;

&lt;h3 id=&#34;an-open-review-process&#34;&gt;An Open Review Process&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Production software review&lt;/strong&gt; occurs within software development teams, open source or not. Contributions to a software project are reviewed by one or more other team members before incorporation into project source code. Contributions are typically small patches, and review serves as a check on quality, as well as an opportunity for training in team standards.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In academic peer review&lt;/strong&gt;, external reviewers critique a complete product – usually a manuscript – with a very broad mandate to address any areas they see as deficient. Academic review is often anonymous and passing through it gives the product, and the author, a public mark of validation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We blend these approaches.&lt;/strong&gt; In our process, authors submit complete R packages to rOpenSci. Editors check that packages fit into our project’s scope, run a series of automated tests to ensure a baseline of code quality and completeness, and then assign two independent reviewers. Reviewers comment on usability, quality, and style of software code as well as documentation. Authors make changes in response, and once reviewers are satisfied with the updates, the package receives a badge of approval and joins our suite.&lt;/p&gt;

&lt;p&gt;This process is quite iterative. After reviewers post a first round of extensive reviews, authors and reviewers chat in an informal back-and-forth, only lightly moderated by an editor. This lets both reviewers and authors pose questions of each other and explain differences of opinion. It can proceed at a much faster pace than typical academic review correspondence. We use the GitHub issues system for this conversation, and responses take minutes to days, rather than weeks to months.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The exchange is also open and public&lt;/strong&gt;. Authors, reviewers, and editors all know each other’s identities. The broader community can view or even participate in the conversation as it happens. This provides an incentive to be thorough and provide non-adversarial, constructive reviews. Both authors and reviewers report that they enjoy and learn more from this open and direct exchange. It also has the benefit of building community. Participants have the opportunity to meaningfully network with new peers, and new collaborations have emerged via ideas spawned during the review process.&lt;/p&gt;

&lt;p&gt;We are aware that open systems can have drawbacks. For instance, in traditional academic review, double-blind peer review &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0169534707002704&#34;&gt;can increase representation of female authors&lt;/a&gt;, suggesting bias in non-blind reviews. It is also possible reviewers are less critical in open review. However, we posit that the openness of the review conversation provides a check on review quality and bias; it’s harder to inject unsupported or subjective comments in public and without the cover of anonymity. Ultimately, we believe the ability of authors and reviewers to have direct but public communication improves quality and fairness.&lt;/p&gt;

&lt;h3 id=&#34;guidance-and-standards&#34;&gt;Guidance and Standards&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;rOpenSci provides guidance on reviewing.&lt;/strong&gt; This falls into two main categories: &lt;strong&gt;high-level best practices&lt;/strong&gt; and &lt;strong&gt;low-level standards&lt;/strong&gt;. High-level best practices are general and broadly applicable across languages and applications. These are practices such as “Write re-usable functions rather than repeating the same code,” “Test edge cases,” or “Write documentation for all of your functions.” Because of their general nature, these can be drawn from other sources and not developed from scratch. Our best practices are based on guidance originally developed by &lt;a href=&#34;https://mozillascience.github.io/codeReview/intro.html&#34;&gt;Mozilla Science Lab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Low-level standards are specific to a language (in our case, R), applications (data interfaces) and user base (researchers). These include specific items such as naming conventions for functions, best choices of dependencies for certain tasks, and adherence to a code style guide. We have an extensive set of standards for our reviewers to check. These change over time as the R software ecosystem evolves, best practices change, and tooling and educational resources make new methods available to developers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Our standards also change based on feedback from reviewers.&lt;/strong&gt; We adopt into our standards suggestions that emerge in multiple reviewers across different packages. Many of these, we’ve found, have to do with with the ease-of-use and consistency of software APIs, and the type and location of information in documentation that make it easiest to find. This highlights one of the advantages of external reviewers – they can provide a fresh perspective on usability, as well as test software under different use-cases than imagined by the author.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As our standards have become more extensive, we have come to rely more on automated tools.&lt;/strong&gt; The R ecosystem, like most languages, has a suite of tools for code linting, function testing, static code analysis and continuous integration. We require authors to use these, and editors run submissions through a suite of tests prior to sending them for review. This frees reviewers from the burden of low-level tasks to focus on high-level critiques where they can add the most value.&lt;/p&gt;

&lt;h3 id=&#34;the-reviewer-community&#34;&gt;The Reviewer Community&lt;/h3&gt;

&lt;p&gt;One of the core challenges and rewards of our work has been developing a community of reviewers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reviewing is a high-skill activity.&lt;/strong&gt; Reviewers need expertise in the programming methods used in a software package and also the scientific field of its application. (“Find me someone who knows sensory ecology and sparse data structures!”) They need good communications skills and the time and willingness to volunteer. Thankfully, the open-science and open-source worlds are filled with generous, expert people. We have been able to expand our reviewer pool as the pace of submissions and the domains of their applications have grown.&lt;/p&gt;

&lt;p&gt;Developing the reviewer pool requires constant recruitment. Our editors actively and broadly engage with developer and research communities to find new reviewers. We recruit from authors of previous submissions, co-workers and colleagues, at conferences, through our other collaborative work and on social media. In the open-source software ecosystem, one can often identify people with particular expertise by looking at their published software or contribution to other projects, and we often will cold-email potential reviewers whose published work suggests they would be a good match for a submission.&lt;/p&gt;

&lt;p&gt;We cultivate our reviewer pool as well as expand it. We bring back reviewers so that they may develop reviewing as a skill, but not so often as to overburden them. We provide guidance and feedback to new recruits. When assigning reviewers to a submission, we aim to pair experienced reviewers with new ones, or reviewers with expertise on a package’s programming methods with those experienced in its field of application. &lt;strong&gt;These reviewers learn from each other, and diversity in perspectives is an advantage&lt;/strong&gt;; less experienced developers often provide insight that more experienced ones do not on software usability, API design, and documentation. More experienced developers will more often identify inefficiencies in code, pitfalls due to edge-cases, or suggest alternate implementation approaches.&lt;/p&gt;

&lt;h3 id=&#34;expanding-peer-review-for-code&#34;&gt;Expanding Peer Review for Code&lt;/h3&gt;

&lt;p&gt;Code review has been one of rOpenSci’s best initiatives. We build software, build skills, and build community, and the peer review process has been a major catalyst for all three. It has made both the software we develop internally and that we accept from outside contributors more reliable, usable, and maintainable. So &lt;strong&gt;we are working to promote open peer review of code by more organizations&lt;/strong&gt; working with scientific software. We helped launch &lt;a href=&#34;http://joss.theoj.org/&#34;&gt;The Journal of Open Source Software&lt;/a&gt;, which uses a version of our review process to provide a developer-friendly publication venue. JOSS’s success has led to a spin-off, the &lt;a href=&#34;http://jose.theoj.org/&#34;&gt;Journal of Open Source Education&lt;/a&gt;, which uses an open, code-review-like processes to provide feedback on curricula and educational materials. We are also developing a pilot program where software papers submitted to a partner academic journal receive a badge for going through rOpenSci review. We are encouraged by other review initiatives like &lt;a href=&#34;https://rescience.github.io/&#34;&gt;ReScience&lt;/a&gt; and &lt;a href=&#34;https://programminghistorian.org/&#34;&gt;The Programming Historian&lt;/a&gt;. &lt;a href=&#34;https://www.bioconductor.org/&#34;&gt;BioConductor&lt;/a&gt;’s code reviews, which predate ours by several years, recently switched to an open model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If your organization is developing or curating scientific code&lt;/strong&gt;, we believe code review, implemented well, can be a great benefit. It can take considerable effort to begin, but &lt;strong&gt;here are some of the key lessons we’ve learned that can help:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Develop standards and guidelines&lt;/strong&gt; for your authors and reviewers to use. Borrow these freely from other projects (feel free to use ours), and modify them iteratively as you go.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use automated tools&lt;/strong&gt; such as code linters, test suites, and test coverage measures to reduce burden on authors, reviewers, and editors as much as possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a clear scope.&lt;/strong&gt; Spell out to yourselves and potential contributors what your project will accept, and why. This will save a lot of confusion and effort in the future.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build a community&lt;/strong&gt; through incentives of networking, opportunities to learn, and kindness.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;rOpenSci is eager to work with other groups interested in developing similar review processes&lt;/strong&gt;, especially if you are interested in reviewing and curating scientific software in languages other than R or beyond our scope of supporting the data life cycle. Software that implements statistical algorithms, for instance, is an area ripe for open peer review of code. Please &lt;a href=&#34;https://ropensci.org/contact.html&#34;&gt;get in touch&lt;/a&gt; if you have questions or wish to co-pilot a review system for your project.&lt;/p&gt;

&lt;p&gt;And of course, if you want to review, we’re always looking for volunteers. Sign up at &lt;a href=&#34;https://ropensci.org/onboarding&#34;&gt;https://ropensci.org/onboarding&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;You can support rOpenSci by &lt;a href=&#34;https://www.numfocus.org/community/donate/&#34;&gt;becoming a NumFOCUS member&lt;/a&gt; or making a &lt;a href=&#34;https://www.numfocus.org/open-source-projects/&#34;&gt;donation to the project&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Community Call - rOpenSci Software Review and Onboarding</title>
      <link>https://ropensci.org/blog/2017/08/31/comm-call-v14/</link>
      <pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/08/31/comm-call-v14/</guid>
      <description>
        
        

&lt;p&gt;Are you thinking about submitting a package to rOpenSci&amp;rsquo;s open peer software review? Considering volunteering to review for the first time? Maybe you&amp;rsquo;re an experienced package author or reviewer and have ideas about how we can improve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Join our &lt;a href=&#34;https://github.com/ropensci/commcalls/issues/15&#34;&gt;Community Call on Wednesday, September 13th&lt;/a&gt;&lt;/strong&gt;. We want to get your feedback and we&amp;rsquo;d love to answer your questions!&lt;/p&gt;

&lt;h3 id=&#34;agenda&#34;&gt;Agenda&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Welcome (Stefanie Butland, rOpenSci Community Manager, 5 min)&lt;/li&gt;
&lt;li&gt;guest: Noam Ross, editor (15 min)
Noam will give an overview of the rOpenSci software review and onboarding, highlighting the role editors play and how decisions are made about policies and changes to the process.&lt;/li&gt;
&lt;li&gt;guest: Andee Kaplan, reviewer (15 min)
Andee will give her perspective as a package reviewer, sharing specifics about her workflow and her motivation for doing this.&lt;/li&gt;
&lt;li&gt;Q &amp;amp; A (25 min, moderated by Noam Ross)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;speaker-bios&#34;&gt;Speaker bios&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Andee Kaplan&lt;/strong&gt; is a Postdoctoral Fellow at Duke University. She is a recent PhD graduate from the Iowa State University Department of Statistics, where she learned a lot about R and reproducibility by developing a class on data stewardship for Agronomists. Andee has reviewed multiple (two!) packages for rOpenSci, &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/107&#34;&gt;&lt;code&gt;iheatmapr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/58&#34;&gt;&lt;code&gt;getlandsat&lt;/code&gt;&lt;/a&gt;, and hopes to one day be on the receiving end of the review process.&lt;/p&gt;

&lt;p&gt;Andee on &lt;a href=&#34;https://github.com/andeek&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/andeekaplan&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Noam Ross&lt;/strong&gt; is one of rOpenSci&amp;rsquo;s four editors for software peer review. Noam is a Senior Research Scientist at EcoHealth Alliance in New York, specializing in mathematical modeling of disease outbreaks, as well as training and standards for data science and reproducibility. Noam earned his Ph.D. in Ecology from the University of California-Davis, where he founded the Davis R Users&amp;rsquo; Group.&lt;/p&gt;

&lt;p&gt;Noam on &lt;a href=&#34;https://github.com/noamross&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/noamross&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;resources&#34;&gt;Resources&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science/&#34;&gt;How rOpenSci uses Code Review to Promote Reproducible Science&lt;/a&gt;; blog post Aug 11, 2017&lt;/li&gt;
&lt;li&gt;The what, why and how of &lt;a href=&#34;http://onboarding.ropensci.org/&#34;&gt;rOpenSci open peer review and onboarding&lt;/a&gt;; guidelines&lt;/li&gt;
&lt;li&gt;rOpenSci &lt;a href=&#34;https://github.com/ropensci/onboarding/issues&#34;&gt;software reviews in progress and completed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/packages/&#34;&gt;rOpenSci onboarded packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read on &lt;a href=&#34;https://ropensci.org/blog/&#34;&gt;our blog&lt;/a&gt; one of ten guest posts (to date) by authors of onboarded packages&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/22/first-package-review&#34;&gt;So you (don&amp;rsquo;t) think you can review a package&lt;/a&gt;; guest blog post by first-time reviewer Mara Averick, Aug 22, 2017&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/blog/blog/2016/03/28/software-review&#34;&gt;Onboarding at rOpenSci: A Year in Reviews&lt;/a&gt;; blog post Mar 28, 2016&lt;/li&gt;
&lt;li&gt;Soon after the Community Call, we&amp;rsquo;ll post the &lt;a href=&#34;https://vimeo.com/ropensci/videos&#34;&gt;video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>rtimicropem: Using an *R* package as platform for harmonized cleaning of data from RTI MicroPEM air quality sensors</title>
      <link>https://ropensci.org/blog/2017/08/29/rtimicropem/</link>
      <pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/08/29/rtimicropem/</guid>
      <description>
        
        

&lt;p&gt;As you might remember from &lt;a href=&#34;https://ropensci.org/blog/blog/2017/02/21/ropenaq&#34;&gt;my blog post about &lt;code&gt;ropenaq&lt;/code&gt;&lt;/a&gt;, I work as a data manager and statistician for an &lt;a href=&#34;http://www.chaiproject.org/&#34;&gt;epidemiology project called CHAI&lt;/a&gt; for Cardio-vascular health effects of air pollution in Telangana, India. One of our interests in CHAI is determining exposure, and sources of exposure, to PM2.5 which are very small particles in the air that have diverse adverse health effects. You can find more details about CHAI &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/28606699&#34;&gt;in our recently published protocol paper&lt;/a&gt;. In this blog post that partly corresponds to the content of &lt;a href=&#34;http://sched.co/AxrS&#34;&gt;my useR! 2017 lightning talk&lt;/a&gt;, I&amp;rsquo;ll present a package we wrote for dealing with the output of a scientific device, which might remind you of similar issues in your experimental work.&lt;/p&gt;

&lt;h3 id=&#34;why-write-the-rtimicropem-package&#34;&gt;Why write the rtimicropem package?&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-29-rtimicropem/Allequipment_Frontview_cropped.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Part of the CHAI project is a panel study involving about 40 people wearing several devices, as you see above. The devices include a GPS, an accelerometer, a wearable camera, and a PM2.5 monitor outputting time-resolved data (the grey box on the left). Basically, with this device, the RTI MicroPEM, we get one PM2.5 exposure value every 10 seconds. This is quite exciting, right? Except that we have two main issues with it&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-29-rtimicropem/screenshot_output2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First of all, the output of the device, a file with a &amp;ldquo;.csv&amp;rdquo; extension corresponding to a session of measurements, in our case 24 hours of measurements, is not really a csv. The header contains information about settings of the device for that session, and then comes the actual table with measurements.&lt;/p&gt;

&lt;p&gt;Second, since the RTI MicroPEMs are nice devices but also a work-in-progress, we had some problems with the data, such as negative relative humidity. Because of these issues, we decided to write an R package whose three goals were to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Transform the output of the device into something more usable.&lt;/li&gt;
&lt;li&gt;Allow the exploration of individual files after a day in the field.&lt;/li&gt;
&lt;li&gt;Document our data cleaning process.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We chose R because everything else in our project, well data processing, documentation and analysis, was to be implemented in R, and because we wanted other teams to be able to use our package.&lt;/p&gt;

&lt;h3 id=&#34;features-of-rtimicropem-transform-explore-and-learn-about-data-cleaning&#34;&gt;Features of &lt;code&gt;rtimicropem&lt;/code&gt;: transform, explore and learn about data cleaning&lt;/h3&gt;

&lt;p&gt;First things first, our package lives &lt;a href=&#34;https://github.com/ropensci/rtimicropem&#34;&gt;here&lt;/a&gt; and is &lt;a href=&#34;https://cran.r-project.org/web/packages/rtimicropem/index.html&#34;&gt;on CRAN&lt;/a&gt;. It has a &lt;a href=&#34;http://ropensci.github.io/rtimicropem/&#34;&gt;nice documentation website&lt;/a&gt; thanks to &lt;a href=&#34;https://github.com/hadley/pkgdown&#34;&gt;&lt;code&gt;pkgdown&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;transform-and-explore-single-files&#34;&gt;Transform and explore single files&lt;/h3&gt;

&lt;p&gt;In &lt;code&gt;rtimicropem&lt;/code&gt; after the use of the &lt;code&gt;convert_output&lt;/code&gt; function, one gets an object of the R6 class &lt;code&gt;micropem&lt;/code&gt; class. Its fields include the settings and measurements as two &lt;code&gt;data.frames&lt;/code&gt;, and it has methods such as &lt;code&gt;summary&lt;/code&gt; and &lt;code&gt;plot&lt;/code&gt; for which you see the static output below (no unit on this exploratory plot).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-29-rtimicropem/plotexample.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The plot method can also outputs an interactive graph thanks to &lt;a href=&#34;http://hafen.github.io/rbokeh/&#34;&gt;&lt;code&gt;rbokeh&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While these methods can be quite helpful to explore single files as an R user, they don&amp;rsquo;t help non R users a lot. Because we wanted members of our team working in the field to be able to explore and check files with no R knowledge, we created a Shiny app that allows to upload individual files and then look at different tabs, including one with a plot, one with the summary of measurements, etc. This way, it was easy to spot a device failure for instance, and to plan a new measurement session with the corresponding participant.&lt;/p&gt;

&lt;h3 id=&#34;transform-a-bunch-of-files&#34;&gt;Transform a bunch of files&lt;/h3&gt;

&lt;p&gt;At the end of the CHAI data collection, we had more than 250 MicroPEM files. In order to prepare them for further processing we wrote the &lt;code&gt;batch_convert&lt;/code&gt; function that saves the content of any number of MicroPEM files as two (real!) csv, one with the measurements, one with the settings.&lt;/p&gt;

&lt;h3 id=&#34;learn-about-data-cleaning&#34;&gt;Learn about data cleaning&lt;/h3&gt;

&lt;p&gt;As mentioned previously, we experienced issues with MicroPEM data quality. Although we had heard other teams complain of similar problems, in the literature there were very few details about data cleaning. We decided to gather information from other teams and the manufacturer and to document our own decisions, e.g. remove entire files based on some criteria, in a &lt;a href=&#34;http://ropensci.github.io/rtimicropem/articles/chai_data_cleaning.html&#34;&gt;vignette of the package&lt;/a&gt;. This is our transparent answer to the question &amp;ldquo;What was your experience with MicroPEMs?&amp;rdquo; which we get often enough from other scientists interested in PM2.5 exposure.&lt;/p&gt;

&lt;h3 id=&#34;place-of-rtimicropem-in-the-r-package-ecosystem&#34;&gt;Place of rtimicropem in the R package ecosystem&lt;/h3&gt;

&lt;p&gt;When preparing &lt;code&gt;rtimicropem&lt;/code&gt; submission to rOpenSci, I started wondering whether one would like to have one R package for each scientific device out there. In our case, having the weird output to deal with, and the lack of a central data issues documentation place, were enough of a motivation. But maybe one could hope that manufacturers of scientific devices would focus a bit more on making the output format analysis-friendly, and that the open documentation of data issues would be language-agnostic and managed by the manufacturers themselves. In the meantime, we&amp;rsquo;re quite proud to have taken the time to create and share our experience with &lt;code&gt;rtimicropem&lt;/code&gt;, and have already heard back from a few users, including one who found the package via googling &amp;ldquo;RTI MicroPEM data&amp;rdquo;! Another argument I in particular have to write R packages for dealing with scientific data is that it might motivate people to learn R, but this is maybe a bit evil.&lt;/p&gt;

&lt;p&gt;What about the place of &lt;code&gt;rtimicropem&lt;/code&gt; in the rOpenSci package collection? After &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/126&#34;&gt;very useful reviews&lt;/a&gt; by &lt;a href=&#34;https://github.com/LucyMcGowan&#34;&gt;Lucy D&amp;rsquo;Agostino McGowan&lt;/a&gt; and &lt;a href=&#34;https://github.com/karawoo&#34;&gt;Kara Woo&lt;/a&gt; our package got onboarded which we were really thankful for and happy about. Another package I can think off the top of my head to deal with the output of a scientific tool is &lt;a href=&#34;https://ropensci.org/blog/blog/2017/02/06/plater-blog-post&#34;&gt;&lt;code&gt;plater&lt;/code&gt;&lt;/a&gt;. Let me switch roles from CHAI team member to rOpenSci onboarding co-editor here and do some advertisement&amp;hellip; Such packages are unlikely to become the new &lt;code&gt;ggplot2&lt;/code&gt; but their specialization doesn&amp;rsquo;t make them less useful and they fit very well in the &amp;ldquo;data extraction&amp;rdquo; of the &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md&#34;&gt;onboarding categories&lt;/a&gt;. So if you have written such a package, please consider submitting it! It&amp;rsquo;ll get better thanks to review and might get more publicity as part of a larger software ecosystem. For the &lt;code&gt;rtimicropem&lt;/code&gt; submission we took advantage of the joint submission process of rOpenSci and the Journal of Open Source Software, JOSS, so now our piece of software has &lt;a href=&#34;http://joss.theoj.org/papers/7ead5e9a445da3e885d99247c5d6e58e&#34;&gt;its JOSS paper with a DOI&lt;/a&gt;. And hopefully, having more submissions of packages for scientific hardware might inspire R users to package up the code they wrote to use the output of their scientific tools!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>FedData - Getting assorted geospatial data into R</title>
      <link>https://ropensci.org/technotes/2017/08/24/feddata-release/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/08/24/feddata-release/</guid>
      <description>
        
        

&lt;p&gt;The package &lt;a href=&#34;https://github.com/ropensci/FedData&#34;&gt;&lt;code&gt;FedData&lt;/code&gt;&lt;/a&gt; has gone through software review and is now part of &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt;. &lt;code&gt;FedData&lt;/code&gt; includes functions to automate downloading geospatial data available from several federated data sources (mainly sources maintained by the US Federal government).&lt;/p&gt;

&lt;p&gt;Currently, the package enables extraction from six datasets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;http://ned.usgs.gov&#34;&gt;National Elevation Dataset (NED)&lt;/a&gt; digital elevation models (1 and &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; arc-second; USGS)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://nhd.usgs.gov&#34;&gt;National Hydrography Dataset (NHD)&lt;/a&gt; (USGS)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://websoilsurvey.sc.egov.usda.gov/&#34;&gt;Soil Survey Geographic (SSURGO) database&lt;/a&gt; from the National Cooperative Soil Survey (NCSS), which is led by the Natural Resources Conservation Service (NRCS) under the USDA,&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn&#34;&gt;Global Historical Climatology Network (GHCN)&lt;/a&gt;, coordinated by National Climatic Data Center at NOAA,&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://daymet.ornl.gov/&#34;&gt;Daymet&lt;/a&gt; gridded estimates of daily weather parameters for North America, version 3, available from the Oak Ridge National Laboratory&amp;rsquo;s Distributed Active Archive Center (DAAC), and&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://www.ncdc.noaa.gov/data-access/paleoclimatology-data/datasets/tree-ring&#34;&gt;International Tree Ring Data Bank (ITRDB)&lt;/a&gt;, coordinated by National Climatic Data Center at NOAA.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; is designed with the large-scale geographic information system (GIS) use-case in mind: cases where the use of dynamic web-services is impractical due to the scale (spatial and/or temporal) of analysis. It functions primarily as a means of downloading tiled or otherwise spatially-defined datasets; additionally, it can preprocess those datasets by extracting data within an area of interest (AoI), defined spatially. It relies heavily on the &lt;a href=&#34;https://cran.r-project.org/package=sp&#34;&gt;&lt;code&gt;sp&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/package=raster&#34;&gt;&lt;code&gt;raster&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://cran.r-project.org/package=rgdal&#34;&gt;&lt;code&gt;rgdal&lt;/code&gt;&lt;/a&gt; packages.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Load &lt;code&gt;FedData&lt;/code&gt; and define a study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# FedData Tester
library(FedData)
library(magrittr)

# Extract data for the Village Ecodynamics Project &amp;quot;VEPIIN&amp;quot; study area:
# http://veparchaeology.org
vepPolygon &amp;lt;- polygon_from_extent(raster::extent(672800, 740000, 4102000, 4170000),
                                  proj4string = &amp;quot;+proj=utm +datum=NAD83 +zone=12&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get and plot the National Elevation Dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NED (USA ONLY)
# Returns a raster
NED &amp;lt;- get_ned(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;)
# Plot with raster::plot
raster::plot(NED)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the Daymet dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the DAYMET (North America only)
# Returns a raster
DAYMET &amp;lt;- get_daymet(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;,
               elements = c(&amp;quot;prcp&amp;quot;,&amp;quot;tmax&amp;quot;),
               years = 1980:1985)
# Plot with raster::plot
raster::plot(DAYMET$tmax$X1985.10.23)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the daily GHCN precipitation data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the daily GHCN data (GLOBAL)
# Returns a list: the first element is the spatial locations of stations,
# and the second is a list of the stations and their daily data
GHCN.prcp &amp;lt;- get_ghcn_daily(template = vepPolygon,
                            label = &amp;quot;VEPIIN&amp;quot;,
                            elements = c(&#39;prcp&#39;))
# Plot the NED again
raster::plot(NED)
# Plot the spatial locations
sp::plot(GHCN.prcp$spatial,
         pch = 1,
         add = TRUE)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend=&amp;quot;GHCN Precipitation Records&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the daily GHCN temperature data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Elements for which you require the same data
# (i.e., minimum and maximum temperature for the same days)
# can be standardized using standardize==T
GHCN.temp &amp;lt;- get_ghcn_daily(template = vepPolygon,
                            label = &amp;quot;VEPIIN&amp;quot;,
                            elements = c(&#39;tmin&#39;,&#39;tmax&#39;),
                            years = 1980:1985,
                            standardize = TRUE)
# Plot the NED again
raster::plot(NED)
# Plot the spatial locations
sp::plot(GHCN.temp$spatial,
         add = TRUE,
         pch = 1)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend = &amp;quot;GHCN Temperature Records&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the National Hydrography Dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NHD (USA ONLY)
NHD &amp;lt;- get_nhd(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;)
# Plot the NED again
raster::plot(NED)
# Plot the NHD data
NHD %&amp;gt;%
  lapply(sp::plot,
         col = &#39;black&#39;,
         add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the NRCS SSURGO data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NRCS SSURGO data (USA ONLY)
SSURGO.VEPIIN &amp;lt;- get_ssurgo(template = vepPolygon,
                     label = &amp;quot;VEPIIN&amp;quot;)
#&amp;gt; Warning: 1 parsing failure.
#&amp;gt; row # A tibble: 1 x 5 col     row     col               expected actual expected   &amp;lt;int&amp;gt;   &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; actual 1  1276 slope.r no trailing characters     .5 file # ... with 1 more variables: file &amp;lt;chr&amp;gt;
# Plot the NED again
raster::plot(NED)
# Plot the SSURGO mapunit polygons
plot(SSURGO.VEPIIN$spatial,
     lwd = 0.1,
     add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the NRCS SSURGO data for particular soil survey areas&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Or, download by Soil Survey Area names
SSURGO.areas &amp;lt;- get_ssurgo(template = c(&amp;quot;CO670&amp;quot;,&amp;quot;CO075&amp;quot;),
                           label = &amp;quot;CO_TEST&amp;quot;)

# Let&#39;s just look at spatial data for CO675
SSURGO.areas.CO675 &amp;lt;- SSURGO.areas$spatial[SSURGO.areas$spatial$AREASYMBOL==&amp;quot;CO075&amp;quot;,]

# And get the NED data under them for pretty plotting
NED.CO675 &amp;lt;- get_ned(template = SSURGO.areas.CO675,
                            label = &amp;quot;SSURGO_CO675&amp;quot;)

# Plot the SSURGO mapunit polygons, but only for CO675
plot(NED.CO675)
plot(SSURGO.areas.CO675,
     lwd = 0.1,
     add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the ITRDB chronology locations in the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the ITRDB records
ITRDB &amp;lt;- get_itrdb(template = vepPolygon,
                        label = &amp;quot;VEPIIN&amp;quot;,
                        makeSpatial = TRUE)
# Plot the NED again
raster::plot(NED)
# Map the locations of the tree ring chronologies
plot(ITRDB$metadata,
     pch = 1,
     add = TRUE)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend = &amp;quot;ITRDB chronologies&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-13-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;todo&#34;&gt;TODO&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The current CRAN version of &lt;code&gt;FedData&lt;/code&gt;, v2.4.6, will (hopefully) be the final CRAN release of &lt;code&gt;FedData&lt;/code&gt; 2. &lt;code&gt;FedData&lt;/code&gt; 3 will be released in the coming months, but some code built on &lt;code&gt;FedData&lt;/code&gt; 2 will not be compatible with FedData 3.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; was initially developed prior to widespread use of modern web mapping services and RESTful APIs by many Federal data-holders. Future releases of &lt;code&gt;FedData&lt;/code&gt; will limit data transfer by utilizing server-side geospatial and data queries. We will also implement &lt;a href=&#34;https://github.com/hadley/dplyr&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/a&gt; verbs, tidy data structures, (&lt;a href=&#34;https://github.com/tidyverse/magrittr&#34;&gt;&lt;code&gt;magrittr&lt;/code&gt;&lt;/a&gt;) piping, functional programming using &lt;a href=&#34;https://github.com/hadley/purrr&#34;&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/a&gt;, simple features for spatial data from &lt;a href=&#34;https://github.com/edzer/sfr&#34;&gt;&lt;code&gt;sf&lt;/code&gt;&lt;/a&gt;, and local data storage in OGC-compliant data formats (probably GeoJSON and NetCDF). I am also aiming for 100% testing coverage.&lt;/p&gt;

&lt;p&gt;All that being said, much of the functionality of the &lt;code&gt;FedData&lt;/code&gt; package could be spun off into more domain-specific packages. For example, ITRDB download functions could be part of the &lt;a href=&#34;https://r-forge.r-project.org/projects/dplr/&#34;&gt;&lt;code&gt;dplR&lt;/code&gt;&lt;/a&gt; dendrochronology package; concepts/functions having to do with the GHCN data integrated into &lt;a href=&#34;https://github.com/ropensci/rnoaa&#34;&gt;&lt;code&gt;rnoaa&lt;/code&gt;&lt;/a&gt;; and Daymet concepts integrated into &lt;a href=&#34;https://github.com/khufkens/daymetr&#34;&gt;&lt;code&gt;daymetr&lt;/code&gt;&lt;/a&gt;. I welcome any and all suggestions about how to improve the utility of FedData; please &lt;a href=&#34;https://github.com/ropensci/FedData/issues&#34;&gt;submit an issue&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; is a product of SKOPE (&lt;a href=&#34;http://www.openskope.org&#34;&gt;Synthesizing Knowledge of Past Environments&lt;/a&gt;) and the &lt;a href=&#34;http://veparchaeology.org/&#34;&gt;Village Ecodynamics Project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; was reviewed for &lt;a href=&#34;https://ropensci.org&#34;&gt;rOpenSci&lt;/a&gt; by &lt;a href=&#34;https://github.com/jooolia&#34;&gt;@jooolia&lt;/a&gt;, with &lt;a href=&#34;https://github.com/sckott&#34;&gt;@sckott&lt;/a&gt; as onboarding editor, and was greatly improved as a result.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Onboarding visdat, a tool for preliminary visualisation of whole dataframes</title>
      <link>https://ropensci.org/blog/2017/08/22/visdat/</link>
      <pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/08/22/visdat/</guid>
      <description>
        
        

&lt;blockquote&gt;
&lt;p&gt;Take a look at the data&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a phrase that comes up when you first get a dataset.&lt;/p&gt;

&lt;p&gt;It is also ambiguous. Does it mean to do some exploratory modelling? Or make some histograms, scatterplots, and boxplots? Is it both?&lt;/p&gt;

&lt;p&gt;Starting down either path, you often encounter the non-trivial growing pains of working with a new dataset. The mix ups of data types - height in cm coded as a factor, categories are numerics with decimals, strings are datetimes, and somehow datetime is one long number. And let&amp;rsquo;s not forget everyone&amp;rsquo;s favourite: missing data.&lt;/p&gt;

&lt;p&gt;These growing pains often get in the way of your basic modelling or graphical exploration. So, sometimes you can&amp;rsquo;t even start to take a look at the data, and that is frustrating.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/ropensci/visdat&#34;&gt;&lt;code&gt;visdat&lt;/code&gt;&lt;/a&gt; package aims to make this preliminary part of analysis easier. It focuses on creating visualisations of whole dataframes, to make it easy and fun for you to &amp;ldquo;get a look at the data&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Making &lt;code&gt;visdat&lt;/code&gt; was fun, and it was easy to use. But I couldn&amp;rsquo;t help but think that maybe &lt;code&gt;visdat&lt;/code&gt; could be more.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I felt like the code was a little sloppy, and that it could be better.&lt;/li&gt;
&lt;li&gt;I wanted to know whether others found it useful.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What I needed was someone to sit down and read over it, and tell me what they thought. And hey, a publication out of this would certainly be great.&lt;/p&gt;

&lt;p&gt;Too much to ask, perhaps? No. Turns out, not at all. This is what the rOpenSci &lt;a href=&#34;https://github.com/ropensci/onboarding&#34;&gt;onboarding process&lt;/a&gt; provides.&lt;/p&gt;

&lt;h3 id=&#34;ropensci-onboarding-basics&#34;&gt;rOpenSci onboarding basics&lt;/h3&gt;

&lt;p&gt;Onboarding a package onto rOpenSci is an open peer review of an R package. If successful, the package is migrated to rOpenSci, with the option of putting it through an accelerated publication with &lt;a href=&#34;http://joss.theoj.org/&#34;&gt;JOSS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What&amp;rsquo;s in it for the author?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Feedback on your package&lt;/li&gt;
&lt;li&gt;Support from rOpenSci members&lt;/li&gt;
&lt;li&gt;Maintain ownership of your package&lt;/li&gt;
&lt;li&gt;Publicity from it being under rOpenSci&lt;/li&gt;
&lt;li&gt;Contribute something to rOpenSci&lt;/li&gt;
&lt;li&gt;Potentially a publication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;What can rOpenSci do that CRAN cannot?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The rOpenSci onboarding process provides a stamp of quality on a package that you do not necessarily get when a package is on CRAN &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Here&amp;rsquo;s what rOpenSci does that CRAN cannot:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Assess documentation readability / usability&lt;/li&gt;
&lt;li&gt;Provide a code review to find weak points / points of improvement&lt;/li&gt;
&lt;li&gt;Determine whether a package is overlapping with another.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So I submitted &lt;code&gt;visdat&lt;/code&gt; to the onboarding process. For me, I did this for three reasons.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;So &lt;code&gt;visdat&lt;/code&gt; could become a better package&lt;/li&gt;
&lt;li&gt;Pending acceptance, I would get a publication in JOSS&lt;/li&gt;
&lt;li&gt;I get to contribute back to rOpenSci&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Submitting the package was actually quite easy - you go to &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/new&#34;&gt;submit an issue&lt;/a&gt; on the onboarding page on GitHub, and it provides a magical template for you to fill out &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, with no submission gotchas - this could be the future &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. Within 2 days of submitting the issue, I had a response from the editor, &lt;a href=&#34;https://github.com/noamross&#34;&gt;Noam Ross&lt;/a&gt;, and two reviewers assigned, &lt;a href=&#34;https://github.com/batpigandme&#34;&gt;Mara Averick&lt;/a&gt;, and &lt;a href=&#34;https://github.com/seaaan&#34;&gt;Sean Hughes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/87&#34;&gt;submitted &lt;strong&gt;visdat&lt;/strong&gt;&lt;/a&gt; and waited, somewhat apprehensively. &lt;em&gt;What would the reviewers think?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In fact, Mara Averick wrote a post: &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/22/first-package-review&#34;&gt;&amp;ldquo;So you (don&amp;rsquo;t) think you can review a package&amp;rdquo;&lt;/a&gt; about her experience evaluating &lt;code&gt;visdat&lt;/code&gt; as a first-time reviewer.&lt;/p&gt;

&lt;h3 id=&#34;getting-feedback&#34;&gt;Getting feedback&lt;/h3&gt;

&lt;h4 id=&#34;unexpected-extras-from-the-review&#34;&gt;Unexpected extras from the review&lt;/h4&gt;

&lt;p&gt;Even before the review started officially, I got some great concrete feedback from Noam Ross, the editor for the &lt;code&gt;visdat&lt;/code&gt; submission.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Noam used the &lt;a href=&#34;https://github.com/MangoTheCat/goodpractice&#34;&gt;goodpractice&lt;/a&gt; package, to identify bad code patterns and other places to immediately improve upon in a concrete way. This resulted in me:

&lt;ul&gt;
&lt;li&gt;Fixing error prone code such as using &lt;code&gt;1:length(...)&lt;/code&gt;, or &lt;code&gt;1:nrow(...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Improving testing using the visualisation testing software &lt;a href=&#34;https://github.com/lionel-/vdiffr&#34;&gt;&lt;code&gt;vdiffr&lt;/code&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Reducing long code lines to improve readability&lt;/li&gt;
&lt;li&gt;Defining global variables to avoid a NOTE (&amp;ldquo;no visible binding for global variable&amp;rdquo;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So before the review even started, &lt;code&gt;visdat&lt;/code&gt; is in better shape, with 99% test coverage, and clearance from &lt;code&gt;goodpractice&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;the-feedback-from-reviewers&#34;&gt;The feedback from reviewers&lt;/h4&gt;

&lt;p&gt;I received prompt replies from the reviewers, and I got to hear really nice things like  &amp;ldquo;I think &lt;code&gt;visdat&lt;/code&gt; is a very worthwhile project and have already started using it in my own work.&amp;ldquo;, and &amp;ldquo;Having now put it to use in a few of my own projects, I can confidently say that it is an incredibly useful early step in the data analysis workflow. &lt;code&gt;vis_miss()&lt;/code&gt;, in particular, is helpful for scoping the task at hand &amp;hellip;&amp;ldquo;. In addition to these nice things, there was also great critical feedback from Sean and Mara.&lt;/p&gt;

&lt;p&gt;A common thread in both reviews was that the way I initially had &lt;code&gt;visdat&lt;/code&gt; set up was to have the first row of the dataset at the bottom left, and the variable names at the bottom. However, this doesn&amp;rsquo;t reflect what a dataframe typically looks like - with the names of the variables at the top, and the first row also at the top. There was also suggestions to add the percentage of missing data in each column.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-compare-1.png&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-compare-2.png&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-compare-3.png&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-compare-4.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;On the left are the old &lt;code&gt;visdat&lt;/code&gt; and vismiss plots, and on the right are the new &lt;code&gt;visdat&lt;/code&gt; and vismiss plots.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Changing this makes the plots make a lot more sense, and read better.&lt;/p&gt;

&lt;p&gt;Mara made me aware of the warning and error messages that I had let crop up in the package. This was something I had grown to accept - the plot worked, right? But Mara pointed out that from a user perspective, seeing these warnings and messages can be a negative experience for the user, and something that might stop them from using it - how do they know if their plot is accurate with all these warnings? Are they using it wrong?&lt;/p&gt;

&lt;p&gt;Sean gave practical advice on reducing code duplication, explaining how to write general construction method to prepare the data for the plots. Sean also explained how to write C++ code to improve the speed of &lt;code&gt;vis_guess()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;From both reviewers I got nitty gritty feedback about my writing - places where documentation that was just a bunch of notes I made, or where I had reversed the order of a statement.&lt;/p&gt;

&lt;h3 id=&#34;what-did-i-think&#34;&gt;What did I think?&lt;/h3&gt;

&lt;p&gt;I think that getting feedback in general on your own work can be a bit hard to take sometimes. We get attached to our ideas, we&amp;rsquo;ve seen them grow from little thought bubbles all the way to &amp;ldquo;all growed up&amp;rdquo; R packages. I was apprehensive about getting feedback on &lt;code&gt;visdat&lt;/code&gt;. But the feedback process from rOpenSci was, as Tina Turner put it, &lt;a href=&#34;https://www.youtube.com/watch?v=mNU3aIJs88g&#34;&gt;&amp;ldquo;simply the best&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Boiling down the onboarding review process down to a few key points, I would say it is &lt;strong&gt;transparent&lt;/strong&gt;, &lt;strong&gt;friendly&lt;/strong&gt;, and &lt;strong&gt;thorough&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Having the entire review process on GitHub means that everyone is accountable for what they say, and means that you can track exactly what everyone said about it &lt;em&gt;in one place&lt;/em&gt;. No email chain hell with (mis)attached documents, accidental reply-alls or single replies. The whole internet is cc&amp;rsquo;d in on this discussion.&lt;/p&gt;

&lt;p&gt;Being an rOpenSci initiative, the process is incredibly &lt;strong&gt;friendly&lt;/strong&gt; and respectful of everyone involved. Comments are upbeat, but are also, importantly &lt;strong&gt;thorough&lt;/strong&gt;, providing constructive feedback.&lt;/p&gt;

&lt;h3 id=&#34;so-what-does-visdat-look-like&#34;&gt;So what does &lt;code&gt;visdat&lt;/code&gt; look like?&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(visdat)

vis_dat(airquality)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-dat-example-1.png&#34; alt=&#34;visdat-example&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This shows us a visual analogue of our data, the variable names are shown on the top, and the class of each variable is shown, along with where missing data.&lt;/p&gt;

&lt;p&gt;You can focus in on missing data with &lt;code&gt;vis_miss()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;vis_miss(airquality)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-22-visdat/blog-vis-miss-aq-1.png&#34; alt=&#34;vis-miss-example&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This shows only missing and present information in the data. In addition to &lt;code&gt;vis_dat()&lt;/code&gt; it shows the percentage of missing data for each variable and also the overall amount of missing data. &lt;code&gt;vis_miss()&lt;/code&gt; will also indicate when a dataset has no missing data at all, or a very small percentage.&lt;/p&gt;

&lt;h3 id=&#34;the-future-of-visdat&#34;&gt;The future of &lt;code&gt;visdat&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;There are some really exciting changes coming up for &lt;code&gt;visdat&lt;/code&gt;. The first is making a plotly version of all of the figures that provides useful tooltips and interactivity. The second and third changes to bring in later down the track are to include the idea of visualising expectations, where the user can search their data for particular things, such as particular characters like &amp;ldquo;~&amp;rdquo; or values like -99, or -0, or conditions &amp;ldquo;x &amp;gt; 101&amp;rdquo;, and visualise them. Another final idea is to make it easy to visually compare two dataframes of differing size. We also want to work on providing consistent palettes for particular datatypes. For example, character, numerics, integers, and datetime would all have different (and consistently different) colours.&lt;/p&gt;

&lt;p&gt;I am very interested to hear how people use &lt;code&gt;visdat&lt;/code&gt; in their work, so if you have suggestions or feedback I would love to hear from you! The best way to leave feedback is by &lt;a href=&#34;https://github.com/ropensci/visdat/issues/new&#34;&gt;filing an issue&lt;/a&gt;, or perhaps sending me an email at nicholas [dot] tierney [at] gmail [dot] com.&lt;/p&gt;

&lt;h3 id=&#34;the-future-of-your-r-package&#34;&gt;The future of your R package?&lt;/h3&gt;

&lt;p&gt;If you have an R package you should give some serious thought about submitting it to the rOpenSci through their onboarding process. There are very clear guidelines on their &lt;a href=&#34;https://github.com/ropensci/onboarding&#34;&gt;onboarding GitHub page&lt;/a&gt;. If you aren&amp;rsquo;t sure about &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md&#34;&gt;package fit&lt;/a&gt;, you can submit a &lt;a href=&#34;https://github.com/ropensci/onboarding/issues?q=is%3Aissue+label%3A0%2Fpresubmission&#34;&gt;pre-submission enquiry&lt;/a&gt; - the editors are nice and friendly, and a positive experience awaits you!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:3&#34;&gt;CRAN is an essential part of what makes the r-project successful and certainly without CRAN R simply would not be the language that it is today. The tasks provided by the rOpenSci onboarding require human hours, and there just isn&amp;rsquo;t enough spare time and energy amongst CRAN managers.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:1&#34;&gt;Never used GitHub? Don&amp;rsquo;t worry, creating an account is easy, and the template is all there for you. You provide very straightforward information, and it&amp;rsquo;s all there at once.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;With some journals, the submission process means you aren&amp;rsquo;t always clear what information you need ahead of time. Gotchas include things like &amp;ldquo;what is the residential address of every co-author&amp;rdquo;, or getting everyone to sign a copyright notice.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>So you (don&#39;t) think you can review a package</title>
      <link>https://ropensci.org/blog/2017/08/22/first-package-review/</link>
      <pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/08/22/first-package-review/</guid>
      <description>
        
        

&lt;p&gt;Contributing to an open-source community &lt;em&gt;without&lt;/em&gt; contributing code is an oft-vaunted idea that can seem nebulous. Luckily, putting vague ideas into action is one of the strengths of the &lt;a href=&#34;https://ropensci.org/community/&#34;&gt;rOpenSci Community&lt;/a&gt;, and their package onboarding system offers a chance to do just that.&lt;/p&gt;

&lt;p&gt;This was my first time reviewing a package, and, as with so many things in life, I went into it worried that I&amp;rsquo;d somehow ruin the package-reviewing process— not just the package itself, but the actual onboarding infrastructure&amp;hellip;maybe even rOpenSci on the whole.&lt;/p&gt;

&lt;p&gt;Barring the destruction of someone else&amp;rsquo;s hard work and/or an entire organization, I was fairly confident that I&amp;rsquo;d have little to offer in the way of useful advice. &lt;em&gt;What if I have absolutely nothing to say other than, yes, this is, in fact, a package?!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/np59m8Z.png&#34; alt=&#34;rOpenSci package review: what I imagined&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So, step one (for me) was: confess my inadequacies and seek advice. It turns out that much of the advice vis-à-vis &lt;em&gt;how to review a package&lt;/em&gt; is baked right into the documents. The &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/reviewer_template.md&#34;&gt;reviewer template&lt;/a&gt; is a great trail map, the utility of which is fleshed out in the &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/reviewing_guide.md&#34;&gt;rOpenSci Package Reviewing Guide&lt;/a&gt;. Giving these a thorough read, and perusing a recommended review or two (links in the reviewing guide) will probably have you raring to go. But, if you&amp;rsquo;re feeling particularly neurotic (as I almost always am), the rOpenSci &lt;a href=&#34;https://github.com/ropensci/onboarding#-editors-and-reviewers&#34;&gt;onboarding editors&lt;/a&gt; and larger community are endless founts of wisdom and resources.&lt;/p&gt;

&lt;h3 id=&#34;visdat&#34;&gt;&lt;code&gt;visdat&lt;/code&gt; 📦👀&lt;/h3&gt;

&lt;p&gt;I knew nothing about &lt;a href=&#34;https://github.com/njtierney&#34;&gt;Nicholas Tierney&lt;/a&gt;&amp;rsquo;s &lt;a href=&#34;http://visdat.njtierney.com/&#34;&gt;&lt;code&gt;visdat&lt;/code&gt;&lt;/a&gt; package prior to receiving my &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/87#issuecomment-270428584&#34;&gt;invitation to review&lt;/a&gt; it. So the first (coding-y) thing I did was play around with it in the same way I do for other cool R packages I encounter. This is a totally unstructured mish-mash of running examples, putting my own data in, and seeing what happens. In addition to being amusing, it&amp;rsquo;s a good way to sort of &amp;ldquo;ground-truth&amp;rdquo; the package&amp;rsquo;s mission, and make sure there isn&amp;rsquo;t some super helpful feature that&amp;rsquo;s going unsung.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re &lt;em&gt;not&lt;/em&gt; familiar with &lt;code&gt;visdat&lt;/code&gt;, it &amp;ldquo;provides a quick way for the user to visually examine the structure of their data set, and, more specifically, where and what kinds of data are missing.&amp;rdquo;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; With early-stage EDA (exploratory data analysis), you&amp;rsquo;re really trying to get a &lt;em&gt;feel&lt;/em&gt; of your data. So, knowing that I couldn&amp;rsquo;t be much help in the &lt;em&gt;&amp;ldquo;here&amp;rsquo;s how you could make this faster with C++&amp;rdquo;&lt;/em&gt; department, I decided to fully embrace my role as &lt;em&gt;&amp;ldquo;naïve user&amp;rdquo;&lt;/em&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h4 id=&#34;questions-i-kept-in-mind-as-del-myself-del-resident-naïf&#34;&gt;Questions I kept in mind as &lt;del&gt;myself&lt;/del&gt;  resident naïf:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;What did I think this thing would do? Did it do it?&lt;/li&gt;
&lt;li&gt;What are things that scare me off?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The latter question is key, and, while I don&amp;rsquo;t have data to back this up, can be a sort of &amp;ldquo;silent&amp;rdquo; usability failure when left unexamined. Someone who tinkers with a package, but finds it confusing doesn&amp;rsquo;t necessarily stop to give feedback. There&amp;rsquo;s also a pseudo &lt;em&gt;curse-of-knowledge&lt;/em&gt; component. While messages and warnings are easily parsed, suppressed, dealt with, and/or dismissed by the veteran R user/programmer, unexpected, brightly-coloured text can easily scream &lt;em&gt;Oh my gosh you broke it all!!&lt;/em&gt; to those with less experience.&lt;/p&gt;

&lt;h3 id=&#34;myriad-lessons-learned&#34;&gt;Myriad lessons learned 💡&lt;/h3&gt;

&lt;p&gt;I can&amp;rsquo;t speak for Nick per the utility or lack thereof of my review (you can see &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/22/visdat&#34;&gt;his take here&lt;/a&gt;, but I &lt;em&gt;can&lt;/em&gt; vouch for the package-reviewing experience as a means of methodically inspecting the innards of an R package. Methodical is really the operative word here. Though &lt;em&gt;&amp;ldquo;read the docs,&amp;rdquo;&lt;/em&gt; or &lt;em&gt;&amp;ldquo;look at the code&amp;rdquo;&lt;/em&gt; sounds straight-forward enough, it&amp;rsquo;s not always easy to coax oneself into going through the task piece-by-piece without an end goal in mind. While a desire to contribute to open-source software is noble enough (and is how I &lt;em&gt;personaly&lt;/em&gt; ended up involved in this process&amp;ndash; with some help/coaxing from &lt;a href=&#34;https://twitter.com/noamross&#34;&gt;Noam Ross&lt;/a&gt;), it&amp;rsquo;s also an abstraction that can leave one feeling overwhelmed, and not knowing where to begin.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;There are also &lt;a href=&#34;https://github.com/ropensci/onboarding#why-review-packages-for-ropensci&#34;&gt;self-serving bonus points&lt;/a&gt; that one simply can&amp;rsquo;t avoid, should you go the rOpenSci-package-reviewing route&amp;ndash; especially if package development is new to you.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; Heck, the &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/reviewing_guide.md&#34;&gt;package reviewing guide&lt;/a&gt; alone was illuminating.&lt;/p&gt;

&lt;p&gt;Furthermore, the wise-sage 🦉 &lt;a href=&#34;https://github.com/ropensci/onboarding#associate-editors&#34;&gt;rOpenSci onboarding editors&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; are excellent matchmakers, and ensure that you&amp;rsquo;re actually reviewing a package authored by someone who &lt;em&gt;wants&lt;/em&gt; their package to be reviewed. This sounds simple enough, but it&amp;rsquo;s a comforting thought to know that your feedback isn&amp;rsquo;t totally unsolicited.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Yes, I&amp;rsquo;m quoting my own review.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;So, basically just playing myself&amp;hellip; Also I knew that, if nothing more, I can proofread and copy edit.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;There &lt;em&gt;are&lt;/em&gt; lots of good resources out there re. overcoming this obstacle, though (e.g. &lt;a href=&#34;http://www.firsttimersonly.com/&#34;&gt;First Timers Only&lt;/a&gt;; or &lt;a href=&#34;https://twitter.com/cvwickham&#34;&gt;Charlotte Wickham&lt;/a&gt;&amp;rsquo;s &lt;a href=&#34;http://cwick.co.nz/talks/collab-code-user17/#/&#34;&gt;Collaborative Coding&lt;/a&gt; from useR!2017 is esp. 👍 for the R-user).
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;OK, so I don&amp;rsquo;t have a parallel world wherein a very experienced package-developer version of me is running around getting &lt;em&gt;less&lt;/em&gt; out of the process, &lt;em&gt;but&lt;/em&gt; if you already deeply understand package structure, you&amp;rsquo;re unlikely to stumble upon quite so many basic &amp;ldquo;a-ha&amp;rdquo; moments.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;👋 &lt;a href=&#34;https://github.com/noamross&#34;&gt;Noam Ross&lt;/a&gt;, &lt;a href=&#34;https://github.com/sckott&#34;&gt;Scott Chamberlain&lt;/a&gt;, &lt;a href=&#34;https://github.com/karthik&#34;&gt;Karthik Ram&lt;/a&gt;, &amp;amp; &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maëlle Salmon&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>Tackling the Research Compendium at runconf17</title>
      <link>https://ropensci.org/blog/2017/06/20/checkers/</link>
      <pubDate>Tue, 20 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/06/20/checkers/</guid>
      <description>
        
        

&lt;p&gt;Two years ago at &lt;a href=&#34;https://twitter.com/hashtag/runconf15&#34;&gt;#runconf15&lt;/a&gt;, there was a great discussion about best practices for organizing R-based analysis projects that yielded a &lt;a href=&#34;https://github.com/ropensci/rrrpkg&#34;&gt;nice guidance document&lt;/a&gt; describing &lt;em&gt;research compendia&lt;/em&gt;. Compendia, as we described them, were minimal products of reproducible research, using parts of R package structure to organize the inputs, analyses, and outputs of research projects.&lt;/p&gt;

&lt;p&gt;Since then, we&amp;rsquo;ve seen more examples and models of research compendia emerge (the organization of such projects is &lt;a href=&#34;https://discuss.ropensci.org/t/resources-on-project-directory-organization/340&#34;&gt;something of an obsession&lt;/a&gt; for some of the community). In parallel, there&amp;rsquo;s been much progress on a number of fronts with R &lt;em&gt;packages&lt;/em&gt;: rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/onboarding&#34;&gt;package review process&lt;/a&gt; has expanded and we&amp;rsquo;ve worked out many kinks. Infrastructure for automated testing of package code has been developed and field tested. So at &lt;a href=&#34;unconf17.ropensci.org&#34;&gt;#runconf17&lt;/a&gt;, we wanted to see how much of this progress in review, testing, and automation could apply to research compendia.&lt;/p&gt;

&lt;p&gt;It turns out there&amp;rsquo;s a &lt;em&gt;lot&lt;/em&gt; to do here, and a lot of interest! We put up a proposal as a &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/5&#34;&gt;GitHub issue&lt;/a&gt;; before the unconf even began, the thread had over 50 posts and the discussion had yielded two design documents led by Hadley Wickham on &lt;a href=&#34;https://docs.google.com/document/d/1LzZKS44y4OEJa4Azg5reGToNAZL0e0HSUwxamNY7E-Y/edit&#34;&gt;research compendium organization&lt;/a&gt; and &lt;a href=&#34;https://docs.google.com/document/d/1avYAqjTS7zSZn7JAAOZhFPkhkPvYwaPVrSpo31Cu0Yc/edit#&#34;&gt;automated build systems&lt;/a&gt;. There were easily four or five projects wrapped up in the proposal.&lt;/p&gt;

&lt;p&gt;The thread also revealed many schools of thought. As &lt;a href=&#34;https://twitter.com/nj_tierney&#34;&gt;Nick&lt;/a&gt; put it, &amp;ldquo;The problem with &lt;a href=&#34;https://www.rstudio.com/resources/videos/opinionated-analysis-development/&#34;&gt;opinionated analysis development&lt;/a&gt; is that everyone has an opinion.&amp;rdquo; We never reached consensus on basic issues like directory structure and build systems.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://imgur.com/RpjyfL8.jpeg&#34; alt=&#34;Wading our way through a thorny bramble of opinions (photo: Alice Daish)&#34; /&gt;
&lt;em&gt;Wading our way through a thorny bramble of opinions (photo: Alice Daish)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the face of a beast of a topic, and so many unresolved decisions, we decided to tackle a modest slice - a guide for compendium review, and a first pass at a package for automated checks of various best practices.&lt;/p&gt;

&lt;h3 id=&#34;reviewing-a-research-compendium&#34;&gt;Reviewing a research compendium&lt;/h3&gt;

&lt;p&gt;How does one do peer review for a research compendium in the face of so much heterogeneity in types and styles of analysis?  We spent the better part of our first day brainstorming everything that one might review, from the meta-topics (&amp;ldquo;Is there a clear question?&amp;rdquo;) to  minutiae (&amp;ldquo;Are &lt;code&gt;library()&lt;/code&gt; calls at the top of the script?&amp;ldquo;), and mapping these out on an ever more data-dense whiteboard:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://imgur.com/vb4E3JV.jpeg&#34; alt=&#34;Nick, Jennifer and Molly discussing analysis best practices (photo: Alice Daish)&#34; /&gt;
 &lt;em&gt;Nick, Jennifer, Molly and Alice discussing analysis best practices (photo: Alice Daish)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/dIT1sjK.jpg&#34; alt=&#34;The team prioritizing data analysis workflow best practice into tiers (photo: Nistara Randhawa)&#34; /&gt;
 &lt;em&gt;The team prioritizing data analysis workflow best practice into tiers (photo: Nistara Randhawa)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://imgur.com/8klgK7Q.jpeg&#34; alt=&#34;With our 7th team member, Wy T. Board (photo: Alice Daish)&#34; /&gt;
&lt;em&gt;With our 7th team member, Wy T. Board (photo: Alice Daish)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These review topics made their way into a &lt;a href=&#34;https://docs.google.com/document/d/1OYcWJUk-MiM2C1TIHB1Rn6rXoF5fHwRX-7_C12Blx8g/edit#heading=h.dyoxrtoo15mm&#34;&gt;Google doc&lt;/a&gt; which will form the basis of a review guide along the lines of rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/onboarding/#-useful-documents-in-this-repository&#34;&gt;package review guides&lt;/a&gt;.  One major organizing dimension we hit upon was &amp;ldquo;Tiers&amp;rdquo; - as the number of best practices can be overwhelming, it is better to prioritize them so that users have a way of advancing through escalating levels of detail.  Another was &amp;ldquo;automatability,&amp;rdquo; which is key to the other half of our work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/lWpcEfb.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;checkers-r-cmd-check-for-your-data-analysis&#34;&gt;&lt;strong&gt;checkers&lt;/strong&gt;: R CMD check for your data analysis&lt;/h3&gt;

&lt;p&gt;One of the lessons of the rOpenSci package review process has been that reviews work better when we let reviewers focus on the tasks that humans are best at, and find ways to automate tedious or tiny tasks. So for the second aim of our project, we built a package to run the automatable components of  review and create reports for analysts and reviewers.  The package, &lt;strong&gt;&lt;a href=&#34;https://github.com/ropenscilabs/checkers&#34;&gt;checkers&lt;/a&gt;&lt;/strong&gt;, is meant to be, as Hadley coined it, &amp;ldquo;&lt;code&gt;R CMD check&lt;/code&gt; for your data analysis.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;checkers&lt;/strong&gt; scans the project directory and delivers advice on how to improve project code, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; checkers::gp_check()
─────────────────────────────────────────────────────────────────────
It is good practice to

  ✖ Place your project under version control. You are using
    neither git nor svn. See http://happygitwithr.com/ for more
    info
  ✖ Use preferred packages. xml2 is preferred to XML.
──────────────────────────────────────────────────────────────────
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One area we had to tackle was the need for both &lt;em&gt;opinionated defaults&lt;/em&gt; and &lt;em&gt;configurability&lt;/em&gt;.  So we built in the ability for individuals or teams to select or define their own best practices in a shared YAML configuration file.&lt;/p&gt;

&lt;h3 id=&#34;stone-soup-development-at-the-unconf&#34;&gt;Stone soup development at the unconf&lt;/h3&gt;

&lt;p&gt;The review guide and &lt;strong&gt;checkers&lt;/strong&gt; are works in progress, but both made great leaps forward in two days thanks to the tremendous catalyzing environment of the unconf.  While our team of six only formed the morning of the first day, the &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/5&#34;&gt;pre-unconf discussion&lt;/a&gt; meant that many more people from the community shaped contours of the project.&lt;/p&gt;

&lt;p&gt;Also, one of the great things about the unconf is that so many experienced developers are on hand to chip in with their areas of expertise. Hadley Wickham joined in to brainstorm some of the initial best practices to include in our guide. Later, we decided to base our checks system on Gábor Csárdi&amp;rsquo;s &lt;strong&gt;&lt;a href=&#34;https://github.com/MangoTheCat/goodpractice/&#34;&gt;goodpractice&lt;/a&gt;&lt;/strong&gt;, and Gábor worked with us to build a flexible extension system into that package. Jim Hester was on hand to answer questions about &lt;strong&gt;&lt;a href=&#34;https://github.com/jimhester/lintr/&#34;&gt;lintr&lt;/a&gt;&lt;/strong&gt; internals, and we&amp;rsquo;ll be sending some changes upstream to that as well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/uWYKR1e.jpg&#34; alt=&#34;Laura and Gábor practicing goodpractices (photo: Nistara Randhawa)&#34; /&gt;
&lt;em&gt;Laura and Gábor practicing goodpractices (photo: Nistara Randhawa)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://imgur.com/Dw0yTwi.jpeg&#34; alt=&#34;Team reviewing framework and package development examples (photo: Alice Daish)&#34; /&gt;
&lt;em&gt;Team reviewing framework and package development examples (photo: Alice Daish)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re excited about the potential for this project, and just as excited about the potential of what else we&amp;rsquo;ll make with our new friends and collaborators. Thanks to rOpenSci and everyone who made #runconf17 such a tremendously productive and fun event!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>ropenaq, a breath of fresh air/R</title>
      <link>https://ropensci.org/blog/2017/02/21/ropenaq/</link>
      <pubDate>Tue, 21 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/02/21/ropenaq/</guid>
      <description>
        
        

&lt;p&gt;Do you fancy open data, R, and breathing? Then you might be interested in &lt;code&gt;ropenaq&lt;/code&gt; which provides access to open air quality data via OpenAQ! Also note that in French, R and air are homophones, therefore we French speakers can make puns like the one in the title. Please re-read it with a French accent and don&amp;rsquo;t judge me.&lt;/p&gt;

&lt;p&gt;In this post I&amp;rsquo;ll motivate the existence of the package, then show you the basics of its use, and finally show off with some pretty figures. You can skip any part but if I were you I wouldn&amp;rsquo;t!&lt;/p&gt;

&lt;h2 id=&#34;discovering-openaq-and-ropensci&#34;&gt;Discovering OpenAQ and rOpenSci&lt;/h2&gt;

&lt;p&gt;I work as a data manager and statistician for an epidemiology project called &lt;a href=&#34;http://www.chaiproject.org/&#34;&gt;CHAI&lt;/a&gt; for &lt;em&gt;Cardio-vascular health effects of air pollution in Telangana, India&lt;/em&gt;. We have generated quite a lot of data of our own, including ambient air quality monitoring at three fixed sites in rural Telangana for one year. Being able to compare these numbers with longer-term measures in the closest city, Hyderabad, was something that probably would be useful at some point, so besides data cleaning, I had a look at other data sources.&lt;/p&gt;

&lt;p&gt;However, in that part of the world, you don&amp;rsquo;t get much air quality data, and even less open and easily accessible air quality data. One pretty easily gets data from the US consulate in Hyderabad (well, easily thanks to &lt;a href=&#34;https://github.com/ropensci/tabulizer&#34;&gt;&lt;code&gt;tabulizer&lt;/code&gt;&lt;/a&gt;, since parts of the data are pdf!). But going on the website of the Indian Central Pollution Control Board I embarked on a kind of scavenger hunt clicking around which felt quite frustrating. This also happens with websites from other countries, with a different scavenger hunt for each website. Sure you learn about &lt;code&gt;tabulizer&lt;/code&gt;, &lt;a href=&#34;https://github.com/ropensci/rselenium&#34;&gt;&lt;code&gt;rSelenium&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/johndharrison/seleniumPipes&#34;&gt;&lt;code&gt;seleniumPipes&lt;/code&gt;&lt;/a&gt; and other awesome &amp;ndash; often rOpenSci-branded &amp;ndash; packages along the way but it just doesn&amp;rsquo;t feel right to have to spend so much time doing this!&lt;/p&gt;

&lt;p&gt;At the end of 2015, during a symposium about the future of environmental epidemiology, &lt;a href=&#34;https://twitter.com/cathryn_tonne&#34;&gt;my boss&lt;/a&gt; mentioned OpenAQ, a platform aggregating and sharing open air quality data from official sources around the world. A bit after that, I found myself looking at the API documentation and got really excited. I contacted OpenAQ founders and asked them whether a R package already existed, and Christa Hasenkopf told me I could open an issue about it which I had zero intention of doing, I wanted to make it happen &lt;em&gt;right now&lt;/em&gt;! So I started writing the package.&lt;/p&gt;

&lt;p&gt;At the same period of my life, a bit earlier, on one week-end I had been googling ways to download scientific literature metadata in R because of a discussion I&amp;rsquo;d had with my husband. Doing that I had discovered the website of rOpenSci (see all the literature access packages &lt;a href=&#34;https://ropensci.org/packages/#literature&#34;&gt;here&lt;/a&gt;) and had really thought it looked like an awesome project, I even saw there was an onboarding system where you could submit your package and make it part of the suite&amp;hellip; I had more urgent things to do on that week-end, like finishing to write my PhD thesis, but the idea stuck with me.&lt;/p&gt;

&lt;p&gt;So really soon after writing the first version of &lt;code&gt;ropenaq&lt;/code&gt;, I submitted my package to rOpenSci! I was a bit scared, I had to google parts of the words of the guidelines, like &amp;ldquo;continuous integration&amp;rdquo;, but there are many resources out there and from all rOpenSci reviews I&amp;rsquo;ve read you can ask for help at any point of the process.&lt;/p&gt;

&lt;p&gt;I received the reviews of &lt;a href=&#34;https://twitter.com/andyteucher&#34;&gt;Andy Teucher&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/polesasunder&#34;&gt;Andrew MacDonald&lt;/a&gt; a few weeks later. Their comments were as nice as they were useful! You can read the review &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/24&#34;&gt;here&lt;/a&gt; and see what I mean by nice and useful! I improved &lt;code&gt;ropenaq&lt;/code&gt; and really dug the rOpenSci review process. Not only did my package get better, but my R skills and knowledge of best practice also improved, which is useful every day of my life as a data manager and statistician. And I became a contributor of both OpenAQ and rOpenSci, two projects whose goals resonated with me!&lt;/p&gt;

&lt;p&gt;So end of the story, now let&amp;rsquo;s move to the more interesting stuff, what can you do with &lt;code&gt;ropenaq&lt;/code&gt;? Download all the data from OpenAQ! Well not all the data in one go, if you really wanted to do that you should look at their &lt;a href=&#34;http://openaq-data.s3.amazonaws.com/index.html&#34;&gt;daily data dumps&lt;/a&gt; or contact them, but here&amp;rsquo;s how you would deal with the query &amp;ldquo;How are PM2.5 values in Hyderabad?&amp;rdquo;. OpenAQ has data for 7 pollutants: PM2.5 (particles smaller than 2.5μm), PM10 (particles smaller than 10μm), &lt;em&gt;SO&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt; (sulfur dioxide), &lt;em&gt;NO&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt; (nitrogen dioxide), &lt;em&gt;O&lt;/em&gt;&lt;sub&gt;3&lt;/sub&gt; (ozone), CO (carbon monoxide), BC (black carbon). All of them are bad for human health with effects than can be revealed in the short or long term. In the whole post I&amp;rsquo;ll only show examples with PM2.5, but other pollutants can be as interesting.&lt;/p&gt;

&lt;h2 id=&#34;getting-data-via-ropenaq&#34;&gt;Getting data via &lt;code&gt;ropenaq&lt;/code&gt;&lt;/h2&gt;

&lt;h3 id=&#34;install-the-package&#34;&gt;Install the package&lt;/h3&gt;

&lt;p&gt;Install the package with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;ropenaq&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or install the development version using devtools with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;devtools&amp;quot;)
install_github(&amp;quot;ropensci/ropenaq&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Currently the development version, 1.0.4 implements OpenAQ new limit per API call of 10,000, while the CRAN version, 1.0.3, only allows to return 1,000 lines per API call. The development version should soon be submitted to CRAN.&lt;/p&gt;

&lt;h2 id=&#34;find-the-air-quality-stations-with-available-data&#34;&gt;Find the air quality stations with available data&lt;/h2&gt;

&lt;p&gt;The package contains three functions that are useful to find the stations at which there is data: &lt;code&gt;aq_countries&lt;/code&gt;, &lt;code&gt;aq_cities&lt;/code&gt; and &lt;code&gt;aq_locations&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So say I&amp;rsquo;m looking for Indian data, I could choose to first check there&amp;rsquo;s data for India.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;ropenaq&amp;quot;)
library(&amp;quot;dplyr&amp;quot;)
import::from(dplyr, filter)
countries &amp;lt;- aq_countries()
filter(countries, name == &amp;quot;India&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 5
##    name  code cities locations   count
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;
## 1 India    IN     93        93 2766369
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the other functions of &lt;code&gt;ropenaq&lt;/code&gt;, what you&amp;rsquo;ll need to use for saying you want data for India is the country code, &lt;code&gt;IN&lt;/code&gt;. By the way if you ever need to convert country names and codes, have a look at the &lt;a href=&#34;https://github.com/vincentarelbundock/countrycode&#34;&gt;&lt;code&gt;countrycode&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now we could look for cities for which there is data in India.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;in_cities &amp;lt;- aq_cities(country = &amp;quot;IN&amp;quot;)
filter(in_cities, city == &amp;quot;Hyderabad&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 5
##        city country locations  count   cityURL
##       &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;     &amp;lt;chr&amp;gt;
## 1 Hyderabad      IN        10 159191 Hyderabad
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In &lt;code&gt;ropenaq&lt;/code&gt; other functions, what you&amp;rsquo;ll use for the city is its &lt;code&gt;cityURL&lt;/code&gt;. Now we can have a look at all stations for Hyderabad.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;aq_locations(city = &amp;quot;Hyderabad&amp;quot;) %&amp;gt;%
  knitr::kable()
&lt;/code&gt;&lt;/pre&gt;

&lt;table class=&#34;table&#34;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; location &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; pm25 &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; pm10 &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; no2 &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; so2 &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; o3 &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; co &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; bc &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; lastUpdated &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; firstUpdated &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Bollaram Industrial Area &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-17 05:15:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-16 07:15:00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Bollaram Industrial Area, Hyderabad - TSPCB &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-20 19:45:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-17 05:45:00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; ICRISAT Patancheru &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-17 05:30:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-15 18:30:00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; ICRISAT Patancheru, Hyderabad - TSPCB &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-20 19:30:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-17 05:30:00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; IDA Pashamylaram,Hyderabad &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2016-09-20 09:45:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2016-09-20 04:45:00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; IDA Pashamylaram, Hyderabad - TSPCB &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-20 19:30:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2016-09-20 10:15:00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Sanathnagar - Hyderabad - TSPCB &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-20 19:30:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2016-03-22 09:50:00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TSPCBPashamylaram &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2016-09-20 04:45:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2016-09-18 18:30:00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; US Diplomatic Post: Hyderabad &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-20 19:30:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2015-12-11 21:30:00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; ZooPark &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; TRUE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; FALSE &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2017-02-20 19:15:00 &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 2016-03-21 18:30:00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In this table you see the parameters available for each station, and the dates for which you can get data. One could also directly filter stations with, say, PM2.5 information:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;aq_locations(city = &amp;quot;Hyderabad&amp;quot;, parameter = &amp;quot;pm25&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 19
##                                       location      city country count
##                                          &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
## 1                     Bollaram Industrial Area Hyderabad      IN     5
## 2  Bollaram Industrial Area, Hyderabad - TSPCB Hyderabad      IN   151
## 3                           ICRISAT Patancheru Hyderabad      IN    38
## 4        ICRISAT Patancheru, Hyderabad - TSPCB Hyderabad      IN   151
## 5                   IDA Pashamylaram,Hyderabad Hyderabad      IN    11
## 6          IDA Pashamylaram, Hyderabad - TSPCB Hyderabad      IN  6694
## 7              Sanathnagar - Hyderabad - TSPCB Hyderabad      IN    74
## 8                            TSPCBPashamylaram Hyderabad      IN    35
## 9                US Diplomatic Post: Hyderabad Hyderabad      IN 10305
## 10                                     ZooPark Hyderabad      IN 13674
## # ... with 15 more variables: sourceNames &amp;lt;list&amp;gt;, lastUpdated &amp;lt;dttm&amp;gt;,
## #   firstUpdated &amp;lt;dttm&amp;gt;, sourceName &amp;lt;chr&amp;gt;, latitude &amp;lt;dbl&amp;gt;,
## #   longitude &amp;lt;dbl&amp;gt;, pm25 &amp;lt;lgl&amp;gt;, pm10 &amp;lt;lgl&amp;gt;, no2 &amp;lt;lgl&amp;gt;, so2 &amp;lt;lgl&amp;gt;,
## #   o3 &amp;lt;lgl&amp;gt;, co &amp;lt;lgl&amp;gt;, bc &amp;lt;lgl&amp;gt;, cityURL &amp;lt;chr&amp;gt;, locationURL &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I do agree that the workflow that I&amp;rsquo;ve just presented is a bit tedious, but I really wanted you to know these three functions and to see how many countries/cities are represented. But actually things can be easier! The &lt;code&gt;aq_measurements&lt;/code&gt; function I&amp;rsquo;m about to present you has a &lt;code&gt;longitude&lt;/code&gt;, &lt;code&gt;latitude&lt;/code&gt; and &lt;code&gt;radius&lt;/code&gt; arguments allowing you to make a query directly inside a circle of your choice on the Earth&amp;rsquo;s surface! So if you have, for instance, names of cities in German, you don&amp;rsquo;t need to worry about their English names, just use your favorite geocoding package (may I suggest &lt;a href=&#34;https://github.com/ropensci/opencage&#34;&gt;&lt;code&gt;opencage&lt;/code&gt;&lt;/a&gt;?) and you&amp;rsquo;ll be good to go.&lt;/p&gt;

&lt;h3 id=&#34;get-air-quality-data&#34;&gt;Get air quality data!&lt;/h3&gt;

&lt;p&gt;For getting measurements themselves, you can either use &lt;code&gt;aq_latest&lt;/code&gt; or &lt;code&gt;aq_measurements&lt;/code&gt;. &lt;code&gt;aq_latest&lt;/code&gt; only gives you the latest measurements for a given place, &lt;code&gt;aq_measurements&lt;/code&gt; gives all the measurements for a given place, and time period if you indicate one, this up to 10,000 data points per page. So if you make a query for a station with loads of data, you&amp;rsquo;ll have to loop or more elegantly/modernly map over pages. Don&amp;rsquo;t worry, &lt;code&gt;ropenaq&lt;/code&gt; helps you know just how many pages there are. Say I want all PM2.5 data for Hyderabad&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# find how many measurements there are
first_test &amp;lt;- aq_measurements(city = &amp;quot;Hyderabad&amp;quot;,
                              date_from = &amp;quot;2016-01-01&amp;quot;,
                              date_to = &amp;quot;2016-12-31&amp;quot;,
                              parameter = &amp;quot;pm25&amp;quot;)
count &amp;lt;- attr(first_test, &amp;quot;meta&amp;quot;)$found
print(count)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 24685
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;purrr&amp;quot;)

# map queries over all pages
allthedata &amp;lt;- (1:ceiling(count/10000)) %&amp;gt;%
  purrr::map(function(page){
    aq_measurements(city = &amp;quot;Hyderabad&amp;quot;,
                   date_from = &amp;quot;2016-01-01&amp;quot;,
                   date_to = &amp;quot;2016-12-31&amp;quot;,
                   parameter = &amp;quot;pm25&amp;quot;,
                   page = page,
                   limit = 10000)
    }) %&amp;gt;%
  bind_rows()

allthedata
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 24,685 × 12
##                               location parameter value  unit country
##                                  &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;
## 1  IDA Pashamylaram, Hyderabad - TSPCB      pm25  56.0 µg/m³      IN
## 2                              ZooPark      pm25  56.0 µg/m³      IN
## 3        US Diplomatic Post: Hyderabad      pm25 158.6 µg/m³      IN
## 4  IDA Pashamylaram, Hyderabad - TSPCB      pm25  56.0 µg/m³      IN
## 5                              ZooPark      pm25  56.0 µg/m³      IN
## 6  IDA Pashamylaram, Hyderabad - TSPCB      pm25  56.0 µg/m³      IN
## 7                              ZooPark      pm25  56.0 µg/m³      IN
## 8  IDA Pashamylaram, Hyderabad - TSPCB      pm25  56.0 µg/m³      IN
## 9        US Diplomatic Post: Hyderabad      pm25 148.8 µg/m³      IN
## 10                             ZooPark      pm25  56.0 µg/m³      IN
## # ... with 24,675 more rows, and 7 more variables: city &amp;lt;chr&amp;gt;,
## #   dateUTC &amp;lt;dttm&amp;gt;, dateLocal &amp;lt;dttm&amp;gt;, latitude &amp;lt;dbl&amp;gt;, longitude &amp;lt;dbl&amp;gt;,
## #   cityURL &amp;lt;chr&amp;gt;, locationURL &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yeah! We got the data! And now we can make a plot! Since all &lt;code&gt;ropenaq&lt;/code&gt; functions return tidy &lt;code&gt;data.frame&lt;/code&gt;s, you can use them with any of your favourite plotting libraries. Mine are &lt;code&gt;ggplot2&lt;/code&gt; coupled with &lt;code&gt;viridis&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll filter out the negative values, which are actually invalid values, because the original data source returns &amp;ldquo;-999&amp;rdquo; instead of NA and OpenAQ doesn&amp;rsquo;t make any change to the original data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;ggplot2&amp;quot;)
library(&amp;quot;viridis&amp;quot;)
allthedata %&amp;gt;%
  filter(value != - 999) %&amp;gt;%
  group_by(day = as.Date(dateLocal), location) %&amp;gt;%
  filter(n() &amp;gt; 0) %&amp;gt;%
  summarize(average = mean(value)) %&amp;gt;%
ggplot() +
  geom_line(aes(x = day, y = average, col = location)) +
  facet_grid(location ~ .) +
  geom_hline(yintercept = 25) +
  scale_color_viridis(discrete = TRUE) +
  theme(legend.position = &amp;quot;none&amp;quot;,
        strip.text.y = element_text(angle=0))+
  ylab(expression(paste(&amp;quot;Average daily PM2.5 concentration (&amp;quot;, mu, &amp;quot;g/&amp;quot;,m^3,&amp;quot;)&amp;quot;))) +
  xlab(&amp;quot;Time (days)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-02-21-ropenaq/unnamed-chunk-6-1.png&#34; alt=&#34;plot of chunk unnamed-chunk-6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What can we conclude from looking at this graph? One point is that the WHO daily limit of 25μg/m3, indicated by the black horizontal line on the plot, is very often exceeded. Another point is that some stations produce so little data that we don&amp;rsquo;t even get a curve for them (note that in some cases gap in the data can be due to an OpenAQ issue rather than a provider issue). Both points can be interesting for fighting air pollution: having proof that the air is unhealthy might help trigger action against air pollution; and knowing the devices for measuring it were broken or that data wasn&amp;rsquo;t communicated is something one can complain about to official authorities.&lt;/p&gt;

&lt;p&gt;Currently OpenAQ doesn&amp;rsquo;t have all the data sources available in the world, nor all the existing historical data. But the number of sources is constantly increasing thanks to volunteers building new adapters between sources and OpenAQ, or uploading their data. Yes, you can be such a &lt;a href=&#34;https://github.com/openaq/openaq-info/blob/master/FAQ.md&#34;&gt;volunteer&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;And with the current data available on OpenAQ, you&amp;rsquo;d already get much more data for the same efforts than on, say, the Indian Central Pollution Control Board website. So the existence of OpenAQ and of &lt;code&gt;ropenaq&lt;/code&gt; are already really good news. For instance as an epidemiologist planning a study about the link between PM2.5 concentration and a disease. For choosing a sample size which would allow you to detect the assumed effect, you need to have a rough idea of the exposure of your study population to PM2.5. Maybe you got data from a collaborator for a rural area and you want to also recruit people in a more exposed, say urban area. With OpenAQ you could already use the average concentration of the last year in e.g. Delhi for your calculations.&lt;/p&gt;

&lt;h2 id=&#34;some-animated-plots-of-openaq-data&#34;&gt;Some animated plots of OpenAQ data&lt;/h2&gt;

&lt;p&gt;I promised I would show off cool plots&amp;hellip; Let&amp;rsquo;s say that in general plotting air quality numbers that kill people isn&amp;rsquo;t that cool, but one can also have fun with air quality data.&lt;/p&gt;

&lt;h3 id=&#34;data-surfing&#34;&gt;Data surfing&lt;/h3&gt;

&lt;p&gt;One day I was testing out &lt;code&gt;gganimate&lt;/code&gt; for decorating a very simple air quality time series and while discussing options with &lt;a href=&#34;https://twitter.com/sciencerely&#34;&gt;Christa Hasenkopf&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/dirk_sch&#34;&gt;Dirk Schumacher&lt;/a&gt;&amp;hellip; the data surfer was born! In the meantime I started using &lt;a href=&#34;https://github.com/ropensci/magick&#34;&gt;&lt;code&gt;magick&lt;/code&gt;&lt;/a&gt; instead of &lt;code&gt;gganimate&lt;/code&gt;, probably because of the elegance of &lt;a href=&#34;https://rud.is/b/2016/07/27/u-s-drought-animations-with-the-witchs-brew-purrr-broom-magick/&#34;&gt;this post&lt;/a&gt;. Also, &lt;code&gt;magick&lt;/code&gt; is an rOpenSci package and this is the rOpenSci blog!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;emojifont&amp;quot;)
library(&amp;quot;magick&amp;quot;)
library(&amp;quot;ggthemes&amp;quot;)

load.emojifont(&#39;OpenSansEmoji.ttf&#39;)

lima &amp;lt;- aq_measurements(country = &amp;quot;PE&amp;quot;, limit = 1000)
lima &amp;lt;- filter(lima, location == &amp;quot;US Diplomatic Post: Lima&amp;quot;)
lima &amp;lt;- mutate(lima, label = emoji(&amp;quot;surfer&amp;quot;))

figure_onetime &amp;lt;- function(now, lima){

  p &amp;lt;- ggplot(lima)+
  geom_area(aes(x = dateLocal,
                 y = value),
             size = 2, fill = &amp;quot;navyblue&amp;quot;)+
  geom_text(aes(x = dateLocal,
                y = value+1,
                label = label),
                col = &amp;quot;goldenrod&amp;quot;,
            family=&amp;quot;OpenSansEmoji&amp;quot;, size=20,
            data = filter_(lima, ~dateLocal == now))+
  ylab(expression(paste(&amp;quot;PM2.5 concentration (&amp;quot;, mu, &amp;quot;g/&amp;quot;,m^3,&amp;quot;)&amp;quot;)))+
  xlab(&#39;Local date and time, Lima, Peru&#39;)+
  ylim(0, 50)+
  ggtitle(as.character(now))+
  theme_hc(bgcolor = &amp;quot;darkunica&amp;quot;) +
  scale_colour_hc(&amp;quot;darkunica&amp;quot;)+
  theme(text = element_text(size=40)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  theme(plot.title=element_text(family=&amp;quot;OpenSansEmoji&amp;quot;,
                                face=&amp;quot;bold&amp;quot;))+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
  outfil &amp;lt;- gsub(&amp;quot;-&amp;quot;, &amp;quot;&amp;quot;, now)
  outfil &amp;lt;- gsub(&amp;quot; &amp;quot;, &amp;quot;&amp;quot;, outfil)
  outfil &amp;lt;- gsub(&amp;quot;[:punct:]&amp;quot;, &amp;quot;&amp;quot;, outfil)
  outfil &amp;lt;- paste0(outfil, &amp;quot;.png&amp;quot;)
  ggsave(outfil, p, width=8, height=5)

  outfil
}

sort(unique(lima$dateLocal)) %&amp;gt;%
  map(figure_onetime, lima = lima)  %&amp;gt;%
  map(image_read) %&amp;gt;%
  image_join() %&amp;gt;%
  image_animate(fps=2) %&amp;gt;%
  image_write(&amp;quot;surf.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-02-21-ropenaq/surf.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The use of such a plot might be to illustrate a very serious talk about the need for open air quality data. I promise you&amp;rsquo;ll get attention from the audience.&lt;/p&gt;

&lt;h3 id=&#34;fireworks-across-the-us-on-the-4th-of-july&#34;&gt;Fireworks across the US on the 4th of July&lt;/h3&gt;

&lt;p&gt;When looking at the time series of PM2.5 over years in say Delhi, one can see peaks corresponding to fireworks for celebrating Diwali. Last summer I decided to explore PM2.5 values on the 4th of July in the US.&lt;/p&gt;

&lt;p&gt;First I got the necessary data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# find how many measurements there are
first_test &amp;lt;- aq_measurements(country = &amp;quot;US&amp;quot;,
                              has_geo = TRUE,
                              parameter = &amp;quot;pm25&amp;quot;,
                              limit = 10000,
                              date_from = &amp;quot;2016-07-04&amp;quot;,
                              date_to = &amp;quot;2016-07-06&amp;quot;,
                              value_from = 0)
count &amp;lt;- attr(first_test, &amp;quot;meta&amp;quot;)$found
print(count)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 25446
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;purrr&amp;quot;)

# map queries over all pages
usdata &amp;lt;- (1:ceiling(count/10000)) %&amp;gt;%
  purrr::map(function(page){
    aq_measurements(country = &amp;quot;US&amp;quot;,
                    has_geo = TRUE,
                    parameter = &amp;quot;pm25&amp;quot;,
                    limit = 10000,
                    date_from = &amp;quot;2016-07-04&amp;quot;,
                    date_to = &amp;quot;2016-07-06&amp;quot;,
                    value_from = 0,
                    page = page)
    }) %&amp;gt;%
  bind_rows()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I then summarize it for having one value by hour only and replace values over 80 by 80, because otherwise it&amp;rsquo;s hard to find a good colour scale for the graph later.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;usdata &amp;lt;- usdata %&amp;gt;%
  group_by(hour = update(dateUTC, minute = 0),
           location, longitude, latitude, dateUTC) %&amp;gt;%
  summarize(value = mean(value))

usdata &amp;lt;- usdata %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(hour = update(hour, hour = lubridate::hour(hour) - 5)) %&amp;gt;%
  mutate(value = ifelse(value &amp;gt; 80, 80, value))
save(usdata, file = &amp;quot;data/4th_july.RData&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is how I make the visualization itself, using once again &lt;code&gt;magick&lt;/code&gt;, and also &lt;a href=&#34;https://github.com/hrbrmstr/albersusa&#34;&gt;&lt;code&gt;albersusa&lt;/code&gt;&lt;/a&gt;. The package has to be installed from Github: &lt;code&gt;devtools::install_github(&amp;quot;hrbrmstr/albersusa&amp;quot;)&lt;/code&gt;. Note that I don&amp;rsquo;t show Alaska and Hawaii.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;load( &amp;quot;data/4th_july.RData&amp;quot;)

mintime &amp;lt;- lubridate::ymd_hms(&amp;quot;2016 07 04 17 00 00&amp;quot;)
maxtime &amp;lt;- lubridate::ymd_hms(&amp;quot;2016 07 05 07 00 00&amp;quot;)

usdata &amp;lt;- filter(usdata, hour &amp;gt;= mintime)
usdata &amp;lt;- filter(usdata, hour &amp;lt;= maxtime)

library(&amp;quot;albersusa&amp;quot;)
us &amp;lt;- usa_composite()
us_map &amp;lt;- fortify(us, region=&amp;quot;name&amp;quot;)
us_map &amp;lt;- filter(us_map, !id %in% c(&amp;quot;Alaska&amp;quot;, &amp;quot;Hawaii&amp;quot;))
gg &amp;lt;- ggplot()
gg &amp;lt;- gg + geom_map(data=us_map, map=us_map,
                    aes(x=long, y=lat, map_id=id),
                    color=&amp;quot;white&amp;quot;, size=0.1, fill=&amp;quot;black&amp;quot;)
gg &amp;lt;- gg + theme_map(base_size = 40)
gg &amp;lt;- gg + theme(plot.title = element_text(color=&amp;quot;white&amp;quot;))
gg &amp;lt;- gg + theme(legend.position = &amp;quot;bottom&amp;quot;)
gg &amp;lt;- gg + theme(panel.background = element_rect(fill = &amp;quot;black&amp;quot;))
gg &amp;lt;- gg + theme(plot.background=element_rect(fill=&amp;quot;black&amp;quot;))
gg &amp;lt;- gg + theme(legend.background= element_rect(fill=&amp;quot;black&amp;quot;, colour= NA))
gg &amp;lt;- gg + theme(legend.text = element_text(colour=&amp;quot;white&amp;quot;))
gg &amp;lt;- gg + theme(legend.title = element_text(colour=&amp;quot;white&amp;quot;))

# find the maximal number of data points for the period
lala &amp;lt;- group_by(usdata, location, latitude) %&amp;gt;% summarize(n = n())
# and keep only stations with data for each hour
usdata &amp;lt;- group_by(usdata, location, latitude) %&amp;gt;%
  filter(n() == max(lala$n),
         latitude &amp;lt; 50, longitude &amp;gt; - 130) %&amp;gt;%
  ungroup()

firework_onehour &amp;lt;- function(now, gg, usdata){
  p &amp;lt;- gg+
  geom_point(data = filter_(usdata, ~ hour == now),
             aes(x=longitude,
                  y =latitude,
                  colour = value,
                  size = value))+
    ggtitle(as.character(now)) +
  coord_map()+
  viridis::scale_color_viridis(expression(paste(&amp;quot;PM2.5 concentration (&amp;quot;, mu, &amp;quot;g/&amp;quot;,m^3,&amp;quot;)Set to 80 if &amp;gt;80&amp;quot;)),
                               option = &amp;quot;inferno&amp;quot;,
                               limits = c(min(usdata$value),
                                          max(usdata$value))) +
    scale_size(limits = c(min(usdata$value),
                                          max(usdata$value)))
outfil &amp;lt;- gsub(&amp;quot;-&amp;quot;, &amp;quot;&amp;quot;, now)
  outfil &amp;lt;- gsub(&amp;quot; &amp;quot;, &amp;quot;&amp;quot;, outfil)
  outfil &amp;lt;- gsub(&amp;quot;[:punct:]&amp;quot;, &amp;quot;&amp;quot;, outfil)
  outfil &amp;lt;- paste0(outfil, &amp;quot;_fireworks.png&amp;quot;)
  ggsave(outfil, p, width=12, height=6)

  outfil
}

sort(unique(usdata$hour)) %&amp;gt;%
  map(firework_onehour, gg = gg, usdata = usdata)  %&amp;gt;%
  map(image_read) %&amp;gt;%
  image_join() %&amp;gt;%
  image_animate(fps=1) %&amp;gt;%
  image_write(&amp;quot;fireworks.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-02-21-ropenaq/fireworks.gif&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;On this gif, where the title indicates the time in New York City, one sees the East to West wave of PM2.5 peaks due to fireworks as soon as it gets dark in each city, which happens at different times across the US. I think it&amp;rsquo;s an interesting way of looking at this holiday. Holidays and fireworks are one thing, but one could also imagine coupling &lt;code&gt;ropenaq&lt;/code&gt; data with data about fires, or weather, for which rOpenSci got you covered with &lt;code&gt;rnoaa&lt;/code&gt; (&lt;a href=&#34;https://github.com/ropensci/rnoaa&#34;&gt;https://github.com/ropensci/rnoaa&lt;/a&gt;) and &lt;code&gt;riem&lt;/code&gt; (&lt;a href=&#34;https://github.com/ropensci/riem&#34;&gt;https://github.com/ropensci/riem&lt;/a&gt;). Or for parts of the world with a high density of locations, why not compare air quality with land-use information from Openstreetmap via &lt;a href=&#34;https://github.com/osmdatar/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt;&lt;/a&gt; and with transit information processed via &lt;a href=&#34;https://github.com/ropenscilabs/gtfsr&#34;&gt;&lt;code&gt;gtfsr&lt;/code&gt;&lt;/a&gt;?&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;h3 id=&#34;what-can-you-do&#34;&gt;What can YOU do?&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;d strongly encourage you to get involved with the open-source projects you think are useful and cool, and in particular with rOpenSci and OpenAQ since I know these are friendly places, which is even made official by &lt;a href=&#34;https://ropensci.org/blog/blog/2016/10/31/comm-call-v12&#34;&gt;Codes&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/ropenaq/blob/master/CONDUCT.md&#34;&gt;of&lt;/a&gt; &lt;a href=&#34;https://github.com/openaq/openaq-info/blob/master/CODE-OF-CONDUCT.md&#34;&gt;conduct&lt;/a&gt;. Tweet at both organizations, look at their website, you&amp;rsquo;ll meet people who&amp;rsquo;ll be more than happy to include you and your contributions. All OpenAQ &lt;a href=&#34;https://github.com/openaq/&#34;&gt;Github repos&lt;/a&gt; have contributing guides.&lt;/p&gt;

&lt;p&gt;If you want to get involved with &lt;code&gt;ropenaq&lt;/code&gt; itself, you&amp;rsquo;re welcome to do so! I&amp;rsquo;ve opened &lt;a href=&#34;https://github.com/ropensci/ropenaq/issues&#34;&gt;issues&lt;/a&gt; of possible enhancements of the package. Currently, two of them are I think more geared towards new-ish users of R that have an air quality background, one of them is more technical. And don&amp;rsquo;t hesitate to open an issue if you notice a bug or think of a new functionality! Also, I like to collect use cases of the package, feel free to share your &lt;code&gt;ropenaq&lt;/code&gt; examples.&lt;/p&gt;

&lt;h3 id=&#34;a-few-concluding-words&#34;&gt;A few concluding words&lt;/h3&gt;

&lt;p&gt;Note that &lt;code&gt;ropenaq&lt;/code&gt; isn&amp;rsquo;t the only R package providing access to open air quality data, you can have a look at:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://github.com/ropensci/rdefra&#34;&gt;&lt;code&gt;rdefra&lt;/code&gt; package&lt;/a&gt;, also part of the rOpenSci project,
allows to  to interact with the UK AIR pollution database from DEFRA, including historical measures. I actually reviewed this package for rOpenSci, see &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/68&#34;&gt;the review here&lt;/a&gt;. I tried to be as nice and helpful as Andy and Andrew and think Claudia did an awesome work with her package!&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://github.com/davidcarslaw/openair&#34;&gt;&lt;code&gt;openair&lt;/code&gt; package&lt;/a&gt;, on top of the plotting tools appreciated by air quality folks, gives access to the same data as &lt;code&gt;rdefra&lt;/code&gt; but relies on a local and compressed copy of the data on servers at King&amp;rsquo;s College (UK), periodically updated.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://github.com/masalmon/usaqmindia&#34;&gt;&lt;code&gt;usaqmindia&lt;/code&gt; package&lt;/a&gt; provides data from the US air quality monitoring program in India for Delhi, Mumbai, Chennai, Hyderabad and Kolkata from 2013. I packaged it up for ease of use, the data is included in the package.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks for reading until here! I also thank Stefanie Butland, Scott Chamberlain and Karthik Ram for their support during the preparation of this post.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>From a million nested `ifelse`s to the plater package</title>
      <link>https://ropensci.org/blog/2017/02/06/plater-blog-post/</link>
      <pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/02/06/plater-blog-post/</guid>
      <description>
        
        

&lt;p&gt;As a lab scientist, I do almost all of my experiments in &lt;a href=&#34;https://en.wikipedia.org/wiki/Microtiter_plate&#34;&gt;microtiter plates&lt;/a&gt;. These tools are an efficient means of organizing many parallel experimental conditions. It&amp;rsquo;s not always easy, however, to translate between the physical plate and a useful data structure for analysis. My first attempts to solve this problem&amp;ndash;nesting one &lt;code&gt;ifelse&lt;/code&gt; call inside of the next to describe which well was which&amp;ndash;were very unsatisfying. Over time, my attempts at solving the problem grew more sophisticated, and eventually, the &lt;code&gt;plater&lt;/code&gt; package was born. Here I will tell the story of how with the help of &lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;R Packages&lt;/a&gt; and the amazing reviewers (&lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt; and &lt;a href=&#34;http://deanattali.com/&#34;&gt;Dean Attali&lt;/a&gt;) and &lt;a href=&#34;https://scottchamberlain.info/&#34;&gt;editors&lt;/a&gt; at rOpenSci, I ended up with a package that makes it easy to work with plate-based data.&lt;/p&gt;

&lt;h2 id=&#34;plates-are-great&#34;&gt;Plates are great&lt;/h2&gt;

&lt;p&gt;Microtiter plates are essential in the lab. Basically an ice cube tray about the size of an index card, they have &amp;ldquo;wells&amp;rdquo; for between 6 and 384 ice cubes (up to 6144 if you&amp;rsquo;re a robot!). Except instead of freezing water, you use each well for a different sample or experimental condition.&lt;/p&gt;

&lt;p&gt;For example, say I have 8 samples and want to test 4 different drugs. Each drug should be tested on each sample three separate times to ensure accurate results. A 96-well plate is perfect for this: it&amp;rsquo;s a grid of 12 columns and 8 rows. So each sample would go in its own row. Each drug would then go in a group of three columns, say Drug A in columns 1-3, Drug B in columns 4-6, and so on. This is shown below, with the numbers 1-8 representing samples and the colors representing groups of wells treated with the same drug.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-02-plater-post/plate-1.png&#34; alt=&#34;Example plate layout&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Typically, I make myself a map like the image above before I start the experiment, then I take it with me into the lab when I&amp;rsquo;m ready to begin. The map creates a powerful mental connection between each experimental condition and its particular physical location on the plate. With large effects, you might even be able to visually see the results of your experiment: all the cells in this column died, or the cells grew like crazy in that row.&lt;/p&gt;

&lt;p&gt;This is very convenient to work with physically and remember mentally.&lt;/p&gt;

&lt;h2 id=&#34;plates-are-not-tidy&#34;&gt;Plates are not tidy&lt;/h2&gt;

&lt;p&gt;The problem is that you can pack a ton of complexity into a small experiment and mapping that back into a &lt;a href=&#34;https://www.jstatsoft.org/article/view/v059i10&#34;&gt;tidy&lt;/a&gt; framework for analysis isn&amp;rsquo;t always easy.&lt;/p&gt;

&lt;p&gt;One way to map from data to plate is through well IDs. Each well has one. For example, the top left well is in row A and column 1, so its ID is A01. The well in the third row down and 5th column over is C05. But how do you say that everything in row A is sample X, everything in row B is sample Y, and so on?&lt;/p&gt;

&lt;p&gt;My first strategy was to put it in the code, with a mess like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data &amp;lt;- dplyr::mutate(data,
            Sample = ifelse(Row == &amp;quot;A&amp;quot;, &amp;quot;Sample X&amp;quot;, ifelse(
                Row == &amp;quot;B&amp;quot;, &amp;quot;Sample Y&amp;quot;, ifelse(
                    ...))),
            Treatment = ifelse(Column %in% 1:3, &amp;quot;Drug A&amp;quot;, ifelse(
                Column %in% 4:6, &amp;quot;Drug B&amp;quot;, ifelse(
                    ...)))
            )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But doing it this way made me want to cry.&lt;/p&gt;

&lt;p&gt;My next strategy was to try to directly make a table and then merge it into the data. The table would look something like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;WellId&lt;/th&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Treatment&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;A01&lt;/td&gt;
&lt;td&gt;X&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;A02&lt;/td&gt;
&lt;td&gt;X&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;A03&lt;/td&gt;
&lt;td&gt;X&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;A04&lt;/td&gt;
&lt;td&gt;X&lt;/td&gt;
&lt;td&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;B01&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;B01&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;While this merges nicely into a data frame and solves the problem of indicating what each well is, it&amp;rsquo;s actually not that easy to create by hand, especially in more realistic experiments with more variables and a more complex plate layout. Even worse, there is a substantial risk of typos and copy-paste errors.&lt;/p&gt;

&lt;h2 id=&#34;plater-to-the-rescue&#34;&gt;&lt;code&gt;plater&lt;/code&gt; to the rescue&lt;/h2&gt;

&lt;p&gt;The solution came from the plates themselves: store the data and mapping in the shape of the plate and then transform it into a tidy shape. Scientific instruments often provide data in the shape of a plate, in fact: you get back a spreadsheet with a grid of numbers shaped like your plate, with a cell for each well.&lt;/p&gt;

&lt;p&gt;My first step was to write a function to convert one of those plate-shaped grids to a data frame with two columns: one of plate IDs and one of the numbers (machine read-out).&lt;/p&gt;

&lt;p&gt;So now I could take a &lt;code&gt;.csv&lt;/code&gt; file with plate-shaped data and convert it into tidy form and connect it with the well ID. It didn&amp;rsquo;t take long for me to start creating &lt;code&gt;.csv&lt;/code&gt; files with sample and treatment information as well and then merging the data frames together. Now I could create plate maps really easily because they looked just like the plate I did the experiment in.&lt;/p&gt;

&lt;h2 id=&#34;a-package-takes-shape&#34;&gt;A package takes shape&lt;/h2&gt;

&lt;p&gt;With time and feedback from &lt;a href=&#34;https://github.com/ClaireLevy&#34;&gt;others in the lab&lt;/a&gt;, I refined the system. Instead of creating separate files for each variable (treatment, sample, data, &amp;hellip;), everything could go in one &lt;code&gt;.csv&lt;/code&gt; file, with sequential plate layouts for each variable (say, treatment or sample). I started calling this the &lt;code&gt;plater&lt;/code&gt; format and storing all of my data that way. These files are especially nice because they give an overview of the whole experiment in a compact format.&lt;/p&gt;

&lt;p&gt;Eventually, I boiled it down to a small set of functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;read_plate&lt;/code&gt;: takes a &lt;code&gt;plater&lt;/code&gt; format file and gives you a tidy data frame&lt;/li&gt;
&lt;li&gt;&lt;code&gt;read_plates&lt;/code&gt;: takes multiple &lt;code&gt;plater&lt;/code&gt; format files and gives you a big tidy data frame&lt;/li&gt;
&lt;li&gt;&lt;code&gt;add_plate&lt;/code&gt;: takes a &lt;code&gt;plater&lt;/code&gt; format file and a tidy data frame and combines them&lt;/li&gt;
&lt;li&gt;&lt;code&gt;view_plate&lt;/code&gt;: takes a tidy data frame and displays selected variables from it back in plate shape&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;is-this-thing-any-good&#34;&gt;Is this thing any good?&lt;/h2&gt;

&lt;p&gt;With the package in place, I started getting ready to submit it to CRAN, but I wanted to get more feedback first and rOpenSci seemed perfect for that.&lt;/p&gt;

&lt;p&gt;It turned out that the improvements started even before I got any feedback. As I prepared to submit the package to rOpenSci, I went through their &lt;a href=&#34;https://github.com/ropensci/onboarding#how-to-submit-your-package-for-review&#34;&gt;thorough guide&lt;/a&gt;  to make sure &lt;code&gt;plater&lt;/code&gt; met all of the requirements. This process made me aware of best practices and motivated me to handle niggling little details like using consistent &lt;code&gt;snake_case&lt;/code&gt;, making sure all of the documentation was clear, and creating a code of conduct for contributors. In all, I made 22 commits preparing for submission.&lt;/p&gt;

&lt;p&gt;The review process itself led to even more improvement. Two generous reviewers (&lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt; and &lt;a href=&#34;http://deanattali.com/&#34;&gt;Dean Attali&lt;/a&gt;) and an &lt;a href=&#34;https://scottchamberlain.info/&#34;&gt;editor&lt;/a&gt; spent multiple hours reading the code, testing the functions, and thinking about how to make it more useful. Their thoughtful suggestions resulted in many changes to the package (I made 61 commits responding to the reviews) that made it more robust and useful. Among others:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Make &lt;code&gt;add_plate&lt;/code&gt; more easily pipeable by reordering the arguments&lt;/li&gt;
&lt;li&gt;Add a function &lt;code&gt;check_plater_format&lt;/code&gt; to test if a file is formatted correctly&lt;/li&gt;
&lt;li&gt;Brainstorm a Shiny tool for non-R users to use &lt;code&gt;plater&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Explore alternative visualizations to &lt;code&gt;view_plate&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reviewing process made &lt;code&gt;plater&lt;/code&gt; a much better package and left me feeling confident in putting it up on CRAN with a stable version 1.0.0.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Since transferring &lt;code&gt;plater&lt;/code&gt; over to rOpenSci and putting it onto CRAN, I&amp;rsquo;ve used the package all the time and have shared it with lab mates and colleagues. It works well and does exactly what I want, seamlessly without my needing to even notice it. This level of convenience and utility is the best testament to the efforts of the reviewers and editors of rOpenSci, who helped to make it a better package.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Onboarding at rOpenSci: A Year in Reviews</title>
      <link>https://ropensci.org/blog/2016/03/28/software-review/</link>
      <pubDate>Mon, 28 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2016/03/28/software-review/</guid>
      <description>
        
        

&lt;p&gt;Code review, in which peers manually inspect the source code of software
written by others, is widely recognized as one of the best tools for finding
bugs in software. Code review is relatively uncommon in &lt;em&gt;scientific&lt;/em&gt; software
development, though. Scientists, despite being familiar with the process of
peer review, often have little exposure to code review due to lack of training and
historically little incentive to share the source code from their research. So
scientific code, from one-off scripts to reusable R packages, is rarely subject
to review. Most R packages are subject only to the automated checks required by
  CRAN, which primarily ensure that packages can be installed on multiple systems.
As such, The burden is on software users to discern well-written and efficient
packages from poorly written ones.&lt;/p&gt;

&lt;p&gt;rOpenSci is a community of developer-scientists, creating R packages for other
scientists, and our package contributors have a mix of backgrounds. We aim to
serve our users with high-quality software, and also promote best practices
among our author base and in the scientific community in general. So for
the past year, rOpenSci has been piloting a system of peer code review for
submissions to &lt;a href=&#34;https://ropensci.org/packages/&#34;&gt;our suite of R packages&lt;/a&gt;. Here
we&amp;rsquo;ll outline how our system works, and what we&amp;rsquo;ve learned from our authors and
reviewers.&lt;/p&gt;

&lt;h2 id=&#34;our-system&#34;&gt;Our System&lt;/h2&gt;

&lt;p&gt;rOpenSci&amp;rsquo;s package review process owes much to the experiments of others
(such as &lt;a href=&#34;http://mcs.open.ac.uk/mp8/&#34;&gt;Marian Petre&lt;/a&gt; and the &lt;a href=&#34;https://mozillascience.org/code-review-for-science-what-we-learned&#34;&gt;Mozilla Science Lab&lt;/a&gt;),
as well as the &lt;a href=&#34;https://discuss.ropensci.org/t/code-review-onboarding-milestones/180&#34;&gt;active feedback&lt;/a&gt; &lt;a href=&#34;https://discuss.ropensci.org/t/how-could-the-onboarding-package-review-process-be-even-better/302&#34;&gt;from our
community&lt;/a&gt;. Here&amp;rsquo;s how it works: When an author submits
a package, our editors evaluate it for fit according to our &lt;a href=&#34;https://github.com/ropensci/policies#package-fit&#34;&gt;criteria&lt;/a&gt;, then assign reviewers who evaluate
the package for usability, quality, and style based on our &lt;a href=&#34;https://github.com/ropensci/packaging_guide#ropensci-packaging-guide&#34;&gt;guidelines&lt;/a&gt;. After the
reviewers evaluate and the author makes recommended changes, the package gets
the rOpenSci stamp in its README and is added to our collection.&lt;/p&gt;

&lt;p&gt;We work entirely through the GitHub issue system. To submit authors &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/new&#34;&gt;open
an issue&lt;/a&gt;. Reviewers post
reviews as comments on that issue. This means the entire process is open and
public from the start. Reviewers and authors are known to each other and free
to communicate directly in the issue thread. GitHub-based reviews have some
other nice features: reviewers can publicly consult others by &lt;strong&gt;\@tagging&lt;/strong&gt;
if outside expertise is wanted. Reviewers can also contribute to the package
directly via a pull request when this is more efficient than describing the
changes they suggest.&lt;/p&gt;

&lt;p&gt;This system deliberately combines elements of traditional academic peer review
(external peers), with practices from open-source software review. One design
goal was to keep reviews &lt;em&gt;non-adversarial&lt;/em&gt; - to focus on improving software
quality rather than judging the package or authors. We think the openness
of the process has something to do with this, as reviews are public and this
incentivizes reviewers to do good work and abide by our &lt;a href=&#34;https://github.com/ropensci/policies#code-of-conduct&#34;&gt;code of conduct&lt;/a&gt;. We also do not
explicitly reject packages, except for turning some away prior review when they
are out-of-scope. We do this because submitted packages are already public and
open-source, so &amp;ldquo;time to publication&amp;rdquo; has not been a concern. Packages that
require significant revisions can just remain on hold until authors incorporate
such changes and update the discussion thread.&lt;/p&gt;

&lt;h2 id=&#34;some-lessons-learned&#34;&gt;Some lessons learned&lt;/h2&gt;

&lt;p&gt;So far, we&amp;rsquo;ve received 16 packages. Of these, only 1 was rejected due to lack
of fit. 11 were reviewed, 6 of which were accepted, and 5 are awaiting changes
requested by reviewers. 4 are still awaiting at least one review.&lt;/p&gt;

&lt;p&gt;We also recently &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1zaE5MvqXyD0I7LWONh1HlQu98wTIZ6Uls4QVmKs2u-w/edit?usp=sharing&#34;&gt;surveyed&lt;/a&gt; our reviewers and
reviewees, asking them how long it took to review, their positive and negative
experiences with the system, and what they learned from the process.&lt;/p&gt;

&lt;h3 id=&#34;reviewers-and-reviewees-like-it&#34;&gt;Reviewers and reviewees like it!&lt;/h3&gt;

&lt;p&gt;Pretty much everyone who responded to the survey, which was most of our
reviewers, found value in the system.  While we didn&amp;rsquo;t ask anyone to rate the
system or quantify their satisfaction, the length of answers to &amp;ldquo;What was the
best thing about the software review process?&amp;rdquo; and the number of superlatives
and exclamation marks indicates a fair bit of enthusiasm.  Here are a couple of
choice quotes:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I don&amp;rsquo;t really see myself writing another serious package without having it go through code review.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;I learnt that code review is the best thing that can ever happen to your
package!&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Authors appreciated that their reviews were thorough, that they were able to
converse with (nice) reviewers, and that they picked up best practices from
other experienced authors. Reviewers also praised the ability to converse
directly with author, expand their community of colleagues and learn about new
and best practices from other authors.&lt;/p&gt;

&lt;p&gt;Interestingly, no one mentioned the credential of an rOpenSci &amp;ldquo;badge&amp;rdquo; as a
positive aspect of review.  While the badge may be a motivating factor,
it seems from the responses that authors primarily value the feedback itself.
There has been some &lt;a href=&#34;http://simplystatistics.org/2013/09/26/how-could-code-review-discourage-code-disclosure-reviewers-with-motivation/&#34;&gt;argument&lt;/a&gt; whether code
review will encourage or discourage scientists to publish their
code.  While our package authors represent a specific subset of scientists - those knowledgeable and motivated enough to create and disseminate packages - we think
our pilot shows that a well-designed review process can be encouraging.&lt;/p&gt;

&lt;h3 id=&#34;review-takes-a-lot-of-time&#34;&gt;Review takes a lot of time&lt;/h3&gt;

&lt;p&gt;We asked reviewers to estimate how much time each review took, and here&amp;rsquo;s what
they reported:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2016-03-28-software-review/reviewer-time-1.png&#34; alt=&#34;plot of chunk reviewer-time&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Answers varied from 1-10 hours with an average of 4. This is comparable
to &lt;a href=&#34;http://publishingresearchconsortium.com/index.php/112-prc-projects/research-reports/peer-review-in-scholarly-journals-research-report/142-peer-review-in-scholarly-journals-perspective-of-the-scholarly-community-an-international-study&#34;&gt;how long it takes researchers to review scholarly papers&lt;/a&gt;, but
it&amp;rsquo;s still a lot of time, and does not include further time corresponding with
the authors or re-reviewing an updated package.&lt;/p&gt;

&lt;p&gt;Package writing and reviewing are generally volunteer activities, and as one
respondent put it, the process &amp;ldquo;still feels more like community service than
a professional obligation.&amp;rdquo; 7 of 16 reviewers respondents mentioned the time to
it took to review and respond as a negative of the process. For this process to
be sustainable, we have to figure out how to limit the burden on our reviewers.&lt;/p&gt;

&lt;h3 id=&#34;we-can-be-clearer-about-the-beginning-and-end-of-the-process&#34;&gt;We can be clearer about the beginning and end of the process&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;It wasn&amp;rsquo;t immediately clear what to do&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A few respondents pointed out we could be better at explaining the review
process, both in how to get started and how it is supposed to wrap up. For the
former, we&amp;rsquo;ve recently updated our &lt;a href=&#34;https://github.com/ropensci/onboarding/wiki/For-Reviewers&#34;&gt;reviewer guide&lt;/a&gt;, including adding links to previous reviews. We
hope as our reviewer pool gets more experienced, and as software reviews become
more common, this gets easier. However, as our pool of editors and reviewers
grows, we&amp;rsquo;ll need to ensure that our communication is clear.&lt;/p&gt;

&lt;p&gt;As for the &lt;em&gt;end&lt;/em&gt; of the review, this can be an area of considerable ambiguity.
There&amp;rsquo;s a clear endpoint when a package is accepted, but with no &amp;ldquo;rejections&amp;rdquo;
some reviewers weren&amp;rsquo;t sure how to respond if authors didn&amp;rsquo;t follow up on their
comments. We realize it can be demotivating to reviewers if their suggestions
aren&amp;rsquo;t acted upon. (One reviewer pointed out that seeing her suggestions
implemented as a positive motivator.) It may be worthwhile to enforce a
deadline for package authors to respond.&lt;/p&gt;

&lt;h3 id=&#34;we-are-helping-drive-best-practices-with-our-author-base&#34;&gt;We are helping drive best practices with our author base&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I had never heard of continuous integration, and it is fantastic!&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We asked both reviewers and reviewees to tell us what they learned. While there
was a lot of variety in the responses, one common thread was learning and
appreciating best practices: continuous integration, documentation,
&amp;ldquo;the right way to do [X]&amp;ldquo;, were the common responses.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:RefC&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:RefC&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Importantly, a number of reviewers and reviewees commented that they &lt;em&gt;learned
the value of review&lt;/em&gt; through this process.&lt;/p&gt;

&lt;h2 id=&#34;questions-and-ideas-for-the-future&#34;&gt;Questions and ideas for the future&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Scaling and reviewer incentives.&lt;/em&gt; Like academic paper review or contributing
to free open-source projects, our package review is a volunteer activity.
How do we build an experienced reviewer base, maintain enthusiasm, and
avoid overburdening our reviewers? We will need to expand our reviewer pool in
order to spread the load. As such, we are moving to a system
of multiple &amp;ldquo;handling editors&amp;rdquo; to assign and keep up with reviews. Hopefully we
will be able to bring in more reviewers through their networks.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Author incentives.&lt;/em&gt; Our small pool of early adopters indicated that they
valued the review process itself, but will this be enough incentive to draw more
package authors to do the extra work it takes? An area to explore is finding
ways to help package authors gain greater visibility and credit for their work
after their packages pass review. This could take the form of &amp;ldquo;badges&amp;rdquo;, such
as those being developed by &lt;a href=&#34;https://osf.io/tvyxz/wiki/home/&#34;&gt;The Center for Open Science&lt;/a&gt;
and &lt;a href=&#34;https://www.mozillascience.org/projects/contributorship-badges&#34;&gt;Mozilla Science Lab&lt;/a&gt;, or providing an easier route to publishing software papers.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Automation.&lt;/em&gt; How can we automate more parts of the review process so as to
get more value out of reviewer and reviewees time? One suggestion has been to submit packages
&lt;a href=&#34;https://discuss.ropensci.org/t/how- could-the-onboarding-package-review-process-be-even-better/302/3&#34;&gt;&lt;em&gt;as&lt;/em&gt; pull requests&lt;/a&gt; to take more advantage of GitHub
review features such as in-line commenting. This may allow us to move the burden
of setting up continuous integration and testing away from the authors and onto
our own pipeline, and allow us to add rOpenSci-specific tests. We&amp;rsquo;ve also started using &lt;a href=&#34;https://github.com/ropenscilabs/heythere&#34;&gt;automated reminders&lt;/a&gt; to keep up with reviewers, which reduces the
burden on our editors to keep up with everyone.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have learned a ton from this experiment and look forward to making review
better! Many, many thanks to the authors who have contributed to the rOpenSci
package ecosystem and the reviewers who have lent their time to this project.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:RefC&#34;&gt;Also, &amp;ldquo;RefClasses are the devil&amp;rdquo;, said one reviewer.  Interpret that as you may.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:RefC&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
  </channel>
</rss>
