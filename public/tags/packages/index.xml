<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Packages on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/packages/</link>
    <description>Recent content in Packages on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 05 Dec 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/packages/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exploratory Data Analysis of Ancient Texts with rperseus</title>
      <link>https://ropensci.org/blog/2017/12/05/rperseus/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/12/05/rperseus/</guid>
      <description>
        
        

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;When I was in grad school at Emory, I had a favorite desk in the library. The desk wasn’t particularly cozy or private, but what it lacked in comfort it made up for in real estate. My books and I needed room to operate. Students of the ancient world require many tools, and when jumping between commentaries, lexicons, and interlinears, additional clutter is additional “friction”, i.e., lapses in thought due to frustration. Technical solutions to this clutter exist, but the best ones are proprietary and expensive. Furthermore, they are somewhat inflexible, and you may have to shoehorn your thoughts into their framework. More friction.&lt;/p&gt;

&lt;p&gt;Interfacing with &lt;a href=&#34;http://www.perseus.tufts.edu/hopper/&#34;&gt;the Perseus Digital Library&lt;/a&gt; was a popular online alternative. The library includes a catalog of classical texts, a Greek and Latin lexicon, and a word study tool for appearances and references in other literature. If the university library’s reference copies of BDAG&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;em&gt;Synopsis Quattuor Evangeliorum&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; were unavailable, Perseus was our next best thing.&lt;/p&gt;

&lt;p&gt;Fast forward several years, and I’ve abandoned my quest to become a biblical scholar. Much to my father’s dismay, I’ve learned writing code is more fun than writing exegesis papers. Still, I enjoy dabbling with dead languages, and it was the desire to wed my two loves, biblical studies and R, that birthed my latest package, &lt;code&gt;rperseus&lt;/code&gt;. The goal of this package is to furnish classicists with texts of the ancient world and a toolkit to unpack them.&lt;/p&gt;

&lt;h3 id=&#34;exploratory-data-analysis-in-biblical-studies&#34;&gt;Exploratory Data Analysis in Biblical Studies&lt;/h3&gt;

&lt;p&gt;Working with the Perseus Digital Library was already a trip down memory lane, but here’s an example of how I would have leveraged &lt;code&gt;rperseus&lt;/code&gt; many years ago.&lt;/p&gt;

&lt;p&gt;My best papers often sprung from the outer margins of my &lt;a href=&#34;https://en.wikipedia.org/wiki/Novum_Testamentum_Graece&#34;&gt;&lt;em&gt;Nestle-Aland Novum Testamentum Graece.&lt;/em&gt;&lt;/a&gt; Here the editors inserted cross references to parallel vocabulary, themes, and even grammatical constructions. Given the intertextuality of biblical literature, the margins are a rich source of questions: Where else does the author use similar vocabulary? How is the source material used differently? Does the literary context affect our interpretation of a particular word? This is exploratory data analysis in biblical studies.&lt;/p&gt;

&lt;p&gt;Unfortunately the excitement of your questions is incommensurate with the tedium of the process&amp;ndash;EDA continues by flipping back and forth between books, dog-earring pages, and avoiding paper cuts. &lt;code&gt;rperseus&lt;/code&gt; aims to streamline this process with two functions: &lt;code&gt;get_perseus_text&lt;/code&gt; and &lt;code&gt;perseus_parallel&lt;/code&gt;. The former returns a data frame containing the text from any work in the Perseus Digital Library, and the latter renders a parallel in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Suppose I am writing a paper on different expressions of love in Paul’s letters. Naturally, I start in 1 Corinthians 13, the famed “Love Chapter” often heard at weddings and seen on bumper stickers. I finish the chapter and turn to the margins. In the image below, I see references to Colossians 1:4, 1 Thessalonians 1:3, 5:8, Hebrews 10:22-24, and Romans 8:35-39.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/nantg.png&#34; alt=&#34;&#34; /&gt;
&lt;em&gt;1 Corinithians 13 in Nestle-Aland Novum Testamentum Graece&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ignoring that some scholars exclude Colossians from the “authentic” letters, let’s see the references alongside each other:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rperseus) #devtools::install_github(“ropensci/rperseus”)
library(tidyverse)

tribble(
  ~label, ~excerpt,
  &amp;quot;Colossians&amp;quot;, &amp;quot;1.4&amp;quot;,
  &amp;quot;1 Thessalonians&amp;quot;, &amp;quot;1.3&amp;quot;,
  &amp;quot;1 Thessalonians&amp;quot;, &amp;quot;5.8&amp;quot;,
  &amp;quot;Romans&amp;quot;, &amp;quot;8.35-8.39&amp;quot;
  ) %&amp;gt;% 
  left_join(perseus_catalog) %&amp;gt;%
  filter(language == &amp;quot;grc&amp;quot;) %&amp;gt;%
  select(urn, excerpt) %&amp;gt;%
  pmap_df(get_perseus_text) %&amp;gt;%
  perseus_parallel(words_per_row = 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/Parallel1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A brief explanation: First, I specify the labels and excerpts within a tibble. Second, I join the lazily loaded &lt;code&gt;perseus_catalog&lt;/code&gt; onto the data frame. Third, I filter for the Greek&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; and select the columns containing the arguments required for &lt;code&gt;get_perseus_text&lt;/code&gt;. Fourth, I map over each urn and excerpt, returning another data frame. Finally, I pipe the output into &lt;code&gt;perseus_parallel&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The key word shared by each passage is &lt;em&gt;agape&lt;/em&gt; (“love”). Without going into detail, it might be fruitful to consider the references alongside each other, pondering how the semantic range of &lt;em&gt;agape&lt;/em&gt; expands or contracts within the Pauline corpus. Paul had a penchant for appropriating and recasting old ideas&amp;ndash;often in slippery and unexpected ways&amp;ndash;and your Greek lexicon provides a mere approximation. In other words, how can we move from the dictionary definition of &lt;em&gt;agape&lt;/em&gt; towards Paul&amp;rsquo;s unique vision?&lt;/p&gt;

&lt;p&gt;If your Greek is rusty, you can parse each word with &lt;code&gt;parse_excerpt&lt;/code&gt; by locating the text&amp;rsquo;s urn within the &lt;code&gt;perseus_catalog&lt;/code&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parse_excerpt(urn = &amp;quot;urn:cts:greekLit:tlg0031.tlg012.perseus-grc2&amp;quot;, excerpt = &amp;quot;1.4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;form&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;verse&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;part_of_speech&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;person&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;number&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;tense&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;mood&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;voice&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;gender&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;case&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;degree&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ἀκούω&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ἀκούσαντες&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;verb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;plural&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aorist&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;participle&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;active&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;masculine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;nominative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ὁ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;τὴν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;article&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;singular&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;feminine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;accusative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;πίστις&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;πίστιν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;noun&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;singular&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;feminine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;accusative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ὑμός&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ὑμῶν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;pronoun&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;plural&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;masculine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;genative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If your Greek is &lt;em&gt;really&lt;/em&gt; rusty, you can also flip the &lt;code&gt;language&lt;/code&gt; filter to “eng” to view an older English translation.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; And if the margin references a text from the Old Testament, you can call the Septuagint as well as the original Hebrew.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tribble(
  ~label, ~excerpt,
  &amp;quot;Genesis&amp;quot;, &amp;quot;32.31&amp;quot;,
  &amp;quot;Genesis, pointed&amp;quot;, &amp;quot;32.31&amp;quot;,
  &amp;quot;Numeri&amp;quot;, &amp;quot;12.8&amp;quot;,
  &amp;quot;Numbers, pointed&amp;quot;, &amp;quot;12.8&amp;quot;
  ) %&amp;gt;% 
  left_join(perseus_catalog) %&amp;gt;%
  filter(language %in% c(&amp;quot;grc&amp;quot;, &amp;quot;hpt&amp;quot;)) %&amp;gt;%
  select(urn, excerpt) %&amp;gt;%
  pmap_df(get_perseus_text) %&amp;gt;%
  perseus_parallel()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/Parallel2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Admittedly, there is some “friction” here in joining the &lt;code&gt;perseus_catalog&lt;/code&gt; onto the initial tibble. There is a learning curve with getting acquainted with the idiosyncrasies of the catalog object. A later release will aim to streamline this workflow.&lt;/p&gt;

&lt;h3 id=&#34;future-work&#34;&gt;Future Work&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://ropensci.github.io/rperseus/articles/rperseus-vignette.html&#34;&gt;Check the vignette&lt;/a&gt; for a more general overview of &lt;code&gt;rperseus&lt;/code&gt;. In the meantime, I look forward to getting more intimately acquainted with the Perseus Digital Library. Tentative plans to extend &lt;code&gt;rperseus&lt;/code&gt; a Shiny interface to further reduce “friction” and a method of creating a “book” of custom parallels with &lt;code&gt;bookdown&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;I want to thank my two rOpenSci reviewers, &lt;a href=&#34;https://www.ildiczeller.com/&#34;&gt;Ildikó Czeller&lt;/a&gt; and &lt;a href=&#34;https://francoismichonneau.net/&#34;&gt;François Michonneau,&lt;/a&gt; for coaching me through the review process. They were the first two individuals to ever scrutinize my code, and I was lucky to hear their feedback. rOpenSci onboarding is truly a wonderful process.&lt;/p&gt;

&lt;!-- references --&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Bauer, Walter. &lt;em&gt;A Greek-English Lexicon of the New Testament and Other Early Christian Literature.&lt;/em&gt; Edited by Frederick W. Danker. 3rd ed. Chicago: University of Chicago Press, 2000.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Aland, Kurt. &lt;em&gt;Synopsis Quattuor Evangeliorum.&lt;/em&gt; Deutsche Bibelgesellschaft, 1997.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;The Greek text from the Perseus Digital Library is from 1885 standards. The advancement of textual criticism in the 20th century led to a more stable text you would find in current editions of the Greek New Testament.&lt;br /&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;The English translation is from Rainbow Missions, Inc. &lt;em&gt;World English Bible.&lt;/em&gt; Rainbow Missions, Inc.; revision of the American Standard Version of 1901. I’ve toyed with the idea of incorporating more modern translations, but that would require require resources beyond the Perseus Digital Library.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;&amp;ldquo;hpt&amp;rdquo; is the pointed Hebrew text from &lt;em&gt;Codex Leningradensis.&lt;/em&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>Magick 1.6: clipping, geometries, fonts, fuzz, and a bit of history</title>
      <link>https://ropensci.org/technotes/2017/12/05/magick-16/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/12/05/magick-16/</guid>
      <description>
        
        &lt;img src=&quot;https://i.imgur.com/tTFk7ig.jpg&quot; alt=&quot;cover image&quot;&gt;
        
        

&lt;p&gt;This week &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick&lt;/a&gt; 1.6 appeared on CRAN. This release is a big all-round maintenance update with lots of tweaks and improvements across the package.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/NEWS&#34;&gt;NEWS&lt;/a&gt; file gives an overview of changes in this version. In this post we highlight some changes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)
stopifnot(packageVersion(&#39;magick&#39;) &amp;gt;= 1.6)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you are new to magick, check out the &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;vignette&lt;/a&gt; for a quick introduction.&lt;/p&gt;

&lt;h2 id=&#34;perfect-graphics-rendering&#34;&gt;Perfect Graphics Rendering&lt;/h2&gt;

&lt;p&gt;I have fixed a few small rendering imperfections in the graphics device. The native magick graphics device &lt;code&gt;image_graph()&lt;/code&gt; now renders identical or better quality images as the R-base bitmap devices &lt;code&gt;png&lt;/code&gt;, &lt;code&gt;jpeg&lt;/code&gt;, etc.&lt;/p&gt;

&lt;p&gt;One issue was that sometimes magick graphics would show a 1px black border around the image. It turned out this is caused by rounding of clipping coordinates.&lt;/p&gt;

&lt;p&gt;When R calculates clipping area it often ends up at non-whole values. It is then up to the graphics device to decide what to do with the pixel that is partially clipped. Let&amp;rsquo;s show clipping in action:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;testplot &amp;lt;- function(title = &amp;quot;&amp;quot;){
  plot(1, main = title)
  abline(0, 1, col = &amp;quot;blue&amp;quot;, lwd = 2, lty = &amp;quot;solid&amp;quot;)
  abline(0.1, 1, col = &amp;quot;red&amp;quot;, lwd = 3, lty = &amp;quot;dotted&amp;quot;)
  abline(0.2, 1, col = &amp;quot;green&amp;quot;, lwd = 4, lty = &amp;quot;twodash&amp;quot;)
  abline(0.3, 1, col = &amp;quot;black&amp;quot;, lwd = 5, lty = &amp;quot;dotdash&amp;quot;)
  abline(0.4, 1, col = &amp;quot;purple&amp;quot;, lwd = 6, lty = &amp;quot;dashed&amp;quot;)
  abline(0.5, 1, col = &amp;quot;yellow&amp;quot;, lwd = 7, lty = &amp;quot;longdash&amp;quot;)
  abline(-0.1, 1, col = &amp;quot;blue&amp;quot;, lwd = 10, lend = &amp;quot;round&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
  abline(-0.2, 1, col = &amp;quot;blue&amp;quot;, lwd = 10, lend = &amp;quot;butt&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
  abline(-0.3, 1, col = &amp;quot;blue&amp;quot;, lwd = 10, lend = &amp;quot;square&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we run it with and without clipping:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img2 &amp;lt;- magick::image_graph(clip = FALSE)
testplot(&amp;quot;Without clipping&amp;quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/TtpjlLq.png&#34; alt=&#34;noclip.png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img1 &amp;lt;- magick::image_graph(clip = TRUE)
testplot(&amp;quot;With clipping&amp;quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/JbWMElL.png&#34; alt=&#34;clip.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see the latter image is now perfectly clipped. The colored lines are truncated exactly at the pixel where the axis starts. This is not always the case in base R ;)&lt;/p&gt;

&lt;h2 id=&#34;font-families&#34;&gt;Font Families&lt;/h2&gt;

&lt;p&gt;In magick there are two ways to render text on an image. You can either open the image or graphic in the magick graphics device and then use base R &lt;code&gt;text()&lt;/code&gt; function to print text. Alternatively there is &lt;code&gt;image_annotate()&lt;/code&gt; which is a simpler version to print some text on an image.&lt;/p&gt;

&lt;p&gt;Wherever text rendering is involved, two major headache arise: encoding and fonts. The latter is tricky because different operating systems have different fonts with different names. In addition a font can be specified as a name, or family name, or alias.&lt;/p&gt;

&lt;p&gt;Below is a simple test that I use to quickly inspect if fonts are working on different systems:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img &amp;lt;- image_graph(width = 800, height = 500, pointsize = 20, res = 96)
graphics::plot.new()
graphics::par(mar = c(0,0,3,0))
graphics::plot.window(xlim = c(0, 20), ylim = c(-.5, 8))
title(expression(Gamma %prop% sum(x[alpha], i==1, n) * sqrt(mu)), expression(hat(x)))

# Standard families as supported by other devices
text(0.95, 7, &amp;quot;abcdefg  - Helvetica&amp;quot;, pos = 4, family = &amp;quot;helvetica&amp;quot;)
text(0.95, 6, &amp;quot;abcdefg  - Sans (Arial)&amp;quot;, pos = 4, family = &amp;quot;sans&amp;quot;)
text(0.95, 5, &amp;quot;abcdefg - Serif (Times)&amp;quot;, pos = 4, family = &amp;quot;serif&amp;quot;)
text(0.95, 4, &amp;quot;abcdefg - Monospace (Courier New)&amp;quot;, pos = 4, family = &amp;quot;mono&amp;quot;)
text(0.95, 3, &amp;quot;abcdefg - Symbol Face&amp;quot;, pos = 4, font = 5)
text(0.95, 2, &amp;quot;abcdefg  - Comic Sans&amp;quot;, pos = 4, family = &amp;quot;Comic Sans&amp;quot;)
text(0.95, 1, &amp;quot;abcdefg - Georgia Serif&amp;quot;, pos = 4, family = &amp;quot;Georgia&amp;quot;)
text(0.95, 0, &amp;quot;abcdefg - Courier&amp;quot;, pos = 4, family = &amp;quot;Courier&amp;quot;)
dev.off()
img &amp;lt;- image_border(img, &#39;red&#39;, geometry = &#39;2x2&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/tzIktip.png&#34; alt=&#34;families&#34; /&gt;&lt;/p&gt;

&lt;p&gt;R requires that a graphics device supports at least 4 font types: &lt;code&gt;serif&lt;/code&gt;, &lt;code&gt;sans&lt;/code&gt;, &lt;code&gt;mono&lt;/code&gt; and &lt;code&gt;symbol&lt;/code&gt;. The latter is a special 8bit font with some Greek letters and other characters needed for rendering math. This set of fonts corresponds to the original &lt;strong&gt;13 base fonts&lt;/strong&gt; from the &lt;a href=&#34;https://en.wikipedia.org/wiki/PostScript_fonts#Core_Font_Set&#34;&gt;1984 postscript standard&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;4x Courier (Regular, Oblique, Bold, Bold Oblique)&lt;/li&gt;
&lt;li&gt;4x Helvetica (Regular, Oblique, Bold, Bold Oblique)&lt;/li&gt;
&lt;li&gt;4x Times (Roman, Italic, Bold, Bold Italic)&lt;/li&gt;
&lt;li&gt;Symbol&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below a photo of the 1985 &lt;a href=&#34;https://en.wikipedia.org/wiki/LaserWriter&#34;&gt;Apple Laser Writer&lt;/a&gt; which was &lt;a href=&#34;https://en.wikipedia.org/wiki/PostScript_fonts#History&#34;&gt;the first laser printer&lt;/a&gt; to use the PostScript language and support all these fonts! Not much later PostScript graphics devices were adopted by R&amp;rsquo;s predecessor &lt;a href=&#34;https://en.wikipedia.org/wiki/S_(programming_language)#.22New_S.22&#34;&gt;&amp;ldquo;The New S&amp;rdquo;&lt;/a&gt; (The New S Language, 1988).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://theappletimeline.com/images/color1000.jpg&#34; alt=&#34;printers&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;geometry-helpers&#34;&gt;Geometry Helpers&lt;/h2&gt;

&lt;p&gt;Another major improvement in this release is the introduction of helper functions for geometry and option strings. Many functions in magick require a special geometry syntax to specify a size, area, or point. For example to resize an image you need to specify a size:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;image_resize(img, &amp;quot;50%&amp;quot;)
image_resize(img, &amp;quot;300x300&amp;quot;)
image_resize(img, &amp;quot;300x300!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or to crop you need to specify an area which consists of a size and offset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;image_crop(img, &amp;quot;300x300+100+100&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We added a few handy &lt;code&gt;?geometry&lt;/code&gt; helper functions to generate proper geometry syntax&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/2jivLxi.png&#34; alt=&#34;geometries&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;magick-options&#34;&gt;Magick Options&lt;/h2&gt;

&lt;p&gt;A lot of the power in ImageMagick is contained in the hundreds of built-in filters, colorspaces, compose operators, disposal types, convolution kernels, noise types and what not. These are specified simply as a string in the function.&lt;/p&gt;

&lt;p&gt;For example in our previous &lt;a href=&#34;https://ropensci.org/technotes/2017/11/02/image-convolve/&#34;&gt;post about Image Convolution&lt;/a&gt; we discussed a few kernel types:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Gaussian Kernel
img %&amp;gt;% image_convolve(&#39;Gaussian:0x5&#39;, scaling = &#39;60,40%&#39;)

# Sobel Kernel
img %&amp;gt;% image_convolve(&#39;Sobel&#39;)

# Difference of Gaussians
img %&amp;gt;% image_convolve(&#39;DoG:0,0,2&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Supported values for each option are described in the online ImageMagick documentation. We now have added functions in the magick package that list all values for each option. This should make it a easier to see what is supported and harness the full power of built-in ImageMagick algorithms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/cid6JqU.png&#34; alt=&#34;options&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So we can now easily list e.g. supported kernel types:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; kernel_types()
 [1] &amp;quot;Undefined&amp;quot;     &amp;quot;Unity&amp;quot;         &amp;quot;Gaussian&amp;quot;      &amp;quot;DoG&amp;quot;          
 [5] &amp;quot;LoG&amp;quot;           &amp;quot;Blur&amp;quot;          &amp;quot;Comet&amp;quot;         &amp;quot;Binomial&amp;quot;     
 [9] &amp;quot;Laplacian&amp;quot;     &amp;quot;Sobel&amp;quot;         &amp;quot;FreiChen&amp;quot;      &amp;quot;Roberts&amp;quot;      
[13] &amp;quot;Prewitt&amp;quot;       &amp;quot;Compass&amp;quot;       &amp;quot;Kirsch&amp;quot;        &amp;quot;Diamond&amp;quot;      
[17] &amp;quot;Square&amp;quot;        &amp;quot;Rectangle&amp;quot;     &amp;quot;Disk&amp;quot;          &amp;quot;Octagon&amp;quot;      
[21] &amp;quot;Plus&amp;quot;          &amp;quot;Cross&amp;quot;         &amp;quot;Ring&amp;quot;          &amp;quot;Peaks&amp;quot;        
[25] &amp;quot;Edges&amp;quot;         &amp;quot;Corners&amp;quot;       &amp;quot;Diagonals&amp;quot;     &amp;quot;ThinDiagonals&amp;quot;
[29] &amp;quot;LineEnds&amp;quot;      &amp;quot;LineJunctions&amp;quot; &amp;quot;Ridges&amp;quot;        &amp;quot;ConvexHull&amp;quot;   
[33] &amp;quot;ThinSe&amp;quot;        &amp;quot;Skeleton&amp;quot;      &amp;quot;Chebyshev&amp;quot;     &amp;quot;Manhattan&amp;quot;    
[37] &amp;quot;Octagonal&amp;quot;     &amp;quot;Euclidean&amp;quot;     &amp;quot;User Defined&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a lot of kernels.&lt;/p&gt;

&lt;h2 id=&#34;fuzz-scaling&#34;&gt;Fuzz Scaling&lt;/h2&gt;

&lt;p&gt;Finally one more (breaking) change: several functions in magick use a &lt;code&gt;fuzz&lt;/code&gt; parameter to specify the max distance between two colors to be considered similar.&lt;/p&gt;

&lt;p&gt;For example the flood fill algorithm (the paint-bucket button in ms-paint) changes the color of a given starting pixel, and then recursively all adjacent pixels that have the same color. However sometimes neighboring pixels are not precisely the same color, but nearly the same. The &lt;code&gt;fuzz&lt;/code&gt; parameter allows the fill to continue when pixels are not the same but similar color.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Paint the shirt orange
frink &amp;lt;- image_read(&amp;quot;https://jeroen.github.io/images/frink.png&amp;quot;) %&amp;gt;%
  image_fill(&amp;quot;orange&amp;quot;, point = &amp;quot;+100+200&amp;quot;, fuzz = 25)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/VwlqYWy.png&#34; alt=&#34;frink&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What has changed in this version is that &lt;code&gt;fuzz&lt;/code&gt; parameter been rescaled to a percentage. Hence you should always provide a value between 0 and 100. Previously it was the absolute distance between colors, but this depends on the type and color depth of the image at hand, which was very confusing.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Using Magick with RMarkdown and Shiny</title>
      <link>https://ropensci.org/technotes/2017/11/07/magick-knitr/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/11/07/magick-knitr/</guid>
      <description>
        
        &lt;img src=&quot;https://i.imgur.com/tTFk7ig.jpg&quot; alt=&quot;cover image&quot;&gt;
        
        

&lt;p&gt;This week &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick&lt;/a&gt; 1.5 appeared on CRAN. The latest update adds support for using images in knitr documents and shiny apps. In this post we show how this nicely ties together a reproducible image workflow in R, from source image or plot directly into your report or application.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)
stopifnot(packageVersion(&#39;magick&#39;) &amp;gt;= 1.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also the magick &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;intro vignette&lt;/a&gt; has been updated in this version to cover the latest features available in the package.&lt;/p&gt;

&lt;h2 id=&#34;magick-in-knitr-rmarkdown-documents&#34;&gt;Magick in Knitr / RMarkdown Documents&lt;/h2&gt;

&lt;p&gt;Magick 1.5 is now fully compatible with knitr. To embed magick images in your rmarkdown report, simply use standard code chunk syntax in your &lt;code&gt;Rmd&lt;/code&gt; file. No special options or packages are required; the image automatically appears in your documents when printed!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Example from our post last week
image_read(&#39;logo:&#39;) %&amp;gt;%
  image_convolve(&#39;DoG:0,0,2&#39;) %&amp;gt;%
  image_negate() %&amp;gt;%
  image_resize(&amp;quot;400x400&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/PhwCJ4k.gif&#34; alt=&#34;fig1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can also combine this with the magick graphics device to post process or animate your plots and figures directly in knitr. Again no special packages or system dependencies are required.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Produce graphic
fig &amp;lt;- image_graph(width = 800, height = 600, res = 96)
ggplot2::qplot(factor(cyl), data = mtcars, fill = factor(gear))
invisible(dev.off())

print(fig)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/zFLcHws.png&#34; alt=&#34;fig2&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
# Some post-processing
frink &amp;lt;- image_read(&amp;quot;https://jeroen.github.io/images/frink.png&amp;quot;)

fig %&amp;gt;%
  image_rotate(10) %&amp;gt;%
  image_implode(.6) %&amp;gt;%
  image_composite(frink, offset = &amp;quot;+140+70&amp;quot;) %&amp;gt;%
  image_annotate(&amp;quot;Very usefull stuff&amp;quot;, size = 40, location = &amp;quot;+300+100&amp;quot;, color = &amp;quot;navy&amp;quot;, boxcolor = &amp;quot;pink&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/0E5cqaz.png&#34; alt=&#34;fig3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Same works for animation with &lt;code&gt;image_animate()&lt;/code&gt;; the figure shows automatically up in the report as a gif image:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;image_read(&amp;quot;https://jeroen.github.io/images/banana.gif&amp;quot;) %&amp;gt;%
  image_apply( function(banana){
    image_composite(fig, banana, offset = &amp;quot;+200+200&amp;quot;)
  }) %&amp;gt;%
  image_resize(&amp;quot;50%&amp;quot;) %&amp;gt;%
  image_animate()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/mi67gjt.gif&#34; alt=&#34;fig4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The magick vignette &lt;a href=&#34;https://raw.githubusercontent.com/ropensci/magick/master/vignettes/intro.Rmd&#34;&gt;source code&lt;/a&gt; is itself written in Rmarkdown, so it&amp;rsquo;s great example to see this in action. Try rendering it in RStudio to see how easy it is!&lt;/p&gt;

&lt;h2 id=&#34;magick-in-shiny-apps&#34;&gt;Magick in Shiny Apps&lt;/h2&gt;

&lt;p&gt;While we&amp;rsquo;re at it, several people had asked how to use magick images in shiny apps. The easiest way is to write the image to a &lt;code&gt;tempfile()&lt;/code&gt; within the &lt;code&gt;renderImage()&lt;/code&gt; callback function. For example the server part could look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;output$img &amp;lt;- renderImage({
    tmpfile &amp;lt;- image %&amp;gt;%
      image_resize(input$size) %&amp;gt;%
      image_implode(input$implode) %&amp;gt;%
      image_blur(input$blur, input$blur) %&amp;gt;%
      image_rotate(input$rotation) %&amp;gt;%
      image_write(tempfile(fileext=&#39;jpg&#39;), format = &#39;jpg&#39;)

  # Return a list
  list(src = tmpfile, contentType = &amp;quot;image/jpeg&amp;quot;)
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is a simple shiny app that demonstrates this. Have a look at the &lt;a href=&#34;https://github.com/jeroen/shinymagick/blob/master/app.R&#34;&gt;source code&lt;/a&gt; or just run it in R:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(shiny)
library(magick)
runGitHub(&amp;quot;jeroen/shinymagick&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://jeroen.shinyapps.io/shinymagick&#34;&gt;&lt;img src=&#34;https://i.imgur.com/tTFk7ig.jpg&#34; alt=&#34;tigrou&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Perhaps there&amp;rsquo;s an even better way to make this work by wrapping magick images into an htmlwidget but I have not figured this out yet.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Image Convolution in R using Magick</title>
      <link>https://ropensci.org/technotes/2017/11/02/image-convolve/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/11/02/image-convolve/</guid>
      <description>
        
        

&lt;p&gt;Release 1.4 of the &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick package&lt;/a&gt; introduces
a new feature called &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_(image_processing)#Convolution&#34;&gt;image convolution&lt;/a&gt; that
was requested by Thomas L. Pedersen. In this post we explain what this is all about.&lt;/p&gt;

&lt;h2 id=&#34;kernel-matrix&#34;&gt;Kernel Matrix&lt;/h2&gt;

&lt;p&gt;The new &lt;code&gt;image_convolve()&lt;/code&gt; function applies a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_(image_processing)&#34;&gt;kernel&lt;/a&gt; over the image. Kernel convolution means that each pixel value is recalculated using the &lt;em&gt;weighted neighborhood sum&lt;/em&gt; defined in the kernel matrix. For example lets look at this simple kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)

kern &amp;lt;- matrix(0, ncol = 3, nrow = 3)
kern[1, 2] &amp;lt;- 0.25
kern[2, c(1, 3)] &amp;lt;- 0.25
kern[3, 2] &amp;lt;- 0.25
kern
##      [,1] [,2] [,3]
## [1,] 0.00 0.25 0.00
## [2,] 0.25 0.00 0.25
## [3,] 0.00 0.25 0.00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This kernel changes each pixel to the mean of its horizontal and vertical neighboring pixels, which results in a slight blurring effect in the right-hand image below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img &amp;lt;- image_read(&#39;logo:&#39;)
img_blurred &amp;lt;- image_convolve(img, kern)
image_append(c(img, img_blurred))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Y6xByUL.gif&#34; alt=&#34;image_appended&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;standard-kernels&#34;&gt;Standard Kernels&lt;/h2&gt;

&lt;p&gt;Many operations in &lt;code&gt;magick&lt;/code&gt;  such as blurring, sharpening, and edge detection are
actually special cases of image convolution. The benefit of explicitly using
&lt;code&gt;image_convolve()&lt;/code&gt; is more control. For example, we can blur an image and then blend
it together with the original image in one step by mixing a blurring kernel with the
unit kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;Gaussian:0x5&#39;, scaling = &#39;60,40%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/6Vf6c2hl.gif&#34; alt=&#34;mixed&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The above requires a bit of explanation. ImageMagick defines several common
&lt;a href=&#34;http://www.imagemagick.org/Usage/convolve/&#34;&gt;standard kernels&lt;/a&gt; such as the
gaussian kernel. Most of the standard kernels take one or more parameters,
e.g. the example above used a gaussian kernel with 0 &lt;em&gt;radius&lt;/em&gt; and 5 &lt;em&gt;sigma&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In addition, &lt;code&gt;scaling&lt;/code&gt; argument defines the magnitude of the kernel, and possibly
how much of the original picture should be mixed in. Here we mix 60% of the
blurring with 40% of the original picture in order to get a diffused lightning effect.&lt;/p&gt;

&lt;h2 id=&#34;edge-detection&#34;&gt;Edge Detection&lt;/h2&gt;

&lt;p&gt;Another area where kernels are of use is in edge detection. A simple example of
a direction-aware edge detection kernel is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sobel_operator&#34;&gt;&lt;em&gt;Sobel&lt;/em&gt;&lt;/a&gt; kernel.
As can be seen below, vertical edges are detected while horizontals are not.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;Sobel&#39;) %&amp;gt;% image_negate()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/i8ndfCu.gif&#34; alt=&#34;edges&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Something less apparent is that the result of the edge detection is truncated.
Edge detection kernels can result in negative color values which get truncated to zero.
To combat this it is possible to add a &lt;code&gt;bias&lt;/code&gt; to the result. Often you&amp;rsquo;ll end up with
scaling the kernel to 50% and adding 50% bias to move the midpoint of the result to 50%
grey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;Sobel&#39;, scaling = &#39;50%&#39;, bias = &#39;50%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/llUawrg.gif&#34; alt=&#34;50pct&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sharpening&#34;&gt;Sharpening&lt;/h2&gt;

&lt;p&gt;ImageMagick has many more edge detection kernels, some of which are insensitive to
the direction of the edge. To emulate a classic high-pass filter from photoshop use
&lt;a href=&#34;https://en.wikipedia.org/wiki/Difference_of_Gaussians&#34;&gt;difference of gaussians&lt;/a&gt; kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;DoG:0,0,2&#39;) %&amp;gt;% image_negate()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/o5kODpc.gif&#34; alt=&#34;dog&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As with the blurring, the original image can be blended in with the transformed one, effectively sharpening the image along edges.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;DoG:0,0,2&#39;, scaling = &#39;100, 100%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/MtcMSn7.gif&#34; alt=&#34;combination&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://www.imagemagick.org/Usage/convolve/&#34;&gt;ImageMagick documentation&lt;/a&gt; has more examples of convolve with various avaiable kernels.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Changes to Internet Connectivity in R on Windows</title>
      <link>https://ropensci.org/technotes/2017/10/10/curl-30/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/10/10/curl-30/</guid>
      <description>
        
        

&lt;p&gt;This week we released version 3.0 of the &lt;a href=&#34;https://cran.r-project.org/web/packages/curl/vignettes/intro.html&#34;&gt;curl&lt;/a&gt; R package to CRAN. You may have never used this package directly, but &lt;code&gt;curl&lt;/code&gt; provides the foundation for most HTTP infrastructure in R, including &lt;code&gt;httr&lt;/code&gt;, &lt;code&gt;rvest&lt;/code&gt;, and all packages that build on it. If R packages need to go online, chances are traffic is going via curl.&lt;/p&gt;

&lt;p&gt;This release introduces an important change for Windows users: we are switching from OpenSSL to Secure Channel on Windows 7 / 2008-R2 and up. Let me explain this in a bit more detail.&lt;/p&gt;

&lt;h2 id=&#34;why-switching-ssl-backends&#34;&gt;Why Switching SSL backends&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://curl.haxx.se/libcurl/&#34;&gt;libcurl&lt;/a&gt; C library requires an external crypto library to provide the SSL layer (the S in HTTPS). On Linux / MacOS, libcurl is included with the OS so we don&amp;rsquo;t worry about this. However on Windows we ship our own build of libcurl so we can choose if we want to build against &lt;a href=&#34;https://www.openssl.org/&#34;&gt;OpenSSL&lt;/a&gt; or Windows native SSL api called &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/aa380123(v=vs.85).aspx&#34;&gt;Secure Channel&lt;/a&gt;, also referred to as just &amp;ldquo;WinSSL&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Thus far we have always used libcurl with OpenSSL, which works consistently on all versions of Windows. However OpenSSL requires that we provide our own CA bundle, which is not ideal. In particular users on corporate / government networks have reported difficulty connecting to the internet in R. The reason is often that their enterprise gateway / proxy uses custom certificates which are installed in the Windows certificate manager, but are not present in R&amp;rsquo;s bundle.&lt;/p&gt;

&lt;p&gt;Moreover shipping our own CA bundle can be a security risk. If a CA gets hacked, the corresponding certificate needs to be revoked immediately. Operating systems can quickly push a security update to all users, but we cannot do this in R.&lt;/p&gt;

&lt;h2 id=&#34;switching-to-winssl&#34;&gt;Switching to WinSSL&lt;/h2&gt;

&lt;p&gt;If we build libcurl against Windows native &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/aa380123(v=vs.85).aspx&#34;&gt;Secure Channel&lt;/a&gt;, it automatically uses the same SSL certificates as Internet Explorer. Hence we do not have to ship and maintain a custom CA bundle. Earlier this year I tried to switch the &lt;code&gt;curl&lt;/code&gt; package to WinSSL, and everything seemed to work great on my machine.&lt;/p&gt;

&lt;p&gt;However when we started checking reverse dependecies on CRAN WinBuilder, many packages depending on curl started to fail! It turned out Windows versions before Windows 7 do not natively support TLS 1.1 and 1.2 by default. Because TLS 1.2 is used by the majority of HTTPS servers today, WinSSL is basically useless on these machines. Unfortunately this also includes CRAN WinBuilder which runs Windows 2008 (the server edition of Vista).&lt;/p&gt;

&lt;p&gt;So we had no choice but to roll back to OpenSSL in order to keep everything working properly on CRAN. Bummer.&lt;/p&gt;

&lt;h2 id=&#34;towards-dual-ssl&#34;&gt;Towards Dual SSL&lt;/h2&gt;

&lt;p&gt;I had almost given up on this when a few weeks ago Daniel Stenberg posted the following &lt;a href=&#34;https://curl.haxx.se/mail/lib-2017-08/0118.html&#34;&gt;announcement&lt;/a&gt; on the libcurl mailing list:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hi friends!
As of minutes ago, libcurl has the ability to change SSL backend dynamically
at run-time - if built with the support enabled. That means that the choice
does no longer only have to happen at build-time.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This new feature gives us exactly the flexibility we need. We can take advantage of native Secure Channel on Windows 7 and up which are almost all users. However we can keep things working in legacy servers by falling back on OpenSSL on these machines, including the CRAN win builder.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cran/curl/blob/3.0/src/ssl.c#L11-L17&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/curl30.png&#34; alt=&#34;code&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So this is where we are. Version 3.0 of the curl R package uses the latest &lt;a href=&#34;https://github.com/rwinlib/libcurl/releases&#34;&gt;libcurl 7.56.0&lt;/a&gt; and automatically switches to native SSL on Windows 7 and up. If all goes well, nobody should not notice any changes, except those people on enterprise networks where things will, hopefully, magically start working.&lt;/p&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;Because each Windows network seems to have a different setup, testing and debugging these things is often difficult. We are interested to hear from Windows users if updating to curl 3.0 has improved the situation, or if any unexpected side effects arise. Please &lt;a href=&#34;https://github.com/jeroen/curl/issues&#34;&gt;open an issue&lt;/a&gt; on Github if you run into problems.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>googleLanguageR - Analysing language through the Google Cloud Machine Learning APIs</title>
      <link>https://ropensci.org/blog/2017/10/03/googlelanguager/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/03/googlelanguager/</guid>
      <description>
        
        

&lt;!-- open source image taken from: https://upload.wikimedia.org/wikipedia/commons/2/21/Bell_System_switchboard.jpg --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/switchboard.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the greatest assets human beings possess is the power of speech and language, from which almost all our other accomplishments flow. To be able to analyse communication offers us a chance to gain a greater understanding of one another.&lt;/p&gt;

&lt;p&gt;To help you with this, &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt;&lt;/a&gt; is an R package that allows you to perform speech-to-text transcription, neural net translation and natural language processing via the &lt;a href=&#34;https://cloud.google.com/products/machine-learning/&#34;&gt;Google Cloud machine learning services&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An introduction to the package is below, but you can find out more details at the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-s-bet&#34;&gt;Google&amp;rsquo;s bet&lt;/h3&gt;

&lt;p&gt;Google predicts that machine learning is to be a fundamental feature of business, and so they are looking to become the infrastructure that makes machine learning possible. Metaphorically speaking: If machine learning is electricity, then Google wants to be the pylons carrying it around the country.&lt;/p&gt;

&lt;!-- open source image taken from: https://pixabay.com/en/pylon-sky-electricity-tower-2515429/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/pylon.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Google may not be the only company with such ambitions, but one advantage Google has is the amount of data it possesses. Twenty years of web crawling has given it an unprecedented corpus to train its models.  In addition, its recent moves into voice and video gives it one of the biggest audio and speech datasets, all of which have been used to help create machine learning applications within its products such as search and Gmail. Further investment in machine learning is shown by Google&amp;rsquo;s purchase of &lt;a href=&#34;https://deepmind.com/&#34;&gt;Deepmind&lt;/a&gt;, a UK based A.I. research firm that recently was in the news for defeating the top Go champion with its neural network trained Go bot.  Google has also taken an open-source route with the creation and publication of &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt;, a leading machine learning framework.&lt;/p&gt;

&lt;p&gt;Whilst you can create your own machine learning models, for those users who haven&amp;rsquo;t the expertise, data or time to do so, Google also offers an increasing range of machine learning APIs that are pre-trained, such as image and video recognition or job search.  &lt;code&gt;googleLanguageR&lt;/code&gt; wraps the subset of those machine learning APIs that are language flavoured - Cloud Speech, Translation and Natural Language.&lt;/p&gt;

&lt;p&gt;Since they carry complementary outputs that can be used in each other&amp;rsquo;s input, all three of the APIs are included in one package. For example, you can transcribe a recording of someone speaking in Danish, translate that to English and then identify how positive or negative the writer felt about its content (sentiment analysis) then identify the most important concepts and objects within the content (entity analysis).&lt;/p&gt;

&lt;h3 id=&#34;motivations&#34;&gt;Motivations&lt;/h3&gt;

&lt;h4 id=&#34;fake-news&#34;&gt;Fake news&lt;/h4&gt;

&lt;p&gt;One reason why I started looking at this area was the growth of &amp;lsquo;fake news&amp;rsquo;, and its effect on political discourse on social media. I wondered if there was some way to put metrics on how much a news story fuelled one&amp;rsquo;s own bias within your own filter bubble.  The entity API provides a way to perform entity and sentiment analysis at scale on tweets, and by then comparing different users and news sources preferences the hope is to be able to judge how much they are in agreement with your own bias, views and trusted reputation sources.&lt;/p&gt;

&lt;h4 id=&#34;make-your-own-alexa&#34;&gt;Make your own Alexa&lt;/h4&gt;

&lt;p&gt;Another motivating application is the growth of voice commands that will become the primary way of user interface with technology.  Already, &lt;a href=&#34;https://www.thinkwithgoogle.com/data-gallery/detail/google-app-voice-search/&#34;&gt;Google reports up to 20% of search in its app&lt;/a&gt; is via voice search.  I&amp;rsquo;d like to be able to say &amp;ldquo;R, print me out that report for client X&amp;rdquo;.  A Shiny app that records your voice, uploads to the API then parses the return text into actions gives you a chance to create your very own Alexa-like infrastructure.&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;text-align:center&#34;&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/alexa.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The voice activated internet connected speaker, Amazon&amp;rsquo;s Alexa - image from www.amazon.co.uk&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;translate-everything&#34;&gt;Translate everything&lt;/h4&gt;

&lt;p&gt;Finally, I live and work in Denmark.  As Danish is only spoken by less than 6 million people, applications that work in English may not be available in Danish very quickly, if at all.  The API&amp;rsquo;s translation service is the one that made the news in 2016 for &lt;a href=&#34;https://research.googleblog.com/2016/09/a-neural-network-for-machine.html&#34;&gt;&amp;ldquo;inventing its own language&amp;rdquo;&lt;/a&gt;, and offers much better English to Danish translations that the free web version and may make services available in Denmark sooner.&lt;/p&gt;

&lt;h3 id=&#34;using-the-library&#34;&gt;Using the library&lt;/h3&gt;

&lt;p&gt;To use these APIs within R, you first need to do a one-time setup to create a Google Project, add a credit card and authenticate which is &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/#installation&#34;&gt;detailed on the package website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After that, you feed in the R objects you want to operate upon.  The &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/127&#34;&gt;rOpenSci review&lt;/a&gt; helped to ensure that this can scale up easily, so that you can feed in large character vectors which the library will parse and rate limit as required.  The functions also work within &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; pipe syntax.&lt;/p&gt;

&lt;h4 id=&#34;speech-to-text&#34;&gt;Speech-to-text&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/speech.html&#34;&gt;Cloud Speech API&lt;/a&gt; is exposed via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_speech.html&#34;&gt;&lt;code&gt;gl_speech&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It supports multiple audio formats and languages, and you can either feed a sub-60 second audio file directly, or perform asynchrnous requests for longer audio files.&lt;/p&gt;

&lt;p&gt;Example code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)

my_audio &amp;lt;- &amp;quot;my_audio_file.wav&amp;quot;
gl_speech(my_audio)
#  A tibble: 1 x 3
#  transcript confidence                 words
#* &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;                &amp;lt;list&amp;gt;
#1 Hello Mum  0.9227779 &amp;lt;data.frame [19 x 3]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;translation&#34;&gt;Translation&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/translation.html&#34;&gt;Cloud Translation API&lt;/a&gt; lets you translate text via &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_translate.html&#34;&gt;&lt;code&gt;gl_translate&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you are charged per character, one tip here if you are working with lots of different languages is to perform detection of language offline first using another rOpenSci package, &lt;a href=&#34;https://github.com/ropensci/cld2&#34;&gt;&lt;code&gt;cld2&lt;/code&gt;&lt;/a&gt;.  That way you can avoid charges for text that is already in your target language i.e. English.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
library(cld2)
library(purrr)

my_text &amp;lt;- c(&amp;quot;Katten sidder på måtten&amp;quot;, &amp;quot;The cat sat on the mat&amp;quot;)

## offline detect language via cld2
detected &amp;lt;- map_chr(my_text, detect_language)
# [1] &amp;quot;DANISH&amp;quot;  &amp;quot;ENGLISH&amp;quot;

## get non-English text
translate_me &amp;lt;- my_text[detected != &amp;quot;ENGLISH&amp;quot;]

## translate
gl_translate(translate_me)
## A tibble: 1 x 3
#                 translatedText detectedSourceLanguage                    text
#*                         &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;
#1 The cat is sitting on the mat                     da Katten sidder på måtten
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/nlp.html&#34;&gt;Natural Language API&lt;/a&gt; reveals the structure and meaning of text, accessible via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_nlp.html&#34;&gt;&lt;code&gt;gl_nlp&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It returns several analysis:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Entity analysis&lt;/em&gt; - finds named entities (currently proper names and common nouns) in the text along with entity types, salience, mentions for each entity, and other properties. If possible, will also return metadata about that entity such as a Wikipedia URL.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Syntax&lt;/em&gt; - analyzes the syntax of the text and provides sentence boundaries and tokenization along with part of speech tags, dependency trees, and other properties.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sentiment&lt;/em&gt; - the overall sentiment of the text, represented by a magnitude [0, +inf] and score between -1.0 (negative sentiment) and 1.0 (positive sentiment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all useful to get an understanding of the meaning of a sentence, and has potentially the greatest number of applications of the APIs featured.  With entity analysis, auto categorisation of text is possible; the syntax returns let you pull out nouns and verbs for parsing into other actions; and the sentiment analysis allows you to get a feeling for emotion within text.&lt;/p&gt;

&lt;p&gt;A demonstration is below which gives an idea of what output you can generate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
quote &amp;lt;- &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
nlp &amp;lt;- gl_nlp(quote)

str(nlp)
#List of 6
# $ sentences        :List of 1
#  ..$ :&#39;data.frame&#39;:	1 obs. of  4 variables:
#  .. ..$ content    : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
#  .. ..$ beginOffset: int 0
#  .. ..$ magnitude  : num 0.6
#  .. ..$ score      : num -0.6
# $ tokens           :List of 1
#  ..$ :&#39;data.frame&#39;:	20 obs. of  17 variables:
#  .. ..$ content       : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;things&amp;quot; &amp;quot;are&amp;quot; &amp;quot;infinite&amp;quot; ...
#  .. ..$ beginOffset   : int [1:20] 0 4 11 15 23 25 29 38 42 48 ...
#  .. ..$ tag           : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NOUN&amp;quot; &amp;quot;VERB&amp;quot; &amp;quot;ADJ&amp;quot; ...
#  .. ..$ aspect        : chr [1:20] &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; ...
#  .. ..$ case          : chr [1:20] &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; ...
#  .. ..$ form          : chr [1:20] &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; ...
#  .. ..$ gender        : chr [1:20] &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; ...
#  .. ..$ mood          : chr [1:20] &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;INDICATIVE&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; ...
#  .. ..$ number        : chr [1:20] &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;PLURAL&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; ...
#  .. ..$ person        : chr [1:20] &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; ...
#  .. ..$ proper        : chr [1:20] &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; ...
#  .. ..$ reciprocity   : chr [1:20] &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; ...
#  .. ..$ tense         : chr [1:20] &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;PRESENT&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; ...
#  .. ..$ voice         : chr [1:20] &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; ...
#  .. ..$ headTokenIndex: int [1:20] 1 2 2 2 2 6 2 6 9 6 ...
#  .. ..$ label         : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NSUBJ&amp;quot; &amp;quot;ROOT&amp;quot; &amp;quot;ACOMP&amp;quot; ...
#  .. ..$ value         : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;thing&amp;quot; &amp;quot;be&amp;quot; &amp;quot;infinite&amp;quot; ...
# $ entities         :List of 1
#  ..$ :Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	6 obs. of  9 variables:
#  .. ..$ name         : chr [1:6] &amp;quot;human stupidity&amp;quot; &amp;quot;things&amp;quot; &amp;quot;universe&amp;quot; &amp;quot;universe&amp;quot; ...
#  .. ..$ type         : chr [1:6] &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; ...
#  .. ..$ salience     : num [1:6] 0.1662 0.4771 0.2652 0.2652 0.0915 ...
#  .. ..$ mid          : Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ wikipedia_url: Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ magnitude    : num [1:6] NA NA NA NA NA NA
#  .. ..$ score        : num [1:6] NA NA NA NA NA NA
#  .. ..$ beginOffset  : int [1:6] 42 4 29 86 29 86
#  .. ..$ mention_type : chr [1:6] &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; ...
# $ language         : chr &amp;quot;en&amp;quot;
# $ text             : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
# $ documentSentiment:Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	1 obs. of  2 variables:
#  ..$ magnitude: num 0.6
#  ..$ score    : num -0.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;This package is 10 times better due to the efforts of the rOpenSci reviewers &lt;a href=&#34;http://enpiar.com/&#34;&gt;Neal Richardson&lt;/a&gt; and &lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt;, who have whipped the documentation, outputs and test cases into the form they are today in &lt;code&gt;0.1.0&lt;/code&gt;.  Many thanks to them.&lt;/p&gt;

&lt;p&gt;Hopefully, this is just the beginning and the package can be further improved by its users - if you do give the package a try and find a potential improvement, &lt;a href=&#34;https://github.com/ropensci/googleLanguageR/issues&#34;&gt;raise an issue on GitHub&lt;/a&gt; and we can try to implement it.  I&amp;rsquo;m excited to see what users can do with these powerful tools.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>The writexl package: zero dependency xlsx writer for R</title>
      <link>https://ropensci.org/technotes/2017/09/08/writexl-release/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/09/08/writexl-release/</guid>
      <description>
        
        

&lt;p&gt;We have started working on a new rOpenSci package called &lt;a href=&#34;https://github.com/ropensci/writexl#readme&#34;&gt;writexl&lt;/a&gt;. This package wraps the very powerful &lt;a href=&#34;https://libxlsxwriter.github.io/&#34;&gt;libxlsxwriter&lt;/a&gt; library which allows for exporting data to Microsoft Excel format.&lt;/p&gt;

&lt;p&gt;The major benefit of writexl over other packages is that it is completely written in C and has absolutely zero dependencies. No Java, Perl or Rtools are required.&lt;/p&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;write_xlsx&lt;/code&gt; function writes a data frame to an xlsx file. You can test that data roundtrips properly by reading it back using the readxl package. Columns containing dates and factors get automatically coerced to character strings.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(writexl)
library(readxl)
write_xlsx(iris, &amp;quot;iris.xlsx&amp;quot;)

# read it back
out &amp;lt;- read_xlsx(&amp;quot;iris.xlsx&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also give it a named list of data frames, in which case each data frame becomes a sheet in the xlsx file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;write_xlsx(list(iris = iris, cars = cars, mtcars = mtcars), &amp;quot;mydata.xlsx&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance is good too; in our benchmarks writexl is about twice as fast as openxlsx:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(microbenchmark)
library(nycflights13)
microbenchmark(
  writexl = writexl::write_xlsx(flights, tempfile()),
  openxlsx = openxlsx::write.xlsx(flights, tempfile()),
  times = 5
)
### Unit: seconds
###      expr       min        lq      mean    median        uq       max neval
###   writexl  8.884712  8.904431  9.103419  8.965643  9.041565  9.720743     5
###  openxlsx 17.166818 18.072527 19.171003 18.669805 18.756661 23.189206     5
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;roadmap&#34;&gt;Roadmap&lt;/h2&gt;

&lt;p&gt;The initial version of writexl implements the most important functionality for R users: exporting data frames. However the underlying &lt;a href=&#34;https://libxlsxwriter.github.io/&#34;&gt;libxlsxwriter&lt;/a&gt; library actually provides far more sophisticated functionality such as custom formatting, writing complex objects, formulas, etc.&lt;/p&gt;

&lt;p&gt;Most of this probably won&amp;rsquo;t be useful to R users. But if you have a well defined use case for exposing some specific features from the library in writexl, &lt;a href=&#34;https://github.com/ropensci/writexl/issues&#34;&gt;open an issue&lt;/a&gt; on Github and we&amp;rsquo;ll look into it!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Spelling 1.0: quick and effective spell checking in R</title>
      <link>https://ropensci.org/technotes/2017/09/07/spelling-release/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/09/07/spelling-release/</guid>
      <description>
        
        

&lt;p&gt;The new rOpenSci &lt;a href=&#34;https://cran.r-project.org/web/packages/spelling/index.html&#34;&gt;spelling&lt;/a&gt; package provides utilities for spell checking common document formats including latex, markdown, manual pages, and DESCRIPTION files. It also includes tools especially for package authors to automate spell checking of R documentation and vignettes.&lt;/p&gt;

&lt;h2 id=&#34;spell-checking-packages&#34;&gt;Spell Checking Packages&lt;/h2&gt;

&lt;p&gt;The main purpose of this package is to quickly find spelling errors in R packages. The &lt;code&gt;spell_check_package()&lt;/code&gt; function extracts all text from your package manual pages and vignettes, compares it against a language (e.g. en_US or en_GB), and lists potential errors in a nice tidy format:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; spelling::spell_check_package(&amp;quot;~/workspace/writexl&amp;quot;)
  WORD       FOUND IN
booleans   write_xlsx.Rd:21
xlsx       write_xlsx.Rd:6,18
           title:1
           description:1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results may contain false positives, i.e. names or technical jargon which does not appear in the English dictionary. Therefore you can create a WORDLIST file, which serves as a package-specific dictionary of allowed words:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; spelling::update_wordlist(&amp;quot;~/workspace/writexl&amp;quot;)
The following words will be added to the wordlist:
 - booleans
 - xlsx
Are you sure you want to update the wordlist?
1: Yes
2: No
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Words added to this file are ignored in the spell check, making it easier to catch actual spelling errors:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; spell_check_package(&amp;quot;~/workspace/writexl&amp;quot;)
No spelling errors found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The package also includes a cool function &lt;code&gt;spell_check_setup()&lt;/code&gt; which adds a unit test to your package that automatically runs the spell check.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; spelling::spell_check_setup(&amp;quot;~/workspace/writexl&amp;quot;)
No changes required to /Users/jeroen/workspace/writexl/inst/WORDLIST
Updated /Users/jeroen/workspace/writexl/tests/spelling.R
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By default this unit test will never actually fail; it merely displays potential spelling errors at the end of a &lt;code&gt;R CMD check&lt;/code&gt;. But you can configure it to fail if you&amp;rsquo;d like, which can be useful to automatically highlight spelling errors on e.g. Travis CI.&lt;/p&gt;

&lt;h2 id=&#34;under-the-hood&#34;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;The spelling package builds on &lt;a href=&#34;https://ropensci.org/blog/technotes/2016/09/12/hunspell-release-20&#34;&gt;hunspell&lt;/a&gt; which has a fully customizable spell checking engine. Most of the code in the spelling package is dedicated to parsing and extracting text from documents before feeding it to the spell checker.
For example, when spell checking an rmarkdown file, we first extract words from headers and paragraphs (but not urls or R syntax).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Spell check this post
&amp;gt; spelling::spell_check_files(&amp;quot;~/workspace/roweb/_posts/2017-09-07-spelling-release.md&amp;quot;, lang = &#39;en_US&#39;)
  WORD         FOUND IN
blog         2017-09-07-spelling-release.md:7
commonmark   2017-09-07-spelling-release.md:88
hunspell     2017-09-07-spelling-release.md:69
Jeroen       2017-09-07-spelling-release.md:7
knitr        2017-09-07-spelling-release.md:88
Ooms         2017-09-07-spelling-release.md:7
rmarkdown    2017-09-07-spelling-release.md:88
rOpenSci     2017-09-07-spelling-release.md:18
urls         2017-09-07-spelling-release.md:88
wordlist     2017-09-07-spelling-release.md:49
WORDLIST     2017-09-07-spelling-release.md:34
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To accomplish this, we use knitr to drop code chunks, and subsequently parse markdown using &lt;a href=&#34;https://ropensci.org/blog/blog/2016/12/02/commonmark&#34;&gt;commonmark&lt;/a&gt; and xml2, which gives us the text nodes and approximate line numbers in the source document.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>FedData - Getting assorted geospatial data into R</title>
      <link>https://ropensci.org/technotes/2017/08/24/feddata-release/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/08/24/feddata-release/</guid>
      <description>
        
        

&lt;p&gt;The package &lt;a href=&#34;https://github.com/ropensci/FedData&#34;&gt;&lt;code&gt;FedData&lt;/code&gt;&lt;/a&gt; has gone through software review and is now part of &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt;. &lt;code&gt;FedData&lt;/code&gt; includes functions to automate downloading geospatial data available from several federated data sources (mainly sources maintained by the US Federal government).&lt;/p&gt;

&lt;p&gt;Currently, the package enables extraction from six datasets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;http://ned.usgs.gov&#34;&gt;National Elevation Dataset (NED)&lt;/a&gt; digital elevation models (1 and &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; arc-second; USGS)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://nhd.usgs.gov&#34;&gt;National Hydrography Dataset (NHD)&lt;/a&gt; (USGS)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://websoilsurvey.sc.egov.usda.gov/&#34;&gt;Soil Survey Geographic (SSURGO) database&lt;/a&gt; from the National Cooperative Soil Survey (NCSS), which is led by the Natural Resources Conservation Service (NRCS) under the USDA,&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn&#34;&gt;Global Historical Climatology Network (GHCN)&lt;/a&gt;, coordinated by National Climatic Data Center at NOAA,&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://daymet.ornl.gov/&#34;&gt;Daymet&lt;/a&gt; gridded estimates of daily weather parameters for North America, version 3, available from the Oak Ridge National Laboratory&amp;rsquo;s Distributed Active Archive Center (DAAC), and&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://www.ncdc.noaa.gov/data-access/paleoclimatology-data/datasets/tree-ring&#34;&gt;International Tree Ring Data Bank (ITRDB)&lt;/a&gt;, coordinated by National Climatic Data Center at NOAA.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; is designed with the large-scale geographic information system (GIS) use-case in mind: cases where the use of dynamic web-services is impractical due to the scale (spatial and/or temporal) of analysis. It functions primarily as a means of downloading tiled or otherwise spatially-defined datasets; additionally, it can preprocess those datasets by extracting data within an area of interest (AoI), defined spatially. It relies heavily on the &lt;a href=&#34;https://cran.r-project.org/package=sp&#34;&gt;&lt;code&gt;sp&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/package=raster&#34;&gt;&lt;code&gt;raster&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://cran.r-project.org/package=rgdal&#34;&gt;&lt;code&gt;rgdal&lt;/code&gt;&lt;/a&gt; packages.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Load &lt;code&gt;FedData&lt;/code&gt; and define a study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# FedData Tester
library(FedData)
library(magrittr)

# Extract data for the Village Ecodynamics Project &amp;quot;VEPIIN&amp;quot; study area:
# http://veparchaeology.org
vepPolygon &amp;lt;- polygon_from_extent(raster::extent(672800, 740000, 4102000, 4170000),
                                  proj4string = &amp;quot;+proj=utm +datum=NAD83 +zone=12&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get and plot the National Elevation Dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NED (USA ONLY)
# Returns a raster
NED &amp;lt;- get_ned(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;)
# Plot with raster::plot
raster::plot(NED)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the Daymet dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the DAYMET (North America only)
# Returns a raster
DAYMET &amp;lt;- get_daymet(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;,
               elements = c(&amp;quot;prcp&amp;quot;,&amp;quot;tmax&amp;quot;),
               years = 1980:1985)
# Plot with raster::plot
raster::plot(DAYMET$tmax$X1985.10.23)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the daily GHCN precipitation data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the daily GHCN data (GLOBAL)
# Returns a list: the first element is the spatial locations of stations,
# and the second is a list of the stations and their daily data
GHCN.prcp &amp;lt;- get_ghcn_daily(template = vepPolygon,
                            label = &amp;quot;VEPIIN&amp;quot;,
                            elements = c(&#39;prcp&#39;))
# Plot the NED again
raster::plot(NED)
# Plot the spatial locations
sp::plot(GHCN.prcp$spatial,
         pch = 1,
         add = TRUE)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend=&amp;quot;GHCN Precipitation Records&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the daily GHCN temperature data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Elements for which you require the same data
# (i.e., minimum and maximum temperature for the same days)
# can be standardized using standardize==T
GHCN.temp &amp;lt;- get_ghcn_daily(template = vepPolygon,
                            label = &amp;quot;VEPIIN&amp;quot;,
                            elements = c(&#39;tmin&#39;,&#39;tmax&#39;),
                            years = 1980:1985,
                            standardize = TRUE)
# Plot the NED again
raster::plot(NED)
# Plot the spatial locations
sp::plot(GHCN.temp$spatial,
         add = TRUE,
         pch = 1)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend = &amp;quot;GHCN Temperature Records&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the National Hydrography Dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NHD (USA ONLY)
NHD &amp;lt;- get_nhd(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;)
# Plot the NED again
raster::plot(NED)
# Plot the NHD data
NHD %&amp;gt;%
  lapply(sp::plot,
         col = &#39;black&#39;,
         add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the NRCS SSURGO data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NRCS SSURGO data (USA ONLY)
SSURGO.VEPIIN &amp;lt;- get_ssurgo(template = vepPolygon,
                     label = &amp;quot;VEPIIN&amp;quot;)
#&amp;gt; Warning: 1 parsing failure.
#&amp;gt; row # A tibble: 1 x 5 col     row     col               expected actual expected   &amp;lt;int&amp;gt;   &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; actual 1  1276 slope.r no trailing characters     .5 file # ... with 1 more variables: file &amp;lt;chr&amp;gt;
# Plot the NED again
raster::plot(NED)
# Plot the SSURGO mapunit polygons
plot(SSURGO.VEPIIN$spatial,
     lwd = 0.1,
     add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the NRCS SSURGO data for particular soil survey areas&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Or, download by Soil Survey Area names
SSURGO.areas &amp;lt;- get_ssurgo(template = c(&amp;quot;CO670&amp;quot;,&amp;quot;CO075&amp;quot;),
                           label = &amp;quot;CO_TEST&amp;quot;)

# Let&#39;s just look at spatial data for CO675
SSURGO.areas.CO675 &amp;lt;- SSURGO.areas$spatial[SSURGO.areas$spatial$AREASYMBOL==&amp;quot;CO075&amp;quot;,]

# And get the NED data under them for pretty plotting
NED.CO675 &amp;lt;- get_ned(template = SSURGO.areas.CO675,
                            label = &amp;quot;SSURGO_CO675&amp;quot;)

# Plot the SSURGO mapunit polygons, but only for CO675
plot(NED.CO675)
plot(SSURGO.areas.CO675,
     lwd = 0.1,
     add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the ITRDB chronology locations in the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the ITRDB records
ITRDB &amp;lt;- get_itrdb(template = vepPolygon,
                        label = &amp;quot;VEPIIN&amp;quot;,
                        makeSpatial = TRUE)
# Plot the NED again
raster::plot(NED)
# Map the locations of the tree ring chronologies
plot(ITRDB$metadata,
     pch = 1,
     add = TRUE)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend = &amp;quot;ITRDB chronologies&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-13-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;todo&#34;&gt;TODO&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The current CRAN version of &lt;code&gt;FedData&lt;/code&gt;, v2.4.6, will (hopefully) be the final CRAN release of &lt;code&gt;FedData&lt;/code&gt; 2. &lt;code&gt;FedData&lt;/code&gt; 3 will be released in the coming months, but some code built on &lt;code&gt;FedData&lt;/code&gt; 2 will not be compatible with FedData 3.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; was initially developed prior to widespread use of modern web mapping services and RESTful APIs by many Federal data-holders. Future releases of &lt;code&gt;FedData&lt;/code&gt; will limit data transfer by utilizing server-side geospatial and data queries. We will also implement &lt;a href=&#34;https://github.com/hadley/dplyr&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/a&gt; verbs, tidy data structures, (&lt;a href=&#34;https://github.com/tidyverse/magrittr&#34;&gt;&lt;code&gt;magrittr&lt;/code&gt;&lt;/a&gt;) piping, functional programming using &lt;a href=&#34;https://github.com/hadley/purrr&#34;&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/a&gt;, simple features for spatial data from &lt;a href=&#34;https://github.com/edzer/sfr&#34;&gt;&lt;code&gt;sf&lt;/code&gt;&lt;/a&gt;, and local data storage in OGC-compliant data formats (probably GeoJSON and NetCDF). I am also aiming for 100% testing coverage.&lt;/p&gt;

&lt;p&gt;All that being said, much of the functionality of the &lt;code&gt;FedData&lt;/code&gt; package could be spun off into more domain-specific packages. For example, ITRDB download functions could be part of the &lt;a href=&#34;https://r-forge.r-project.org/projects/dplr/&#34;&gt;&lt;code&gt;dplR&lt;/code&gt;&lt;/a&gt; dendrochronology package; concepts/functions having to do with the GHCN data integrated into &lt;a href=&#34;https://github.com/ropensci/rnoaa&#34;&gt;&lt;code&gt;rnoaa&lt;/code&gt;&lt;/a&gt;; and Daymet concepts integrated into &lt;a href=&#34;https://github.com/khufkens/daymetr&#34;&gt;&lt;code&gt;daymetr&lt;/code&gt;&lt;/a&gt;. I welcome any and all suggestions about how to improve the utility of FedData; please &lt;a href=&#34;https://github.com/ropensci/FedData/issues&#34;&gt;submit an issue&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; is a product of SKOPE (&lt;a href=&#34;http://www.openskope.org&#34;&gt;Synthesizing Knowledge of Past Environments&lt;/a&gt;) and the &lt;a href=&#34;http://veparchaeology.org/&#34;&gt;Village Ecodynamics Project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; was reviewed for &lt;a href=&#34;https://ropensci.org&#34;&gt;rOpenSci&lt;/a&gt; by &lt;a href=&#34;https://github.com/jooolia&#34;&gt;@jooolia&lt;/a&gt;, with &lt;a href=&#34;https://github.com/sckott&#34;&gt;@sckott&lt;/a&gt; as onboarding editor, and was greatly improved as a result.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Tesseract and Magick: High Quality OCR in R</title>
      <link>https://ropensci.org/technotes/2017/08/17/tesseract-16/</link>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/08/17/tesseract-16/</guid>
      <description>
        
        

&lt;p&gt;Last week we released an update of the tesseract package to CRAN. This package provides R bindings to Google&amp;rsquo;s OCR library &lt;a href=&#34;https://en.wikipedia.org/wiki/Tesseract_(software)&#34;&gt;Tesseract&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;tesseract&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The new version ships with the latest libtesseract 3.05.01 on Windows and MacOS. Furthermore it includes enhancements for managing language data and using tesseract together with the magick package.&lt;/p&gt;

&lt;h2 id=&#34;installing-language-data&#34;&gt;Installing Language Data&lt;/h2&gt;

&lt;p&gt;The new version has several improvements for installing additional language data. On Windows and MacOS you use the &lt;code&gt;tesseract_download()&lt;/code&gt; function to install additional languages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tesseract_download(&amp;quot;fra&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Language data are now stored in &lt;code&gt;rappdirs::user_data_dir(&#39;tesseract&#39;)&lt;/code&gt; which makes it persist across updates of the package. To OCR french text:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;french &amp;lt;- tesseract(&amp;quot;fra&amp;quot;)
text &amp;lt;- ocr(&amp;quot;https://jeroen.github.io/images/french_text.png&amp;quot;, engine = french)
cat(text)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Très Bien! Note that on Linux you should not use &lt;code&gt;tesseract_download&lt;/code&gt; but instead install languages using apt-get (e.g. &lt;a href=&#34;https://packages.debian.org/testing/tesseract-ocr-fra&#34;&gt;tesseract-ocr-fra&lt;/a&gt;) or yum (e.g. &lt;a href=&#34;https://apps.fedoraproject.org/packages/tesseract-langpack-fra&#34;&gt;tesseract-langpack-fra&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;tesseract-and-magick&#34;&gt;Tesseract and Magick&lt;/h2&gt;

&lt;p&gt;The tesseract developers &lt;a href=&#34;https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality&#34;&gt;recommend&lt;/a&gt; to clean up the image before OCR&amp;rsquo;ing it to improve the quality of the output. This involves things like cropping out the text area, rescaling, increasing contrast, etc.&lt;/p&gt;

&lt;p&gt;The rOpenSci &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/15/magick-10&#34;&gt;magick&lt;/a&gt; package is perfectly suitable for this task. The latest version contains a convenient wrapper &lt;code&gt;image_ocr()&lt;/code&gt; that works with pipes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/magick&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s give it a try on some &lt;a href=&#34;https://courses.cs.vt.edu/csonline/AI/Lessons/VisualProcessing/OCRscans.html&#34;&gt;example scans&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://courses.cs.vt.edu/csonline/AI/Lessons/VisualProcessing/OCRscans_files/bowers.jpg&#34; alt=&#34;example&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Requires devel version of magick
# devtools::install_github(&amp;quot;ropensci/magick&amp;quot;)

# Test it
library(magick)
library(magrittr)

text &amp;lt;- image_read(&amp;quot;https://courses.cs.vt.edu/csonline/AI/Lessons/VisualProcessing/OCRscans_files/bowers.jpg&amp;quot;) %&amp;gt;%
  image_resize(&amp;quot;2000&amp;quot;) %&amp;gt;%
  image_convert(colorspace = &#39;gray&#39;) %&amp;gt;%
  image_trim() %&amp;gt;%
  image_ocr()

cat(text)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The Llfe and Work of
Fredson Bowers
by
G. THOMAS TANSELLE

N EVERY FIELD OF ENDEAVOR THERE ARE A FEW FIGURES WHOSE ACCOM-
plishment and inﬂuence cause them to be the symbols of their age;
their careers and oeuvres become the touchstones by which the
ﬁeld is measured and its history told. In the related pursuits of
analytical and descriptive bibliography, textual criticism, and scholarly
editing, Fredson Bowers was such a ﬁgure, dominating the four decades
after 1949, when his Principles of Bibliographical Description was pub-
lished. By 1973 the period was already being called “the age of Bowers”:
in that year Norman Sanders, writing the chapter on textual scholarship
for Stanley Wells&#39;s Shakespeare: Select Bibliographies, gave this title to
a section of his essay. For most people, it would be achievement enough
to rise to such a position in a ﬁeld as complex as Shakespearean textual
studies; but Bowers played an equally important role in other areas.
Editors of ninetcemh-cemury American authors, for example, would
also have to call the recent past “the age of Bowers,&amp;quot; as would the writers
of descriptive bibliographies of authors and presses. His ubiquity in
the broad ﬁeld of bibliographical and textual study, his seemingly com-
plete possession of it, distinguished him from his illustrious predeces-
sors and made him the personiﬁcation of bibliographical scholarship in

his time.

\Vhen in 1969 Bowers was awarded the Gold Medal of the Biblio-
graphical Society in London, John Carter’s citation referred to the
Principles as “majestic,&amp;quot; called Bowers&#39;s current projects “formidable,&amp;quot;
said that he had “imposed critical discipline&amp;quot; on the texts of several
authors, described Studies in Bibliography as a “great and continuing
achievement,&amp;quot; and included among his characteristics &amp;quot;uncompromising
seriousness of purpose” and “professional intensity.&amp;quot; Bowers was not
unaccustomed to such encomia, but he had also experienced his share of
attacks: his scholarly positions were not universally popular, and he
expressed them with an aggressiveness that almost seemed calculated to
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not bad but not perfect. Can you do a better job?&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Magick 1.0: 🎩 ✨🐇 Advanced Graphics and Image Processing in R </title>
      <link>https://ropensci.org/blog/2017/08/15/magick-10/</link>
      <pubDate>Tue, 15 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/08/15/magick-10/</guid>
      <description>
        
        

&lt;p&gt;Last week, version 1.0 of the &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/index.html&#34;&gt;magick&lt;/a&gt; package appeared on CRAN: an ambitious effort to modernize and simplify high quality image processing in R. This R package builds upon the &lt;a href=&#34;https://www.imagemagick.org/Magick++/STL.html&#34;&gt;Magick++ STL&lt;/a&gt; which exposes a powerful C++ API to the famous ImageMagick library.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jeroen.github.io/images/magick.png&#34; alt=&#34;RStudio Screenshot&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The best place to start learning about magick is the &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;vignette&lt;/a&gt; which gives a brief overview of the overwhelming amount of functionality in this package.&lt;/p&gt;

&lt;h3 id=&#34;towards-release-1-0&#34;&gt;Towards Release 1.0&lt;/h3&gt;

&lt;p&gt;Last year around this time rOpenSci &lt;a href=&#34;https://ropensci.org/blog/blog/2016/08/23/z-magick-release&#34;&gt;announced&lt;/a&gt; the first release of the magick package: a new powerful toolkit for image reading, writing, converting, editing, transformation, annotation, and animation in R. Since the initial release there have been several updates with additional functionality, and many useRs have started to discover the power of this package to take visualization in R to the next level.&lt;/p&gt;

&lt;p&gt;For example &lt;a href=&#34;https://twitter.com/hrbrmstr/status/758304420224466944&#34;&gt;Bob Rudis&lt;/a&gt; uses magick to visualize California drought data from the U.S. Drought Monitor (click on the image to go find out more):&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://rud.is/b/2016/07/27/u-s-drought-animations-with-the-witchs-brew-purrr-broom-magick/&#34;&gt;&lt;img src=&#34;https://jeroen.github.io/images/drought.gif&#34; alt=&#34;drought&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;R-ladies &lt;a href=&#34;http://www.lucymcgowan.com/&#34;&gt;Lucy D&amp;rsquo;Agostino McGowan&lt;/a&gt; and &lt;a href=&#34;http://www.masalmon.eu/&#34;&gt;Maëlle Salmon&lt;/a&gt; demonstrate how to make a beautiful collage:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://livefreeordichotomize.com/2017/07/18/the-making-of-we-r-ladies/&#34;&gt;&lt;img src=&#34;http://livefreeordichotomize.com/images/we-r-ladies.jpeg&#34; alt=&#34;collage&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And &lt;a href=&#34;https://twitter.com/danielphadley/status/884845188979359744&#34;&gt;Daniel P. Hadley&lt;/a&gt; lets Vincent Vega explains Cars:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://danielphadley.com/ggplot-Logo/&#34;&gt;&lt;img src=&#34;http://danielphadley.com/images/Cars_Travolta.gif&#34; alt=&#34;travolta&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now, 1 year later, the 1.0 release marks an important milestone: the addition of a new native graphics device (which serves as a hybrid between a magick image object and an R plot) bridges the gap between graphics and image processing in R.&lt;/p&gt;

&lt;p&gt;This blog post explains how the magick device allows you to seamlessly combine graphing with image processing in R. You can either use it to post-process your R graphics, or draw on imported images using the native R plotting machinery. We hope that this unified interface will make it easier to produce beautiful, reproducible images with R.&lt;/p&gt;

&lt;h3 id=&#34;native-magick-graphics&#34;&gt;Native Magick Graphics&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;image_graph()&lt;/code&gt; function opens a new graphics device similar to e.g. &lt;code&gt;png()&lt;/code&gt; or &lt;code&gt;x11()&lt;/code&gt;. It returns an image object to which the plot(s) will be written. Each page in the plotting device will become a frame (layer) in the image object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Produce image using graphics device
fig &amp;lt;- image_graph(res = 96)
ggplot2::qplot(mpg, wt, data = mtcars, colour = cyl)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;fig&lt;/code&gt; object now contains the image that we can easily post-process. For example we can overlay another image:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;logo &amp;lt;- image_read(&amp;quot;https://www.r-project.org/logo/Rlogo.png&amp;quot;)
out &amp;lt;- image_composite(fig, image_scale(logo, &amp;quot;x150&amp;quot;), offset = &amp;quot;+80+380&amp;quot;)

# Show preview
image_browse(out)

# Write to file
image_write(out, &amp;quot;myplot.png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-15-magick-10/out.png&#34; alt=&#34;out&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;drawing-device&#34;&gt;Drawing Device&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;image_draw()&lt;/code&gt; function opens a graphics device to draw on top of an existing image using pixel coordinates.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Open a file
library(magick)
frink &amp;lt;- image_read(&amp;quot;https://jeroen.github.io/images/frink.png&amp;quot;)
drawing &amp;lt;- image_draw(frink)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://jeroen.github.io/images/frink.png&#34; alt=&#34;frink&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can now use R&amp;rsquo;s native low-level graphics functions for drawing on top of the image:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rect(20, 20, 200, 100, border = &amp;quot;red&amp;quot;, lty = &amp;quot;dashed&amp;quot;, lwd = 5)
abline(h = 300, col = &#39;blue&#39;, lwd = &#39;10&#39;, lty = &amp;quot;dotted&amp;quot;)
text(10, 250, &amp;quot;Hoiven-Glaven&amp;quot;, family = &amp;quot;courier&amp;quot;, cex = 4, srt = 90)
palette(rainbow(11, end = 0.9))
symbols(rep(200, 11), seq(0, 400, 40), circles = runif(11, 5, 35),
  bg = 1:11, inches = FALSE, add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At any point you can inspect the current result:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;image_browse(drawing)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-15-magick-10/drawing.png&#34; alt=&#34;drawing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Once you are done you can close the device and save the result.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dev.off()
image_write(drawing, &#39;drawing.png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By default &lt;code&gt;image_draw()&lt;/code&gt; sets all margins to 0 and uses graphics coordinates to match image size in pixels (width x height) where (0,0) is the top left corner. Note that this means the y axis increases from top to bottom which is the opposite of typical graphics coordinates. You can override all this by passing custom &lt;code&gt;xlim&lt;/code&gt;, &lt;code&gt;ylim&lt;/code&gt; or &lt;code&gt;mar&lt;/code&gt; values to &lt;code&gt;image_draw()&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;animated-graphics&#34;&gt;Animated Graphics&lt;/h3&gt;

&lt;p&gt;The graphics device supports multiple frames which makes it easy to create animated graphics. The example below shows how you would implement the example from the very cool &lt;a href=&#34;https://github.com/dgrtwo/gganimate&#34;&gt;gganimate&lt;/a&gt; package using the magick.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(gapminder)
library(ggplot2)
library(magick)
img &amp;lt;- image_graph(res = 96)
datalist &amp;lt;- split(gapminder, gapminder$year)
out &amp;lt;- lapply(datalist, function(data){
  p &amp;lt;- ggplot(data, aes(gdpPercap, lifeExp, size = pop, color = continent)) +
    scale_size(&amp;quot;population&amp;quot;, limits = range(gapminder$pop)) +
    scale_x_log10(limits = range(gapminder$gdpPercap)) +
    geom_point() + ylim(20, 90) +  ggtitle(data$year) + theme_classic()
  print(p)
})
dev.off()
animation &amp;lt;- image_animate(img, fps = 2)
image_write(animation, &amp;quot;animation.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-08-15-magick-10/animation.gif&#34; alt=&#34;animation&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We hope that the magick package can provide a more robust back-end for packages like gganimate to produce interactive graphics in R without requiring the user to manually install external image editing software.&lt;/p&gt;

&lt;h3 id=&#34;porting-imagemagick-commands-to-r&#34;&gt;Porting ImageMagick Commands to R&lt;/h3&gt;

&lt;p&gt;The magick 1.0 release now has the core image processing functionality that you expect from an image processing package. But there is still a lot of room for improvement to make magick &lt;em&gt;the&lt;/em&gt; image processing package in R.&lt;/p&gt;

&lt;p&gt;A lot of R users and packages currently shell out to ImageMagick command line tools for performing image manipulations. The goal is to support all these operations in the magick package, so that the images can be produced (and reproduced!) on any platform without requiring the user to install additional software.&lt;/p&gt;

&lt;p&gt;Note that ImageMagick library is over 26 years old and has accumulated an enormous number of features in those years. Porting all of this to R is quite a bit of work, for which feedback from users is important. If there is an imagemagick operation that you like to do in R but you can&amp;rsquo;t figure out how, please &lt;a href=&#34;https://github.com/ropensci/magick/issues&#34;&gt;open an issue&lt;/a&gt; on GitHub. If the functionality is currently not supported yet, we will try to add it to the next version.&lt;/p&gt;

&lt;h3 id=&#34;image-analysis&#34;&gt;Image Analysis&lt;/h3&gt;

&lt;p&gt;Currently magick is focused on generating and editing images. There is yet another entirely different set of features which we like to support related to analyzing images. Image analysis can involve anything from calculating color distributions to more sophisticated feature extraction and vision tools. I am not very familiar with this field, so again we could use suggestions from users and experts.&lt;/p&gt;

&lt;p&gt;One feature that is already available is the &lt;code&gt;image_ocr()&lt;/code&gt; function which extracts text from the image using the rOpenSci &lt;a href=&#34;https://ropensci.org/blog/blog/2016/11/16/tesseract&#34;&gt;tesseract&lt;/a&gt; package. Another cool example of using image analysis is the &lt;a href=&#34;https://github.com/ThinkRstat/collage&#34;&gt;collage&lt;/a&gt; package which calculates &lt;a href=&#34;https://github.com/ThinkRstat/collage#histograms&#34;&gt;color histograms&lt;/a&gt; to select appropriate tile images for creating a collage.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ThinkRstat/collage#histograms&#34;&gt;&lt;img src=&#34;https://github.com/ThinkRstat/collage/raw/master/README-histograms-2.png&#34; alt=&#34;histogram&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As part of supporting supporting analysis tools we plan to extract the bitmap (raster) classes into a separate package. This will enable package authors to write R extensions to analyze and manipulate on the raw image data, without necessarily depending on magick. Yet the user can always rely on magick as a powerful toolkit to import/export images and graphics into such low level bitmaps.&lt;/p&gt;

&lt;div class=&#34;col-sm-11 col-sm-offset-1&#34;&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;a href=&#34;https://twitter.com/grbails/status/885543687559811073&#34;&gt;&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>The rOpenSci Taxonomy Suite</title>
      <link>https://ropensci.org/blog/2017/07/27/taxonomy-suite/</link>
      <pubDate>Thu, 27 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/07/27/taxonomy-suite/</guid>
      <description>
        
        

&lt;h2 id=&#34;what-is-taxonomy&#34;&gt;What is Taxonomy?&lt;/h2&gt;

&lt;p&gt;Taxonomy in its most general sense is &lt;a href=&#34;https://en.wikipedia.org/wiki/Taxonomy_(general)&#34;&gt;the practice and science of classification&lt;/a&gt;. It can refer to many things. You may have heard or used the word &lt;em&gt;taxonomy&lt;/em&gt; used to indicate any sort of classification of things, whether it be companies or widgets. Here, we&amp;rsquo;re talking about &lt;a href=&#34;https://en.wikipedia.org/wiki/Taxonomy_(biology)&#34;&gt;biological taxonomy&lt;/a&gt;, the science of defining and naming groups of biological organisms.&lt;/p&gt;

&lt;p&gt;In case you aren&amp;rsquo;t familiar with the terminology, here&amp;rsquo;s a brief intro.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;species&lt;/code&gt; - the term you are likely most familiar with, usually defined as a group of individuals in which any 2 individuals can produce fertile offspring, although definitions can vary.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;genus&lt;/code&gt;/&lt;code&gt;family&lt;/code&gt;/&lt;code&gt;order&lt;/code&gt;/&lt;code&gt;class&lt;/code&gt;/&lt;code&gt;phylum&lt;/code&gt;/&lt;code&gt;kingdom&lt;/code&gt; -  These are nested groupings of similar species. &lt;code&gt;genus&lt;/code&gt; (e.g. &lt;em&gt;Homo&lt;/em&gt;) is  restrictive grouping and &lt;code&gt;kingdom&lt;/code&gt; (e.g. &lt;em&gt;Animalia&lt;/em&gt;) is a much more inclusive grouping. There are genera in families, families in orders, etc.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;taxon&lt;/code&gt; - a species or grouping of species. e.g. &lt;em&gt;Homo sapiens&lt;/em&gt;, &lt;em&gt;Primates&lt;/em&gt;, and &lt;em&gt;Animalia&lt;/em&gt; are all taxa.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;taxa&lt;/code&gt; - the plural of &lt;code&gt;taxon&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;taxonomic hierarchy&lt;/code&gt; or &lt;code&gt;taxonomic classification&lt;/code&gt; - the list of groups a species (or other taxon) belongs to. For example the taxonomic classification of humans is: &lt;code&gt;Animalia;Chordata;Mammalia;Primates;Hominidae;Homo;sapiens&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ubiquity-and-importance-of-taxonomic-names&#34;&gt;Ubiquity and Importance of Taxonomic Names&lt;/h2&gt;

&lt;p&gt;We put a lot of time into our suite of taxonomic software for a good reason - probably all naturalists/biologists/environmental consultants/etc. will be confronted with taxonomic names in their research/work/surveys/etc. at some point or all along the way. Some people study a single species their entire career, likely having little trouble with taxonomic names - while others study entire communities or ecosystems, dealing with thousands of taxonomic names.&lt;/p&gt;

&lt;p&gt;Taxonomic names are not only ubiquitous but are incredibly important to get right. Just as the URL points to the correct page you want to view on the internet (an incorrect URL will not get you where you want to go), taxonomic names point to the right definition/description of a taxon, leading to lots of resources increasingly online including text, images, sounds, etc. If you get the taxonomic name wrong, all information downstream is likely to be wrong.&lt;/p&gt;

&lt;h2 id=&#34;why-r-for-taxonomic-names&#34;&gt;Why R for taxonomic names?&lt;/h2&gt;

&lt;p&gt;R is gaining in popularity in general (&lt;a href=&#34;https://www.tiobe.com/tiobe-index//&#34;&gt;TIOBE index&lt;/a&gt;, &lt;a href=&#34;http://r4stats.com/articles/popularity/&#34;&gt;Muenchen 2017&lt;/a&gt;), and in &lt;a href=&#34;http://www.nature.com/news/programming-tools-adventures-with-r-1.16609&#34;&gt;academia&lt;/a&gt;. At least in my graduate school experience (&amp;lsquo;06 - &amp;lsquo;12), most graduate students used R - despite their bosses often using other things.&lt;/p&gt;

&lt;p&gt;Given that R is widely used among biologists that have to deal with taxonomic names, it makes a lot of sense to build taxonomic tools in R.&lt;/p&gt;

&lt;h2 id=&#34;ropensci-taxonomy-suite&#34;&gt;rOpenSci Taxonomy Suite&lt;/h2&gt;

&lt;p&gt;We have an ever-growing suite of packages that enable users to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Search for taxonomic names&lt;/li&gt;
&lt;li&gt;Correct taxonomic names&lt;/li&gt;
&lt;li&gt;Embed their taxonomic names in R classes that enable powerful downstream manipulations&lt;/li&gt;
&lt;li&gt;Leverage dozens of taxonomic data sources&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The packages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;taxize&lt;/code&gt; - taxonomic data from many sources&lt;/li&gt;
&lt;li&gt;&lt;code&gt;taxizedb&lt;/code&gt; - work with taxonomic SQL databases locally&lt;/li&gt;
&lt;li&gt;&lt;code&gt;taxa&lt;/code&gt; - taxonomic classes and manipulation functions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;binomen&lt;/code&gt; - taxonomic name classes and parsing methods (getting folded into &lt;code&gt;taxa&lt;/code&gt;, will be archived on CRAN soon)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wikitaxa&lt;/code&gt; - taxonomic data from Wikipedia/Wikidata/Wikispecies&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ritis&lt;/code&gt; - get ITIS (Integrated Taxonomic Information Service) taxonomic data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worrms&lt;/code&gt; - get WORMS (World Register of Marine Species) taxonomic data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pegax&lt;/code&gt; - taxonomy PEG (Parsing Expression Grammar)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;For each package below, there are 2-3 badges. One for whether the package is on CRAN
&lt;br&gt;
&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312; color:white&#34;&gt;cran&lt;/span&gt;
&lt;br&gt;
a link to source on GitHub
&lt;br&gt;
&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB; color:white&#34;&gt;github&lt;/span&gt;
&lt;br&gt;
and another for when the package is community contributed:&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#19B698; color:white&#34;&gt;community&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;For each package we show a very brief example - all packages have much more functionality - check them out on CRAN or GitHub.&lt;/p&gt;

&lt;h2 id=&#34;taxize&#34;&gt;taxize&lt;/h2&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/taxize/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/taxize&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;This was our first package for taxonomy. It is a one stop shop for lots of different taxonomic data sources online, including NCBI, ITIS, GBIF, EOL, IUCN, and more - up to 22 data sources now.&lt;/p&gt;

&lt;p&gt;The canonical reference for &lt;code&gt;taxize&lt;/code&gt; is the paper we published in 2013:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Chamberlain, S. A., &amp;amp; Szöcs, E. (2013). taxize: taxonomic search and retrieval in R. F1000Research.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Check it out at &lt;a href=&#34;https://doi.org/10.12688/f1000research.2-191.v1&#34;&gt;https://doi.org/10.12688/f1000research.2-191.v1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We released a new version (&lt;code&gt;v0.8.8&lt;/code&gt;) about a month ago (a tiny bug fix was pushed more recently (&lt;code&gt;v0.8.9&lt;/code&gt;)) with some new features requested by users:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can now get downstream taxa from NCBI, see &lt;code&gt;ncbi_downstream&lt;/code&gt; and &lt;code&gt;downstream&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Wikipedia/Wikidata/Wikispecies are now data sources! via the &lt;code&gt;wikitaxa&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;Now you can get IUCN IDs for taxa, see &lt;code&gt;get_iucn&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tax_rank&lt;/code&gt; now works with many more data sources: ncbi, itis, eol, col, tropicos, gbif, nbn,
worms, natserv, and bold&lt;/li&gt;
&lt;li&gt;Many improvements and bug fixes&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;A quick example of the power of &lt;code&gt;taxize&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;taxize&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;taxize&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get WORMS identifiers for three taxa:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ids &amp;lt;- get_wormsid(c(&amp;quot;Platanista gangetica&amp;quot;, &amp;quot;Lichenopora neapolitana&amp;quot;, &#39;Gadus morhua&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get classifications for each taxon&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;clazz &amp;lt;- classification(ids, db = &#39;worms&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Combine all three into a single data.frame&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(rbind(clazz))
#&amp;gt;            name       rank     id  query
#&amp;gt; 1      Animalia    Kingdom      2 254967
#&amp;gt; 2      Chordata     Phylum   1821 254967
#&amp;gt; 3    Vertebrata  Subphylum 146419 254967
#&amp;gt; 4 Gnathostomata Superclass   1828 254967
#&amp;gt; 5     Tetrapoda Superclass   1831 254967
#&amp;gt; 6      Mammalia      Class   1837 254967
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;taxizedb&#34;&gt;taxizedb&lt;/h2&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/taxizedb/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/taxizedb&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;taxizedb&lt;/code&gt; is a relatively new package. We just released a new version (&lt;code&gt;v0.1.4&lt;/code&gt;) about one month ago, with fixes for the new &lt;code&gt;dplyr&lt;/code&gt; version.&lt;/p&gt;

&lt;p&gt;The sole purpose of &lt;code&gt;taxizedb&lt;/code&gt; is to solve the use case where a user has a lot of taxonomic names, and thus using &lt;code&gt;taxize&lt;/code&gt; is too slow. Although &lt;code&gt;taxize&lt;/code&gt; is a powerful tool, every request is a transaction over the internet, and the speed of that transaction can vary from very fast to very slow, depending on three factors: data provider speed (including many things), your internet speed, and how much data you requested. &lt;code&gt;taxizedb&lt;/code&gt; gets around this problem by using a local SQL database of the same stuff the data providers have, so you can get things done much faster.&lt;/p&gt;

&lt;p&gt;The trade-off with &lt;code&gt;taxizedb&lt;/code&gt; is that the interface is quite different from &lt;code&gt;taxize&lt;/code&gt;.  So there is a learning curve. There are two options in &lt;code&gt;taxizedb&lt;/code&gt;: you can use SQL syntax, or &lt;code&gt;dplyr&lt;/code&gt; commands. I&amp;rsquo;m guessing people are more familiar with the latter.&lt;/p&gt;

&lt;h3 id=&#34;example-1&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;Install &lt;code&gt;taxizedb&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;taxizedb&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;taxizedb&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, we show working with the ITIS SQL database. Other sources work with the same workflow of function calls.&lt;/p&gt;

&lt;p&gt;Download ITIS SQL database&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- db_download_itis()
#&amp;gt; downloading...
#&amp;gt; unzipping...
#&amp;gt; cleaning up...
#&amp;gt; [1] &amp;quot;/Users/sacmac/Library/Caches/R/taxizedb/ITIS.sql&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;db_load_tpl()&lt;/code&gt; loads the SQL database into Postgres. Data sources vary in the SQL database used, see help for more.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;db_load_tpl(x, &amp;quot;&amp;lt;your Postgresql user name&amp;gt;&amp;quot;, &amp;quot;your Postgresql password, if any&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a &lt;code&gt;src&lt;/code&gt; object to connect to the SQL database.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;src &amp;lt;- src_itis(&amp;quot;&amp;lt;your Postgresql user name&amp;gt;&amp;quot;, &amp;quot;your Postgresql password, if any&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Query!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dbplyr)
library(dplyr)
tbl(src, sql(&amp;quot;select * from taxonomic_units limit 10&amp;quot;))
# Source:   SQL [?? x 26]
# Database: postgres 9.6.0 [sacmac@localhost:5432/ITIS]
     tsn unit_ind1                          unit_name1 unit_ind2 unit_name2 unit_ind3 unit_name3 unit_ind4 unit_name4
   &amp;lt;int&amp;gt;     &amp;lt;chr&amp;gt;                               &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;
 1    50      &amp;lt;NA&amp;gt; Bacteria                                 &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
 2    51      &amp;lt;NA&amp;gt; Schizomycetes                            &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
 3    52      &amp;lt;NA&amp;gt; Archangiaceae                            &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
 4    53      &amp;lt;NA&amp;gt; Pseudomonadales                          &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
 5    54      &amp;lt;NA&amp;gt; Rhodobacteriineae                        &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
 6    55      &amp;lt;NA&amp;gt; Pseudomonadineae                         &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
 7    56      &amp;lt;NA&amp;gt; Nitrobacteraceae                         &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
 8    57      &amp;lt;NA&amp;gt; Nitrobacter                              &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
 9    58      &amp;lt;NA&amp;gt; Nitrobacter                              &amp;lt;NA&amp;gt;     agilis      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
10    59      &amp;lt;NA&amp;gt; Nitrobacter                              &amp;lt;NA&amp;gt;     flavus      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;
# ... with more rows, and 17 more variables: unnamed_taxon_ind &amp;lt;chr&amp;gt;, name_usage &amp;lt;chr&amp;gt;, unaccept_reason &amp;lt;chr&amp;gt;,
#   credibility_rtng &amp;lt;chr&amp;gt;, completeness_rtng &amp;lt;chr&amp;gt;, currency_rating &amp;lt;chr&amp;gt;, phylo_sort_seq &amp;lt;int&amp;gt;, initial_time_stamp &amp;lt;dttm&amp;gt;,
#   parent_tsn &amp;lt;int&amp;gt;, taxon_author_id &amp;lt;int&amp;gt;, hybrid_author_id &amp;lt;int&amp;gt;, kingdom_id &amp;lt;int&amp;gt;, rank_id &amp;lt;int&amp;gt;, update_date &amp;lt;date&amp;gt;,
#   uncertain_prnt_ind &amp;lt;chr&amp;gt;, n_usage &amp;lt;chr&amp;gt;, complete_name &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;taxa&#34;&gt;taxa&lt;/h2&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/taxa/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/taxa&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;taxa&lt;/code&gt; is our newest entry (hit CRAN just a few weeks ago) into the taxonomic R package space. It defines taxonomic classes for R, and basic, but powerful manipulations on those classes.&lt;/p&gt;

&lt;p&gt;It defines two broad types of classes: those with just taxonomic data, and a class with taxonomic data plus other associated data (such as traits, environmental data, etc.) called &lt;code&gt;taxmap&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;taxa&lt;/code&gt; package includes functions to do various operations with these taxonomic classes. With the taxonomic classes, you can filter out or keep taxa based on various criteria. In the case of the &lt;code&gt;taxmap&lt;/code&gt; class, when you filter on taxa, the associated data is filtered the same way so taxa and data are in sync.&lt;/p&gt;

&lt;p&gt;A manuscript about &lt;code&gt;taxa&lt;/code&gt; is being prepared at the moment - so look out for that.&lt;/p&gt;

&lt;p&gt;Most of the hard work in &lt;code&gt;taxa&lt;/code&gt; has been done by my co-maintainer &lt;a href=&#34;https://github.com/zachary-foster&#34;&gt;Zachary Foster&lt;/a&gt;!&lt;/p&gt;

&lt;h3 id=&#34;example-2&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;A quick example of the power of &lt;code&gt;taxa&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;taxa&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;taxa&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An example &lt;code&gt;Hierarchy&lt;/code&gt; data object that comes with the package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ex_hierarchy1
#&amp;gt; &amp;lt;Hierarchy&amp;gt;
#&amp;gt;   no. taxon&#39;s:  3
#&amp;gt;   Poaceae / family / 4479
#&amp;gt;   Poa / genus / 4544
#&amp;gt;   Poa annua / species / 93036
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can remove taxa like the following, combining criteria targeting ranks, taxonomic names, or IDs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ex_hierarchy1 %&amp;gt;% pop(ranks(&amp;quot;family&amp;quot;), ids(4544))
#&amp;gt; &amp;lt;Hierarchy&amp;gt;
#&amp;gt;   no. taxon&#39;s:  1
#&amp;gt;   Poa annua / species / 93036
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An example &lt;code&gt;taxmap&lt;/code&gt; class:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ex_taxmap
#&amp;gt; &amp;lt;Taxmap&amp;gt;
#&amp;gt;   17 taxa: b. Mammalia ... q. lycopersicum, r. tuberosum
#&amp;gt;   17 edges: NA-&amp;gt;b, NA-&amp;gt;c, b-&amp;gt;d ... j-&amp;gt;o, k-&amp;gt;p, l-&amp;gt;q, l-&amp;gt;r
#&amp;gt;   4 data sets:
#&amp;gt;     info:
#&amp;gt;       # A tibble: 6 x 4
#&amp;gt;           name n_legs dangerous taxon_id
#&amp;gt;         &amp;lt;fctr&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;lgl&amp;gt;    &amp;lt;chr&amp;gt;
#&amp;gt;       1  tiger      4      TRUE        m
#&amp;gt;       2    cat      4     FALSE        n
#&amp;gt;       3   mole      4     FALSE        o
#&amp;gt;       # ... with 3 more rows
#&amp;gt;     phylopic_ids:  e148eabb-f138-43c6-b1e4-5cda2180485a ... 63604565-0406-460b-8cb8-1abe954b3f3a
#&amp;gt;     foods: a list with 6 items
#&amp;gt;     And 1 more data sets: abund
#&amp;gt;   1 functions:
#&amp;gt;  reaction
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, filter by taxonomic names to those starting with the letter &lt;code&gt;t&lt;/code&gt; (notice the taxa, edgelist, and datasets have changed)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;filter_taxa(ex_taxmap, startsWith(taxon_names, &amp;quot;t&amp;quot;))
#&amp;gt; &amp;lt;Taxmap&amp;gt;
#&amp;gt;   3 taxa: m. tigris, o. typhlops, r. tuberosum
#&amp;gt;   3 edges: NA-&amp;gt;m, NA-&amp;gt;o, NA-&amp;gt;r
#&amp;gt;   4 data sets:
#&amp;gt;     info:
#&amp;gt;       # A tibble: 3 x 4
#&amp;gt;           name n_legs dangerous taxon_id
#&amp;gt;         &amp;lt;fctr&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;lgl&amp;gt;    &amp;lt;chr&amp;gt;
#&amp;gt;       1  tiger      4      TRUE        m
#&amp;gt;       2   mole      4     FALSE        o
#&amp;gt;       3 potato      0     FALSE        r
#&amp;gt;     phylopic_ids:  e148eabb-f138-43c6-b1e4-5cda2180485a ... 63604565-0406-460b-8cb8-1abe954b3f3a
#&amp;gt;     foods: a list with 3 items
#&amp;gt;     And 1 more data sets: abund
#&amp;gt;   1 functions:
#&amp;gt;  reaction
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;wikitaxa&#34;&gt;wikitaxa&lt;/h2&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/wikitaxa/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/wikitaxa&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;wikitaxa&lt;/code&gt; is a client that allows you to get taxonomic data from four different Wiki-* sites:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Wikipedia&lt;/li&gt;
&lt;li&gt;Wikispecies&lt;/li&gt;
&lt;li&gt;Wikidata&lt;/li&gt;
&lt;li&gt;Wikicommons&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Only Wikispecies is focused on taxonomy - for the others you could use &lt;code&gt;wikitaxa&lt;/code&gt; to do any searches, but we look for and parse out taxonomic specific items in the wiki objects that are returned.&lt;/p&gt;

&lt;p&gt;We released a new version (&lt;code&gt;v0.1.4&lt;/code&gt;) earlier this year. Big thanks to &lt;a href=&#34;https://github.com/ezwelty&#34;&gt;Ethan Welty&lt;/a&gt; for help on this package.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;wikitaxa&lt;/code&gt; is used in &lt;code&gt;taxize&lt;/code&gt; to get Wiki* data.&lt;/p&gt;

&lt;h3 id=&#34;example-3&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;A quick example of the power of &lt;code&gt;wikitaxa&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;wikitaxa&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;wikitaxa&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Search for &lt;em&gt;Malus domestica&lt;/em&gt; (apple):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- wt_wikispecies(name = &amp;quot;Malus domestica&amp;quot;)
# links to language sites for the taxon
res$langlinks
#&amp;gt; # A tibble: 12 x 5
#&amp;gt;     lang                                                   url    langname
#&amp;gt;  * &amp;lt;chr&amp;gt;                                                 &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;
#&amp;gt;  1   ast        https://ast.wikipedia.org/wiki/Malus_domestica    Asturian
#&amp;gt;  2    es         https://es.wikipedia.org/wiki/Malus_domestica     Spanish
#&amp;gt;  3    hu              https://hu.wikipedia.org/wiki/Nemes_alma   Hungarian
#&amp;gt;  4    ia         https://ia.wikipedia.org/wiki/Malus_domestica Interlingua
#&amp;gt;  5    it         https://it.wikipedia.org/wiki/Malus_domestica     Italian
#&amp;gt;  6   nds              https://nds.wikipedia.org/wiki/Huusappel  Low German
#&amp;gt;  7    nl           https://nl.wikipedia.org/wiki/Appel_(plant)       Dutch
#&amp;gt;  8    pl https://pl.wikipedia.org/wiki/Jab%C5%82o%C5%84_domowa      Polish
#&amp;gt;  9   pms        https://pms.wikipedia.org/wiki/Malus_domestica Piedmontese
#&amp;gt; 10    pt         https://pt.wikipedia.org/wiki/Malus_domestica  Portuguese
#&amp;gt; 11    sk https://sk.wikipedia.org/wiki/Jablo%C5%88_dom%C3%A1ca      Slovak
#&amp;gt; 12    vi         https://vi.wikipedia.org/wiki/Malus_domestica  Vietnamese
#&amp;gt; # ... with 2 more variables: autonym &amp;lt;chr&amp;gt;, `*` &amp;lt;chr&amp;gt;
# any external links on the page
res$externallinks
#&amp;gt; [1] &amp;quot;https://web.archive.org/web/20090115062704/http://www.ars-grin.gov/cgi-bin/npgs/html/taxon.pl?104681&amp;quot;
# any common names, and the language they are from
res$common_names
#&amp;gt; # A tibble: 19 x 2
#&amp;gt;               name   language
#&amp;gt;              &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;
#&amp;gt;  1          Ябълка  български
#&amp;gt;  2    Poma, pomera     català
#&amp;gt;  3           Apfel    Deutsch
#&amp;gt;  4     Aed-õunapuu      eesti
#&amp;gt;  5           Μηλιά   Ελληνικά
#&amp;gt;  6           Apple    English
#&amp;gt;  7         Manzano    español
#&amp;gt;  8           Pomme   français
#&amp;gt;  9           Melâr     furlan
#&amp;gt; 10        사과나무     한국어
#&amp;gt; 11          ‘Āpala    Hawaiʻi
#&amp;gt; 12            Melo   italiano
#&amp;gt; 13           Aapel Nordfriisk
#&amp;gt; 14  Maçã, Macieira  português
#&amp;gt; 15 Яблоня домашняя    русский
#&amp;gt; 16   Tarhaomenapuu      suomi
#&amp;gt; 17            Elma     Türkçe
#&amp;gt; 18  Яблуня домашня українська
#&amp;gt; 19          Pomaro     vèneto
# the taxonomic hierarchy - or classification
res$classification
#&amp;gt; # A tibble: 8 x 2
#&amp;gt;          rank          name
#&amp;gt;         &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;
#&amp;gt; 1 Superregnum     Eukaryota
#&amp;gt; 2      Regnum       Plantae
#&amp;gt; 3      Cladus   Angiosperms
#&amp;gt; 4      Cladus      Eudicots
#&amp;gt; 5      Cladus Core eudicots
#&amp;gt; 6      Cladus        Rosids
#&amp;gt; 7      Cladus    Eurosids I
#&amp;gt; 8        Ordo       Rosales
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ritis&#34;&gt;ritis&lt;/h2&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/ritis/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/ritis&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;ritis&lt;/code&gt; is a client for ITIS (Integrated Taxonomic Information Service), part of &lt;a href=&#34;https://www.usgs.gov/&#34;&gt;USGS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a number of different ways to get ITIS data, one of which (local SQL dump) is available in &lt;code&gt;taxizedb&lt;/code&gt;, while the others are covered in &lt;code&gt;ritis&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SOLR web service &lt;a href=&#34;https://www.itis.gov/solr_documentation.html&#34;&gt;https://www.itis.gov/solr_documentation.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RESTful web service &lt;a href=&#34;https://www.itis.gov/web_service.html&#34;&gt;https://www.itis.gov/web_service.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The functions that use the SOLR service are: &lt;code&gt;itis_search&lt;/code&gt;, &lt;code&gt;itis_facet&lt;/code&gt;, &lt;code&gt;itis_group&lt;/code&gt;, and &lt;code&gt;itis_highlight&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;All other functions interact with the RESTful web service.&lt;/p&gt;

&lt;p&gt;We released a new version (&lt;code&gt;v0.5.4&lt;/code&gt;) late last year.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ritis&lt;/code&gt; is used in &lt;code&gt;taxize&lt;/code&gt; to get ITIS data.&lt;/p&gt;

&lt;h3 id=&#34;example-4&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;A quick example of the power of &lt;code&gt;ritis&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;ritis&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;ritis&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Search for blue oak ( &lt;em&gt;Quercus douglasii&lt;/em&gt; )&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;search_scientific(&amp;quot;Quercus douglasii&amp;quot;)
#&amp;gt; # A tibble: 1 x 12
#&amp;gt;         author      combinedName kingdom   tsn unitInd1 unitInd2 unitInd3
#&amp;gt; *        &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;lgl&amp;gt;    &amp;lt;lgl&amp;gt;    &amp;lt;lgl&amp;gt;
#&amp;gt; 1 Hook. &amp;amp; Arn. Quercus douglasii Plantae 19322       NA       NA       NA
#&amp;gt; # ... with 5 more variables: unitInd4 &amp;lt;lgl&amp;gt;, unitName1 &amp;lt;chr&amp;gt;,
#&amp;gt; #   unitName2 &amp;lt;chr&amp;gt;, unitName3 &amp;lt;lgl&amp;gt;, unitName4 &amp;lt;lgl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get taxonomic hierarchy down from the Oak genus - that is, since it&amp;rsquo;s a genus, get all species in the Oak genus&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- search_scientific(&amp;quot;Quercus&amp;quot;)
hierarchy_down(res[1,]$tsn)
#&amp;gt; # A tibble: 207 x 5
#&amp;gt;    parentname parenttsn rankname          taxonname   tsn
#&amp;gt;  *      &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
#&amp;gt;  1    Quercus     19276  Species    Quercus falcata 19277
#&amp;gt;  2    Quercus     19276  Species     Quercus lyrata 19278
#&amp;gt;  3    Quercus     19276  Species  Quercus michauxii 19279
#&amp;gt;  4    Quercus     19276  Species      Quercus nigra 19280
#&amp;gt;  5    Quercus     19276  Species  Quercus palustris 19281
#&amp;gt;  6    Quercus     19276  Species    Quercus phellos 19282
#&amp;gt;  7    Quercus     19276  Species Quercus virginiana 19283
#&amp;gt;  8    Quercus     19276  Species Quercus macrocarpa 19287
#&amp;gt;  9    Quercus     19276  Species   Quercus coccinea 19288
#&amp;gt; 10    Quercus     19276  Species  Quercus agrifolia 19289
#&amp;gt; # ... with 197 more rows
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;worrms&#34;&gt;worrms&lt;/h2&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/worrms/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/worrms&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;worrms&lt;/code&gt; is a client for working with data from World Register of Marine Species (WoRMS).&lt;/p&gt;

&lt;p&gt;WoRMS is the most authoritative list of names of all marine species globally.&lt;/p&gt;

&lt;p&gt;We released our first version (&lt;code&gt;v0.1.0&lt;/code&gt;) earlier this year.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;worrms&lt;/code&gt; is used in &lt;code&gt;taxize&lt;/code&gt; to get WoRMS data.&lt;/p&gt;

&lt;h3 id=&#34;example-5&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;A quick example of the power of &lt;code&gt;worrms&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;worrms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;worrms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get taxonomic name synonyms for salmon ( &lt;em&gt;Oncorhynchus&lt;/em&gt; )&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xx &amp;lt;- wm_records_name(&amp;quot;Oncorhynchus&amp;quot;, fuzzy = FALSE)
wm_synonyms(id = xx$AphiaID)
#&amp;gt; # A tibble: 4 x 25
#&amp;gt;   AphiaID                                                           url
#&amp;gt; *   &amp;lt;int&amp;gt;                                                         &amp;lt;chr&amp;gt;
#&amp;gt; 1  296858 http://www.marinespecies.org/aphia.php?p=taxdetails&amp;amp;id=296858
#&amp;gt; 2  397908 http://www.marinespecies.org/aphia.php?p=taxdetails&amp;amp;id=397908
#&amp;gt; 3  397909 http://www.marinespecies.org/aphia.php?p=taxdetails&amp;amp;id=397909
#&amp;gt; 4  297397 http://www.marinespecies.org/aphia.php?p=taxdetails&amp;amp;id=297397
#&amp;gt; # ... with 23 more variables: scientificname &amp;lt;chr&amp;gt;, authority &amp;lt;chr&amp;gt;,
#&amp;gt; #   status &amp;lt;chr&amp;gt;, unacceptreason &amp;lt;chr&amp;gt;, rank &amp;lt;chr&amp;gt;, valid_AphiaID &amp;lt;int&amp;gt;,
#&amp;gt; #   valid_name &amp;lt;chr&amp;gt;, valid_authority &amp;lt;chr&amp;gt;, kingdom &amp;lt;chr&amp;gt;, phylum &amp;lt;chr&amp;gt;,
#&amp;gt; #   class &amp;lt;chr&amp;gt;, order &amp;lt;chr&amp;gt;, family &amp;lt;chr&amp;gt;, genus &amp;lt;chr&amp;gt;, citation &amp;lt;chr&amp;gt;,
#&amp;gt; #   lsid &amp;lt;chr&amp;gt;, isMarine &amp;lt;int&amp;gt;, isBrackish &amp;lt;lgl&amp;gt;, isFreshwater &amp;lt;lgl&amp;gt;,
#&amp;gt; #   isTerrestrial &amp;lt;int&amp;gt;, isExtinct &amp;lt;lgl&amp;gt;, match_type &amp;lt;chr&amp;gt;, modified &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pegax&#34;&gt;pegax&lt;/h2&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://github.com/ropensci/pegax&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;pegax&lt;/code&gt; aims to be a powerful taxonomic name parser for R. This package started at &lt;a href=&#34;http://unconf17.ropensci.org/&#34;&gt;#runconf17&lt;/a&gt; - was made possible because the talented &lt;a href=&#34;https://github.com/Ironholds/&#34;&gt;Oliver Keyes&lt;/a&gt; created a &lt;a href=&#34;https://en.wikipedia.org/wiki/Parsing_expression_grammar&#34;&gt;Parsing Expression Grammar&lt;/a&gt; package for R: &lt;a href=&#34;https://github.com/Ironholds/piton&#34;&gt;piton&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From &lt;code&gt;piton&lt;/code&gt; PEGs are:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;a way of defining formal grammars for formatted data that allow you to identify matched structures and then take actions on them&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some great taxonomic name parsing does exist already. &lt;a href=&#34;https://github.com/GlobalNamesArchitecture/gnparser&#34;&gt;Global Names Parser, gnparser&lt;/a&gt; is a great effort by &lt;a href=&#34;https://github.com/dimus&#34;&gt;Dmitry Mozzherin&lt;/a&gt; and others. The only problem is Java does not play nice with R - thus &lt;code&gt;pegax&lt;/code&gt;, implementing in C++. We&amp;rsquo;ll definitely try to learn alot from the work they have done on &lt;code&gt;gnparser&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pegax&lt;/code&gt; is not on CRAN yet.  The package is in very very early days, so expect lots of changes.&lt;/p&gt;

&lt;h3 id=&#34;example-6&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;A quick example of the power of &lt;code&gt;pegax&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropenscilabs/pegax&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;pegax&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Parse out authority name&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;authority_names(&amp;quot;Linnaeus, 1758&amp;quot;)
#&amp;gt; [1] &amp;quot;Linnaeus&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Parse out authority year&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;authority_years(&amp;quot;Linnaeus, 1758&amp;quot;)
#&amp;gt; [1] &amp;quot;1758&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;taxonomic-adjacent-packages&#34;&gt;Taxonomic adjacent packages&lt;/h2&gt;

&lt;p&gt;These packages do not primarily deal with taxonomy, but do include taxonomic data. No examples are included below, but do check out their vignettes and other documentation to get started.&lt;/p&gt;

&lt;h3 id=&#34;rotl&#34;&gt;rotl&lt;/h3&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/rotl/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/rotl&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt; &lt;span class=&#34;label&#34; style=&#34;background-color:#19B698&#34;&gt;community&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;rotl&lt;/code&gt; is maintained by &lt;a href=&#34;https://github.com/fmichonneau&#34;&gt;Francois Michonneau&lt;/a&gt;, &lt;a href=&#34;https://github.com/josephwb&#34;&gt;Joseph Brown&lt;/a&gt;, and &lt;a href=&#34;https://github.com/dwinter&#34;&gt;David Winter&lt;/a&gt;, and is a package to interact with the &lt;a href=&#34;https://opentreeoflife.org/&#34;&gt;Open Tree of Life (OTL)&lt;/a&gt;. OTL main purpose is perhaps about phylogeny data, but they do have a taxonomy they maintain, and &lt;code&gt;rotl&lt;/code&gt; has functions that let you access that taxonomic data.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rotl&lt;/code&gt; is used in &lt;code&gt;taxize&lt;/code&gt; to get OTL data.&lt;/p&gt;

&lt;h3 id=&#34;rredlist&#34;&gt;rredlist&lt;/h3&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/rredlist/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/rredlist&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;rredlist&lt;/code&gt; is an interface to the &lt;a href=&#34;http://www.iucnredlist.org/&#34;&gt;IUCN Redlist of Threatened Species&lt;/a&gt;,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;which provides taxonomic, conservation status and distribution information on plants, fungi and animals that have been globally evaluated using the IUCN Red List Categories and Criteria.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;rredlist&lt;/code&gt; is used in &lt;code&gt;taxize&lt;/code&gt; to get IUCN Redlist Taxonomy data.&lt;/p&gt;

&lt;h3 id=&#34;bold&#34;&gt;bold&lt;/h3&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/bold/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/bold&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;bold&lt;/code&gt; is an interface to the &lt;a href=&#34;http://www.iucnredlist.org/&#34;&gt;IUCN Redlist of Threatened Species&lt;/a&gt;,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;which provides taxonomic, conservation status and distribution information on plants, fungi and animals that have been globally evaluated using the IUCN Red List Categories and Criteria.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;bold&lt;/code&gt; is used in &lt;code&gt;taxize&lt;/code&gt; to get BOLD taxonomy data.&lt;/p&gt;

&lt;h3 id=&#34;rgbif&#34;&gt;rgbif&lt;/h3&gt;

&lt;div class=&#34;labels&#34;&gt;
&lt;a href=&#34;https://cran.rstudio.com/web/packages/rgbif/&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#F1C312&#34;&gt;cran&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ropensci/rgbif&#34;&gt;&lt;span class=&#34;label&#34; style=&#34;background-color:#3598DB&#34;&gt;github&lt;/span&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;rgbif&lt;/code&gt; is an interface to the &lt;a href=&#34;http://www.gbif.org/&#34;&gt;Global Biodiversity Information Facility&lt;/a&gt;, the largest provider of free and open access biodiversity data.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rgbif&lt;/code&gt; is used in &lt;code&gt;taxize&lt;/code&gt; to get GBIF taxonomy data.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Together, the rOpenSci taxonomy suite of packages make it much easier to work with taxonomy data in R. We hope you agree :)&lt;/p&gt;

&lt;p&gt;Despite all of the above, we still have some things to work on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;taxa&lt;/code&gt; taxonomy classes where appropriate. We plan to deploy &lt;code&gt;taxa&lt;/code&gt; classes inside of the &lt;code&gt;taxize&lt;/code&gt; package very soon, but they may be appropriate elsewhere as well. Using the same classes in many packages will make working with taxonomic data more consistent across packages.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;taxizedb&lt;/code&gt; needs to be more robust. Given that the package not only touches your file system, and for some databases depends on different SQL databases, we likely will run into many problems with various operating system + database combinations. Please do kick the tires and get back to us!&lt;/li&gt;
&lt;li&gt;Once &lt;code&gt;pegax&lt;/code&gt; is ready for use, we&amp;rsquo;ll be able to use it in many packages whenever we need to parse taxonomic names.&lt;/li&gt;
&lt;li&gt;They&amp;rsquo;ll always be more data sources that we can potentially add to &lt;code&gt;taxize&lt;/code&gt; - get in touch and let us know.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What do you think about the taxonomic suite of packages?  Anything we&amp;rsquo;re missing? Anything we can be doing better with any of the packages?  Are you working on a taxonomic R package? Consider &lt;a href=&#34;https://github.com/ropensci/onboarding&#34;&gt;submitting to rOpenSci&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>notary - Signing &amp; Verification of R Packages</title>
      <link>https://ropensci.org/blog/2017/07/25/notary/</link>
      <pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/07/25/notary/</guid>
      <description>
        
        

&lt;p&gt;Most of us who work in R just want to Get Stuff Done&amp;trade;. We want a minimum amount of friction between ourselves and the data we need to wrangle, analyze, and visualize. We&amp;rsquo;re focused on solving a problem or gaining insights into a new area of research. We rely on a rich, community-driven ecosystem of packages to help get our work done and likely make an unconscious assumption that there is a safety net out there, protecting us from harm.&lt;/p&gt;

&lt;p&gt;Unfortunately, I get to be &amp;ldquo;that guy&amp;rdquo; who comes along and shatters such assumptions. It&amp;rsquo;s time to put our hard hats on, get our clipboards out, and take a safety inspection tour of R. Along the way, we&amp;rsquo;ll introduce features and design concepts of our rOpenSci #runconf17 project &amp;mdash; the &lt;a href=&#34;https://github.com/ropenscilabs/notary&#34;&gt;&lt;code&gt;notary&lt;/code&gt; package&lt;/a&gt; &amp;mdash; that are aimed at making working in R a bit safer and more secure.&lt;/p&gt;

&lt;h2 id=&#34;meet-the-team&#34;&gt;Meet The Team&lt;/h2&gt;

&lt;p&gt;Since we say &amp;ldquo;we&amp;rdquo; quite a bit in this post, here are the folks that are represented by those two letters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/richfitz&#34;&gt;Rich FitzJohn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ironholds&#34;&gt;Oliver Keyes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/stephlocke&#34;&gt;Stephanie Locke&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jeroen&#34;&gt;Jeroen Ooms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hrbrmstr/&#34;&gt;Bob Rudis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(Since &amp;ldquo;Bob&amp;rdquo; typed out the post, I get to insert what a privilege it was to work with those four folks. They&amp;rsquo;re incredibly talented individuals doing really great work for the R community.)&lt;/p&gt;

&lt;h2 id=&#34;cran-trust-needs-verification&#34;&gt;CRAN : Trust Needs Verification&lt;/h2&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ropenscilabs/notary/master/img/trust.jpg&#34;/&gt;&lt;/center&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Before we go into the concept of package trust, we&amp;rsquo;d like you to put one finger on this blog post (to hold the page) and switch over to your R console and verify what CRAN mirror you are using. Since you&amp;rsquo;re down to one hand you can copy and paste this snippet: &lt;code&gt;options(&amp;quot;repos&amp;quot;)&lt;/code&gt; and review the results.&lt;/p&gt;

&lt;p&gt;If any URL in that list doesn&amp;rsquo;t start with &lt;code&gt;https://&lt;/code&gt; replace it with one from &lt;a href=&#34;https://cran.rstudio.com/mirrors.html&#34;&gt;this official mirror list&lt;/a&gt; that does (you will likely need to use both hands for that, so make sure you leave the browser tab open). If you don&amp;rsquo;t use a crytographically secure method of installing packages, then everyone from your ISP, to your employer, to the government (depending on where you reside) can see what packages you&amp;rsquo;re downloading and installing. Furthermore, using plain ol&amp;rsquo; &lt;code&gt;http://&lt;/code&gt; means it&amp;rsquo;s far easier for those who would seek to do you harm to intercept and switch out the contents of what you&amp;rsquo;re retrieving.&lt;/p&gt;

&lt;p&gt;Now, that you&amp;rsquo;re sure you&amp;rsquo;re using &lt;code&gt;https://&lt;/code&gt;, consider how much you know about the CRAN mirror you just picked. Are you &lt;em&gt;sure&lt;/em&gt; that you can either trust the site or at least trust that the site is maintained sufficiently to deter attackers who would seek to do you (or the community) harm? Running a secure site is non-trivial and, like it or not, &amp;ldquo;data science&amp;rdquo; is one of the fastest growth areas in virtually every modern organization (commercial or academic). Such a condition is a natural attractor for attackers and while the R package ecosystem may not be in the top ten most sinister threat scenarios (for now), it will be easy to take advantage of in its current state.&lt;/p&gt;

&lt;p&gt;To that end, the team came up with the concept of &lt;em&gt;signing&lt;/em&gt; packages (hence the &lt;code&gt;notary&lt;/code&gt; name). Without taking you down a deep dive into &lt;a href=&#34;https://en.wikipedia.org/wiki/Digital_signature&#34;&gt;digital signatures&lt;/a&gt;, you&amp;rsquo;re already familiar with this concept if you use something like an iOS-based device (i.e. iPhone or iPad) and have downloaded an app from the Apple app store. A developer applies for a developer account with Apple. They get a key. They make an app. They digitally sign the app with the key they received. Apple reviews the app and (usually)eventually approves it. The signed app goes into the store and your iOS device (if you haven&amp;rsquo;t &amp;ldquo;rooted&amp;rdquo; it) is configured to only run signed and approved apps.&lt;/p&gt;

&lt;p&gt;There are three functions in &lt;code&gt;notary&lt;/code&gt; to help facilitate a more secure package ecosystem &amp;mdash; &lt;code&gt;install_packages()&lt;/code&gt;, &lt;code&gt;download_packages()&lt;/code&gt; &amp;amp; &lt;code&gt;available_packages()&lt;/code&gt; &amp;mdash; each of which is a thin wrapper around their base, dotted counterparts which ultimately will require modifications to CRAN mirrors to house digital signatures for packages and CRAN mirror sites themselves.&lt;/p&gt;

&lt;p&gt;Why all this extra infrastructure and scaffolding? If we think of the R Core/CRAN team as the R equivalent of the Apple app store guardians, then when they review and approve a package that version becomes the gold standard. But, there&amp;rsquo;s no current, easy, complete way to know for sure that what&amp;rsquo;s on &lt;code&gt;cran.r-project.org&lt;/code&gt; is also what&amp;rsquo;s on one of the mirror sites.&lt;/p&gt;

&lt;p&gt;By having a similar set of signing and validation idioms, it will be possible to ensure that what you think you&amp;rsquo;re getting from a CRAN repository is what was approved by the CRAN team. We still need to get one &amp;ldquo;secure&amp;rdquo; mirror setup to enable a proof-of-concept, so stay tuned for advancements in this area.&lt;/p&gt;

&lt;h2 id=&#34;a-sheriff-for-the-wild-wild-west-i-e-making-github-safer&#34;&gt;A Sheriff for the Wild, Wild West (i.e. Making GitHub Safer)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ropenscilabs/notary/master/img/badge.png&#34; align=&#34;right&#34;/&gt; While the CRAN distribution model is not perfect, it&amp;rsquo;s Fort Knox compared to GitHub.&lt;/p&gt;

&lt;p&gt;Oh, but before we go into that, you should check out some extremely cutting edge functionality &lt;a href=&#34;https://github.com/hadley&#34;&gt;Hadley&lt;/a&gt; and others are putting into &lt;code&gt;purrr&lt;/code&gt;. Just do a quick &lt;code&gt;devtools::install_github(&amp;quot;hadlley/purrr&amp;quot;)&lt;/code&gt; bring up the help for the new &lt;em&gt;threaded&lt;/em&gt; parallel execution of &lt;code&gt;map()&lt;/code&gt;: &lt;code&gt;map_t()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now, you know this is a post about security &amp;amp; R so hopefully your Spidey-sense was triggered and you knew enough not to even try that or you caught the &lt;code&gt;ll&lt;/code&gt; before you did the copy/paste. If you did end up doing the install attempt, be a &lt;em&gt;teensy&lt;/em&gt; bit thankful that I deleted the &lt;code&gt;hadlley&lt;/code&gt; account before I wrote the post.&lt;/p&gt;

&lt;p&gt;GitHub (and other public code repositories) are wonderful places where folks can collaborate and share creations. They are also fraught with peril. This is easily demonstrated by this proof-of-concept R package &lt;a href=&#34;https://github.com/hrbrmstr/rpwnd&#34;&gt;&lt;code&gt;rpwnd&lt;/code&gt;&lt;/a&gt;. Since GitHub is the most popular public R package development area, we&amp;rsquo;ll focus on it for the remainder of this section.&lt;/p&gt;

&lt;p&gt;One way to begin to mitigate the threat of GitHub package distribution is to impose some rules and provide a means to ensure some level of authenticity at the author and release level. To that end, we have two core functions: &lt;code&gt;install_release()&lt;/code&gt; and &lt;code&gt;validate_release()&lt;/code&gt; that rely on a setting that most of you likely do not have enabled in GitHub - &lt;a href=&#34;https://github.com/settings/keys&#34;&gt;PGP keys&lt;/a&gt;. You can read up on &lt;a href=&#34;https://help.github.com/articles/signing-commits-with-gpg/&#34;&gt;GitHub &amp;amp; PGP&lt;/a&gt; but you should really keep one finger on this page (again) and go check out &lt;a href=&#34;https://mikegerwitz.com/papers/git-horror-story.html&#34;&gt;&lt;em&gt;A Git Horror Story: Repository Integrity With Signed Commits&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Back? Good. Let&amp;rsquo;s continue.&lt;/p&gt;

&lt;p&gt;The premise is simple: only install actual releases (which is a good idea anyway) and only, then, install &lt;em&gt;signed&lt;/em&gt; releases. &lt;em&gt;This is not a panacea&lt;/em&gt; and does not fix all the security &amp;amp; integrity problems associated with the GitHub distribution model, but if combined with some manual inspection of the repository and repo owner profile it will help ensure that you&amp;rsquo;re somewhat closer to getting benign code.&lt;/p&gt;

&lt;p&gt;This functionality is available today. So go setup your own PGP keys, add them to your owner profile and start generating signed releases.&lt;/p&gt;

&lt;h2 id=&#34;source-sources-safely-with-signatures&#34;&gt;Source Sources Safely With Signatures&lt;/h2&gt;

&lt;p&gt;Rounding out the feature set are two functions &lt;code&gt;source_safe_sign()&lt;/code&gt; and &lt;code&gt;sys_source_safe_sign()&lt;/code&gt; which are more secure (well, at least safer) wrappers for their dotted base siblings.&lt;/p&gt;

&lt;p&gt;I literally break down into tears when I see a &lt;code&gt;source()&lt;/code&gt; suggestion posted anywhere, especially to non-&lt;code&gt;https://&lt;/code&gt; URLs. Why? Even if you did a manual inspection at one point in time that the code is not malicious, how do you know that it hasn&amp;rsquo;t been modified since then? Their &lt;code&gt;devtools&lt;/code&gt; counterparts (&lt;code&gt;source_gist()&lt;/code&gt;/&lt;code&gt;source_url()&lt;/code&gt;) are a tad better, provided they you use &lt;code&gt;sha1&lt;/code&gt; parameter to ensure that what you think you are sourcing hasn&amp;rsquo;t changed.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;notary&lt;/code&gt; sourcers go one step further and use a &lt;a href=&#34;https://github.com/jeroen/sodium&#34;&gt;&lt;code&gt;sodium&lt;/code&gt;&lt;/a&gt;-based signature to verify the integrity of the source code you so desperately want to use via this methodology. These functions need some kinder, gentler companion functions to make it easier for all users to sign scripts, so you&amp;rsquo;ll have to check back for those as we continue to poke at the project.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ropenscilabs/notary/master/img/reliable.png&#34;&gt;&lt;/center&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h2 id=&#34;collaborating-for-community-safety&#34;&gt;Collaborating for Community Safety&lt;/h2&gt;

&lt;p&gt;While we have a great start at building a foundation of safer and more secure R package and code delivery, the best part of building the &lt;code&gt;notary&lt;/code&gt; package was working with a team who genuinely wants to help ensure that the R community can operate as safely as possible without garish, creativity-crushing impediments. Rich, Oliver, Stephanie and Jeroen all had clever ideas for tough problems and we&amp;rsquo;ll hopefully be able to continue to make small steps towards progress.&lt;/p&gt;

&lt;h2 id=&#34;jump-on-the-crazy-train-with-us&#34;&gt;Jump on the Crazy Train With Us!&lt;/h2&gt;

&lt;p&gt;Hopefully we&amp;rsquo;ve helped folks understand some of the dangers that are out there and further demonstrated that we&amp;rsquo;ve begun to address some of them with the &lt;code&gt;notary&lt;/code&gt; package. If the idea of helping find ways to keep data science folks safer has piqued your interest, please do not hesitate to contact any of the team. We&amp;rsquo;d love to engage with more of the community on &lt;code&gt;notary&lt;/code&gt;, and would love feedback on usability and ideas for new or improved functionality.&lt;/p&gt;

&lt;p&gt;Thank you, again, to rOpenSci for the opportunity to come together and collaborate on this project.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Tackling the Research Compendium at runconf17</title>
      <link>https://ropensci.org/blog/2017/06/20/checkers/</link>
      <pubDate>Tue, 20 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/06/20/checkers/</guid>
      <description>
        
        

&lt;p&gt;Two years ago at &lt;a href=&#34;https://twitter.com/hashtag/runconf15&#34;&gt;#runconf15&lt;/a&gt;, there was a great discussion about best practices for organizing R-based analysis projects that yielded a &lt;a href=&#34;https://github.com/ropensci/rrrpkg&#34;&gt;nice guidance document&lt;/a&gt; describing &lt;em&gt;research compendia&lt;/em&gt;. Compendia, as we described them, were minimal products of reproducible research, using parts of R package structure to organize the inputs, analyses, and outputs of research projects.&lt;/p&gt;

&lt;p&gt;Since then, we&amp;rsquo;ve seen more examples and models of research compendia emerge (the organization of such projects is &lt;a href=&#34;https://discuss.ropensci.org/t/resources-on-project-directory-organization/340&#34;&gt;something of an obsession&lt;/a&gt; for some of the community). In parallel, there&amp;rsquo;s been much progress on a number of fronts with R &lt;em&gt;packages&lt;/em&gt;: rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/onboarding&#34;&gt;package review process&lt;/a&gt; has expanded and we&amp;rsquo;ve worked out many kinks. Infrastructure for automated testing of package code has been developed and field tested. So at &lt;a href=&#34;unconf17.ropensci.org&#34;&gt;#runconf17&lt;/a&gt;, we wanted to see how much of this progress in review, testing, and automation could apply to research compendia.&lt;/p&gt;

&lt;p&gt;It turns out there&amp;rsquo;s a &lt;em&gt;lot&lt;/em&gt; to do here, and a lot of interest! We put up a proposal as a &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/5&#34;&gt;GitHub issue&lt;/a&gt;; before the unconf even began, the thread had over 50 posts and the discussion had yielded two design documents led by Hadley Wickham on &lt;a href=&#34;https://docs.google.com/document/d/1LzZKS44y4OEJa4Azg5reGToNAZL0e0HSUwxamNY7E-Y/edit&#34;&gt;research compendium organization&lt;/a&gt; and &lt;a href=&#34;https://docs.google.com/document/d/1avYAqjTS7zSZn7JAAOZhFPkhkPvYwaPVrSpo31Cu0Yc/edit#&#34;&gt;automated build systems&lt;/a&gt;. There were easily four or five projects wrapped up in the proposal.&lt;/p&gt;

&lt;p&gt;The thread also revealed many schools of thought. As &lt;a href=&#34;https://twitter.com/nj_tierney&#34;&gt;Nick&lt;/a&gt; put it, &amp;ldquo;The problem with &lt;a href=&#34;https://www.rstudio.com/resources/videos/opinionated-analysis-development/&#34;&gt;opinionated analysis development&lt;/a&gt; is that everyone has an opinion.&amp;rdquo; We never reached consensus on basic issues like directory structure and build systems.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://imgur.com/RpjyfL8.jpeg&#34; alt=&#34;Wading our way through a thorny bramble of opinions (photo: Alice Daish)&#34; /&gt;
&lt;em&gt;Wading our way through a thorny bramble of opinions (photo: Alice Daish)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the face of a beast of a topic, and so many unresolved decisions, we decided to tackle a modest slice - a guide for compendium review, and a first pass at a package for automated checks of various best practices.&lt;/p&gt;

&lt;h3 id=&#34;reviewing-a-research-compendium&#34;&gt;Reviewing a research compendium&lt;/h3&gt;

&lt;p&gt;How does one do peer review for a research compendium in the face of so much heterogeneity in types and styles of analysis?  We spent the better part of our first day brainstorming everything that one might review, from the meta-topics (&amp;ldquo;Is there a clear question?&amp;rdquo;) to  minutiae (&amp;ldquo;Are &lt;code&gt;library()&lt;/code&gt; calls at the top of the script?&amp;ldquo;), and mapping these out on an ever more data-dense whiteboard:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://imgur.com/vb4E3JV.jpeg&#34; alt=&#34;Nick, Jennifer and Molly discussing analysis best practices (photo: Alice Daish)&#34; /&gt;
 &lt;em&gt;Nick, Jennifer, Molly and Alice discussing analysis best practices (photo: Alice Daish)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/dIT1sjK.jpg&#34; alt=&#34;The team prioritizing data analysis workflow best practice into tiers (photo: Nistara Randhawa)&#34; /&gt;
 &lt;em&gt;The team prioritizing data analysis workflow best practice into tiers (photo: Nistara Randhawa)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://imgur.com/8klgK7Q.jpeg&#34; alt=&#34;With our 7th team member, Wy T. Board (photo: Alice Daish)&#34; /&gt;
&lt;em&gt;With our 7th team member, Wy T. Board (photo: Alice Daish)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These review topics made their way into a &lt;a href=&#34;https://docs.google.com/document/d/1OYcWJUk-MiM2C1TIHB1Rn6rXoF5fHwRX-7_C12Blx8g/edit#heading=h.dyoxrtoo15mm&#34;&gt;Google doc&lt;/a&gt; which will form the basis of a review guide along the lines of rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/onboarding/#-useful-documents-in-this-repository&#34;&gt;package review guides&lt;/a&gt;.  One major organizing dimension we hit upon was &amp;ldquo;Tiers&amp;rdquo; - as the number of best practices can be overwhelming, it is better to prioritize them so that users have a way of advancing through escalating levels of detail.  Another was &amp;ldquo;automatability,&amp;rdquo; which is key to the other half of our work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/lWpcEfb.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;checkers-r-cmd-check-for-your-data-analysis&#34;&gt;&lt;strong&gt;checkers&lt;/strong&gt;: R CMD check for your data analysis&lt;/h3&gt;

&lt;p&gt;One of the lessons of the rOpenSci package review process has been that reviews work better when we let reviewers focus on the tasks that humans are best at, and find ways to automate tedious or tiny tasks. So for the second aim of our project, we built a package to run the automatable components of  review and create reports for analysts and reviewers.  The package, &lt;strong&gt;&lt;a href=&#34;https://github.com/ropenscilabs/checkers&#34;&gt;checkers&lt;/a&gt;&lt;/strong&gt;, is meant to be, as Hadley coined it, &amp;ldquo;&lt;code&gt;R CMD check&lt;/code&gt; for your data analysis.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;checkers&lt;/strong&gt; scans the project directory and delivers advice on how to improve project code, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; checkers::gp_check()
─────────────────────────────────────────────────────────────────────
It is good practice to

  ✖ Place your project under version control. You are using
    neither git nor svn. See http://happygitwithr.com/ for more
    info
  ✖ Use preferred packages. xml2 is preferred to XML.
──────────────────────────────────────────────────────────────────
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One area we had to tackle was the need for both &lt;em&gt;opinionated defaults&lt;/em&gt; and &lt;em&gt;configurability&lt;/em&gt;.  So we built in the ability for individuals or teams to select or define their own best practices in a shared YAML configuration file.&lt;/p&gt;

&lt;h3 id=&#34;stone-soup-development-at-the-unconf&#34;&gt;Stone soup development at the unconf&lt;/h3&gt;

&lt;p&gt;The review guide and &lt;strong&gt;checkers&lt;/strong&gt; are works in progress, but both made great leaps forward in two days thanks to the tremendous catalyzing environment of the unconf.  While our team of six only formed the morning of the first day, the &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/5&#34;&gt;pre-unconf discussion&lt;/a&gt; meant that many more people from the community shaped contours of the project.&lt;/p&gt;

&lt;p&gt;Also, one of the great things about the unconf is that so many experienced developers are on hand to chip in with their areas of expertise. Hadley Wickham joined in to brainstorm some of the initial best practices to include in our guide. Later, we decided to base our checks system on Gábor Csárdi&amp;rsquo;s &lt;strong&gt;&lt;a href=&#34;https://github.com/MangoTheCat/goodpractice/&#34;&gt;goodpractice&lt;/a&gt;&lt;/strong&gt;, and Gábor worked with us to build a flexible extension system into that package. Jim Hester was on hand to answer questions about &lt;strong&gt;&lt;a href=&#34;https://github.com/jimhester/lintr/&#34;&gt;lintr&lt;/a&gt;&lt;/strong&gt; internals, and we&amp;rsquo;ll be sending some changes upstream to that as well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/uWYKR1e.jpg&#34; alt=&#34;Laura and Gábor practicing goodpractices (photo: Nistara Randhawa)&#34; /&gt;
&lt;em&gt;Laura and Gábor practicing goodpractices (photo: Nistara Randhawa)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://imgur.com/Dw0yTwi.jpeg&#34; alt=&#34;Team reviewing framework and package development examples (photo: Alice Daish)&#34; /&gt;
&lt;em&gt;Team reviewing framework and package development examples (photo: Alice Daish)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re excited about the potential for this project, and just as excited about the potential of what else we&amp;rsquo;ll make with our new friends and collaborators. Thanks to rOpenSci and everyone who made #runconf17 such a tremendously productive and fun event!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>New rOpenSci Packages for Text Processing in R</title>
      <link>https://ropensci.org/blog/2017/06/13/ropensci_text_tools/</link>
      <pubDate>Tue, 13 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/06/13/ropensci_text_tools/</guid>
      <description>
        
        

&lt;p&gt;Textual data and natural language processing are still a niche domain within the R ecosytstem. The &lt;a href=&#34;https://cran.r-project.org/view=NaturalLanguageProcessing&#34;&gt;NLP task view&lt;/a&gt; gives an overview of existing work however a lot of basic infrastructure is still missing.
At the rOpenSci &lt;a href=&#34;https://ropensci.org/blog/blog/2017/05/03/textworkshop17&#34;&gt;text workshop&lt;/a&gt; in April we discussed many ideas for improving text processing in R which revealed several core areas that need improvement:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reading: better tools for extracing text and metadata from documents in various formats (doc, rtf, pdf, etc).&lt;/li&gt;
&lt;li&gt;Encoding: many text packages work well for ascii text but rapidly break down when text contains Hungarian, Korean or emojis.&lt;/li&gt;
&lt;li&gt;Interchange: packages don&amp;rsquo;t work well together due to lack of data classes or conventions for textual data (see also &lt;a href=&#34;https://github.com/ropensci/tif&#34;&gt;ropensci/tif&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Participants also had many good suggestions for C/C++ libraries that text researchers in R might benefit from. Over the past weeks I was able to look into these suggestions and work on a few packages for reading and analyzing text. Below is an update on new and improved rOpenSci tools for text processsing in R!&lt;/p&gt;

&lt;h2 id=&#34;google-language-detector-2-and-3&#34;&gt;Google language detector 2 and 3&lt;/h2&gt;

&lt;p&gt;New packages &lt;code&gt;cld2&lt;/code&gt; and &lt;code&gt;cld3&lt;/code&gt; are wrappers C++ libraries by Google for language identification. &lt;a href=&#34;https://github.com/cld2owners/cld2#internals&#34;&gt;CLD2&lt;/a&gt; is a Naïve Bayesian classifier, whereas &lt;a href=&#34;https://github.com/google/cld3#model&#34;&gt;CLD3&lt;/a&gt; uses a neural network model. I found &lt;code&gt;cld2&lt;/code&gt; to give better results for short text.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the latest versions
install.packages(c(&amp;quot;cld2&amp;quot;, &amp;quot;cld3&amp;quot;))

# Vectorized function
text &amp;lt;- c(&amp;quot;À chaque fou plaît sa marotte.&amp;quot;, &amp;quot;猿も木から落ちる&amp;quot;,
	&amp;quot;Алты́нного во́ра ве́шают, а полти́нного че́ствуют.&amp;quot;, &amp;quot;Nou breekt mijn klomp!&amp;quot;)

cld2::detect_language(text)
# [1] &amp;quot;fr&amp;quot; &amp;quot;ja&amp;quot; &amp;quot;ru&amp;quot; &amp;quot;nl&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://maelle.github.io&#34;&gt;Maëlle&lt;/a&gt; has written a &lt;a href=&#34;http://www.masalmon.eu/2017/06/10/rolandgarros/&#34;&gt;cool post&lt;/a&gt; comparing language classification methods using 18000 &lt;code&gt;&amp;quot;#RolandGarros2017&amp;quot;&lt;/code&gt; tweets and &lt;a href=&#34;https://www.stat.auckland.ac.nz/people/tlum005&#34;&gt;Thomas&lt;/a&gt; &lt;a href=&#34;http://notstatschat.tumblr.com/post/161449071226/stupid-word-games&#34;&gt;reminds us&lt;/a&gt; that algorithms can easily be fooled. Still I found the accuracy on real text quite astonishing given the relatively small size of these libraries.&lt;/p&gt;

&lt;p&gt;Note that the algorithm for CLD3 is still under development and the engineers at Google have recently &lt;a href=&#34;https://github.com/google/cld3/issues&#34;&gt;opened&lt;/a&gt; their Github issues page for feedback.&lt;/p&gt;

&lt;h2 id=&#34;anti-word-and-un-rtf&#34;&gt;(anti) word and (un)rtf&lt;/h2&gt;

&lt;p&gt;Many archived documents are only available in legacy formats such as &lt;code&gt;.doc&lt;/code&gt; and &lt;code&gt;.rtf&lt;/code&gt;. The only tools available for extracting text from these documents were difficult to install and could not be imported from packages and scripts.&lt;/p&gt;

&lt;p&gt;To make this a little easier we have packaged up utilities &lt;a href=&#34;http://www.winfield.demon.nl/&#34;&gt;antiword&lt;/a&gt; and &lt;a href=&#34;https://www.gnu.org/software/unrtf/&#34;&gt;UnRTF&lt;/a&gt; to read MS &lt;code&gt;doc&lt;/code&gt; and &lt;code&gt;rtf&lt;/code&gt; files respectively.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the latest versions
install.packages(c(&amp;quot;antiword&amp;quot;, &amp;quot;unrtf&amp;quot;))

# Extract text from &#39;rtf&#39; file
text &amp;lt;- unrtf::unrtf(&amp;quot;https://jeroen.github.io/files/sample.rtf&amp;quot;, format = &amp;quot;text&amp;quot;)
cat(text)
### Lots of text...

# Extract text from &#39;doc&#39; file
text &amp;lt;- antiword::antiword(&amp;quot;https://jeroen.github.io/files/UDHR-english.doc&amp;quot;)
cat(text)
### Lots of text...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also have a look at meta packages &lt;code&gt;readtext&lt;/code&gt; or &lt;code&gt;textreadr&lt;/code&gt; which wrap these and other packages for automatically reading text in many different formats.&lt;/p&gt;

&lt;h2 id=&#34;pdf-utilities&#34;&gt;pdf utilities&lt;/h2&gt;

&lt;p&gt;Our &lt;a href=&#34;https://cran.r-project.org/web/packages/pdftools/index.html&#34;&gt;pdftools&lt;/a&gt; package now supports reading pdf (extracting text or metadata) and rendering pdf to png, jpeg, tiff, or raw vectors on all platforms (incl. Windows).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Read some text
text &amp;lt;- pdftools::pdf_text(&#39;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#39;)
cat(text[1])
# An Introduction to R
#             Notes on R: A Programming Environment for Data Analysis and Graphics
#                                                        Version 3.4.0 (2017-04-21)
# W. N. Venables, D. M. Smith
# and the R Core Team

# Read meta data
pdftools::pdf_info(&#39;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#39;)
# $version
# [1] &amp;quot;1.5&amp;quot;
#
# $pages
# [1] 105
#
# .... much more :)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use either render a page into a raw bitmap array, or directly to an image format such as png, jpeg or tiff.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;files &amp;lt;- pdftools::pdf_convert(&#39;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#39;, format = &amp;quot;png&amp;quot;, pages = 1:5)
# Converting page 1 to R-intro_1.png... done!
# Converting page 2 to R-intro_2.png... done!
# Converting page 3 to R-intro_3.png... done!
# Converting page 4 to R-intro_4.png... done!
# Converting page 5 to R-intro_5.png... done!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To extract text from scanned images, also check out our &lt;a href=&#34;https://cran.r-project.org/web/packages/tesseract/index.html&#34;&gt;tesseract&lt;/a&gt; package which wraps Google&amp;rsquo;s powerful OCR engine.&lt;/p&gt;

&lt;h2 id=&#34;stemming-tokenizing-and-spell-checking&#34;&gt;Stemming, tokenizing and spell checking&lt;/h2&gt;

&lt;p&gt;Our &lt;a href=&#34;https://cran.r-project.org/web/packages/hunspell/index.html&#34;&gt;hunspell&lt;/a&gt; package has had a few updates recently as well. The package is a wrapper for &lt;a href=&#34;http://hunspell.github.io/&#34;&gt;libhunspell&lt;/a&gt; which is a popular library for spell checking:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Extract incorrect from a piece of text
bad &amp;lt;- hunspell(&amp;quot;spell checkers are not neccessairy for langauge ninja&#39;s&amp;quot;)
print(bad[[1]])
# [1] &amp;quot;neccessairy&amp;quot; &amp;quot;langauge&amp;quot;
hunspell_suggest(bad[[1]])
# [[1]]
# [1] &amp;quot;necessary&amp;quot;    &amp;quot;necessarily&amp;quot;  &amp;quot;necessaries&amp;quot;  &amp;quot;recessionary&amp;quot; &amp;quot;accessory&amp;quot;    &amp;quot;incarcerate&amp;quot;
#
# [[2]]
# [1] &amp;quot;language&amp;quot;  &amp;quot;Langeland&amp;quot; &amp;quot;Lagrange&amp;quot;  &amp;quot;Lange&amp;quot;     &amp;quot;gaugeable&amp;quot; &amp;quot;linkage&amp;quot;   &amp;quot;Langland&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The package is also used by &lt;code&gt;devtools&lt;/code&gt; to spell-check manual pages in R packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::spell_check()
#   WORD          FOUND IN
# alltogether   pdftools.Rd:36
# cairo         pdf_render_page.Rd:42
# jpeg          pdf_render_page.Rd:40
# libpoppler    pdf_render_page.Rd:42, pdftools.Rd:30, description:1
# png           pdf_render_page.Rd:40
# Poppler       pdftools.Rd:34
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally hunspell also exposes the underlying methods needed for spell checking such as stemming words:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Find possible stems for each word
words &amp;lt;- c(&amp;quot;loving&amp;quot;, &amp;quot;loved&amp;quot;, &amp;quot;lover&amp;quot;, &amp;quot;lovely&amp;quot;, &amp;quot;love&amp;quot;)
hunspell_analyze(words)
# [[1]]
# [1] &amp;quot; st:loving&amp;quot;    &amp;quot; st:love fl:G&amp;quot;
#
# [[2]]
# [1] &amp;quot; st:loved&amp;quot;     &amp;quot; st:love fl:D&amp;quot;
#
# [[3]]
# [1] &amp;quot; st:lover&amp;quot;     &amp;quot; st:love fl:R&amp;quot;
#
# [[4]]
# [1] &amp;quot; st:lovely&amp;quot;    &amp;quot; st:love fl:Y&amp;quot;
#
# [[5]]
# [1] &amp;quot; st:love&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hunspell also suppors tokenizing words from html, latex, man, or plain text. For more advanced word extraction, check out the rOpenSci &lt;a href=&#34;https://github.com/ropensci/tokenizers#readme&#34;&gt;tokenizers&lt;/a&gt; package.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Release mongolite 1.0</title>
      <link>https://ropensci.org/blog/2017/03/10/mongolite/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/03/10/mongolite/</guid>
      <description>
        
        

&lt;p&gt;After 2.5 years of development, version 1.0 of the &lt;a href=&#34;https://cran.r-project.org/web/packages/mongolite/index.html&#34;&gt;mongolite&lt;/a&gt; package has been released to CRAN. The package is now stable, well documented, and will soon be submitted for peer review to be onboarded in the rOpenSci suite.&lt;/p&gt;

&lt;h2 id=&#34;mongodb-in-r-and-mongolite&#34;&gt;MongoDB in R and mongolite&lt;/h2&gt;

&lt;p&gt;I started working on mongolite in September 2014, and it was first announced at the rOpenSci &lt;a href=&#34;https://twitter.com/_inundata/status/581605601882480640/photo/1&#34;&gt;unconf 2015&lt;/a&gt;. At this time, there were already two Mongo clients on CRAN: &lt;a href=&#34;https://cran.r-project.org/web/packages/rmongodb/index.html&#34;&gt;rmongodb&lt;/a&gt; (no longer works) and &lt;a href=&#34;https://cran.r-project.org/web/packages/RMongo/index.html&#34;&gt;RMongo&lt;/a&gt; (depends on Java). However I found both of them pretty clunky, and the MongoDB folks had just released 1.0 of their new C driver, so I decided to write a new client from scratch.&lt;/p&gt;

&lt;p&gt;Mongolite aims to provide a &lt;em&gt;simple&lt;/em&gt; R client for MongoDB, based on the excellent &lt;a href=&#34;https://github.com/mongodb/mongo-c-driver&#34;&gt;mongo-c-driver&lt;/a&gt; combined with super-powers from the &lt;a href=&#34;https://cran.r-project.org/web/packages/jsonlite/index.html&#34;&gt;jsonlite&lt;/a&gt; package. Simple means insert and query data in R using data-frames with a single command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Create a connection
con &amp;lt;- mongolite::mongo(&amp;quot;diamonds&amp;quot;,
  url = &amp;quot;mongodb://readwrite:test@ds043942.mongolab.com:43942/jeroen_test&amp;quot;)

# Find diamonds with: cut == Premium &amp;amp; price &amp;lt; 500
mydata &amp;lt;- con$find(&#39;{&amp;quot;cut&amp;quot; : &amp;quot;Premium&amp;quot;, &amp;quot;price&amp;quot; : { &amp;quot;$lt&amp;quot; : 500 } }&#39;)
print(mydata)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running your own MongoDB server is easy. Either &lt;a href=&#34;https://www.mongodb.com/download-center&#34;&gt;download&lt;/a&gt; it from the website or install it with your favorite package manager. To start the server simply run the &lt;code&gt;mongod&lt;/code&gt; command (d for daemon) in a shell:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Install mongoDB server
brew install mongodb

# Run the server dameon
mongod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;mongolite::mongo()&lt;/code&gt; function wil default to the localhost server if no URI is specified. Try inserting and reading some data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Create a connection
con &amp;lt;- mongolite::mongo(&amp;quot;iris&amp;quot;)

# Insert some data
con$insert(datasets::iris)

# Count how much data is in the DB
con$count()

# Read the data back
con$find(&#39;{}&#39;)

# Wipe the collection
con$drop()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In my experience, a simple interface is critical to get started. Obviously, advanced features are available in mongolite as well, but this will get you up to speed right way if you just need the data and get on with your job.&lt;/p&gt;

&lt;h2 id=&#34;bookdown-documentation&#34;&gt;Bookdown documentation&lt;/h2&gt;

&lt;p&gt;The 1.0 release has fresh documentation based on the awesome bookdown system. You can find documentation on the mongolite &lt;a href=&#34;https://jeroen.github.io/mongolite/&#34;&gt;github homepage&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://jeroen.github.io/mongolite/&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/mongolite-docs.png&#34; alt=&#34;mongolite-docs&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The bookdown is now the primary documentation source for mongolite.&lt;/p&gt;

&lt;h2 id=&#34;why-mongodb&#34;&gt;Why MongoDB&lt;/h2&gt;

&lt;p&gt;MongoDB is the &lt;a href=&#34;(http://db-engines.com/en/ranking)&#34;&gt;most popular&lt;/a&gt; nosql database (by market share), and the 5th most popular database allround. Mongo is relatively young in comparison with the traditional engines (Oracle, Microsoft, MySQL, Postgres), yet well established, fully open source, and backed by a professional company.&lt;/p&gt;

&lt;p&gt;MongoDB provides a modern high-performance DB engine with cool features that cannot be found anywhere else. The high quality client drivers are a pleasure to work with, and actively maintained by professional engineers. Writing bindings, it quickly became obvious that Mongo does not suffer from the legacy bloat that I have come to associate with traditional DB engines.&lt;/p&gt;

&lt;p&gt;At the same time the ecosystem is mature and offers reliability and continuity that makes it stand out from the proliferation of nosql systems. MongoDB has been widely adopted by users and distributions, so I am pretty confident it will still be around 5 or 10 years from now.&lt;/p&gt;

&lt;h2 id=&#34;changes-in-1-0&#34;&gt;Changes in 1.0&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/jeroen/mongolite/blob/master/NEWS&#34;&gt;NEWS&lt;/a&gt; file on Github lists what has changed in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;New mongo bookdown docs at &lt;a href=&#34;https://jeroen.github.io/mongolite&#34;&gt;https://jeroen.github.io/mongolite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Update mongo-c-driver to upstream 1.6.0&lt;/li&gt;
&lt;li&gt;Add basic decimal 128 support (coerce to double)&lt;/li&gt;
&lt;li&gt;Improve enterprise authentication for LDAP, X509 and Kerberos&lt;/li&gt;
&lt;li&gt;Windows: build with SSPI instead of SASL&lt;/li&gt;
&lt;li&gt;Added &lt;code&gt;allow_invalid_hostname&lt;/code&gt; parameter to &lt;code&gt;ssl_options()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Option &lt;code&gt;bigint_as_char&lt;/code&gt; to parse int64 into string instead of double&lt;/li&gt;
&lt;li&gt;New function &lt;code&gt;mongo_options()&lt;/code&gt; to get/set global options&lt;/li&gt;
&lt;li&gt;Function &lt;code&gt;mongo_log_level()&lt;/code&gt; is removed (use &lt;code&gt;mongo_options()&lt;/code&gt; instead)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;insert()&lt;/code&gt; now substitutes dots in key (column) names with underscores&lt;/li&gt;
&lt;li&gt;Various fixes in &lt;code&gt;update()&lt;/code&gt;, support for upsert&lt;/li&gt;
&lt;li&gt;Add unit tests from bson spec (some tests fail in mongo-c-driver)&lt;/li&gt;
&lt;li&gt;Switch to new C driver API mongoc_collection_find_with_opts()&lt;/li&gt;
&lt;li&gt;Add R_registerRoutines() call to please CMD check&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tell-us-what-you-think&#34;&gt;Tell us what you think&lt;/h2&gt;

&lt;p&gt;At rOpenSci we&amp;rsquo;re interested to hear who is using R and how. If you decide to give mongolite a try, please share your experiences and suggestions. Open an &lt;a href=&#34;https://github.com/jeroen/mongolite/issues&#34;&gt;issue on github&lt;/a&gt; for specific problems and feature requests, or join the rOpenSci &lt;a href=&#34;https://ropensci.slack.com&#34;&gt;slack&lt;/a&gt; to talk about mongolite or other rOpenSci packages. Or just come say hi and hang out :)&lt;/p&gt;

&lt;p&gt;We both love hearing from academic users, but also industry applications of R and synergy between the industry and open source scientific software.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>From a million nested `ifelse`s to the plater package</title>
      <link>https://ropensci.org/blog/2017/02/06/plater-blog-post/</link>
      <pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/02/06/plater-blog-post/</guid>
      <description>
        
        

&lt;p&gt;As a lab scientist, I do almost all of my experiments in &lt;a href=&#34;https://en.wikipedia.org/wiki/Microtiter_plate&#34;&gt;microtiter plates&lt;/a&gt;. These tools are an efficient means of organizing many parallel experimental conditions. It&amp;rsquo;s not always easy, however, to translate between the physical plate and a useful data structure for analysis. My first attempts to solve this problem&amp;ndash;nesting one &lt;code&gt;ifelse&lt;/code&gt; call inside of the next to describe which well was which&amp;ndash;were very unsatisfying. Over time, my attempts at solving the problem grew more sophisticated, and eventually, the &lt;code&gt;plater&lt;/code&gt; package was born. Here I will tell the story of how with the help of &lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;R Packages&lt;/a&gt; and the amazing reviewers (&lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt; and &lt;a href=&#34;http://deanattali.com/&#34;&gt;Dean Attali&lt;/a&gt;) and &lt;a href=&#34;https://scottchamberlain.info/&#34;&gt;editors&lt;/a&gt; at rOpenSci, I ended up with a package that makes it easy to work with plate-based data.&lt;/p&gt;

&lt;h2 id=&#34;plates-are-great&#34;&gt;Plates are great&lt;/h2&gt;

&lt;p&gt;Microtiter plates are essential in the lab. Basically an ice cube tray about the size of an index card, they have &amp;ldquo;wells&amp;rdquo; for between 6 and 384 ice cubes (up to 6144 if you&amp;rsquo;re a robot!). Except instead of freezing water, you use each well for a different sample or experimental condition.&lt;/p&gt;

&lt;p&gt;For example, say I have 8 samples and want to test 4 different drugs. Each drug should be tested on each sample three separate times to ensure accurate results. A 96-well plate is perfect for this: it&amp;rsquo;s a grid of 12 columns and 8 rows. So each sample would go in its own row. Each drug would then go in a group of three columns, say Drug A in columns 1-3, Drug B in columns 4-6, and so on. This is shown below, with the numbers 1-8 representing samples and the colors representing groups of wells treated with the same drug.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-02-plater-post/plate-1.png&#34; alt=&#34;Example plate layout&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Typically, I make myself a map like the image above before I start the experiment, then I take it with me into the lab when I&amp;rsquo;m ready to begin. The map creates a powerful mental connection between each experimental condition and its particular physical location on the plate. With large effects, you might even be able to visually see the results of your experiment: all the cells in this column died, or the cells grew like crazy in that row.&lt;/p&gt;

&lt;p&gt;This is very convenient to work with physically and remember mentally.&lt;/p&gt;

&lt;h2 id=&#34;plates-are-not-tidy&#34;&gt;Plates are not tidy&lt;/h2&gt;

&lt;p&gt;The problem is that you can pack a ton of complexity into a small experiment and mapping that back into a &lt;a href=&#34;https://www.jstatsoft.org/article/view/v059i10&#34;&gt;tidy&lt;/a&gt; framework for analysis isn&amp;rsquo;t always easy.&lt;/p&gt;

&lt;p&gt;One way to map from data to plate is through well IDs. Each well has one. For example, the top left well is in row A and column 1, so its ID is A01. The well in the third row down and 5th column over is C05. But how do you say that everything in row A is sample X, everything in row B is sample Y, and so on?&lt;/p&gt;

&lt;p&gt;My first strategy was to put it in the code, with a mess like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data &amp;lt;- dplyr::mutate(data,
            Sample = ifelse(Row == &amp;quot;A&amp;quot;, &amp;quot;Sample X&amp;quot;, ifelse(
                Row == &amp;quot;B&amp;quot;, &amp;quot;Sample Y&amp;quot;, ifelse(
                    ...))),
            Treatment = ifelse(Column %in% 1:3, &amp;quot;Drug A&amp;quot;, ifelse(
                Column %in% 4:6, &amp;quot;Drug B&amp;quot;, ifelse(
                    ...)))
            )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But doing it this way made me want to cry.&lt;/p&gt;

&lt;p&gt;My next strategy was to try to directly make a table and then merge it into the data. The table would look something like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;WellId&lt;/th&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Treatment&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;A01&lt;/td&gt;
&lt;td&gt;X&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;A02&lt;/td&gt;
&lt;td&gt;X&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;A03&lt;/td&gt;
&lt;td&gt;X&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;A04&lt;/td&gt;
&lt;td&gt;X&lt;/td&gt;
&lt;td&gt;Drug B&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;B01&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;B01&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Drug A&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;While this merges nicely into a data frame and solves the problem of indicating what each well is, it&amp;rsquo;s actually not that easy to create by hand, especially in more realistic experiments with more variables and a more complex plate layout. Even worse, there is a substantial risk of typos and copy-paste errors.&lt;/p&gt;

&lt;h2 id=&#34;plater-to-the-rescue&#34;&gt;&lt;code&gt;plater&lt;/code&gt; to the rescue&lt;/h2&gt;

&lt;p&gt;The solution came from the plates themselves: store the data and mapping in the shape of the plate and then transform it into a tidy shape. Scientific instruments often provide data in the shape of a plate, in fact: you get back a spreadsheet with a grid of numbers shaped like your plate, with a cell for each well.&lt;/p&gt;

&lt;p&gt;My first step was to write a function to convert one of those plate-shaped grids to a data frame with two columns: one of plate IDs and one of the numbers (machine read-out).&lt;/p&gt;

&lt;p&gt;So now I could take a &lt;code&gt;.csv&lt;/code&gt; file with plate-shaped data and convert it into tidy form and connect it with the well ID. It didn&amp;rsquo;t take long for me to start creating &lt;code&gt;.csv&lt;/code&gt; files with sample and treatment information as well and then merging the data frames together. Now I could create plate maps really easily because they looked just like the plate I did the experiment in.&lt;/p&gt;

&lt;h2 id=&#34;a-package-takes-shape&#34;&gt;A package takes shape&lt;/h2&gt;

&lt;p&gt;With time and feedback from &lt;a href=&#34;https://github.com/ClaireLevy&#34;&gt;others in the lab&lt;/a&gt;, I refined the system. Instead of creating separate files for each variable (treatment, sample, data, &amp;hellip;), everything could go in one &lt;code&gt;.csv&lt;/code&gt; file, with sequential plate layouts for each variable (say, treatment or sample). I started calling this the &lt;code&gt;plater&lt;/code&gt; format and storing all of my data that way. These files are especially nice because they give an overview of the whole experiment in a compact format.&lt;/p&gt;

&lt;p&gt;Eventually, I boiled it down to a small set of functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;read_plate&lt;/code&gt;: takes a &lt;code&gt;plater&lt;/code&gt; format file and gives you a tidy data frame&lt;/li&gt;
&lt;li&gt;&lt;code&gt;read_plates&lt;/code&gt;: takes multiple &lt;code&gt;plater&lt;/code&gt; format files and gives you a big tidy data frame&lt;/li&gt;
&lt;li&gt;&lt;code&gt;add_plate&lt;/code&gt;: takes a &lt;code&gt;plater&lt;/code&gt; format file and a tidy data frame and combines them&lt;/li&gt;
&lt;li&gt;&lt;code&gt;view_plate&lt;/code&gt;: takes a tidy data frame and displays selected variables from it back in plate shape&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;is-this-thing-any-good&#34;&gt;Is this thing any good?&lt;/h2&gt;

&lt;p&gt;With the package in place, I started getting ready to submit it to CRAN, but I wanted to get more feedback first and rOpenSci seemed perfect for that.&lt;/p&gt;

&lt;p&gt;It turned out that the improvements started even before I got any feedback. As I prepared to submit the package to rOpenSci, I went through their &lt;a href=&#34;https://github.com/ropensci/onboarding#how-to-submit-your-package-for-review&#34;&gt;thorough guide&lt;/a&gt;  to make sure &lt;code&gt;plater&lt;/code&gt; met all of the requirements. This process made me aware of best practices and motivated me to handle niggling little details like using consistent &lt;code&gt;snake_case&lt;/code&gt;, making sure all of the documentation was clear, and creating a code of conduct for contributors. In all, I made 22 commits preparing for submission.&lt;/p&gt;

&lt;p&gt;The review process itself led to even more improvement. Two generous reviewers (&lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt; and &lt;a href=&#34;http://deanattali.com/&#34;&gt;Dean Attali&lt;/a&gt;) and an &lt;a href=&#34;https://scottchamberlain.info/&#34;&gt;editor&lt;/a&gt; spent multiple hours reading the code, testing the functions, and thinking about how to make it more useful. Their thoughtful suggestions resulted in many changes to the package (I made 61 commits responding to the reviews) that made it more robust and useful. Among others:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Make &lt;code&gt;add_plate&lt;/code&gt; more easily pipeable by reordering the arguments&lt;/li&gt;
&lt;li&gt;Add a function &lt;code&gt;check_plater_format&lt;/code&gt; to test if a file is formatted correctly&lt;/li&gt;
&lt;li&gt;Brainstorm a Shiny tool for non-R users to use &lt;code&gt;plater&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Explore alternative visualizations to &lt;code&gt;view_plate&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reviewing process made &lt;code&gt;plater&lt;/code&gt; a much better package and left me feeling confident in putting it up on CRAN with a stable version 1.0.0.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Since transferring &lt;code&gt;plater&lt;/code&gt; over to rOpenSci and putting it onto CRAN, I&amp;rsquo;ve used the package all the time and have shared it with lab mates and colleagues. It works well and does exactly what I want, seamlessly without my needing to even notice it. This level of convenience and utility is the best testament to the efforts of the reviewers and editors of rOpenSci, who helped to make it a better package.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Using xml schema and xslt in R</title>
      <link>https://ropensci.org/blog/2017/01/10/xslt-release/</link>
      <pubDate>Tue, 10 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/01/10/xslt-release/</guid>
      <description>
        
        

&lt;p&gt;This week an update for &lt;a href=&#34;https://cran.r-project.org/web/packages/xml2/index.html&#34;&gt;xml2&lt;/a&gt; and a new &lt;a href=&#34;https://cran.r-project.org/web/packages/xslt/index.html&#34;&gt;xslt&lt;/a&gt; package have appeared on CRAN. A full announcement for xml2 version 1.1 will appear on the &lt;a href=&#34;https://blog.rstudio.org/&#34;&gt;rstudio blog&lt;/a&gt;. This post explains xml &lt;em&gt;validation&lt;/em&gt; (via xsd schema) and xml &lt;em&gt;transformation&lt;/em&gt; (via xslt stylesheets) which have been added in this release.&lt;/p&gt;

&lt;p&gt;XML schemas and stylesheets are not exactly new; both &lt;a href=&#34;https://www.w3.org/TR/xslt11/&#34;&gt;xslt 1.1&lt;/a&gt; (2001) and &lt;a href=&#34;https://www.w3.org/TR/xmlschema-1/&#34;&gt;xsd 1.0&lt;/a&gt; (2004) have been available in browsers for over a decade. Revised specifications for xsd/xslt are still developed, but not widely implemented due to declined popularity of xml itself. Our R implementation builds on &lt;a href=&#34;http://xmlsoft.org/libxslt/&#34;&gt;libxslt&lt;/a&gt; which supports XSLT 1.0 features plus most of the EXSLT set of processor-portable extensions functions.&lt;/p&gt;

&lt;h2 id=&#34;xml-validation-with-xsd&#34;&gt;XML Validation with XSD&lt;/h2&gt;

&lt;p&gt;XML schema, also referred to as XSD (XML Schema Definition) is standard for defining the fields and formats that are supposed to appear within an XML document. This provides a formal method for validating XML messages. The schema itself is also written in XML (there is even an &lt;a href=&#34;https://www.w3.org/2001/XMLSchema.xsd&#34;&gt;xsd schema&lt;/a&gt; for validating xml schemas).&lt;/p&gt;

&lt;p&gt;This &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/ms256129(v=vs.110).aspx&#34;&gt;example&lt;/a&gt; from msdn illustrates the idea using a schema for a hypothetical purchase order. Imagine a vendor has an XML api for retailers to automatically order products. The order can be quite complex but the schema formally describes what constitutes a valid XML order message. It contains fields like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt; &amp;lt;xs:complexType name=&amp;quot;PurchaseOrderType&amp;quot;&amp;gt;
  &amp;lt;xs:sequence&amp;gt;
   &amp;lt;xs:element name=&amp;quot;shipTo&amp;quot; type=&amp;quot;USAddress&amp;quot;/&amp;gt;
   &amp;lt;xs:element name=&amp;quot;billTo&amp;quot; type=&amp;quot;USAddress&amp;quot;/&amp;gt;
   &amp;lt;xs:element ref=&amp;quot;comment&amp;quot; minOccurs=&amp;quot;0&amp;quot;/&amp;gt;
   &amp;lt;xs:element name=&amp;quot;items&amp;quot;  type=&amp;quot;Items&amp;quot;/&amp;gt;
  &amp;lt;/xs:sequence&amp;gt;
  &amp;lt;xs:attribute name=&amp;quot;orderDate&amp;quot; type=&amp;quot;xs:date&amp;quot;/&amp;gt;
 &amp;lt;/xs:complexType&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both the client and server can easily validate an XML order against this schema to ensure that all required fields are present and contain the correct format. A copy of this example is included with the &lt;code&gt;xml2&lt;/code&gt; package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Example order
doc &amp;lt;- read_xml(system.file(&amp;quot;extdata/order-doc.xml&amp;quot;, package = &amp;quot;xml2&amp;quot;))

# Example schema
schema &amp;lt;- read_xml(system.file(&amp;quot;extdata/order-schema.xml&amp;quot;, package = &amp;quot;xml2&amp;quot;))
xml_validate(doc, schema)
# TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;xml_validate&lt;/code&gt; function returns TRUE or FALSE. If FALSE it also contains an attribute with a data frame listing invalid elements in the XML document. Let&amp;rsquo;s replace some text in the XML document to make it invalid:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Create invalid order to test
str &amp;lt;- readLines(system.file(&amp;quot;extdata/order-doc.xml&amp;quot;, package = &amp;quot;xml2&amp;quot;))
str &amp;lt;- sub(&amp;quot;&amp;lt;quantity&amp;gt;1&amp;quot;, &amp;quot;&amp;lt;quantity&amp;gt;&amp;quot;, str)
str &amp;lt;- sub(&amp;quot;95819&amp;quot;, &amp;quot;ABC95819&amp;quot;, str)
str &amp;lt;- sub(&#39;partNum=&amp;quot;926-AA&amp;quot;&#39;, &amp;quot;&amp;quot;, str)
doc &amp;lt;- read_xml(paste(str, collapse = &amp;quot;\n&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This new document will fail validation. The return object from &lt;code&gt;xml_validate&lt;/code&gt; contains an &lt;code&gt;error&lt;/code&gt; attribute with a dataframe containing the validation errors.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Fails validation
out &amp;lt;- xml_validate(doc, schema)

# Show the errors
attr(out, &amp;quot;errors&amp;quot;)
#[1] &amp;quot;Element &#39;zip&#39;: &#39;ABC95819&#39; is not a valid value of the atomic type &#39;xs:decimal&#39;.&amp;quot;
#[2] &amp;quot;Element &#39;quantity&#39;: &#39;&#39; is not a valid value of the local atomic type.&amp;quot;
#[3] &amp;quot;Element &#39;item&#39;: The attribute &#39;partNum&#39; is required but missing.&amp;quot;
#[4] &amp;quot;Element &#39;quantity&#39;: &#39;&#39; is not a valid value of the local atomic type.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When implementing an R client for a system with an XML API which also provides a schema, it is good practice to validate your messages before submitting them to the server. Thereby you catch problems with your XML document locally.&lt;/p&gt;

&lt;h2 id=&#34;xml-transformation-with-xsl&#34;&gt;XML Transformation with XSL&lt;/h2&gt;

&lt;p&gt;Extensible Stylesheet Language (XSL) Transformation provides a standardized language for converting a certain XML structure into another XML or HTML structure. Usually the original xml document provides the raw data, and the stylesheet contains a template for a HTML page that presents this content. Again, the XSLT document itself is also written in XML.&lt;/p&gt;

&lt;p&gt;We have decided to implement this in a separate package called &lt;code&gt;xslt&lt;/code&gt; because it requires another C library. Try the example from the &lt;code&gt;xml_xslt&lt;/code&gt; manual page:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(xslt)
doc &amp;lt;- read_xml(system.file(&amp;quot;examples/cd_catalog.xml&amp;quot;, package = &amp;quot;xslt&amp;quot;))
style &amp;lt;- read_xml(system.file(&amp;quot;examples/cd_catalog.xsl&amp;quot;, package = &amp;quot;xslt&amp;quot;))
html &amp;lt;- xml_xslt(doc, style)
cat(as.character(html))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example is explained in more detail on &lt;a href=&#34;http://www.w3schools.com/xml/xsl_transformation.asp&#34;&gt;w3schools&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;why-use-xslt&#34;&gt;Why Use XSLT?&lt;/h2&gt;

&lt;p&gt;As the name implies, XSLT is designed to apply styling so that we can separate data of a document from its presentation markup. Take this &lt;a href=&#34;https://msdn.microsoft.com/nl-nl/library/ms765388(v=vs.85).aspx&#34;&gt;example&lt;/a&gt; of an XSLT document from the msdn homepage:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;xsl:stylesheet xmlns:xsl=&amp;quot;http://www.w3.org/1999/XSL/Transform&amp;quot; version=&amp;quot;1.0&amp;quot;&amp;gt;
  &amp;lt;xsl:template match=&amp;quot;/hello-world&amp;quot;&amp;gt;
    &amp;lt;HTML&amp;gt;
      &amp;lt;HEAD&amp;gt;
        &amp;lt;TITLE&amp;gt;&amp;lt;/TITLE&amp;gt;
      &amp;lt;/HEAD&amp;gt;
      &amp;lt;BODY&amp;gt;
        &amp;lt;H1&amp;gt;
          &amp;lt;xsl:value-of select=&amp;quot;greeting&amp;quot;/&amp;gt;
        &amp;lt;/H1&amp;gt;
        &amp;lt;xsl:apply-templates select=&amp;quot;greeter&amp;quot;/&amp;gt;
      &amp;lt;/BODY&amp;gt;
    &amp;lt;/HTML&amp;gt;
  &amp;lt;/xsl:template&amp;gt;
  &amp;lt;xsl:template match=&amp;quot;greeter&amp;quot;&amp;gt;
    &amp;lt;DIV&amp;gt;from &amp;lt;I&amp;gt;&amp;lt;xsl:value-of select=&amp;quot;.&amp;quot;/&amp;gt;&amp;lt;/I&amp;gt;&amp;lt;/DIV&amp;gt;
  &amp;lt;/xsl:template&amp;gt;
&amp;lt;/xsl:stylesheet&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now if we apply this to a document like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&amp;quot;text/xsl&amp;quot; href=&amp;quot;hello.xsl&amp;quot;?&amp;gt;
&amp;lt;hello-world&amp;gt;   &amp;lt;greeter&amp;gt;An XSLT Programmer&amp;lt;/greeter&amp;gt;   &amp;lt;greeting&amp;gt;Hello, World!&amp;lt;/greeting&amp;gt;&amp;lt;/hello-world&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We get the following output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;HTML&amp;gt;
&amp;lt;HEAD&amp;gt;
&amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset=UTF-8&amp;quot;&amp;gt;
&amp;lt;TITLE&amp;gt;&amp;lt;/TITLE&amp;gt;
&amp;lt;/HEAD&amp;gt;
&amp;lt;BODY&amp;gt;
&amp;lt;H1&amp;gt;Hello, World!&amp;lt;/H1&amp;gt;
&amp;lt;DIV&amp;gt;from &amp;lt;I&amp;gt;An XSLT Programmer&amp;lt;/I&amp;gt;
&amp;lt;/DIV&amp;gt;
&amp;lt;/BODY&amp;gt;
&amp;lt;/HTML&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When XSLT was introduced in &lt;a href=&#34;https://www.w3.org/TR/xslt&#34;&gt;1999&lt;/a&gt;, it was expected that xml would replace html. Computer scientists envisioned that dynamic content of websites would be served via semantically structured xmls feeds (such as RSS), and presentation markup (i.e. a nice html page) could be added on the client by applying a fixed transformation.&lt;/p&gt;

&lt;p&gt;Unfortunately that&amp;rsquo;s now how it went. It turned out that xslt was overly complex and never really found wide adoption. Instead people started writing dynamic HTML pages using PHP, which was slow and insecure, but considerably easier to learn. And that brings us back to R :)&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Update jsonlite 1.2</title>
      <link>https://ropensci.org/technotes/2017/01/04/jsonlite-12/</link>
      <pubDate>Wed, 04 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/01/04/jsonlite-12/</guid>
      <description>
        
        

&lt;p&gt;A new version of &lt;a href=&#34;https://cran.r-project.org/web/packages/jsonlite/index.html&#34;&gt;jsonlite&lt;/a&gt; package to CRAN. This is a maintenance release with enhancements and bug fixes. A summary of changes in v1.2 from the &lt;a href=&#34;https://cran.r-project.org/web/packages/jsonlite/NEWS&#34;&gt;NEWS&lt;/a&gt; file:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Add &lt;code&gt;read_json&lt;/code&gt; and &lt;code&gt;write_json&lt;/code&gt; convenience wrappers, &lt;a href=&#34;https://github.com/jeroen/jsonlite/issues/161&#34;&gt;#161&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Update &lt;code&gt;modp_numtoa&lt;/code&gt; from upstream, fixes a rounding issue in &lt;a href=&#34;https://github.com/jeroen/jsonlite/issues/148&#34;&gt;#148&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Ensure &lt;code&gt;asJSON.POSIXt&lt;/code&gt; does not use sci notation for negative values, &lt;a href=&#34;https://github.com/jeroen/jsonlite/issues/155&#34;&gt;#155&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tweak &lt;code&gt;num_to_char&lt;/code&gt; to properly print large negative numbers&lt;/li&gt;
&lt;li&gt;Performance optimization for simplyfing data frames (see below)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Use the &lt;em&gt;Github compare&lt;/em&gt; page to see the full diff on &lt;a href=&#34;https://github.com/cran/jsonlite/compare/1.1...1.2&#34;&gt;metacran&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;new-read-write-api&#34;&gt;New read/write API&lt;/h2&gt;

&lt;p&gt;The package has gained new high level functions &lt;code&gt;read_json&lt;/code&gt; and &lt;code&gt;write_json&lt;/code&gt;. These are &lt;a href=&#34;https://github.com/cran/jsonlite/blob/1.2/R/read_json.R#L18-L29&#34;&gt;wrappers&lt;/a&gt; for &lt;code&gt;fromJSON&lt;/code&gt; and &lt;code&gt;toJSON&lt;/code&gt; which read/write json directly from/to disk. This API is consistent with tidyverse packages like readr, readxl and haven (see &lt;a href=&#34;https://github.com/jeroen/jsonlite/issues/161&#34;&gt;#161&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The only thing to note is that &lt;code&gt;read_json&lt;/code&gt; does not simplify by default, as is done by &lt;code&gt;fromJSON&lt;/code&gt;. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Write Data frame to a temp file
tmp &amp;lt;- tempfile()
write_json(iris, tmp)

# Nested lists
read_json(tmp)

# A data frame
read_json(tmp, simplifyVector = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice how &lt;code&gt;read_json&lt;/code&gt; only returns a data frame when &lt;code&gt;simplifyVector&lt;/code&gt; is explicitly set to &lt;code&gt;TRUE&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;performance-enhancements&#34;&gt;Performance enhancements&lt;/h2&gt;

&lt;p&gt;We have ported a bit of C code to optimize simplification for data frame structures. This script compares performance for both versions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# example json
json &amp;lt;- jsonlite::toJSON(ggplot2::diamonds)

# Test with jsonlite 1.1
devtools::install_github(&amp;quot;cran/jsonlite@1.1&amp;quot;)
microbenchmark::microbenchmark(jsonlite::fromJSON(json), times = 50)

# Unload jsonlite 1.1 (might need restart R on windows)
unloadNamespace(&amp;quot;jsonlite&amp;quot;)
library.dynam.unload(&#39;jsonlite&#39;, find.package(&#39;jsonlite&#39;))

# Test with jsonlite 1.2
devtools::install_github(&amp;quot;cran/jsonlite@1.2&amp;quot;)
microbenchmark::microbenchmark(jsonlite::fromJSON(json), times = 50)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On my Macbook this has reduced the median time from approx 0.91s to 0.76s.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Announcing pdftools 1.0</title>
      <link>https://ropensci.org/technotes/2016/12/09/pdftools-10/</link>
      <pubDate>Fri, 09 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2016/12/09/pdftools-10/</guid>
      <description>
        
        

&lt;p&gt;This week we released version &lt;code&gt;1.0&lt;/code&gt; of the ropensci &lt;a href=&#34;https://cran.r-project.org/web/packages/pdftools/index.html&#34;&gt;pdftools&lt;/a&gt; package to CRAN. Pdftools provides utilities for extracting text, fonts, attachments and other data from PDF files. It also supports rendering of PDF files into bitmap images.&lt;/p&gt;

&lt;p&gt;This release has a few internal enhancements and fixes an annoying bug for landscape PDF pages. The version bump to &lt;code&gt;1.0&lt;/code&gt; signifies that the package has undergone sufficient testing and the API is stable.&lt;/p&gt;

&lt;h2 id=&#34;extracting-text&#34;&gt;Extracting Text&lt;/h2&gt;

&lt;p&gt;As described in our previous &lt;a href=&#34;https://ropensci.org/blog/blog/2016/03/01/pdftools-and-jeroen&#34;&gt;post&lt;/a&gt;, the most common use of &lt;code&gt;pdftools&lt;/code&gt; is extracting text from (scientific) articles for searching / indexing. But let&amp;rsquo;s try a somewhat more unusual PDF file this time: a poster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(pdftools)
url &amp;lt;- &amp;quot;https://www.rstudio.com/wp-content/uploads/2016/02/advancedR.pdf&amp;quot;

# Display author, editor
pdf_info(url)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;pdf_info&lt;/code&gt; file returns all kind of metadata from the pdf file. For example we can read that this PDF was created on 2016-02-12 by Arianne Colton using Acrobat PDFMaker 11 for PowerPoint.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# extract text vector
text &amp;lt;- pdf_text(url)

# Print text from page 1
cat(text[1])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;pdf_text&lt;/code&gt; function extracts text into an R character vector if length equal to the number of pages in the PDF.
Note how the text is spaced to match the position in the PDF page.&lt;/p&gt;

&lt;h2 id=&#34;rendering-pdf&#34;&gt;Rendering PDF&lt;/h2&gt;

&lt;p&gt;Recent versions of pdftools allow rendering of PDF pages into bitmap images. The &lt;code&gt;pdf_render_page&lt;/code&gt; function returns the bitmap as a raw vector array of size channels * width * height (in pixels).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(pdftools)
bitmap &amp;lt;- pdf_render_page(url, page = 1, dpi = 72)
dim(bitmap)
### 4 1100  850
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here we can use for example the rOpenSci &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick&lt;/a&gt; package to read the bitmap and manipulate/export it to various formats.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)
poster &amp;lt;- image_read(bitmap)
print(poster)
image_write(poster, &amp;quot;out.png&amp;quot;, format = &amp;quot;png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or have some fun with the other magick tools :)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Download dancing banana
banana &amp;lt;- image_read(&amp;quot;https://jeroen.github.io/images/banana.gif&amp;quot;)
banana &amp;lt;- image_scale(banana, &amp;quot;300&amp;quot;)

# Combine and flatten frames
frames &amp;lt;- lapply(banana, function(frame) {
  image_composite(poster, frame, offset = &amp;quot;+70+30&amp;quot;)
})

# Turn frames into animation
animation &amp;lt;- image_animate(image_join(frames))
print(animation)

# Save as gif
image_write(animation, &amp;quot;output.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

      </description>
    </item>
    
  </channel>
</rss>
