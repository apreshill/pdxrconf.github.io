<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Openscience on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/openscience/</link>
    <description>Recent content in Openscience on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 06 Oct 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/openscience/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Governance, Engagement, and Resistance in the Open Science Movement: A Comparative Study</title>
      <link>https://ropensci.org/blog/2017/10/06/sholler-plan/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/06/sholler-plan/</guid>
      <description>
        
        

&lt;p&gt;A growing community of scientists from a variety of disciplines is moving the norms of scientific research toward open practices. Supporters of open science hope to increase the quality and efficiency of research by enabling the widespread sharing of datasets, research software source code, publications, and other processes and products of research. The speed at which the open science community seems to be growing mirrors the rapid development of technological capabilities, including robust open source scientific software, new services for data sharing and publication, and novel data science techniques for working with massive datasets. Organizations like rOpenSci harness such capabilities and deploy various combinations of these research tools, or what I refer to here as open science infrastructures, to facilitate open science.&lt;/p&gt;

&lt;p&gt;As studies of other digital infrastructures have pointed out, developing and deploying the technological capabilities that support innovative work within a community of practitioners constitutes just one part of making innovation happen. As quickly as the technical solutions to improving scientific research may be developing, a host of organizational and social issues are lagging behind and hampering the open science community’s ability to inscribe open practices in the culture of scientific research. Remedying organizational and social issues requires paying attention to open science infrastructures’ human components, such as designers, administrators, and users, as well as the policies, practices, and organizational structures that contribute to the smooth functioning of these systems.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; These elements of infrastructure development have, in the past, proven to be equal to or more important than technical capabilities in determining the trajectory the infrastructure takes (e.g., whether it “succeeds” or “fails”).&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;As a postdoc with rOpenSci, I have begun a qualitative, ethnographic project to explore the organizational and social processes involved in making open science the norm in two disciplines: astronomy and ecology. I focus on these two disciplines to narrow, isolate, and compare the set of contextual factors (e.g., disciplinary histories, research norms, and the like) that might influence perceptions of open science. Specifically, I aim to answer the following research questions (RQ):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;RQ1a: What are the primary motivations of scientists who actively engage with open science infrastructures?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RQ1b: What are the factors that influence resistance to open science among some scientists?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RQ2: What strategies do open science infrastructure leaders use to encourage participation, govern contributions, and overcome resistance to open science infrastructure use?&lt;/p&gt;

&lt;p&gt;a. To what extent do governance strategies balance standardization and flexibility, centralization and decentralization, and voluntary and mandatory contributions?&lt;/p&gt;

&lt;p&gt;b. By what mechanisms are open science policies and practices enforced?&lt;/p&gt;

&lt;p&gt;c. What are the commonalities and differences in the rationale behind choices of governance strategies?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below, I describe how I am systematically investigating these questions in two parts. In Part 1, I am identifying the issues raised by scientists who engage with or resist the open science movement. In Part 2, I am studying the governance strategies open science leaders and decision-makers use to elicit engagement with open science infrastructures in these disciplines.&lt;/p&gt;

&lt;h3 id=&#34;part-1-engagement-with-and-resistance-to-open-science&#34;&gt;Part 1: Engagement with and Resistance to Open Science&lt;/h3&gt;

&lt;p&gt;I am firmly rooted in a research tradition which emphasizes that studying the uptake of a new technology or technological approach, no matter the type of work or profession, begins with capturing how the people charged with changing their activities respond to the change “on the ground.” In this vein, Part 1 of the study aims to lend empirical support or opposition to arguments for and against open science that are commonly found in opinion pieces, on social media, and in organizational mission statements. A holistic reading of such documents reveals several commonalities in the positions for and against open science. Supporters of open science often cite increased transparency, reproducibility, and collaboration as the overwhelming benefits of making scientific research processes and products openly available. Detractors highlight concerns over “scooping,” ownership, and the time costs of curating and publishing code and data.&lt;/p&gt;

&lt;p&gt;I am seeking to verify and test these claims by interviewing and surveying astronomers and ecologists or, more broadly, earth and environmental scientists who fall on various parts of the open science engagement-to-resistance spectrum. I am conducting interviews using a semi-structured interview protocol&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; across all interviewees. I will then use a qualitative data analysis approach based on the grounded theory method&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; to extract themes from the responses, focusing on the factors that promote engagement (e.g., making data available, spending time developing research software, or making publications openly accessible) or resistance (e.g., unwillingness to share code used in a study or protecting access to research data). Similar questions will be asked at scale via a survey.&lt;/p&gt;

&lt;p&gt;Armed with themes from the responses, I will clarify and refine the claims often made in the public sphere about the benefits and drawbacks of open science. I hope to develop this part of the study into actionable recommendations for promoting open science, governing contributions to open science repositories, and addressing the concerns of scientists who are hesitant about engagement.&lt;/p&gt;

&lt;h3 id=&#34;part-2-focusing-on-governance&#34;&gt;Part 2: Focusing on Governance&lt;/h3&gt;

&lt;p&gt;Even with interviews and surveys of scientists on the ground, it is difficult to systematically trace and analyze the totality of social and political processes that support open science infrastructure development because the processes occur across geographic, disciplinary, and other boundaries.&lt;/p&gt;

&lt;p&gt;However, as others have pointed out,&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; the organizational and social elements of digital infrastructure development often become visible and amenable to study through infrastructure governance. Governance refers to the combination of “executive and management roles, program oversight functions organized into structures, and policies that define management principles and decision making.”&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; Effective governance provides projects with the direction and oversight necessary to achieve desired outcomes of infrastructure development while allowing room for creativity and innovation.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:9&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:10&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:10&#34;&gt;10&lt;/a&gt;&lt;/sup&gt; Studying a project’s governance surfaces the negotiation processes that occur among stakeholders—users, managers, organizations, policymakers, and the like—throughout the development process. Outcomes include agreements about the types of technologies used, standards defining the best practices for technology use, and other policies to ensure that a robust, sustainable infrastructure evolves.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:9&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:11&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:11&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Despite the scientific research community’s increasing reliance on open science infrastructures, few studies compare different infrastructure governance strategies&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and even fewer develop new or revised strategies for governing infrastructure development and use.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:12&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:12&#34;&gt;12&lt;/a&gt;&lt;/sup&gt; The primary goal of this part of the project is to address this gap in our understanding of the governance strategies used to create, maintain, and grow open science infrastructures.&lt;/p&gt;

&lt;p&gt;I am administering this part of the study by conducting in-depth, semi-structured interviews with leaders of various open science infrastructure projects supporting work in astronomy and ecology. I define “leaders” in this context as individuals or small groups of individuals who make decisions about the management of open science infrastructures and their component parts. This set of leaders includes founders and administrators of widely-used scientific software packages and collections of packages, of open data repositories, of open access publication and preprint services, and various combinations of open science tools. Furthermore, I intend to interview the leaders of organizations with which the open science community interacts—top publication editors, for example—to gauge how open science practices and processes are being governed outside of active open science organizations.&lt;/p&gt;

&lt;p&gt;I will conduct qualitative coding as described above to develop themes from the responses of open science leaders. I will then ground these themes in the literature on digital infrastructure governance—which emphasizes gradual, decentralized, and voluntary development—and look for avenues to improve governance strategies.&lt;/p&gt;

&lt;p&gt;Alongside the interview and survey methods, I am actively observing and retaining primary documents from the ongoing discourse around open science in popular scientific communication publications (e.g., &lt;i&gt;Nature&lt;/i&gt; and &lt;i&gt;Science&lt;/i&gt;), conferences and meetings (e.g., OpenCon and discipline-specific hackweeks), and in the popular media/social media (e.g., &lt;i&gt;The New York Times&lt;/i&gt; and Twitter threads).&lt;/p&gt;

&lt;h3 id=&#34;preliminary-themes&#34;&gt;Preliminary Themes&lt;/h3&gt;

&lt;p&gt;I entered this project with a very basic understanding of how open science “works”—the technical and social mechanisms by which scientists make processes and outputs publicly available. In learning about the open science movement, in general and in particular instantiations, I’ve begun to see the intricacies involved in efforts to change scientific research and its modes of communication: research data publication, citation, and access; journal publication availability; and research software development and software citation standards. Within the community trying to sustain these changes are participants and leaders who are facing and tackling several important issues head-on. I list some of the most common engagement, resistance, and governance challenges appearing in interview and observation transcripts below.&lt;/p&gt;

&lt;h4 id=&#34;participation-challenges&#34;&gt;Participation challenges&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Overcoming the fear of sharing code and data, specifically the fear of sharing “messy” code and the fear of being shamed for research errors.&lt;/li&gt;
&lt;li&gt;Defending the time and financial costs of participation in open science—particularly open source software development—to supervisors, collaborators, or tenure and promotion panels who are not engaged with open science.&lt;/li&gt;
&lt;li&gt;Finding time to make code and data usable for others (e.g., through good documentation or complete metadata) and, subsequently, finding a home where code and data can easily be searched and found.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;governance-challenges&#34;&gt;Governance challenges&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Navigating the issue of convincing researchers that software development and data publication/archiving “count” as research products, even though existing funding, publication, and tenure and promotion models may not yet value those contributions.&lt;/li&gt;
&lt;li&gt;Developing guidelines and processes for conducting peer review on research publication, software, and data contributions, especially the tensions involved in “open review.”&lt;/li&gt;
&lt;li&gt;Deciding whose responsibility it is to enforce code and data publication standards or policies, both within open science organizations and in traditional outlets like academic journals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The points raised in this post and the questions guiding my project might seem like discussions you’ve had too many times over coffee during a hackathon break or over beers after a conference session. If so, I’d love to hear from you, even if you are not an astronomer, an ecologist, or an active leader of an open science infrastructure (dsholler at berkeley dot edu). I am always looking for new ideas, both confirming and disconfirming, to refine my approach to this project.&lt;/p&gt;

&lt;!-- references --&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Braa, J., Hanseth, O., Heywood, A., Mohammed, W., Shaw, V. 2007. Developing health information systems in developing countries: The flexible standards strategy. &lt;i&gt;MIS Quarterly&lt;/i&gt;, 31(2), 381-402. &lt;a href=&#34;https://doi.org/10.2307/25148796&#34;&gt;https://doi.org/10.2307/25148796&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Tilson, D., Lyytinen, K., Sørensen, C. 2010 Digital infrastructures: The missing IS research agenda. &lt;i&gt;Information Systems Research&lt;/i&gt;, 21(4), 748-759. &lt;a href=&#34;https://doi.org/10.1287/isre.1100.0318&#34;&gt;https://doi.org/10.1287/isre.1100.0318&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Borgman, C. L. 2010. &lt;i&gt;Scholarship in the digital age: Information, infrastructure, and the Internet.&lt;/i&gt; MIT Press, Cambridge, MA. ISBN: 9780262250863
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Vaast, E., Walsham, G. 2009. Trans-situated learning: Supporting a network of practice with an information infrastructure. &lt;i&gt;Information Systems Research&lt;/i&gt;, 20(4), 547-564. &lt;a href=&#34;https://doi.org/10.1287/isre.1080.0228&#34;&gt;https://doi.org/10.1287/isre.1080.0228&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;Spradley, J. P. (2016). &lt;i&gt;The ethnographic interview&lt;/i&gt;. Longegrove, IL: Waveland Press. ISBN: 0030444969
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;Corbin, J., Strauss, A.,  1990. Grounded theory research: Procedures, canons, and evaluative criteria. &lt;i&gt;Qualitative Sociology&lt;/i&gt;, 13(1), 3-21. &lt;a href=&#34;https://doi.org/10.1007/BF00988593&#34;&gt;https://doi.org/10.1007/BF00988593&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;Barrett, M., Davidson, E., Prabhu, J., Vargo, S. L. 2015. Service innovation in the digital age: Key contributions and future directions. &lt;i&gt;MIS Quarterly&lt;/i&gt;, 39(1) 135-154. DOI: 10.25300/MISQ/2015/39:1.03
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;Hanford, M. 2005. Defining program governance and structure. &lt;i&gt;IBM developerWorks&lt;/i&gt;. Available at: &lt;a href=&#34;https://www.ibm.com/developerworks/rational/library/apr05/hanford/&#34;&gt;https://www.ibm.com/developerworks/rational/library/apr05/hanford/&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;Star, S. L., Ruhleder, K. 1996. Steps toward an ecology of infrastructure: Design and access for large information spaces. &lt;i&gt;Information Systems Research&lt;/i&gt;, 7(1), 111-134. &lt;a href=&#34;https://doi.org/10.1287/isre.7.1.111&#34;&gt;https://doi.org/10.1287/isre.7.1.111&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:9&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;Edwards, P. N., Jackson, S. J., Bowker, G. C., Knobel, C. P. 2007. &lt;i&gt;Understanding infrastructure: Dynamics, tensions, and design. Final report for Workshop on History and Theory of Infrastructure: Lessons for New Scientific Cyberinfrastructures&lt;/i&gt;. NSF Report. Available at: &lt;a href=&#34;https://deepblue.lib.umich.edu/handle/2027.42/49353&#34;&gt;https://deepblue.lib.umich.edu/handle/2027.42/49353&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:10&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;Hanseth, O., Jacucci, E., Grisot, M., Aanestad, M. 2006. Reflexive standardization: Side effects and complexity in standard making. &lt;i&gt;MIS Quarterly&lt;/i&gt;, 30(1), 563-581. &lt;a href=&#34;https://doi.org/10.2307/25148773&#34;&gt;https://doi.org/10.2307/25148773&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:11&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;Hanseth, O., &amp;amp; Lyytinen, K. (2010). Design theory for dynamic complexity in information infrastructures: the case of building internet. &lt;i&gt;Journal of Information Technology&lt;/i&gt;, 25(1), 1-19.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:12&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>The challenge of combining 176 x &amp;#35;otherpeoplesdata to create the Biomass And Allometry Database</title>
      <link>https://ropensci.org/blog/2015/06/03/baad/</link>
      <pubDate>Wed, 03 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2015/06/03/baad/</guid>
      <description>
        
        

&lt;p&gt;Despite the hype around &amp;ldquo;big data&amp;rdquo;, a more immediate problem facing many scientific analyses is that large-scale databases must be assembled from a collection of small independent and heterogeneous fragments &amp;ndash; the outputs of many and isolated scientific studies conducted around the globe.&lt;/p&gt;

&lt;p&gt;Collecting and compiling these fragments is challenging at both political and technical levels. The political challenge is to manage the carrots and sticks needed to promote sharing of data within the scientific community. The politics of data sharing have been the primary focus for debate over the last 5 years, but now that many journals and funding agencies are requiring data to be archived at the time of publication, the availability of these data fragments is increasing. But little progress has been made on the technical challenge: &lt;strong&gt;how can you combine a collection of independent fragments, each with its own peculiarities, into a single quality database?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Together with 92 other co-authors, we recently published the &lt;a href=&#34;https://github.com/dfalster/baad&#34;&gt;Biomass And Allometry Database (BAAD)&lt;/a&gt; as a &lt;a href=&#34;http://www.esapubs.org/archive/ecol/E096/128/&#34;&gt;data paper&lt;/a&gt; in the journal Ecology, combining data from 176 different scientific studies into a single unified database. We built BAAD for several reasons: i) we needed it for our own work ii) we perceived a strong need within the vegetation modelling community for such a database and iii) because it allowed us to road-test some new methods for building and maintaining a database &lt;sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:database&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:database&#34;&gt;0&lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Until now, every other data compilation we are aware of has been assembled in the dark. By this we mean, end-users are provided with a finished product, but remain unaware of the diverse modifications that have been made to components in assembling the unified database. Thus users have limited insight into the quality of methods used, nor are they able to build on the compilation themselves.&lt;/p&gt;

&lt;p&gt;The approach we took with BAAD is quite different: our database is built from raw inputs using scripts; plus the entire work-flow and history of modifications is available for users to inspect, run themselves and ultimately build upon. We believe this is a better way for managing lots of #otherpeoplesdata and so below share some of the key insights from our experience.&lt;/p&gt;

&lt;h2 id=&#34;1-script-everything-and-rebuild-from-source&#34;&gt;1. Script everything and rebuild from source&lt;/h2&gt;

&lt;p&gt;From the beginning of the project, we decided to script everything. We wanted the entire work-flow of transforming raw data files into a unified database to be completely scripted and able to be rerun at any point. When your work-flow is scripted, you can make a small change and then rebuild the database in an instant. Another reason for scripting is that it ensures all the modifications to the data are well documented. This simply isn&amp;rsquo;t possible in Excel. Looking at our code, you can see exactly how we modified the data to arrive at the end product.&lt;/p&gt;

&lt;p&gt;The only potential cost of continually rebuilding the database is that the process of rebuilding can take time. In the end, the time taken to make all the transformations and combine all 176 studies was pretty minimal &amp;ndash; ~9 seconds all-up. But the job of continually rebuilding the database became a lot quicker once we started using &lt;a href=&#34;https://github.com/richfitz/remake&#34;&gt;remake&lt;/a&gt; &lt;sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:remake&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:remake&#34;&gt;0&lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt;. Remake caches built objects (e.g. the transformed data from each study) and only rebuilds each of them if either the data or code generating that particular object has changed. So after the first longer run, rebuilding the entire database takes in the range of 1&amp;ndash;2 seconds.&lt;/p&gt;

&lt;p&gt;Another advantage of constantly rebuilding is that we were forced to make our code more robust and portable, so that it would run safely on all the collaborators machines. Recently we took this one step further by setting up some automated builds, using a continuous integration system (&lt;a href=&#34;https://travis-ci.or&#34;&gt;Travis&lt;/a&gt;) that automatically rebuilds the database on a fresh remote virtual machine &lt;sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:TravisCI&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:TravisCI&#34;&gt;0&lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt;. This approach allows us to detect undocumented dependencies or changes to dependencies that would prevent others rebuilding the database.&lt;/p&gt;

&lt;p&gt;The current status of the BAAD is: &lt;a href=&#34;https://travis-ci.org/dfalster/baad&#34;&gt;&lt;img src=&#34;https://travis-ci.org/dfalster/baad.svg?branch=master&#34; alt=&#34;Build Status&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-establish-a-data-processing-pipeline&#34;&gt;2. Establish a data-processing pipeline&lt;/h2&gt;

&lt;p&gt;The hashtag &lt;a href=&#34;https://twitter.com/search?q=%23otherpeoplesdata&#34;&gt;#otherpeoplesdata&lt;/a&gt; documents the challenge and frustrations of working with data that were curated by others. (We each have our own ways of preparing a dataset, but often the logic we bring to the problem cannot be inferred by others from the spreadsheet alone.) For us, the trick to working with large amounts of #otherpeoplesdata was to establish a solid processing pipeline, and then focus on getting every new study into that pipeline. Once in the pipeline, a common set of operations is applied (Figure 1). So the challenge for each new study was reduced from &amp;ldquo;transform into final output&amp;rdquo;, to &amp;ldquo;get it into the pipeline&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Work flow for building the BAAD. Data from each study is processed in the same way, using a standardised set of input files, resulting in a single database with a common format.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2015-06-03-baad/stuff.png&#34; alt=&#34;baad figure 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The following principles were applied in establishing our processing pipeline.&lt;/p&gt;

&lt;h3 id=&#34;don-t-modify-raw-data-files&#34;&gt;Don&amp;rsquo;t modify raw data files&lt;/h3&gt;

&lt;p&gt;Raw data is holy. A back-of-the-envelope calculation suggests the data we are managing would cost millions to collect afresh. We decided early on that we would aim to keep the original files sent to us unchanged, as much as possible. In many cases it was necessary to export an Excel spreadsheet as a csv file, but beyond that, the file should be basically as it was provided. A limited number of actions were performed on raw data files such as (click on links for examples) &lt;a href=&#34;https://github.com/dfalster/baad/commit/7d10aede58080d83d59fe3be5043829b15f0236b&#34;&gt;incorporating an updated dataset from a contributor&lt;/a&gt;, or minor modifications allowing data
to be loaded into R, including &lt;a href=&#34;https://github.com/dfalster/baad/commit/5bb9044e7e4b63ad2febca986ebf1e45f24cdd0e&#34;&gt;modifying line endings&lt;/a&gt; &lt;sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:line-endings&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:line-endings&#34;&gt;0&lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt;, &lt;a href=&#34;https://github.com/dfalster/baad/commit/ec82e83d1b50f4e6bc2df2a780d2bb1684530652&#34;&gt;removing a string of trailing empty columns&lt;/a&gt;, &lt;a href=&#34;https://github.com/dfalster/baad/commit/d22bc1ee1db3870a7e281de22862eaa1ced4ddd1&#34;&gt;removing special characters causing R to crash&lt;/a&gt;, and &lt;a href=&#34;https://github.com/dfalster/baad/commit/4c83c70eb965bfd9c3b7c30f88312e646476836b&#34;&gt;making column names unique&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The types of operations that were not allowed include data-transformations and creation of new columns &amp;ndash; these were all handled in our pipeline.&lt;/p&gt;

&lt;h3 id=&#34;encode-meta-data-as-data-not-as-code&#34;&gt;Encode meta-data as data, not as code&lt;/h3&gt;

&lt;p&gt;In the early stages of our project, we encoded a lot of the changes we wanted to make to the data into our R scripts. For example, the code below is taken from &lt;a href=&#34;https://github.com/dfalster/baad/blob/912163bb371e280340dee2bb4cf872a1d7ede81b/R/makeCleanDataFiles.R&#34;&gt;early in the project&amp;rsquo;s history&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;	if(names[i]==&amp;quot;Kohyama1987&amp;quot;){
		raw        &amp;lt;-  read.csv(paste(dir.rawData,&amp;quot;/&amp;quot;,names[i],&amp;quot;/data.csv&amp;quot;, sep=&#39;&#39;), h=T, stringsAsFactors=FALSE)
		raw$SpecCode[raw$SpecCode==&#39;Cs&#39;]  &amp;lt;-  &amp;quot;Camellia sasanqua&amp;quot;
		raw$SpecCode[raw$SpecCode==&#39;Cj&#39;]  &amp;lt;-  &amp;quot;Camellia japonia&amp;quot;
		...
		...
		raw$leaf.mass  &amp;lt;-  raw$Wtl.g + raw$Wbl.g
		raw$m.st       &amp;lt;-  raw$Wts.g + raw$Wbs.g
		new[[i]]   &amp;lt;-  cbind(dataset=names[i], species=raw$SpecCode, raw[,c(5:8, 14:ncol(raw))], latitude=30.31667, longitude=130.4333, location=&amp;quot;Ohkou River, Yakushima Island, Kyushu, Japan&amp;quot;, reference=&amp;quot;Kohyama T (1987) Significance of architecture and allometry in saplings. Functional Ecology 1:399-404.&amp;quot;, growingCondition=&amp;quot;FW&amp;quot;, vegetation=&amp;quot;TempRf&amp;quot;, stringsAsFactors=FALSE)
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code above shows operations for a single study: loading raw data, making new columns, save the compiled object. The &lt;strong&gt;problem&lt;/strong&gt; with this code is that it mixes in a bunch of useful data with our R code. We had not yet identified a common pipeline for processing data. Eventually we moved all this extra data into their own &lt;em&gt;.csv&lt;/em&gt; files and treated them as we should, as data. We then use functions
to modify the raw data, using the &lt;a href=&#34;http://nicercode.github.io/blog/2013-07-09-modifying-data-with-lookup-tables/&#34;&gt;new files as input&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Each study in the database was therefore required to have a standard set of files to enter the data-processing pipeline:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data.csv&lt;/code&gt;: raw data table provided by authors.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataMatchColumns.csv&lt;/code&gt;: for each column in &lt;code&gt;data.csv&lt;/code&gt;, provides units of the incoming variable, and the name of the variable onto which we want to map this data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataNew.csv&lt;/code&gt;: allows for addition of any new data not present in &lt;code&gt;data.csv&lt;/code&gt;, or modification of existing values based on a find-and-replace logic.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;studyMetadata.csv&lt;/code&gt;: information about the methods used to collect the data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;studyContact.csv&lt;/code&gt;: contacts and affiliations information for contributors.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;studyRef.bib&lt;/code&gt;: bibliographic record of primary source, in &lt;a href=&#34;https://en.wikipedia.org/wiki/Bibtex&#34;&gt;bibtex format&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Changes are then made using &lt;a href=&#34;http://nicercode.github.io/blog/2013-07-09-modifying-data-with-lookup-tables/&#34;&gt;lookup tables&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are several important benefits to this approach of separating code from data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;it is highly scalable.&lt;/li&gt;
&lt;li&gt;it separates data from code, so that potentially someone could replace the R code using the exact same data.&lt;/li&gt;
&lt;li&gt;it drastically reduced the amount of R code needed.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;establish-a-formal-process-for-processing-and-reviewing-each-data-set&#34;&gt;Establish a formal process for processing and reviewing each data set&lt;/h3&gt;

&lt;p&gt;We established a system for tracking the progress of each dataset entering BAAD&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initial screening (basic meta-data extracted from paper).&lt;/li&gt;
&lt;li&gt;Primary authors contacted (asking if they wish to contribute).&lt;/li&gt;
&lt;li&gt;Initial response from authors (indicating interest or not).&lt;/li&gt;
&lt;li&gt;Email sent requesting raw data from authors.&lt;/li&gt;
&lt;li&gt;Raw data received from authors.&lt;/li&gt;
&lt;li&gt;Data processed and entered into BAAD (we filled out as much of information as we could ourselves).&lt;/li&gt;
&lt;li&gt;A review of data, including any queries, sent to authors for error checking.&lt;/li&gt;
&lt;li&gt;Data approved.&lt;/li&gt;
&lt;li&gt;Data excluded because of issues that arose (no response, not interested, could not locate data, data not suitable etc.).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At each stage we automated as much as possible. We used a script to generate emails in R based on information in our database, and made it as easy as possible for the contributors to fulfil their tasks and get back to us.&lt;/p&gt;

&lt;p&gt;Step 7, where we inspected data for errors, was still time consuming. To make this easier for both us and original contributors, we used the package &lt;a href=&#34;http://cran.r-project.org/package=knitr&#34;&gt;knitr&lt;/a&gt; (using &lt;a href=&#34;https://github.com/dfalster/baad/blob/841c346d5c90181b47b0757994901fc520f5e4c6/reports/report.Rmd&#34;&gt;this Rmd template&lt;/a&gt;) to create a standardised report for each study. Each report includes a processed version of the data and metadata, including maps of study site locations and bivariate plots of all variables provided in this study, overlayed against the rest of the data from BAAD. The current set of reports can be viewed &lt;a href=&#34;https://github.com/dfalster/baad/wiki&#34;&gt;on our wiki&lt;/a&gt;, with one page for each study (&lt;a href=&#34;for example&#34;&gt;https://github.com/dfalster/baad/wiki/BondLamberty2002&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;The generated reports are useful in two key ways: i) they provide a nice overview of the data contributed from any single study, and ii) they were invaluable in identifying errors (Figure 2).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Example plot from report on &lt;a href=&#34;https://github.com/dfalster/baad/wiki/Kitazawa1959&#34;&gt;Kitazawa1959 dataset&lt;/a&gt;, showing how data from this study (red) is displaced from the rest of the dataset (grey). The problem was fixed in this &lt;a href=&#34;https://github.com/dfalster/baad/commit/220272b79ceb3aa792523b0c66629be0f23d4468&#34;&gt;commit by changing &lt;code&gt;cm&lt;/code&gt;  to &lt;code&gt;m&lt;/code&gt; as the unit description in the meatadata&lt;/a&gt; (i.e. we did not change the data itself but the transformation used in the processing pipeline).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2015-06-03-baad/plot.png&#34; alt=&#34;Figure: Example figure showing problematic data&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-use-version-control-git-to-track-changes-and-code-sharing-website-github-for-effective-collaboration&#34;&gt;3. Use version control (git) to track changes and code sharing website (github) for effective collaboration&lt;/h2&gt;

&lt;p&gt;The BAAD project began in July 2012, in Feb 2013 Rich FitzJohn got involved and introduced us to version control. You can see the structure of our database at that time &lt;a href=&#34;https://github.com/dfalster/baad/tree/912163bb371e280340dee2bb4cf872a1d7ede81b&#34;&gt;here&lt;/a&gt;. We can&amp;rsquo;t recall that much about what happened prior 13 Feb 2013, but since that day, every single change to the BAAD has been recorded. We know who changed what lines of code or data and when. Many people have been extolling the virtues of git for managing computer code (e.g. &lt;a href=&#34;http://git-scm.com/book&#34;&gt;Chacon 2009&lt;/a&gt;), but others have noted that git is equally good for managing data (&lt;a href=&#34;http://doi.org/10.1186/1751-0473-8-7&#34;&gt;Ram et al 2013&lt;/a&gt;) &lt;sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:git-for-data&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:git-for-data&#34;&gt;0&lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Alongside git, we used the code-sharing website &lt;a href=&#34;www.github.com&#34;&gt;Github&lt;/a&gt; to host our git repository. Github facilitates seamless collaboration by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;syncing changes to scripts and data among collaborators.&lt;/li&gt;
&lt;li&gt;allowing us track the &lt;a href=&#34;https://github.com/dfalster/baad/issues?q=is%3Aissue+is%3Aclosed&#34;&gt;many issues&lt;/a&gt; identified while reviewing data sets.&lt;/li&gt;
&lt;li&gt;providing a nice interface for seeing who changed what and when.&lt;/li&gt;
&lt;li&gt;allowing others to make changes to their data.&lt;/li&gt;
&lt;li&gt;releasing compiled versions of the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;4-embrace-openness&#34;&gt;4. Embrace openness&lt;/h2&gt;

&lt;p&gt;BAAD is far from the first compilation in our field, but as far we know, it possibly the first to be entirely open. By entirely open, we mean&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the entire work flow, including its history, is open and transparent;&lt;/li&gt;
&lt;li&gt;the raw data and meta-data are made available for others to reuse in new and different contexts;&lt;/li&gt;
&lt;li&gt;the data is immediately available on the web, without need to register or login into a site, or submit a project approval.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyone can use the compiled data in whatever way they see fit. Our goal was to create a database that many scientists would immediately want to use, and that would therefore get cited.&lt;/p&gt;

&lt;p&gt;Another concern was that the database would be sustainable. By making the entire process open and scripted, we are effectively allowing ourselves to step away from the project at some point in the future, if that&amp;rsquo;s what we want to do. Moreover, it allows future researchers who are out in the field collecting more raw data to contribute to the this existing unified database.&lt;/p&gt;

&lt;h2 id=&#34;5-a-living-database&#34;&gt;5. A living database&lt;/h2&gt;

&lt;p&gt;We hope that BAAD will continue to grow.  To that end, we have written a very small package &lt;a href=&#34;https://github.com/traitecoevo/baad.data&#34;&gt;baad.data&lt;/a&gt; for accessing data by version in &lt;code&gt;R&lt;/code&gt;.  After installing the package (instructions &lt;a href=&#34;https://github.com/traitecoevo/baad.data&#34;&gt;here&lt;/a&gt;), users can run&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(baad.data)
x &amp;lt;- baad_data(&amp;quot;1.0.0&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to download the version stored in Ecological Archives, or&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- baad_data(&amp;quot;x.y.z&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to download an earlier or more recent version (where version numbers will follow the &lt;a href=&#34;http://semver.org&#34;&gt;semantic versioning&lt;/a&gt; guidelines, eg. &amp;ldquo;0.9.0&amp;rdquo;). The &lt;code&gt;baad.data&lt;/code&gt; package caches everything so subsequent calls, even across sessions, are very fast.  This should facilitate greater reproducibility by making it easy to depend on the version used for a particular analysis, and allowing different analyses to use different versions of the database.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We really hope that the techniques used in building BAAD will help others develop open and transparent compilations of #otherpeoplesdata. On that point, we conclude by thanking all our wonderful &lt;a href=&#34;http://www.esajournals.org/doi/abs/10.1890/14-1889.1&#34;&gt;co-authors&lt;/a&gt; who were willing to put their data out there for others to use.&lt;/p&gt;

&lt;h3 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h3&gt;

&lt;p&gt;&lt;a id=&#34;database&#34;&gt;&lt;sup&gt;^1&lt;/sup&gt;&lt;/a&gt; BAAD is a database in the sense that it is an &lt;a href=&#34;http://en.wikipedia.org/wiki/Database&#34;&gt;organized collection of data&lt;/a&gt;, but we do not use common database tools like SQL or Microsoft Access etc. These are simply not needed and prevent other features like version control.&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;remake&#34;&gt;&lt;sup&gt;^2&lt;/sup&gt;&lt;/a&gt; The package &lt;code&gt;remake&lt;/code&gt; was originally called maker and was introduced on &lt;a href=&#34;https://github.com/dfalster/baad/tree/82b0b1c832e9fcfd7c1d1e6cf42f7c8b97e5d323&#34;&gt;Nov 19 2014&lt;/a&gt;, relatively late in development of BAAD. Earlier we experimented we building a package &lt;a href=&#34;https://github.com/dfalster/dataMashR&#34;&gt;dataMashR&lt;/a&gt; to implement the conversions, but eventually settled on the remake work-flow. DataMashR lives on as a working prototype.&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;TravisCI&#34;&gt;&lt;sup&gt;^3&lt;/sup&gt;&lt;/a&gt;  You can see the record of the automated &lt;a href=&#34;https://travis-ci.org/dfalster/baad/builds/&#34;&gt;builds here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;line_endings&#34;&gt;&lt;sup&gt;^4&lt;/sup&gt;&lt;/a&gt; Excel makes a mess of line endings on Mac and has done for a long time - see &lt;a href=&#34;http://nicercode.github.io/blog/2013-04-30-excel-and-line-endings/&#34;&gt;here&lt;/a&gt; for our thoughts and an early solution.&lt;/p&gt;

&lt;p&gt;&lt;a id=&#34;git_for_data&#34;&gt;&lt;sup&gt;^5&lt;/sup&gt;&lt;/a&gt; Provided the data is not too large. &lt;a href=&#34;https://help.github.com/articles/what-is-my-disk-quota/&#34;&gt;Github works&lt;/a&gt; with files &amp;lt; 100MB and for git repositories &amp;lt; 1GB. Although there are &lt;a href=&#34;https://help.github.com/articles/what-is-my-disk-quota/&#34;&gt;strategies for larger sizes&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Reproducible research is still a challenge</title>
      <link>https://ropensci.org/blog/2014/06/09/reproducibility/</link>
      <pubDate>Mon, 09 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2014/06/09/reproducibility/</guid>
      <description>
        
        

&lt;p&gt;Science is reportedly in the middle of a &lt;a href=&#34;http://theconversation.com/science-is-in-a-reproducibility-crisis-how-do-we-resolve-it-16998&#34;&gt;reproducibility crisis&lt;/a&gt;.  Reproducibility seems laudable and is frequently called for (e.g., &lt;a href=&#34;http://www.nature.com/nature/focus/reproducibility/&#34;&gt;nature&lt;/a&gt; and &lt;a href=&#34;http://www.sciencemag.org/content/334/6060/1226&#34;&gt;science&lt;/a&gt;).  In general the argument is that research that can be independently reproduced is more reliable than research that cannot be independently reproduced.  It is also worth noting that reproducing research is not solely a checking process, and it can provide useful jumping-off points for future research questions.  It is difficult to find a counter-argument to these claims, but arguing that reproducibility is laudable in general glosses over the fact that for each research group it is a significant amount of work to make their research (easily) reproducible for independent scientists.  While much of the attention has focused on &lt;a href=&#34;http://www.nature.com/nature/journal/v483/n7391/full/483531a.html&#34;&gt;entirely repeating laboratory experiments&lt;/a&gt;, there are many simpler forms of reproducibility including, for example, the ability to recompute analyses on known sets of data.&lt;/p&gt;

&lt;p&gt;Different types of scientific research are inherently easier or harder to reproduce.  At one extreme is analytic mathematical research, which should in many cases allow for straightforward reproduction based on the equations in the manuscript.  At the other extreme are field-based studies, which may depend upon factors that are not under the control of the scientist. To use an extreme example, it will always be effectively impossible to entirely reproduce a before and after study of the effects of a hurricane.&lt;/p&gt;

&lt;p&gt;The current frontier of reproducibility is somewhere between these two extreme examples, and the location of this frontier at any given time depends upon the set of tools available to researchers.  Open source software, cloud computing, data archiving, standardised biological materials, and widely available computing resources have all pushed this frontier to allow for the reproduction of &lt;em&gt;more types&lt;/em&gt; of research than was previously the case.   However, the  &lt;a href=&#34;http://theconversation.com/science-is-in-a-reproducibility-crisis-how-do-we-resolve-it-16998&#34;&gt;&amp;ldquo;reproducibility crisis&amp;rdquo;&lt;/a&gt; rhetoric suggests that the current set of tools, while substantial, has not completely solved the problem.&lt;/p&gt;

&lt;p&gt;We recently worked on a project &amp;ndash; a moderately complex analysis of a moderately sized database (&lt;a href=&#34;https://datadryad.org/resource/doi:10.5061/dryad.63q27&#34;&gt;49061 rows&lt;/a&gt;) &amp;ndash; that we treated as an experiment to determine what it would take to make it fully reproducible. (This was to answer a very simple research question: &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/1365-2745.12260/abstract&#34;&gt;what proportion of the world&amp;rsquo;s plant species are woody?&lt;/a&gt;.) Our specific experiences in trying to make this research reproducible may be useful for the on-going discussion of how to allow scientists with less time and fewer technical skills than we had available to make their research reproducible.  In other words, how do we usefully move the &amp;ldquo;frontier of reproducibility&amp;rdquo; to include more types of studies and in doing so make more science more reliable.&lt;/p&gt;

&lt;p&gt;In the end, our analysis and paper have been reproduced independently and it is relatively easy for anyone who wants to do so, but &lt;em&gt;implementing&lt;/em&gt; this level of reproducibility was not without considerable effort.  For those interested, the entirety of our the code and documentation is available &lt;a href=&#34;https://github.com/richfitz/wood&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are two parts to the reproducibility of a project such as this: the data and the analysis. We should note that the fact that this project was even possible is due to the &lt;a href=&#34;http://en.wikipedia.org/wiki/Scientific_data_archiving&#34;&gt;recent developments in data archiving&lt;/a&gt;.  It was relatively straightforward to write a script that downloads the main data from &lt;a href=&#34;http://datadryad.org/&#34;&gt;Dryad&lt;/a&gt; and prepare it for analysis.  However, this proved to be only the beginning of the challenge: the analysis portion turned out to be much more challenging.  The following is essentially a list of lessons learned from that experience.  Each point below details one of the challenges we faced in making our research reproducible and the tool we chose to address that challenge.&lt;/p&gt;

&lt;h2 id=&#34;challenges-and-tools-for-reproducibility&#34;&gt;Challenges and tools for reproducibility&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Using canonical data sources&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We downloaded data from canonical sources (&lt;a href=&#34;http://datadryad.org&#34;&gt;Dryad&lt;/a&gt; and &lt;a href=&#34;http://theplantlist.org&#34;&gt;The Plant List&lt;/a&gt;) and only modified them programmatically, so that the chain of modification was preserved.  The benefits of open data will only be realised if we preserve identity of data sets and do not end up re-archiving hundreds of slightly modified versions.  This also helps ensure credit for data contributors.  However, issues such as taxonomic standardisation remain a real stumbling block for ecological data reuse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Combining thoughts and code&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We used &lt;a href=&#34;http://yihui.name/knitr/&#34;&gt;knitr&lt;/a&gt; to implement the analysis in a &lt;a href=&#34;http://en.wikipedia.org/wiki/Literate_programming&#34;&gt;literate programming&lt;/a&gt; style.  The entire analysis, including justification of the core functions, is &lt;a href=&#34;http://richfitz.github.io/wood/wood.html&#34;&gt;available to interested people&lt;/a&gt;.  However, working with blocks of ugly data-wrangling code, or with long-running calculations, remains a challenge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dynamic generation of figures&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All of our data manipulation was handled with scripts, and we could delete all figures/outputs and recreate them at will.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automated caching of dependencies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We used &lt;a href=&#34;http://en.wikipedia.org/wiki/Make_%28software%29&#34;&gt;make&lt;/a&gt; to document dependencies within components of the projects, rebuilding only the sections that required changing.  This also makes the build process somewhat self documenting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Version control&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All of our scripts were under version control using &lt;a href=&#34;http://git-scm.com&#34;&gt;git&lt;/a&gt; from &lt;a href=&#34;https://github.com/richfitz/wood/commit/8ed0c8c10dfda2a8f11f169ec528b7e161832eeb&#34;&gt;the beginning&lt;/a&gt;, enabling us to dig back through old versions.  This was central to everything we did!  See &lt;a href=&#34;http://www.scfbm.org/content/8/1/7&#34;&gt;this article&lt;/a&gt; for much more on how version control facilitates research.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automated checking that modifications don&amp;rsquo;t break things&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We used the &amp;ldquo;&lt;a href=&#34;http://en.wikipedia.org/wiki/Continuous_integration&#34;&gt;continuous integration&lt;/a&gt;&amp;rdquo; environment &lt;a href=&#34;http://travis-ci.org&#34;&gt;Travis CI&lt;/a&gt; to guard against changes in the analysis causing it to fail.  Every time we made a change, this system downloads the source code, all relevant data and runs the analysis, sending us an email if anything failed.  It even &lt;a href=&#34;http://richfitz.github.io/wood&#34;&gt;uploads the compiled versions of the analysis and manuscript&lt;/a&gt; each time it runs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Documenting dependencies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We used &lt;a href=&#34;https://github.com/rstudio/packrat&#34;&gt;packrat&lt;/a&gt; for managing and archiving R package dependencies to ensure future repeatability.  In theory, this means that if software versions change enough to break our scripts, we have an archived set of packages that can be used.  This is a very new tool; only time will tell if this will work.&lt;/p&gt;

&lt;h2 id=&#34;remaining-challenges&#34;&gt;Remaining challenges&lt;/h2&gt;

&lt;p&gt;We found that moving from running analyses on one person&amp;rsquo;s computer (with their particular constellation of software locations) to another was difficult. For example, see &lt;a href=&#34;https://github.com/richfitz/wood/issues/1&#34;&gt;this issue&lt;/a&gt; for the trouble that we had running the analyses on our own computers, knowing the scope of the project.  It&amp;rsquo;s hard to anticipate all possible causes for confusion: one initial try at replication by &lt;a href=&#34;http://carlboettiger.info&#34;&gt;Carl Boettiger&lt;/a&gt; had &lt;a href=&#34;https://github.com/richfitz/wood/issues/12&#34;&gt;trouble&lt;/a&gt; due to incomplete documentation of required package versions.&lt;/p&gt;

&lt;p&gt;The set of scripts that manages the above jobs is comparable in size to the actual analysis; this is a large overhead to place on researchers.  There are also many different languages and frameworks involved, increasing both the technical knowledge required and the chance that something will break.  Automating as much of this process as possible is essential for reproducibility to become standard practice.&lt;/p&gt;

&lt;p&gt;The continuous integration approach has a huge potential to save headaches in managing computational research projects.  However, while our analysis acts as a proof-of-concept, it will be of limited general use: it requires that the project is open source (in a &lt;em&gt;public&lt;/em&gt; &lt;a href=&#34;https://github.com&#34;&gt;github&lt;/a&gt; repository), and that the analysis is relatively quick to run (under an hour).  These limitations are reasonable given that it is a free service, but they don&amp;rsquo;t match well with many research projects where development does not occur &amp;ldquo;in the open&amp;rdquo;, and where computation can take many hours or days.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We found our reproducibility goals for this paper to be a useful exercise, and it forms the basis of &lt;a href=&#34;https://github.com/richfitz/modeladequacy&#34;&gt;ongoing research projects&lt;/a&gt;.  However, the process is far too complicated at the moment. It is not going to be enough to simply tell people to make their projects reproducible. We need to develop tools that are &lt;em&gt;at least&lt;/em&gt; as easy to use as version control before we can expect project reproducibility to become mainstream.&lt;/p&gt;

&lt;p&gt;We don&amp;rsquo;t disagree with Titus Brown that &lt;a href=&#34;http://ivory.idyll.org/blog/2014-myths-of-computational-reproducibility.html&#34;&gt;partial reproducibility is better than nothing&lt;/a&gt; (50% of people making their work 50% reproducible would be better than 5% of people making their work 100% reproducible!).  However, we disagree with Titus in his contention that new tools are not needed. The current tools are very raw and too numerous to expect widespread adoption from scientists whose main aim is not reproducibility.  Given that reproducibility &lt;a href=&#34;http://software-carpentry.org/blog/2013/02/correctness-isnt-compelling.html&#34;&gt;isn&amp;rsquo;t compelling&lt;/a&gt;, we can&amp;rsquo;t expect people to pour their time into it just for some public greater good, especially if it comes with a large time cost.&lt;/p&gt;

&lt;p&gt;Other efforts for this simple goal of recomputibility are not much more encouraging than ours.  A study in the &lt;a href=&#34;http://www.zoology.ubc.ca/~repro&#34;&gt;UBC Reproducibility Group&lt;/a&gt; found that they &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1365-294X.2012.05754.x/abstract&#34;&gt;could not reproduce the results in 30%&lt;/a&gt; of published analyses using the population genetic package STRUCTURE, using the same data as provided by the authors.  In an even more trivial case, a &lt;a href=&#34;http://reproducibility.cs.arizona.edu/&#34;&gt;research group at Arizona University&lt;/a&gt; found that they could only &lt;em&gt;build&lt;/em&gt; about half of the published software that they could download, without even testing that the software did what it was intended to do (note that this study is currently &lt;a href=&#34;http://cs.brown.edu/~sk/Memos/Examining-Reproducibility/&#34;&gt;being reproduced!&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The process of making our study reproducible reveals that we are only part of the way to making reproducible research broadly accessible to practicing scientists.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Open Science with R</title>
      <link>https://ropensci.org/blog/2013/12/02/open-science-with-r/</link>
      <pubDate>Mon, 02 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2013/12/02/open-science-with-r/</guid>
      <description>
        
        &lt;p&gt;&lt;strong&gt;Upcoming Book on Open Science with R&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re pleased to announce that the rOpenSci core team has just signed a contract with &lt;a href=&#34;http://www.taylorandfrancis.com/books/series/CRCTHERSER/&#34;&gt;CRC Press/Taylor and Francis R series&lt;/a&gt; to publish a new book on practical ways to implement open science into your own research using R. Given all the talk about the importance of open science, the discussion often lacks practical suggestions on how one might actually incorporate these practices into their day to day research workflow.&lt;/p&gt;

&lt;p&gt;In many ways writing this book will be an exercise for us to share our research workflow with the rest of the community. If R plays an important role in your research this book will help you learn how to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Share your computational methods using sound scientific software guidelines.&lt;/li&gt;
&lt;li&gt;Document your datasets with valid metadata and deposit them into persistent repositories.&lt;/li&gt;
&lt;li&gt;Write reports, manuscripts, and presentations.&lt;/li&gt;
&lt;li&gt;Maintain an open lab notebook.&lt;/li&gt;
&lt;li&gt;Build packages that interface with data sources on the web.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The publisher has been kind enough to allow us to maintain a public version the book as a GitHub repository. Anyone will be able to read our chapters, review, comment, and send pull requests. The final edited and nicely formatted version with a complete index will of course only be available via the publisher copy. But that is a small trade off in this case. As an added advantage, we will be able to keep the material current and up-to-date long after publication.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ropensci/open-science-with-R&#34;&gt;GitHub repository for the book&lt;/a&gt;
&lt;a href=&#34;https://github.com/ropensci/open-science-with-R/issues?labels=chapter-01&amp;amp;state=open&#34;&gt;Issues, comments and suggestions&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We expect the book to be out shortly after July 2014.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>rOpenSci awarded 180K from The Sloan Foundation</title>
      <link>https://ropensci.org/blog/2013/06/12/sloan/</link>
      <pubDate>Wed, 12 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2013/06/12/sloan/</guid>
      <description>
        
        &lt;p&gt;Today we are pleased to announce that rOpenSci has been awarded a generous 180K grant from the &lt;a href=&#34;http://www.sloan.org/&#34;&gt;Alfred P. Sloan foundation&lt;/a&gt;. This funding will allow us to develop a whole new suite of tools and provide scientists with general purpose toolkits to access various kinds of scientific data. We will also be traveling a whole bunch this year and running workshops at several conferences and universities. If you&amp;rsquo;d like us to speak to your research group, please &lt;a href=&#34;http://ropensci.org/contact.html&#34;&gt;get in touch&lt;/a&gt;. We&amp;rsquo;ll be at several events over the coming months including &lt;a href=&#34;http://www.nceas.ucsb.edu/news/nceas-leads-hands-primer-ecoinformatics-ecological-society-americas-2013-conference&#34;&gt;The Ecological Society of America annual meeting&lt;/a&gt;, &lt;a href=&#34;http://okcon.org/&#34;&gt;The Open knowledge conference&lt;/a&gt;, Science Online Climate, &lt;a href=&#34;http://sites.agu.org/meetings/&#34;&gt;American Geophysical Union&lt;/a&gt; and several others. Stay tuned for announcements on &lt;a href=&#34;http://twitter.com/ropensci&#34;&gt;Twitter&lt;/a&gt;, our &lt;a href=&#34;http://ropensci.org/blog/&#34;&gt;blog&lt;/a&gt; (&lt;a href=&#34;http://ropensci.org/feed.xml&#34;&gt;rss&lt;/a&gt;) and new mailing list.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
