<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Opendata on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/opendata/</link>
    <description>Recent content in Opendata on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 13 Jun 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/opendata/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>New rOpenSci Packages for Text Processing in R</title>
      <link>https://ropensci.org/blog/2017/06/13/ropensci_text_tools/</link>
      <pubDate>Tue, 13 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/06/13/ropensci_text_tools/</guid>
      <description>
        
        

&lt;p&gt;Textual data and natural language processing are still a niche domain within the R ecosytstem. The &lt;a href=&#34;https://cran.r-project.org/view=NaturalLanguageProcessing&#34;&gt;NLP task view&lt;/a&gt; gives an overview of existing work however a lot of basic infrastructure is still missing.
At the rOpenSci &lt;a href=&#34;https://ropensci.org/blog/blog/2017/05/03/textworkshop17&#34;&gt;text workshop&lt;/a&gt; in April we discussed many ideas for improving text processing in R which revealed several core areas that need improvement:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reading: better tools for extracing text and metadata from documents in various formats (doc, rtf, pdf, etc).&lt;/li&gt;
&lt;li&gt;Encoding: many text packages work well for ascii text but rapidly break down when text contains Hungarian, Korean or emojis.&lt;/li&gt;
&lt;li&gt;Interchange: packages don&amp;rsquo;t work well together due to lack of data classes or conventions for textual data (see also &lt;a href=&#34;https://github.com/ropensci/tif&#34;&gt;ropensci/tif&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Participants also had many good suggestions for C/C++ libraries that text researchers in R might benefit from. Over the past weeks I was able to look into these suggestions and work on a few packages for reading and analyzing text. Below is an update on new and improved rOpenSci tools for text processsing in R!&lt;/p&gt;

&lt;h2 id=&#34;google-language-detector-2-and-3&#34;&gt;Google language detector 2 and 3&lt;/h2&gt;

&lt;p&gt;New packages &lt;code&gt;cld2&lt;/code&gt; and &lt;code&gt;cld3&lt;/code&gt; are wrappers C++ libraries by Google for language identification. &lt;a href=&#34;https://github.com/cld2owners/cld2#internals&#34;&gt;CLD2&lt;/a&gt; is a Naïve Bayesian classifier, whereas &lt;a href=&#34;https://github.com/google/cld3#model&#34;&gt;CLD3&lt;/a&gt; uses a neural network model. I found &lt;code&gt;cld2&lt;/code&gt; to give better results for short text.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the latest versions
install.packages(c(&amp;quot;cld2&amp;quot;, &amp;quot;cld3&amp;quot;))

# Vectorized function
text &amp;lt;- c(&amp;quot;À chaque fou plaît sa marotte.&amp;quot;, &amp;quot;猿も木から落ちる&amp;quot;,
	&amp;quot;Алты́нного во́ра ве́шают, а полти́нного че́ствуют.&amp;quot;, &amp;quot;Nou breekt mijn klomp!&amp;quot;)

cld2::detect_language(text)
# [1] &amp;quot;fr&amp;quot; &amp;quot;ja&amp;quot; &amp;quot;ru&amp;quot; &amp;quot;nl&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://maelle.github.io&#34;&gt;Maëlle&lt;/a&gt; has written a &lt;a href=&#34;http://www.masalmon.eu/2017/06/10/rolandgarros/&#34;&gt;cool post&lt;/a&gt; comparing language classification methods using 18000 &lt;code&gt;&amp;quot;#RolandGarros2017&amp;quot;&lt;/code&gt; tweets and &lt;a href=&#34;https://www.stat.auckland.ac.nz/people/tlum005&#34;&gt;Thomas&lt;/a&gt; &lt;a href=&#34;http://notstatschat.tumblr.com/post/161449071226/stupid-word-games&#34;&gt;reminds us&lt;/a&gt; that algorithms can easily be fooled. Still I found the accuracy on real text quite astonishing given the relatively small size of these libraries.&lt;/p&gt;

&lt;p&gt;Note that the algorithm for CLD3 is still under development and the engineers at Google have recently &lt;a href=&#34;https://github.com/google/cld3/issues&#34;&gt;opened&lt;/a&gt; their Github issues page for feedback.&lt;/p&gt;

&lt;h2 id=&#34;anti-word-and-un-rtf&#34;&gt;(anti) word and (un)rtf&lt;/h2&gt;

&lt;p&gt;Many archived documents are only available in legacy formats such as &lt;code&gt;.doc&lt;/code&gt; and &lt;code&gt;.rtf&lt;/code&gt;. The only tools available for extracting text from these documents were difficult to install and could not be imported from packages and scripts.&lt;/p&gt;

&lt;p&gt;To make this a little easier we have packaged up utilities &lt;a href=&#34;http://www.winfield.demon.nl/&#34;&gt;antiword&lt;/a&gt; and &lt;a href=&#34;https://www.gnu.org/software/unrtf/&#34;&gt;UnRTF&lt;/a&gt; to read MS &lt;code&gt;doc&lt;/code&gt; and &lt;code&gt;rtf&lt;/code&gt; files respectively.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the latest versions
install.packages(c(&amp;quot;antiword&amp;quot;, &amp;quot;unrtf&amp;quot;))

# Extract text from &#39;rtf&#39; file
text &amp;lt;- unrtf::unrtf(&amp;quot;https://jeroen.github.io/files/sample.rtf&amp;quot;, format = &amp;quot;text&amp;quot;)
cat(text)
### Lots of text...

# Extract text from &#39;doc&#39; file
text &amp;lt;- antiword::antiword(&amp;quot;https://jeroen.github.io/files/UDHR-english.doc&amp;quot;)
cat(text)
### Lots of text...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also have a look at meta packages &lt;code&gt;readtext&lt;/code&gt; or &lt;code&gt;textreadr&lt;/code&gt; which wrap these and other packages for automatically reading text in many different formats.&lt;/p&gt;

&lt;h2 id=&#34;pdf-utilities&#34;&gt;pdf utilities&lt;/h2&gt;

&lt;p&gt;Our &lt;a href=&#34;https://cran.r-project.org/web/packages/pdftools/index.html&#34;&gt;pdftools&lt;/a&gt; package now supports reading pdf (extracting text or metadata) and rendering pdf to png, jpeg, tiff, or raw vectors on all platforms (incl. Windows).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Read some text
text &amp;lt;- pdftools::pdf_text(&#39;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#39;)
cat(text[1])
# An Introduction to R
#             Notes on R: A Programming Environment for Data Analysis and Graphics
#                                                        Version 3.4.0 (2017-04-21)
# W. N. Venables, D. M. Smith
# and the R Core Team

# Read meta data
pdftools::pdf_info(&#39;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#39;)
# $version
# [1] &amp;quot;1.5&amp;quot;
#
# $pages
# [1] 105
#
# .... much more :)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use either render a page into a raw bitmap array, or directly to an image format such as png, jpeg or tiff.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;files &amp;lt;- pdftools::pdf_convert(&#39;https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf&#39;, format = &amp;quot;png&amp;quot;, pages = 1:5)
# Converting page 1 to R-intro_1.png... done!
# Converting page 2 to R-intro_2.png... done!
# Converting page 3 to R-intro_3.png... done!
# Converting page 4 to R-intro_4.png... done!
# Converting page 5 to R-intro_5.png... done!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To extract text from scanned images, also check out our &lt;a href=&#34;https://cran.r-project.org/web/packages/tesseract/index.html&#34;&gt;tesseract&lt;/a&gt; package which wraps Google&amp;rsquo;s powerful OCR engine.&lt;/p&gt;

&lt;h2 id=&#34;stemming-tokenizing-and-spell-checking&#34;&gt;Stemming, tokenizing and spell checking&lt;/h2&gt;

&lt;p&gt;Our &lt;a href=&#34;https://cran.r-project.org/web/packages/hunspell/index.html&#34;&gt;hunspell&lt;/a&gt; package has had a few updates recently as well. The package is a wrapper for &lt;a href=&#34;http://hunspell.github.io/&#34;&gt;libhunspell&lt;/a&gt; which is a popular library for spell checking:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Extract incorrect from a piece of text
bad &amp;lt;- hunspell(&amp;quot;spell checkers are not neccessairy for langauge ninja&#39;s&amp;quot;)
print(bad[[1]])
# [1] &amp;quot;neccessairy&amp;quot; &amp;quot;langauge&amp;quot;
hunspell_suggest(bad[[1]])
# [[1]]
# [1] &amp;quot;necessary&amp;quot;    &amp;quot;necessarily&amp;quot;  &amp;quot;necessaries&amp;quot;  &amp;quot;recessionary&amp;quot; &amp;quot;accessory&amp;quot;    &amp;quot;incarcerate&amp;quot;
#
# [[2]]
# [1] &amp;quot;language&amp;quot;  &amp;quot;Langeland&amp;quot; &amp;quot;Lagrange&amp;quot;  &amp;quot;Lange&amp;quot;     &amp;quot;gaugeable&amp;quot; &amp;quot;linkage&amp;quot;   &amp;quot;Langland&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The package is also used by &lt;code&gt;devtools&lt;/code&gt; to spell-check manual pages in R packages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::spell_check()
#   WORD          FOUND IN
# alltogether   pdftools.Rd:36
# cairo         pdf_render_page.Rd:42
# jpeg          pdf_render_page.Rd:40
# libpoppler    pdf_render_page.Rd:42, pdftools.Rd:30, description:1
# png           pdf_render_page.Rd:40
# Poppler       pdftools.Rd:34
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally hunspell also exposes the underlying methods needed for spell checking such as stemming words:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Find possible stems for each word
words &amp;lt;- c(&amp;quot;loving&amp;quot;, &amp;quot;loved&amp;quot;, &amp;quot;lover&amp;quot;, &amp;quot;lovely&amp;quot;, &amp;quot;love&amp;quot;)
hunspell_analyze(words)
# [[1]]
# [1] &amp;quot; st:loving&amp;quot;    &amp;quot; st:love fl:G&amp;quot;
#
# [[2]]
# [1] &amp;quot; st:loved&amp;quot;     &amp;quot; st:love fl:D&amp;quot;
#
# [[3]]
# [1] &amp;quot; st:lover&amp;quot;     &amp;quot; st:love fl:R&amp;quot;
#
# [[4]]
# [1] &amp;quot; st:lovely&amp;quot;    &amp;quot; st:love fl:Y&amp;quot;
#
# [[5]]
# [1] &amp;quot; st:love&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hunspell also suppors tokenizing words from html, latex, man, or plain text. For more advanced word extraction, check out the rOpenSci &lt;a href=&#34;https://github.com/ropensci/tokenizers#readme&#34;&gt;tokenizers&lt;/a&gt; package.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Release &#39;open&#39; data from their PDF prisons using tabulizer</title>
      <link>https://ropensci.org/blog/2017/04/18/tabulizer/</link>
      <pubDate>Tue, 18 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/04/18/tabulizer/</guid>
      <description>
        
        

&lt;p&gt;There is no problem in science quite as frustrating as &lt;em&gt;other peoples&amp;rsquo; data&lt;/em&gt;. Whether it&amp;rsquo;s malformed spreadsheets, disorganized documents, proprietary file formats, data without metadata, or any other data scenario created by someone else, &lt;a href=&#34;https://twitter.com/hashtag/otherpeoplesdata?src=hash&#34;&gt;scientists have taken to Twitter to complain about it&lt;/a&gt;. As a political scientist who regularly encounters so-called &amp;ldquo;open data&amp;rdquo; in PDFs, this problem is particularly irritating. PDFs may have &amp;ldquo;portable&amp;rdquo; in their name, making them display consistently on various platforms, but that portability means any information contained in a PDF is irritatingly difficult to extract computationally. Encountering &amp;ldquo;open data&amp;rdquo; PDFs therefore makes me shout things like this repeatedly:&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;HEY US GOVERNMENT! Tables in PDF documents aren&amp;#39;t &amp;quot;Open Data.&amp;quot; Please provide machine-readable formats or it doesn&amp;#39;t count.&lt;/p&gt;&amp;mdash; Anthony A. Boyles (@AABoyles) &lt;a href=&#34;https://twitter.com/AABoyles/status/776428077123506176&#34;&gt;September 15, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;What can we do about such data other than extract it by hand? One answer is rely on &lt;a href=&#34;https://github.com/ropensci/tabulizer&#34;&gt;&lt;code&gt;tabulizer&lt;/code&gt;&lt;/a&gt; a package I submitted to rOpenSci that reduces some and often all of the hassle of extracting tabular data locked inside PDFs.&lt;/p&gt;

&lt;h2 id=&#34;what-is-tabulizer&#34;&gt;What is &lt;code&gt;tabulizer&lt;/code&gt;?&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;tabulizer&lt;/code&gt; provides R bindings to &lt;a href=&#34;https://github.com/tabulapdf/tabula-java&#34;&gt;the tabula-java library&lt;/a&gt;, the open-source java library that powers &lt;a href=&#34;http://tabula.technology/&#34;&gt;Tabula&lt;/a&gt; (source code on &lt;a href=&#34;https://github.com/tabulapdf/tabula&#34;&gt;GitHub&lt;/a&gt;). What this means is that &lt;code&gt;tabulizer&lt;/code&gt; relies directly on the underlying java classes that power Tabula without the need for system calls or the need to explicitly install Tabula on your system. (A potential downside is the need to handle intricacies of rJava - R&amp;rsquo;s interface to Java, which I discuss in depth below.)&lt;/p&gt;

&lt;p&gt;Tabula is an extremely powerful tool for extracting tabular data locked in PDFs. It&amp;rsquo;s an incredibly valuable tool because the PDF file specification does not have a &amp;ldquo;table&amp;rdquo; representation. Instead, PDFs simply represent tables through the fixed positioning of text into rows and columns. Thus, unlike HTML, Word (.docx), or Open Document (.odt) file formats, there is no easy programmatic way to identify a table in a PDF. Tabula thus implements novel algorithms for identifying rows and columns of data and extracting them. &lt;code&gt;tabulizer&lt;/code&gt; just provides a thin R layer on top of this power Java code.&lt;/p&gt;

&lt;p&gt;Unfortunately, this means that &lt;code&gt;tabulizer&lt;/code&gt; is not a universal solution to data trapped in PDFs. In particular, it can only identify and extract tables that are represented as &lt;em&gt;text&lt;/em&gt; in a PDF:&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Oh no no no no no! Just received &lt;a href=&#34;https://twitter.com/hashtag/otherpeoplesdata?src=hash&#34;&gt;#otherpeoplesdata&lt;/a&gt; as a 276 page set of printed tables scanned in to a PDF&lt;/p&gt;&amp;mdash; Dr Elizabeth Sargent (@esargent184) &lt;a href=&#34;https://twitter.com/esargent184/status/510056437033091074&#34;&gt;September 11, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If a PDF is a scan of a document or the table is actually an image embedded in the PDF, tabula - and thus &lt;code&gt;tabulizer&lt;/code&gt; - are useless. In those cases, users might want to check out the OCR functionality of &lt;a href=&#34;https://github.com/ropensci/tesseract&#34;&gt;tesseract&lt;/a&gt;, which Jeroen Ooms developed for rOpenSci and &lt;a href=&#34;https://ropensci.org/blog/blog/2016/11/16/tesseract&#34;&gt;discussed previously on this blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But it does mean that a substantial amount of difficult-to-parse tabular information in PDFs is now readily and quickly accessible via just one &lt;code&gt;tabulizer&lt;/code&gt; function: &lt;code&gt;extract_tables()&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;tabulizer&lt;/code&gt; is not yet on CRAN. (It&amp;rsquo;s CRAN-ready but due to some underlying developments that are ongoing in the tabula-java library, I&amp;rsquo;m waiting for a formal release.) In the meantime, it&amp;rsquo;s possible to install &lt;code&gt;tabulizer&lt;/code&gt; directly from GitHub.&lt;/p&gt;

&lt;p&gt;Before doing that, I would encourage users to make sure they have rJava installed (from CRAN) and that it works correctly on their platform. A lot of users report difficulties installing &lt;code&gt;tabulizer&lt;/code&gt; that ultimately boil down to being Java and rJava issues that need to be resolved first. The &lt;a href=&#34;https://github.com/ropensci/tabulizer#installation&#34;&gt;package README&lt;/a&gt; provides a number of details on installation, which requires a strictly ordered set of steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Install the Java Development Kit, if you don&amp;rsquo;t already have it on your system. (Note that the JDK is different from the Java Runtime Environment (JRE) that you almost certainly already have.) Details of how to do this vary a lot between platforms, so see the &lt;a href=&#34;https://github.com/ropensci/tabulizer#installation&#34;&gt;README&lt;/a&gt; for details.&lt;/li&gt;
&lt;li&gt;Install rJava using &lt;code&gt;install.packages(&amp;quot;rJava&amp;quot;)&lt;/code&gt; and resolve any issues surrounding the &lt;code&gt;JAVA_HOME&lt;/code&gt; environment variable that may need to be set before and/or after installing rJava. Again, see the &lt;a href=&#34;https://github.com/ropensci/tabulizer#installation&#34;&gt;README&lt;/a&gt; or &lt;a href=&#34;http://stackoverflow.com/search?q=%5Br%5D+rjava+install&#34;&gt;various question/answer pairs on StackOverflow&lt;/a&gt; for platform-specific instructions.&lt;/li&gt;
&lt;li&gt;Install &lt;code&gt;tabulizer&lt;/code&gt; and &lt;code&gt;tabulizerjars&lt;/code&gt; (the package containing the tabula java library) using your favorite GitHub package installer:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(&amp;quot;ghit&amp;quot;)
ghit::install_github(c(&amp;quot;ropensci/tabulizerjars&amp;quot;,&amp;quot;ropensci/tabulizer&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should work. If not, set &lt;code&gt;verbose = TRUE&lt;/code&gt; in &lt;code&gt;ghit::install_github()&lt;/code&gt; to identify the source of any issues. Some common problems are the dependency on the &lt;strong&gt;png&lt;/strong&gt; package, which might need to be installed first. On Windows (depending on your version of R and how it was installed) may require setting &lt;code&gt;INSTALL_opts = &amp;quot;--no-multiarch&amp;quot;&lt;/code&gt; in &lt;code&gt;ghit::install_github()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If none of these steps work, scroll through &lt;a href=&#34;https://github.com/ropensci/tabulizer/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aissue&#34;&gt;the GitHub issues page&lt;/a&gt; for anyone experiencing a similar problem and, if not resolved in any of those discussions, feel free to open an issue on GitHub describing your problem including the fully verbose output of &lt;code&gt;install_github()&lt;/code&gt; and your &lt;code&gt;sessionInfo()&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;unlocking-elections-data-with-tabulizer&#34;&gt;Unlocking elections data with &lt;code&gt;tabulizer&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Elections data are the bread and butter of a lot of quantitative political science research. Many researchers in my field need to know how many citizens voted and for whom in order to make sense of campaigns, election integrity, partisanship, and so forth. Yet a substantial amount of election-related data is locked in government-produced PDFs. Worse, national, state, and local governments have little to no standardization in the formatting of elections data, meaning even if one could figure out a computational strategy for extracting one kind of data about elections in one year from one state, that computational strategy would likely be useless in the same state in another year or in any other state. Elections provide a fantastic and highly visible example of &amp;ldquo;open&amp;rdquo; government data that&amp;rsquo;s not really open or usable at all.&lt;/p&gt;

&lt;p&gt;As a simple example, &lt;a href=&#34;http://elections.cdn.sos.ca.gov/sov/2016-general/sov/04-historical-voter-reg-participation.pdf&#34;&gt;this PDF from the California Secretary of State&amp;rsquo;s office&lt;/a&gt; contains historical voter registration and turnout data in a well-formatted table. Why this is a PDF nobody knows. But extracting the tables using &lt;code&gt;tabulizer&lt;/code&gt;&amp;rsquo;s &lt;code&gt;extract_tables()&lt;/code&gt; function is a breeze with no need to even download the file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;tabulizer&amp;quot;)
sos_url &amp;lt;- &amp;quot;http://elections.cdn.sos.ca.gov/sov/2016-general/sov/04-historical-voter-reg-participation.pdf&amp;quot;
tab1 &amp;lt;- extract_tables(sos_url)
str(tab1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## List of 2
##  $ : chr [1:58, 1:9] &amp;quot;&amp;quot; &amp;quot;Election Date&amp;quot; &amp;quot;Nov. 8, 1910&amp;quot; &amp;quot;Nov. 5, 1912 P&amp;quot; ...
##  $ : chr [1:6, 1:9] &amp;quot;&amp;quot; &amp;quot;Election Date&amp;quot; &amp;quot;Nov. 2, 2010&amp;quot; &amp;quot;Nov. 6, 2012 P&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The (default) result is a list of two matrices, each containing the tables from pages 1 and 2 of the document, respectively. A couple of quick cleanups and this becomes a well-formatted data frame:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# save header
h &amp;lt;- tab1[[1]][2,]
# remove headers in first table
tab1[[1]] &amp;lt;- tab1[[1]][-c(1,2),]
# remove duplicated header in second table
tab1[[2]] &amp;lt;- tab1[[2]][-c(1,2),]
# merge into one table
tab1df &amp;lt;- setNames(as.data.frame(do.call(&amp;quot;rbind&amp;quot;, tab1), stringsAsFactors = FALSE), h)
str(tab1df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	60 obs. of  9 variables:
##  $ Election Date: chr  &amp;quot;Nov. 8, 1910&amp;quot; &amp;quot;Nov. 5, 1912 P&amp;quot; &amp;quot;Nov. 3, 1914&amp;quot; &amp;quot;Nov. 7, 1916 P&amp;quot; ...
##  $ Eligible     : chr  &amp;quot;725,000&amp;quot; &amp;quot;1,569,000&amp;quot; &amp;quot;1,726,000&amp;quot; &amp;quot;1,806,000&amp;quot; ...
##  $ Democratic   : chr  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; ...
##  $ Republican   : chr  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; ...
##  $ Other        : chr  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; ...
##  $ Total        : chr  &amp;quot;*&amp;quot; &amp;quot;987,368&amp;quot; &amp;quot;1,219,345&amp;quot; &amp;quot;1,314,446&amp;quot; ...
##  $ Total Votes  : chr  &amp;quot;393,893&amp;quot; &amp;quot;707,776&amp;quot; &amp;quot;961,868&amp;quot; &amp;quot;1,045,858&amp;quot; ...
##  $ Registered   : chr  &amp;quot;*&amp;quot; &amp;quot;71.68%&amp;quot; &amp;quot;78.88%&amp;quot; &amp;quot;79.57%&amp;quot; ...
##  $ Eligible     : chr  &amp;quot;54.33%&amp;quot; &amp;quot;45.11%&amp;quot; &amp;quot;55.73%&amp;quot; &amp;quot;57.91%&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which is very easy to then quickly turn into a time-series visualization of registration rates:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;ggplot2&amp;quot;)
years &amp;lt;- regexpr(&amp;quot;[[:digit:]]{4}&amp;quot;,tab1df[[&amp;quot;Election Date&amp;quot;]])
tab1df$Year &amp;lt;- as.numeric(regmatches(tab1df[[&amp;quot;Election Date&amp;quot;]], years))
tab1df$RegPerc &amp;lt;- as.numeric(gsub(&amp;quot;%&amp;quot;, &amp;quot;&amp;quot;, tab1df$Registered))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(tab1df, aes(x = Year, y = RegPerc)) +
  geom_line() + ylim(c(0,100)) + ylab(&amp;quot;% Registered&amp;quot;) +
  ggtitle(&amp;quot;California Voter Registration, by Year&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/Gkbc1VO.png&#34; alt=&#34;plot of chunk example1plot&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;optional-arguments&#34;&gt;Optional arguments&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;extract_tables()&lt;/code&gt; has several arguments that control extraction and the return value of the function. They performed reasonably well here, but it&amp;rsquo;s worth seeing a few of the other options. The &lt;code&gt;method&lt;/code&gt; argument controls the return value. For extremely well-formatted tables, setting this to &amp;ldquo;data.frame&amp;rdquo; can be convenient, though it doesn&amp;rsquo;t work perfectly here:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tab2 &amp;lt;- extract_tables(sos_url, method = &amp;quot;data.frame&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## List of 2
##  $ :&#39;data.frame&#39;:	57 obs. of  9 variables:
##   ..$ X        : chr [1:57] &amp;quot;Election Date&amp;quot; &amp;quot;Nov. 8, 1910&amp;quot; &amp;quot;Nov. 5, 1912 P&amp;quot; &amp;quot;Nov. 3, 1914&amp;quot; ...
##   ..$ X.1      : chr [1:57] &amp;quot;Eligible&amp;quot; &amp;quot;725,000&amp;quot; &amp;quot;1,569,000&amp;quot; &amp;quot;1,726,000&amp;quot; ...
##   ..$ X.2      : chr [1:57] &amp;quot;Democratic&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; ...
##   ..$ X.3      : chr [1:57] &amp;quot;Republican&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; ...
##   ..$ X.4      : chr [1:57] &amp;quot;Other&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; ...
##   ..$ X.5      : chr [1:57] &amp;quot;Total&amp;quot; &amp;quot;*&amp;quot; &amp;quot;987,368&amp;quot; &amp;quot;1,219,345&amp;quot; ...
##   ..$ X.6      : chr [1:57] &amp;quot;Total Votes&amp;quot; &amp;quot;393,893&amp;quot; &amp;quot;707,776&amp;quot; &amp;quot;961,868&amp;quot; ...
##   ..$ Turnout  : chr [1:57] &amp;quot;Registered&amp;quot; &amp;quot;*&amp;quot; &amp;quot;71.68%&amp;quot; &amp;quot;78.88%&amp;quot; ...
##   ..$ Turnout.1: chr [1:57] &amp;quot;Eligible&amp;quot; &amp;quot;54.33%&amp;quot; &amp;quot;45.11%&amp;quot; &amp;quot;55.73%&amp;quot; ...
##  $ :&#39;data.frame&#39;:	5 obs. of  9 variables:
##   ..$ X        : chr [1:5] &amp;quot;Election Date&amp;quot; &amp;quot;Nov. 2, 2010&amp;quot; &amp;quot;Nov. 6, 2012 P&amp;quot; &amp;quot;Nov. 4, 2014&amp;quot; ...
##   ..$ X.1      : chr [1:5] &amp;quot;Eligible&amp;quot; &amp;quot;23,551,699&amp;quot; &amp;quot;23,802,577&amp;quot; &amp;quot;24,288,145&amp;quot; ...
##   ..$ X.2      : chr [1:5] &amp;quot;Democratic&amp;quot; &amp;quot;7,620,240&amp;quot; &amp;quot;7,966,422&amp;quot; &amp;quot;7,708,683&amp;quot; ...
##   ..$ X.3      : chr [1:5] &amp;quot;Republican&amp;quot; &amp;quot;5,361,875&amp;quot; &amp;quot;5,356,608&amp;quot; &amp;quot;5,005,422&amp;quot; ...
##   ..$ X.4      : chr [1:5] &amp;quot;Other&amp;quot; &amp;quot;4,303,768&amp;quot; &amp;quot;4,922,940&amp;quot; &amp;quot;5,089,718&amp;quot; ...
##   ..$ X.5      : chr [1:5] &amp;quot;Total&amp;quot; &amp;quot;17,285,883&amp;quot; &amp;quot;18,245,970&amp;quot; &amp;quot;17,803,823&amp;quot; ...
##   ..$ X.6      : chr [1:5] &amp;quot;Total Votes&amp;quot; &amp;quot;10,300,392&amp;quot; &amp;quot;13,202,158&amp;quot; &amp;quot;7,513,972&amp;quot; ...
##   ..$ Turnout  : chr [1:5] &amp;quot;Registered&amp;quot; &amp;quot;59.59%&amp;quot; &amp;quot;72.36%&amp;quot; &amp;quot;42.20%&amp;quot; ...
##   ..$ Turnout.1: chr [1:5] &amp;quot;Eligible&amp;quot; &amp;quot;43.74%&amp;quot; &amp;quot;55.47%&amp;quot; &amp;quot;30.94%&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Setting &lt;code&gt;method = &amp;quot;character&amp;quot;&lt;/code&gt; returns a list of character vectors with white space reflecting the positioning of text within the PDF&amp;rsquo;s tabular representation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tab3 &amp;lt;- extract_tables(sos_url, method = &amp;quot;character&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## List of 2
##  $ : chr &amp;quot;\t\t\t\t\t\t\tTurnout\tTurnout\nElection Date\tEligible\tDemocratic\tRepublican\tOther\tTotal\tTotal Votes\tRegistered\tEligibl&amp;quot;| __truncated__
##  $ : chr &amp;quot;\t\t\t\t\t\t\tTurnout\tTurnout\nElection Date\tEligible\tDemocratic\tRepublican\tOther\tTotal\tTotal Votes\tRegistered\tEligibl&amp;quot;| __truncated__
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This argument can also be set to &lt;code&gt;&amp;quot;csv&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;tsv&amp;quot;&lt;/code&gt;, or &lt;code&gt;&amp;quot;json&amp;quot;&lt;/code&gt; to use a java-level utility to write the table to files in the working directory but this tends to be inconvenient. (For advanced users, &lt;code&gt;method = &amp;quot;asis&amp;quot;&lt;/code&gt; returns an rJava object reference for those who want to manipulate the Java representation of the table directly.)&lt;/p&gt;

&lt;p&gt;The other most important option to be aware of is &lt;code&gt;guess&lt;/code&gt;, which indicates whether a column-finding algorithm should be used to identify column breaks. This should almost always be &lt;code&gt;TRUE&lt;/code&gt;, setting it to &lt;code&gt;FALSE&lt;/code&gt; will tend to return a less useful structure:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(extract_tables(sos_url, guess = FALSE)[[1]], 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##       [,1]
##  [1,] &amp;quot;&amp;quot;
##  [2,] &amp;quot;&amp;quot;
##  [3,] &amp;quot;&amp;quot;
##  [4,] &amp;quot;&amp;quot;
##  [5,] &amp;quot;Election Date&amp;quot;
##  [6,] &amp;quot;Nov. 8, 1910&amp;quot;
##  [7,] &amp;quot;Nov. 5, 1912 P&amp;quot;
##  [8,] &amp;quot;Nov. 3, 1914&amp;quot;
##  [9,] &amp;quot;Nov. 7, 1916 P&amp;quot;
## [10,] &amp;quot;Nov. 5, 1918&amp;quot;
##       [,2]
##  [1,] &amp;quot;HISTORICAL VOTER REGISTRATION AND&amp;quot;
##  [2,] &amp;quot;PARTICIPATION IN STATEWIDE GENERAL ELECTIONS 1910-2016&amp;quot;
##  [3,] &amp;quot;Registration Votes Cast&amp;quot;
##  [4,] &amp;quot;Turnout&amp;quot;
##  [5,] &amp;quot;Eligible Democratic Republican Other Total Total Votes Registered&amp;quot;
##  [6,] &amp;quot;725,000 * * * *393,893*&amp;quot;
##  [7,] &amp;quot;1,569,000 * * * 987,368 707,776 71.68%&amp;quot;
##  [8,] &amp;quot;1,726,000 * * * 1,219,345 961,868 78.88%&amp;quot;
##  [9,] &amp;quot;1,806,000 * * * 1,314,446 1,045,858 79.57%&amp;quot;
## [10,] &amp;quot;1,918,000 * * * 1,203,898 714,525 59.35%&amp;quot;
##       [,3]
##  [1,] &amp;quot;&amp;quot;
##  [2,] &amp;quot;&amp;quot;
##  [3,] &amp;quot;&amp;quot;
##  [4,] &amp;quot;Turnout&amp;quot;
##  [5,] &amp;quot;Eligible&amp;quot;
##  [6,] &amp;quot;54.33%&amp;quot;
##  [7,] &amp;quot;45.11%&amp;quot;
##  [8,] &amp;quot;55.73%&amp;quot;
##  [9,] &amp;quot;57.91%&amp;quot;
## [10,] &amp;quot;37.25%&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, it can be useful if users want to specify the locations of tables manually. The &lt;code&gt;area&lt;/code&gt; argument allows users to specifying a &lt;code&gt;c(top,left,bottom,right)&lt;/code&gt; vector of coordinates for the location of tables on a page (which is useful if pages also contain other non-tabular content); setting &lt;code&gt;columns&lt;/code&gt; with &lt;code&gt;guess = FALSE&lt;/code&gt; indicates where the column breaks are within a table. With a little care in specifying column positions we can successfully separate the &amp;ldquo;P&amp;rdquo; flags specifying Presidential elections that were earlier concatenated with the election dates:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cols &amp;lt;- list(c(76,123,126,203,249,297,342,392,453,498,548))
tab4 &amp;lt;- extract_tables(sos_url, guess = FALSE, columns = cols)

# save header
h &amp;lt;- tab4[[1]][5,-1]
# clean tables
tab4[[1]] &amp;lt;- tab4[[1]][-c(1:5,62),-1]
tab4[[2]] &amp;lt;- tab4[[2]][-c(1:5,10:17),-1]
# merge into one table
tab4df &amp;lt;- setNames(as.data.frame(do.call(&amp;quot;rbind&amp;quot;, tab4), stringsAsFactors = FALSE), h)
str(tab4df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:	60 obs. of  10 variables:
##  $ Election Date: chr  &amp;quot;Nov. 8, 1910&amp;quot; &amp;quot;Nov. 5, 1912&amp;quot; &amp;quot;Nov. 3, 1914&amp;quot; &amp;quot;Nov. 7, 1916&amp;quot; ...
##  $              : chr  &amp;quot;&amp;quot; &amp;quot;P&amp;quot; &amp;quot;&amp;quot; &amp;quot;P&amp;quot; ...
##  $ Eligible     : chr  &amp;quot;725,000&amp;quot; &amp;quot;1,569,000&amp;quot; &amp;quot;1,726,000&amp;quot; &amp;quot;1,806,000&amp;quot; ...
##  $ Democratic   : chr  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; ...
##  $ Republican   : chr  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; ...
##  $ Other        : chr  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; ...
##  $ Total        : chr  &amp;quot;*&amp;quot; &amp;quot;987,368&amp;quot; &amp;quot;1,219,345&amp;quot; &amp;quot;1,314,446&amp;quot; ...
##  $ Total Votes  : chr  &amp;quot;393,893&amp;quot; &amp;quot;707,776&amp;quot; &amp;quot;961,868&amp;quot; &amp;quot;1,045,858&amp;quot; ...
##  $ Registered   : chr  &amp;quot;*&amp;quot; &amp;quot;71.68%&amp;quot; &amp;quot;78.88%&amp;quot; &amp;quot;79.57%&amp;quot; ...
##  $ Eligible     : chr  &amp;quot;54.33%&amp;quot; &amp;quot;45.11%&amp;quot; &amp;quot;55.73%&amp;quot; &amp;quot;57.91%&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figuring out columns positions and/or table areas is quite challenging to do by hand, so the &lt;code&gt;locate_areas()&lt;/code&gt; provides an interactive interface for identifying areas. It returns lists of coordinates for specific table areas. A higher-level function, &lt;code&gt;extract_areas()&lt;/code&gt;, connects that GUI directly to &lt;code&gt;extract_tables()&lt;/code&gt; to return the tables within specified areas. Two other functions can be useful in this respect: &lt;code&gt;get_n_pages()&lt;/code&gt; indicates the number of pages in a PDF and &lt;code&gt;get_page_dims()&lt;/code&gt; indicates the dimensions of the pages.&lt;/p&gt;

&lt;h2 id=&#34;some-other-functionality&#34;&gt;Some other functionality&lt;/h2&gt;

&lt;p&gt;In addition to the core functionality around &lt;code&gt;extract_tables()&lt;/code&gt;, &lt;code&gt;tabulizer&lt;/code&gt; also provides some functions for working with PDFs that might be useful to those trapped in other peoples&amp;rsquo; data. We&amp;rsquo;ll download the file first just to save some time:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tmp &amp;lt;- tempfile(fileext = &amp;quot;.pdf&amp;quot;)
download.file(sos_url, destfile = tmp, mode = &amp;quot;wb&amp;quot;, quiet = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;extract_text()&lt;/code&gt; function extracts text content of the PDF, separately by page, as character strings:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;extract_text(tmp)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;4Election Date Eligible Democratic Republican Other         Total Total Votes\r\nTurnout \r\nRegistered\r\nTurnout \r\nEligible\r\nNov. 8, 1910 725,000 * * *             *    393,893              * 54.33%\r\nNov. 5, 1912 P 1,569,000 * * * 987,368 707,776 71.68% 45.11%\r\nNov. 3, 1914 1,726,000 * * * 1,219,345 961,868 78.88% 55.73%\r\nNov. 7, 1916 P 1,806,000 * * * 1,314,446 1,045,858 79.57% 57.91%\r\nNov. 5, 1918 1,918,000 * * * 1,203,898 714,525 59.35% 37.25%\r\nNov. 2, 1920 P 2,090,000 * * * 1,374,184 987,632 71.87% 47.26%\r\nNov. 7, 1922 2,420,000 319,107 968,429 244,848 1,532,384 1,000,997 65.32% 41.36%\r\nNov. 4, 1924 P 2,754,000 397,962 1,183,672 240,723 1,822,357 1,336,598 73.34% 48.53%\r\nNov. 2, 1926 2,989,000 410,290 1,298,062 204,510 1,912,862 1,212,452 63.38% 40.56%\r\nNov. 6, 1928 P 3,240,000 592,161 1,535,751 185,904 2,313,816 1,846,077 79.78% 56.98%\r\nNov. 4, 1930 3,463,000 456,096 1,638,575 150,557 2,245,228 1,444,872 64.35% 41.72%\r\nNov. 8, 1932 P 3,573,000 1,161,482 1,565,264 162,267 2,889,013 2,330,132 80.65% 65.22%\r\nNov. 6, 1934 3,674,000 1,555,705 1,430,198 154,211 3,140,114 2,360,916 75.19% 64.26%\r\nNov. 3, 1936 P 3,844,000 1,882,014 1,244,507 127,300 3,253,821 2,712,342 83.36% 70.56%\r\nNov. 8, 1938 4,035,000 2,144,360 1,293,929 173,127 3,611,416 2,695,904 74.65% 66.81%\r\nNov. 5, 1940 P 4,214,000 2,419,628 1,458,373 174,394 4,052,395 3,300,410 81.44% 78.32%\r\nNov. 3, 1942 4,693,000 2,300,206 1,370,069 150,491 3,820,776 2,264,288 59.26% 48.25%\r\nNov. 7, 1944 P 5,427,000 2,418,965 1,548,395 173,971 4,141,331 3,566,734 86.13% 65.72%\r\nNov. 5, 1946 5,800,000 2,541,720 1,637,246 204,997 4,383,963 2,759,641 62.95% 47.58%\r\nNov. 2, 1948 P 6,106,000 2,892,222 1,908,170 261,605 5,061,997 4,076,981 80.54% 66.77%\r\nNov. 7, 1950 6,458,000 3,062,205 1,944,812 237,820 5,244,837 3,845,757 73.32% 59.55%\r\nNov. 4, 1952 P 7,033,000 3,312,668 2,455,713 229,919 5,998,300 5,209,692 86.85% 74.07%\r\nNov. 2, 1954 7,565,000 3,266,831 2,415,249 203,157 5,885,237 4,101,692 69.69% 54.22%\r\nNov. 6, 1956 P 8,208,000 3,575,635 2,646,249 186,937 6,408,821 5,547,621 86.56% 67.59%\r\nNov. 4, 1958 8,909,000 3,875,630 2,676,565 200,226 6,752,421 5,366,053 79.47% 60.23%\r\nNov. 8, 1960 P 9,587,000 4,295,330 2,926,408 242,888 7,464,626 6,592,591 88.32% 68.77%\r\nNov. 6, 1962 10,305,000 4,289,997 3,002,038 239,176 7,531,211 5,929,602 78.73% 57.54%\r\nNov. 3, 1964 P 10,959,000 4,737,886 3,181,272 264,985 8,184,143 7,233,067 88.38% 66.00%\r\nNov. 8, 1966 11,448,000 4,720,597 3,350,990 269,281 8,340,868 6,605,866 79.20% 57.70%\r\nNov. 5, 1968 P 11,813,000 4,682,661 3,462,131 442,881 8,587,673 7,363,711 85.75% 62.34%\r\nNov. 3, 1970 12,182,000 4,781,282 3,469,046 456,019 8,706,347 6,633,400 76.19% 54.45%\r\nNov. 7, 1972 P 13,322,000 5,864,745 3,840,620 760,850 10,466,215 8,595,950 82.13% 64.52%\r\nNov. 6, 1973 S 13,512,000 5,049,959 3,422,291 617,569 9,089,819 4,329,017 47.62% 32.04%\r\nNov. 5, 1974 13,703,000 5,623,831 3,574,624 729,909 9,928,364 6,364,597 64.11% 46.45%\r\nNov. 2, 1976 P 14,196,000 5,725,718 3,468,439 786,331 9,980,488 8,137,202 81.53% 57.32%\r\nNov. 7, 1978 14,781,000 5,729,959 3,465,384 934,643 10,129,986 7,132,210 70.41% 48.25%\r\nNov. 6, 1979 S 15,083,000 5,594,018 3,406,854 1,006,085 10,006,957 3,740,800 37.38% 24.80%\r\nNov. 4, 1980 P 15,384,000 6,043,262 3,942,768 1,375,593 11,361,623 8,775,459 77.24% 57.04%\r\nNov. 2, 1982 15,984,000 6,150,716 4,029,684 1,378,699 11,559,099 8,064,314 69.78% 50.45%\r\nNov. 6, 1984 P 16,582,000 6,804,263 4,769,129 1,500,238 13,073,630 9,796,375 74.93% 59.08%\r\nNov. 4, 1986 17,561,000 6,524,496 4,912,581 1,396,843 12,833,920 7,617,142 59.35% 43.38%\r\nNov. 8, 1988 P 19,052,000 7,052,368 5,406,127 1,546,378 14,004,873 10,194,539 72.81% 53.51%\r\nNov. 6, 1990 19,245,000 6,671,747 5,290,202 1,516,078 13,478,027 7,899,131 58.61% 41.05%\r\nNov. 3, 1992 P 20,864,000 7,410,914 5,593,555 2,097,004 15,101,473 11,374,565 75.32% 54.52%\r\nNov. 2, 1993 S 20,797,000 7,110,142 5,389,313 2,043,168 14,524,623 5,282,443 36.37% 27.73%\r\nNov. 8, 1994 18,946,000 7,219,635 5,472,391 2,031,758 14,723,784 8,900,593 60.45% 46.98%\r\nNov. 5, 1996 P 19,526,991 7,387,504 5,704,536 2,570,035 15,662,075 10,263,490 65.53% 52.56%\r\nNov. 3, 1998 20,806,462 6,989,006 5,314,912 2,665,267 14,969,185 8,621,121 57.59% 41.43%\r\nNov. 7, 2000 P 21,461,275 7,134,601 5,485,492 3,087,214 15,707,307 11,142,843 70.94% 51.92%\r\nNov. 5, 2002 21,466,274 6,825,400 5,388,895 3,089,174 15,303,469 7,738,821 50.57% 36.05%\r\nOct.  7, 2003 S 21,833,141 6,718,111 5,429,256 3,236,059 15,383,526 9,413,494 61.20% 43.12%\r\nNov. 2, 2004 P 22,075,036 7,120,425 5,745,518 3,691,330 16,557,273 12,589,683 76.04% 57.03%\r\nNov. 8, 2005 S 22,487,768 6,785,188 5,524,609 3,581,685 15,891,482 7,968,757 50.14% 35.44%\r\nNov. 7, 2006 22,652,190 6,727,908 5,436,314 3,672,886 15,837,108 8,899,059 56.19% 39.29%\r\nNov. 4, 2008 P 23,208,710 7,683,495 5,428,052 4,192,544 17,304,091 13,743,177 79.42% 59.22%\r\nMay 19, 2009 S 23,385,819 7,642,108 5,325,558 4,185,346 17,153,012 4,871,945 28.40% 20.80%\r\nHISTORICAL VOTER REGISTRATION AND\r\nPARTICIPATION IN STATEWIDE GENERAL ELECTIONS 1910-2016\r\nVotes CastRegistration\r\n5Election Date Eligible Democratic Republican Other         Total Total Votes\r\nTurnout \r\nRegistered\r\nTurnout \r\nEligible\r\nNov. 2, 2010 23,551,699 7,620,240 5,361,875 4,303,768 17,285,883 10,300,392 59.59% 43.74%\r\nNov. 6, 2012 P 23,802,577 7,966,422 5,356,608 4,922,940 18,245,970 13,202,158 72.36% 55.47%\r\nNov. 4, 2014 24,288,145 7,708,683 5,005,422 5,089,718 17,803,823 7,513,972 42.20% 30.94%\r\nNov. 8, 2016 P 24,875,293 8,720,417 5,048,398 5,642,956 19,411,771 14,610,509 75.27% 58.74%\r\nNotes\r\n* Indicates information not available. \r\nIn 1911, women gained the right to vote in California.\r\nIn 1972, the voting age was lowered from 21 to 18.\r\nRegistration Votes Cast\r\nP indicates a presidential election year.\r\nThe first statewide record of party affiliations was reported in 1922.\r\nHISTORICAL VOTER REGISTRATION AND\r\nPARTICIPATION IN STATEWIDE GENERAL ELECTIONS 1910-2016 (continued)\r\nS indicates a statewide special election.\r\n&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be useful for non-tabular content, getting a sense of the document&amp;rsquo;s contents, or troubleshooting the main extraction function (e.g., sometimes there is non-visible text that confuses &lt;code&gt;extract_tables()&lt;/code&gt;). &lt;code&gt;extract_metadata()&lt;/code&gt; returns a list of the PDF&amp;rsquo;s embedded document metadata:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(extract_metadata(tmp))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## List of 10
##  $ pages   : int 2
##  $ title   : chr &amp;quot;Statement of Vote - General Election, November 8, 2016&amp;quot;
##  $ author  : NULL
##  $ subject : chr &amp;quot;Statement of Vote - General Election, November 8, 2016&amp;quot;
##  $ keywords: chr &amp;quot;Statement of Vote - General Election, November 8, 2016&amp;quot;
##  $ creator : chr &amp;quot;Acrobat PDFMaker 11 for Excel&amp;quot;
##  $ producer: chr &amp;quot;Adobe PDF Library 11.0&amp;quot;
##  $ created : chr &amp;quot;Fri Dec 16 18:54:13 GMT 2016&amp;quot;
##  $ modified: chr &amp;quot;Fri Dec 16 18:54:44 GMT 2016&amp;quot;
##  $ trapped : NULL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;make_thumbnails()&lt;/code&gt; function produces images (by default PNG) of pages, which can also be useful for debugging or just for the mundane purpose of image conversion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;thumb &amp;lt;- make_thumbnails(tmp, pages = 1)
library(&amp;quot;png&amp;quot;)
thispng &amp;lt;- readPNG(thumb, native = TRUE)
d &amp;lt;- get_page_dims(tmp, pages = 1)[[1]]
plot(c(0, d[1]), c(0, d[2]), type = &amp;quot;n&amp;quot;, xlab = &amp;quot;&amp;quot;, ylab = &amp;quot;&amp;quot;, asp = 1)
rasterImage(thispng, 0, 0, d[1], d[2])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/mJZSM3P.png&#34; alt=&#34;plot of chunk example2d&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And, lastly, the &lt;code&gt;split_pdf()&lt;/code&gt; and &lt;code&gt;merge_pdf()&lt;/code&gt; functions can extract specific pages from a PDF or merge multiple PDFs together. Those functions should find multiple uses cases beyond the challenges of working with other peoples&amp;rsquo; data.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;tabulizer&lt;/code&gt; can&amp;rsquo;t solve all your PDF problems. More likely than not you&amp;rsquo;ll at some point encounter a PDF that contains scanned tables or tables that tabula-java&amp;rsquo;s algorithms can&amp;rsquo;t identify well. But for a wide array of well-formatted PDF tables, &lt;code&gt;tabulizer&lt;/code&gt; should provide a much simpler and much faster initial extraction of data than attempting to transcribe their contents manually.&lt;/p&gt;

&lt;h3 id=&#34;contribute&#34;&gt;Contribute&lt;/h3&gt;

&lt;p&gt;As always, the &lt;a href=&#34;https://github.com/ropensci/tabulizer/issues&#34;&gt;issue tracker&lt;/a&gt; on Github is open for suggestions, bug reports, and package support. &lt;a href=&#34;https://github.com/ropensci/tabulizer/pulls&#34;&gt;Pull requests&lt;/a&gt; are always welcome.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve flagged some specific issues on GitHub which interested users might want to help out with. These range from some basic issues:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Identifying and creating &lt;a href=&#34;https://github.com/ropensci/tabulizer/issues/47&#34;&gt;example use cases for the new &lt;code&gt;tabulizer&lt;/code&gt; wiki&lt;/a&gt; to showcase how the package works&lt;/li&gt;
&lt;li&gt;Adding &lt;a href=&#34;https://github.com/ropensci/tabulizer/issues/46&#34;&gt;comprehensive, cross-platform installation instructions&lt;/a&gt; to deal with the various intricacies of Java and rJava on various platforms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To moderately difficult issues, like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ropensci/tabulizer/issues/49&#34;&gt;Improving the functionality and attractiveness of the Shiny-based &lt;code&gt;extract_areas()&lt;/code&gt; graphical interface&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To more advanced topics that more experienced developers - especially those with Java experience - might be interested in working on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Improving handling of &lt;a href=&#34;https://github.com/ropensci/tabulizer/issues/10&#34;&gt;non-latin encodings&lt;/a&gt; including adding tests thereof&lt;/li&gt;
&lt;li&gt;Preparing &lt;code&gt;tabulizer&lt;/code&gt; for &lt;a href=&#34;https://github.com/ropensci/tabulizer/issues/48&#34;&gt;the migration of the tabula-java library to PDFBox 2.0&lt;/a&gt;, which will change some of the underlying classes (and methods thereof) that &lt;code&gt;tabulizer&lt;/code&gt; calls from both tabula-java and PDFBox&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Help of any kind on these issues will be very useful for getting the package ready for CRAN release!&lt;/p&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;Many, many thanks to the Tabula team who have done considerable work to make the tabula-java library on which &lt;code&gt;tabulizer&lt;/code&gt; depends. I also want to express considerable thanks to &lt;a href=&#34;https://github.com/davidgohel&#34;&gt;David Gohel&lt;/a&gt; and &lt;a href=&#34;https://github.com/lmullen&#34;&gt;Lincoln Mullen&lt;/a&gt; for their feedback during the &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/42&#34;&gt;rOpenSci onboarding process&lt;/a&gt;, which resulted in numerous improvements to the package and its usability, not least of which is the interactive shiny widget. Thanks, too, to &lt;a href=&#34;https://github.com/sckott&#34;&gt;Scott Chamberlain&lt;/a&gt; for overseeing the review process and to the whole of rOpenSci for their support of the R community.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Discover hydrological data using the hddtools R package</title>
      <link>https://ropensci.org/blog/2017/03/07/hddtools/</link>
      <pubDate>Tue, 07 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/03/07/hddtools/</guid>
      <description>
        
        

&lt;p&gt;I&amp;rsquo;ve worked for over 12 years in hydrology and natural hazard modelling and one of the things that still fascinates me is the variety of factors that come into play in trying to predict phenomena such as river floods. From local observations of meteorological and hydrological variables and their spatio-temporal patterns to the type and condition of soils and vegetation/land use as well as the geometry and state of river channels and engineering structures affecting the flow.&lt;/p&gt;

&lt;p&gt;This large number of well known predictors directly translates in the need to collect a wide variety of information such as in-situ sensor observations, river survey data, satellite imagery and, more recently, social media information (reporting in real-time the evolution of on-going events). In this field, data collection is challenging on many levels: data are available from different providers under a variety of licences, in various data types and formats that need to be handled and homogenised.&lt;/p&gt;

&lt;p&gt;On the one side, not much can be done to overcome licensing issues and the learning curve to become an expert hydrologist is rather steep. On the other side, the R community is working hard to provide tools to make data standards more user-friendly and the convenience of data APIs available to everyone, not only web developers. Here is where &lt;code&gt;hddtools&lt;/code&gt; comes into play! This R package is a proof of concept that hydrological data can be made more accessible and consists of a collection of functions to retrieve and homogenise hydrological information. Let me walk you through the main functionalities!&lt;/p&gt;

&lt;h2 id=&#34;the-r-package-hddtools&#34;&gt;The R package &lt;code&gt;hddtools&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;The name &lt;code&gt;hddtools&lt;/code&gt; stands for &lt;strong&gt;H&lt;/strong&gt;ydrological &lt;strong&gt;D&lt;/strong&gt;ata &lt;strong&gt;D&lt;/strong&gt;iscovery &lt;strong&gt;Tools&lt;/strong&gt;. This is an open source project designed to facilitate access to on-line data sources. This typically implies the download of a metadata catalogue, selection of information needed, formal request for dataset(s), de-compression, conversion, manual filtering and parsing. All those operation are made more efficient by re-usable functions.&lt;/p&gt;

&lt;p&gt;Depending on the data license, functions can provide offline and/or on-line modes. When redistribution is allowed, for instance, a copy of the dataset is cached within the package and updated twice a year. This is the fastest option and also allows offline use of functions. When re-distribution is not allowed, only on-line mode is provided.&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;hddtools&lt;/code&gt; package and the examples in this blog depend on other CRAN packages. Before attempting to install &lt;code&gt;hddtools&lt;/code&gt;, solve any missing dependencies using the commands below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;packs &amp;lt;- c(&amp;quot;zoo&amp;quot;, &amp;quot;sp&amp;quot;, &amp;quot;RCurl&amp;quot;, &amp;quot;XML&amp;quot;, &amp;quot;rnrfa&amp;quot;, &amp;quot;Hmisc&amp;quot;, &amp;quot;raster&amp;quot;,
           &amp;quot;stringr&amp;quot;, &amp;quot;devtools&amp;quot;, &amp;quot;leaflet&amp;quot;)
new_packages &amp;lt;- packs[!(packs %in% installed.packages()[,&amp;quot;Package&amp;quot;])]
if(length(new_packages)) install.packages(new_packages)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The package is available from the Comprehensive R Archive Network (&lt;a href=&#34;https://cran.r-project.org/index.html&#34;&gt;CRAN&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;hddtools&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, the development of this package is very dynamic. Data sources become often unavailable or migrate to new systems. URLs change and, as a consequence, functions may stop working. For this reason, I always suggest to use the development version of this package, which is available from github using &lt;a href=&#34;https://github.com/hadley/devtools&#34;&gt;devtools&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/hddtools&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Load the &lt;code&gt;hddtools&lt;/code&gt; package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;hddtools&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;data-sources-and-functions&#34;&gt;Data sources and Functions&lt;/h2&gt;

&lt;p&gt;In my quest for hydrological data I found that there are tons of open datasets available but the problem is that &lt;strong&gt;you need to know where to look!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is the list of data sources available within the &lt;code&gt;hddtools&lt;/code&gt; package:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Koppen Climate Classification map&lt;/li&gt;
&lt;li&gt;The Global Runoff Data Centre&lt;/li&gt;
&lt;li&gt;NASA&amp;rsquo;s Tropical Rainfall Measuring Mission&lt;/li&gt;
&lt;li&gt;The Top-Down modelling Working Group:

&lt;ul&gt;
&lt;li&gt;Data60UK&lt;/li&gt;
&lt;li&gt;MOPEX
-SEPA river level data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For each source, functions are available to obtain and/or filter relevant data. Their usage is described below.&lt;/p&gt;

&lt;h3 id=&#34;the-koppen-climate-classification-map&#34;&gt;The Koppen Climate Classification map&lt;/h3&gt;

&lt;p&gt;The Koppen Climate Classification is the most widely used system for classifying the world&amp;rsquo;s climates. Its categories are based on the annual and monthly averages of temperature and precipitation. This climate classification is used, in hydrological studies, to explain the continental-scale variability in
annual runoff. The &lt;code&gt;hddtools&lt;/code&gt; package contains a function to identify the updated Koppen-Greiger climate zone, given a bounding box. In the example below I&amp;rsquo;m getting the climate class for my beautiful home town in Italy: Pompeii.
Country borders are retrieved using the &lt;code&gt;getData()&lt;/code&gt; function from the raster package, which retrieves global administrative areas from the following website: &lt;a href=&#34;http://www.gadm.org/&#34;&gt;http://www.gadm.org/&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Plot Pompeii on a map
library(raster)
Italy &amp;lt;- getData(&amp;quot;GADM&amp;quot;, country = &amp;quot;IT&amp;quot;, level = 0)
Pompeii &amp;lt;- SpatialPoints(coords = data.frame(x = 14.4870, y = 40.7510))
plot(Italy, col = NA, border = &amp;quot;darkgrey&amp;quot;)
plot(Pompeii, add = TRUE, col = &amp;quot;red&amp;quot;)

# Define and plot a bounding box centred in Pompeii (Italy)
areaBox &amp;lt;- raster::extent(Pompeii@coords[[1]] - 0.5, Pompeii@coords[[1]] + 0.5,
                          Pompeii@coords[[2]] - 0.5, Pompeii@coords[[2]] + 0.5)
plot(areaBox, add = TRUE, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-03-07-hddtools/unnamed-chunk-5-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Extract climate zones from Kottek&#39;s map:
KGClimateClass(areaBox = areaBox, updatedBy = &amp;quot;Kottek&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##   ID Class Frequency
## 1 34   Csa       131
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the column &lt;code&gt;Class&lt;/code&gt; in the above table, it can be derived that the area falls in a warm temperate climate &amp;copy; with dry (s) and hot summers (a). A description of the retrieved class and related criterion can be printed setting the argument &lt;code&gt;verbose = TRUE&lt;/code&gt; in the function &lt;code&gt;KGClimateClass()&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;the-global-runoff-data-centre&#34;&gt;The Global Runoff Data Centre&lt;/h3&gt;

&lt;p&gt;The Global Runoff Data Centre (GRDC) is an international archive hosted by the Federal Institute of Hydrology in Koblenz, Germany. The Centre operates under the auspices of the World Meteorological Organisation and retains services and datasets for all the major rivers in the world. The data catalogue, kml files and the Long-Term Mean Monthly Discharges are open data and accessible via &lt;code&gt;hddtools&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Information on all the GRDC stations can be retrieved using the function &lt;code&gt;catalogueGRDC&lt;/code&gt; with no input arguments, as in the example below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# GRDC full catalogue
GRDC_catalogue_all &amp;lt;- catalogueGRDC()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, there are a number of options to filter the catalogue and return only a subset of stations. For instance, the catalogue can be filtered based on a geographical bounding box as in the function &lt;code&gt;KGClimateClass()&lt;/code&gt;. It might also be interesting to subset only stations in a given country, e.g. Italy. As the 8th column in the catalogue lists the country codes, the catalogue can be filtered passing two input arguments to the &lt;code&gt;catalogueGRDC()&lt;/code&gt; function: &lt;code&gt;columnName = &amp;quot;country_code&amp;quot;&lt;/code&gt; and &lt;code&gt;columnValue  = &amp;quot;IT&amp;quot;&lt;/code&gt;, as in the example below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Filter GRDC catalogue based on a country code
GRDC_IT &amp;lt;- catalogueGRDC(columnName = &amp;quot;country_code&amp;quot;, columnValue = &amp;quot;IT&amp;quot;)

# Convert the table to a SpatialPointsDataFrame
hydro &amp;lt;- SpatialPointsDataFrame(coords = GRDC_IT[, c(&amp;quot;long&amp;quot;, &amp;quot;lat&amp;quot;)],
                                data = GRDC_IT)

# Plot the stations on the map
plot(Italy, col = NA, border = &amp;quot;darkgrey&amp;quot;)
plot(hydro, add = TRUE, col = &amp;quot;blue&amp;quot;, pch = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-03-07-hddtools/unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The arguments &lt;code&gt;columnName&lt;/code&gt; and &lt;code&gt;columnValue&lt;/code&gt; can be used to filter over other columns. For instance, the example below shows how to subset stations along the Tanaro River which source in the Ligurian Alps.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Filter GRDC catalogue based on rivername, search is not case sensitive!
Tanaro &amp;lt;- catalogueGRDC(columnName = &amp;quot;river&amp;quot;, columnValue = &amp;quot;Tanaro&amp;quot;)

# Convert the table to a SpatialPointsDataFrame
TanaroSP &amp;lt;- SpatialPointsDataFrame(coords = Tanaro[, c(&amp;quot;long&amp;quot;, &amp;quot;lat&amp;quot;)],
                                   data = Tanaro)

# Highight in red the stations on the Tanaro River
plot(Italy, col = NA, border = &amp;quot;darkgrey&amp;quot;)
plot(hydro, add = TRUE, col = &amp;quot;blue&amp;quot;, pch = 1)
plot(TanaroSP, add = TRUE, col = &amp;quot;red&amp;quot;, pch = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-03-07-hddtools/unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If &lt;code&gt;columnName&lt;/code&gt; refers to a numeric field, in the &lt;code&gt;columnValue&lt;/code&gt; other terms of comparison can be specified. For example, it is straightforward to find out that the country with the most longstanding monitoring stations is Germany (with 11 out of 15 stations with more than 150 years of recordings).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Filter GRDC stations with more than 150 years of recordings
GRDC150 &amp;lt;- catalogueGRDC(columnName = &amp;quot;d_yrs&amp;quot;, columnValue = &amp;quot;&amp;gt;150&amp;quot;)

# Which country has the most longstanding monitoring stations?
table(GRDC150$country_code)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##
## DE FI LT RO SE
## 11  1  1  1  1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Where is the oldest station?
oldest &amp;lt;- GRDC150[which(GRDC150$d_yrs == max(GRDC150$d_yrs)),
                  c(&amp;quot;grdc_no&amp;quot;, &amp;quot;river&amp;quot;, &amp;quot;station&amp;quot;, &amp;quot;d_yrs&amp;quot;)]
oldest
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 4
##   grdc_no      river station d_yrs
##     &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1 6340120 ELBE RIVER DRESDEN   208
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The oldest stastion is in Dresden on the Elbe River. Let&amp;rsquo;s now find out whether monthly data is available. For this we need the GRDC identification number (this is stored in the column grdc_no of the catalogue) and the function &lt;code&gt;tsGRDC()&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Monthly data extraction
DresdenStation &amp;lt;- tsGRDC(stationID = oldest$grdc_no, plotOption = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-03-07-hddtools/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The plot above, shows the time on the x-axis and the flow (in m&lt;sup&gt;3&lt;/sup&gt;/s) on the y-axis. The river flow has had huge oscillations over time, is this an effect of anthropogenic changes and/or climate change? This is well out of the scope of this blog post, I leave the reader to look at trends from the mean monthly values &lt;code&gt;DresdenStation$mddPerYear&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;nasa-s-tropical-rainfall-measuring-mission&#34;&gt;NASA&amp;rsquo;s Tropical Rainfall Measuring Mission&lt;/h3&gt;

&lt;p&gt;The Tropical Rainfall Measuring Mission (TRMM) is a joint mission between NASA and the Japan Aerospace Exploration Agency (JAXA) that uses a research satellite to measure precipitation within the tropics in order to improve our understanding of climate and its variability.&lt;/p&gt;

&lt;p&gt;The TRMM satellite records global historical rainfall estimation in a gridded format since 1998 with a daily temporal resolution and a spatial resolution of 0.25 degrees (spatial extent goes from -50 to +50 degrees latitude). This information is openly available for educational purposes and downloadable from an FTP server. The &lt;code&gt;hddtools&lt;/code&gt; provides a function, called TRMM(), to download and convert a selected portion of the TRMM dataset into a raster-brick that can be opened in any GIS software.&lt;/p&gt;

&lt;p&gt;As an example, I&amp;rsquo;m going to download precipitation maps for three months in 2016 using the &lt;code&gt;areabox&lt;/code&gt; defined previously to locate Pompeii and surrounding areas. But remember, values become less reliable moving away from the tropics!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Define a temporal extent
twindow &amp;lt;- seq(as.Date(&amp;quot;2016-01-01&amp;quot;), as.Date(&amp;quot;2016-03-31&amp;quot;), by = &amp;quot;months&amp;quot;)

# Retrieve mean monthly precipitations based on a bounding box and time extent
TRMMfile &amp;lt;- TRMM(twindow = twindow, areaBox = areaBox)

plot(TRMMfile)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-03-07-hddtools/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each of the above plots shows the average precipitation over Pompeii during January (top left), February (top right) and March (bottom left) 2016. Precipitation seems generally more abundant in the south (coastal areas) where the plots show prevalence of green, especially during January and February.&lt;/p&gt;

&lt;h3 id=&#34;top-down-modelling-working-group&#34;&gt;Top-Down modelling Working Group&lt;/h3&gt;

&lt;p&gt;The Top-Down modelling Working Group (TDWG) for the Prediction in Ungauged Basins (PUB) Decade (2003-2012) is an initiative of the International Association of Hydrological Sciences (IAHS) which collected datasets for hydrological modelling free-of-charge, available &lt;a href=&#34;http://tdwg.catchment.org/datasets.html&#34;&gt;here&lt;/a&gt;. This package provides a common interface to retrieve, browse and filter two datasets: Data60UK and MOPEX.&lt;/p&gt;

&lt;h4 id=&#34;the-data60uk-dataset&#34;&gt;The Data60UK dataset&lt;/h4&gt;

&lt;p&gt;The Data60UK initiative collated datasets of areal precipitation and streamflow discharge across 61 gauging sites in England and Wales (UK). The database was prepared from source databases for research purposes, with the intention to make it re-usable. This is now available in the public domain free of charge.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;hddtools&lt;/code&gt; contain two functions to interact with this database: one to retrieve the catalogue and another to retrieve time series of areal precipitation and streamflow discharge.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;UK &amp;lt;- getData(&amp;quot;GADM&amp;quot;, country = &amp;quot;GBR&amp;quot;, level = 0)
plot(UK, col = NA, border = &amp;quot;darkgrey&amp;quot;)

# Data60UK full catalogue
allData60UK &amp;lt;- catalogueData60UK()
hydroALL &amp;lt;- SpatialPointsDataFrame(coords = allData60UK[, c(&amp;quot;Longitude&amp;quot;,
                                                         &amp;quot;Latitude&amp;quot;)],
                                data = allData60UK)
plot(hydroALL, add = TRUE, col = &amp;quot;blue&amp;quot;, pch = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-03-07-hddtools/unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(allData60UK)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##   stationID        River          Location gridReference Latitude
## 1     22001       Coquet           Morwick      NU234044 55.33310
## 2     22006        Blyth   Hartford Bridge      NZ242800 55.11381
## 3     23006   South Tyne      Featherstone      NY672610 54.94257
## 4     24004 Bedburn Beck           Bedburn      NZ117321 54.68382
## 5     25005        Leven      Leven Bridge      NZ444120 54.50139
## 6     25006        Greta Rutherford Bridge      NZ033122 54.50511
##   Longitude
## 1 -1.632691
## 2 -1.622163
## 3 -2.513539
## 4 -1.820046
## 5 -1.315911
## 6 -1.950550
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given the station identification number (in the column stationID), time series of areal precipitation and streamflow discharge can be downloaded using the function &lt;code&gt;tsData60UK()&lt;/code&gt;. In the example below I show how to get the time series for the first station in the table, for a given temporal window.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Extract time series for the first station
stationID &amp;lt;- catalogueData60UK()$stationID[1]

# Extract time series for a specified temporal window
twindow &amp;lt;- seq(as.Date(&amp;quot;1988-01-01&amp;quot;), as.Date(&amp;quot;1989-12-31&amp;quot;), by = &amp;quot;days&amp;quot;)
MorwickTSplot &amp;lt;- tsData60UK(stationID = stationID,
                            plotOption = TRUE,
                            twindow = twindow)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-03-07-hddtools/unnamed-chunk-13-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The above figure is divided into two plots: precipitation over time (top) and river flow over time (bottom). High flows and related precipitation events seem to be clearly identifiable. The baseflow is around 10 m&lt;sup&gt;3&lt;/sup&gt;/s, but during the most important events the flow reached 120 m&lt;sup&gt;3&lt;/sup&gt;/s. The shape of the hydrograph, however, suggests that the flow was contained by the embankments and did not cause floods.&lt;/p&gt;

&lt;h4 id=&#34;mopex&#34;&gt;MOPEX&lt;/h4&gt;

&lt;p&gt;This source contains historical hydrometeorological data and river basin characteristics for hundreds of river basins and from a range of climates in the US. As with the previous dataset, &lt;code&gt;hddtools&lt;/code&gt; contains functions to download the catalogue and time series. The example below shows how to download the MOPEX catalogue and the time series for the first station in the catalogue.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# MOPEX full catalogue
allMOPEX &amp;lt;- catalogueMOPEX()

# Extract time series
BroadRiver &amp;lt;- tsMOPEX(stationID = allMOPEX$stationID[1],
                      plotOption = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-03-07-hddtools/unnamed-chunk-14-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the plots above P stands for precipitation, E for potential evapotranspiration, Q for river flow, T&lt;sub&gt;min&lt;/sub&gt; for minimum temperature and T&lt;sub&gt;max&lt;/sub&gt; for maximum temperature.&lt;/p&gt;

&lt;h3 id=&#34;sepa-river-level-data&#34;&gt;SEPA river level data&lt;/h3&gt;

&lt;p&gt;The Scottish Environment Protection Agency (SEPA) manages river level data for hundreds of gauging stations in the UK. The catalogue of stations was derived from this &lt;a href=&#34;http://pennine.ddns.me.uk/riverlevels/ConciseList.html&#34;&gt;list&lt;/a&gt;. The time series of the last few days is available from the SEPA website and can be downloaded using the following function &lt;code&gt;tsSEPA()&lt;/code&gt;, as in the example below. Plese note that this data is updated every 15 minutes and the code will always generate different plots.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# SEPA unofficial catalogue
allSEPA &amp;lt;- catalogueSEPA()

# Single time series extraction
Kilphedir &amp;lt;- tsSEPA(stationID = catalogueSEPA()$stationId[1],
                    plotOption = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-03-07-hddtools/unnamed-chunk-15-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;example-applications&#34;&gt;Example applications&lt;/h2&gt;

&lt;p&gt;There are a number of possible research applications for the &lt;code&gt;hddtools&lt;/code&gt;
package. Retrieved precipitation and streamflow data could, for
instance, be used to draw spatial trends, as done by (Vitolo et al.,
2016b) using the &lt;code&gt;rnrfa&lt;/code&gt; package but over larger areas. The package
could also be used to compare hydrological behaviours in different areas
of the world, to run and calibrate hydrological models such as fuse
(Clark et al., 2008, Vitolo et al. 2016a) as well as to undertake
regionalisation studies.&lt;/p&gt;

&lt;h2 id=&#34;future-developments&#34;&gt;Future developments&lt;/h2&gt;

&lt;p&gt;If you have suggestions, please add them to the &lt;a href=&#34;https://github.com/ropensci/hddtools/issues&#34;&gt;issue
tracker&lt;/a&gt; on github. Also,
feel free to contribute to the package sending a &lt;a href=&#34;https://github.com/ropensci/hddtools/pulls&#34;&gt;pull
request&lt;/a&gt;, that would be
greatly appreciated!&lt;/p&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m very grateful to &lt;a href=&#34;https://github.com/ledell&#34;&gt;Erin Le Dell&lt;/a&gt; and
&lt;a href=&#34;https://github.com/mdsumner&#34;&gt;Michael Sumner&lt;/a&gt; who reviewed, on behalf of
rOpenSci, the &lt;code&gt;hddtools&lt;/code&gt; package and the related paper published in the
Journal of Open Source Software (Vitolo, 2017). Both reviewers provided
very constructive suggestions that grately improved this package. I&amp;rsquo;d
like to also thank &lt;a href=&#34;http://ropensci.org/about/#staff&#34;&gt;Stefanie
Butland&lt;/a&gt; and &lt;a href=&#34;https://github.com/sckott&#34;&gt;Scott
Chamberlain&lt;/a&gt; for providing invaluable advice
when reviewing this post.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;Clark, M. P., Slater, A. G., Rupp, D. E., Woods, R. A., Vrugt, J. A.,
Gupta, H. V., Wagener, T. and Hay, L. E.: Framework for understanding
structural errors (fuse): A modular framework to diagnose differences
between hydrological models, Water Resources Research, 44(12), n/a–n/a,
doi:&lt;a href=&#34;https://doi.org/10.1029/2007WR006735&#34;&gt;10.1029/2007WR006735&lt;/a&gt;, 2008.&lt;/p&gt;

&lt;p&gt;Vitolo, C.: Hddtools: Hydrological data discovery tools, The Journal of
Open Source Software, 2(9),
doi:&lt;a href=&#34;https://doi.org/10.21105/joss.00056&#34;&gt;10.21105/joss.00056&lt;/a&gt;, 2017.&lt;/p&gt;

&lt;p&gt;Vitolo, C., Wells, P., Dobias, M. and Buytaert, W.: fuse: An R package
for ensemble Hydrological Modelling, The Journal of Open Source
Software, 1(8),
doi:&lt;a href=&#34;https://doi.org/10.21105/joss.00052&#34;&gt;10.21105/joss.00052&lt;/a&gt;, 2016a.&lt;/p&gt;

&lt;p&gt;Vitolo, C., Fry, M. and Buytaert, W.: rnrfa: An R package to Retrieve,
Filter and Visualize Data from the UK National River Flow Archive, The R
Journal, 8(2) [online] Available from:
&lt;a href=&#34;https://journal.r-project.org/archive/2016-2/vitolo-fry-buytaert.pdf&#34;&gt;https://journal.r-project.org/archive/2016-2/vitolo-fry-buytaert.pdf&lt;/a&gt;,
2016b.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
