<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Textmining on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/tags/textmining/</link>
    <description>Recent content in Textmining on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 05 Dec 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ropensci.org/tags/textmining/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exploratory Data Analysis of Ancient Texts with rperseus</title>
      <link>https://ropensci.org/blog/2017/12/05/rperseus/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/12/05/rperseus/</guid>
      <description>
        
        

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;When I was in grad school at Emory, I had a favorite desk in the library. The desk wasn’t particularly cozy or private, but what it lacked in comfort it made up for in real estate. My books and I needed room to operate. Students of the ancient world require many tools, and when jumping between commentaries, lexicons, and interlinears, additional clutter is additional “friction”, i.e., lapses in thought due to frustration. Technical solutions to this clutter exist, but the best ones are proprietary and expensive. Furthermore, they are somewhat inflexible, and you may have to shoehorn your thoughts into their framework. More friction.&lt;/p&gt;

&lt;p&gt;Interfacing with &lt;a href=&#34;http://www.perseus.tufts.edu/hopper/&#34;&gt;the Perseus Digital Library&lt;/a&gt; was a popular online alternative. The library includes a catalog of classical texts, a Greek and Latin lexicon, and a word study tool for appearances and references in other literature. If the university library’s reference copies of BDAG&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;em&gt;Synopsis Quattuor Evangeliorum&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; were unavailable, Perseus was our next best thing.&lt;/p&gt;

&lt;p&gt;Fast forward several years, and I’ve abandoned my quest to become a biblical scholar. Much to my father’s dismay, I’ve learned writing code is more fun than writing exegesis papers. Still, I enjoy dabbling with dead languages, and it was the desire to wed my two loves, biblical studies and R, that birthed my latest package, &lt;code&gt;rperseus&lt;/code&gt;. The goal of this package is to furnish classicists with texts of the ancient world and a toolkit to unpack them.&lt;/p&gt;

&lt;h3 id=&#34;exploratory-data-analysis-in-biblical-studies&#34;&gt;Exploratory Data Analysis in Biblical Studies&lt;/h3&gt;

&lt;p&gt;Working with the Perseus Digital Library was already a trip down memory lane, but here’s an example of how I would have leveraged &lt;code&gt;rperseus&lt;/code&gt; many years ago.&lt;/p&gt;

&lt;p&gt;My best papers often sprung from the outer margins of my &lt;a href=&#34;https://en.wikipedia.org/wiki/Novum_Testamentum_Graece&#34;&gt;&lt;em&gt;Nestle-Aland Novum Testamentum Graece.&lt;/em&gt;&lt;/a&gt; Here the editors inserted cross references to parallel vocabulary, themes, and even grammatical constructions. Given the intertextuality of biblical literature, the margins are a rich source of questions: Where else does the author use similar vocabulary? How is the source material used differently? Does the literary context affect our interpretation of a particular word? This is exploratory data analysis in biblical studies.&lt;/p&gt;

&lt;p&gt;Unfortunately the excitement of your questions is incommensurate with the tedium of the process&amp;ndash;EDA continues by flipping back and forth between books, dog-earring pages, and avoiding paper cuts. &lt;code&gt;rperseus&lt;/code&gt; aims to streamline this process with two functions: &lt;code&gt;get_perseus_text&lt;/code&gt; and &lt;code&gt;perseus_parallel&lt;/code&gt;. The former returns a data frame containing the text from any work in the Perseus Digital Library, and the latter renders a parallel in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Suppose I am writing a paper on different expressions of love in Paul’s letters. Naturally, I start in 1 Corinthians 13, the famed “Love Chapter” often heard at weddings and seen on bumper stickers. I finish the chapter and turn to the margins. In the image below, I see references to Colossians 1:4, 1 Thessalonians 1:3, 5:8, Hebrews 10:22-24, and Romans 8:35-39.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/nantg.png&#34; alt=&#34;&#34; /&gt;
&lt;em&gt;1 Corinithians 13 in Nestle-Aland Novum Testamentum Graece&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ignoring that some scholars exclude Colossians from the “authentic” letters, let’s see the references alongside each other:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rperseus) #devtools::install_github(“ropensci/rperseus”)
library(tidyverse)

tribble(
  ~label, ~excerpt,
  &amp;quot;Colossians&amp;quot;, &amp;quot;1.4&amp;quot;,
  &amp;quot;1 Thessalonians&amp;quot;, &amp;quot;1.3&amp;quot;,
  &amp;quot;1 Thessalonians&amp;quot;, &amp;quot;5.8&amp;quot;,
  &amp;quot;Romans&amp;quot;, &amp;quot;8.35-8.39&amp;quot;
  ) %&amp;gt;% 
  left_join(perseus_catalog) %&amp;gt;%
  filter(language == &amp;quot;grc&amp;quot;) %&amp;gt;%
  select(urn, excerpt) %&amp;gt;%
  pmap_df(get_perseus_text) %&amp;gt;%
  perseus_parallel(words_per_row = 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/Parallel1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A brief explanation: First, I specify the labels and excerpts within a tibble. Second, I join the lazily loaded &lt;code&gt;perseus_catalog&lt;/code&gt; onto the data frame. Third, I filter for the Greek&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; and select the columns containing the arguments required for &lt;code&gt;get_perseus_text&lt;/code&gt;. Fourth, I map over each urn and excerpt, returning another data frame. Finally, I pipe the output into &lt;code&gt;perseus_parallel&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The key word shared by each passage is &lt;em&gt;agape&lt;/em&gt; (“love”). Without going into detail, it might be fruitful to consider the references alongside each other, pondering how the semantic range of &lt;em&gt;agape&lt;/em&gt; expands or contracts within the Pauline corpus. Paul had a penchant for appropriating and recasting old ideas&amp;ndash;often in slippery and unexpected ways&amp;ndash;and your Greek lexicon provides a mere approximation. In other words, how can we move from the dictionary definition of &lt;em&gt;agape&lt;/em&gt; towards Paul&amp;rsquo;s unique vision?&lt;/p&gt;

&lt;p&gt;If your Greek is rusty, you can parse each word with &lt;code&gt;parse_excerpt&lt;/code&gt; by locating the text&amp;rsquo;s urn within the &lt;code&gt;perseus_catalog&lt;/code&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parse_excerpt(urn = &amp;quot;urn:cts:greekLit:tlg0031.tlg012.perseus-grc2&amp;quot;, excerpt = &amp;quot;1.4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;form&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;verse&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;part_of_speech&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;person&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;number&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;tense&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;mood&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;voice&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;gender&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;case&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;degree&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ἀκούω&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ἀκούσαντες&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;verb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;plural&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aorist&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;participle&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;active&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;masculine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;nominative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ὁ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;τὴν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;article&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;singular&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;feminine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;accusative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;πίστις&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;πίστιν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;noun&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;singular&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;feminine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;accusative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ὑμός&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ὑμῶν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;pronoun&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;plural&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;masculine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;genative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If your Greek is &lt;em&gt;really&lt;/em&gt; rusty, you can also flip the &lt;code&gt;language&lt;/code&gt; filter to “eng” to view an older English translation.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; And if the margin references a text from the Old Testament, you can call the Septuagint as well as the original Hebrew.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tribble(
  ~label, ~excerpt,
  &amp;quot;Genesis&amp;quot;, &amp;quot;32.31&amp;quot;,
  &amp;quot;Genesis, pointed&amp;quot;, &amp;quot;32.31&amp;quot;,
  &amp;quot;Numeri&amp;quot;, &amp;quot;12.8&amp;quot;,
  &amp;quot;Numbers, pointed&amp;quot;, &amp;quot;12.8&amp;quot;
  ) %&amp;gt;% 
  left_join(perseus_catalog) %&amp;gt;%
  filter(language %in% c(&amp;quot;grc&amp;quot;, &amp;quot;hpt&amp;quot;)) %&amp;gt;%
  select(urn, excerpt) %&amp;gt;%
  pmap_df(get_perseus_text) %&amp;gt;%
  perseus_parallel()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/Parallel2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Admittedly, there is some “friction” here in joining the &lt;code&gt;perseus_catalog&lt;/code&gt; onto the initial tibble. There is a learning curve with getting acquainted with the idiosyncrasies of the catalog object. A later release will aim to streamline this workflow.&lt;/p&gt;

&lt;h3 id=&#34;future-work&#34;&gt;Future Work&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://ropensci.github.io/rperseus/articles/rperseus-vignette.html&#34;&gt;Check the vignette&lt;/a&gt; for a more general overview of &lt;code&gt;rperseus&lt;/code&gt;. In the meantime, I look forward to getting more intimately acquainted with the Perseus Digital Library. Tentative plans to extend &lt;code&gt;rperseus&lt;/code&gt; a Shiny interface to further reduce “friction” and a method of creating a “book” of custom parallels with &lt;code&gt;bookdown&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;I want to thank my two rOpenSci reviewers, &lt;a href=&#34;https://www.ildiczeller.com/&#34;&gt;Ildikó Czeller&lt;/a&gt; and &lt;a href=&#34;https://francoismichonneau.net/&#34;&gt;François Michonneau,&lt;/a&gt; for coaching me through the review process. They were the first two individuals to ever scrutinize my code, and I was lucky to hear their feedback. rOpenSci onboarding is truly a wonderful process.&lt;/p&gt;

&lt;!-- references --&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Bauer, Walter. &lt;em&gt;A Greek-English Lexicon of the New Testament and Other Early Christian Literature.&lt;/em&gt; Edited by Frederick W. Danker. 3rd ed. Chicago: University of Chicago Press, 2000.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Aland, Kurt. &lt;em&gt;Synopsis Quattuor Evangeliorum.&lt;/em&gt; Deutsche Bibelgesellschaft, 1997.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;The Greek text from the Perseus Digital Library is from 1885 standards. The advancement of textual criticism in the 20th century led to a more stable text you would find in current editions of the Greek New Testament.&lt;br /&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;The English translation is from Rainbow Missions, Inc. &lt;em&gt;World English Bible.&lt;/em&gt; Rainbow Missions, Inc.; revision of the American Standard Version of 1901. I’ve toyed with the idea of incorporating more modern translations, but that would require require resources beyond the Perseus Digital Library.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;&amp;ldquo;hpt&amp;rdquo; is the pointed Hebrew text from &lt;em&gt;Codex Leningradensis.&lt;/em&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>googleLanguageR - Analysing language through the Google Cloud Machine Learning APIs</title>
      <link>https://ropensci.org/blog/2017/10/03/googlelanguager/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/03/googlelanguager/</guid>
      <description>
        
        

&lt;!-- open source image taken from: https://upload.wikimedia.org/wikipedia/commons/2/21/Bell_System_switchboard.jpg --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/switchboard.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the greatest assets human beings possess is the power of speech and language, from which almost all our other accomplishments flow. To be able to analyse communication offers us a chance to gain a greater understanding of one another.&lt;/p&gt;

&lt;p&gt;To help you with this, &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt;&lt;/a&gt; is an R package that allows you to perform speech-to-text transcription, neural net translation and natural language processing via the &lt;a href=&#34;https://cloud.google.com/products/machine-learning/&#34;&gt;Google Cloud machine learning services&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An introduction to the package is below, but you can find out more details at the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-s-bet&#34;&gt;Google&amp;rsquo;s bet&lt;/h3&gt;

&lt;p&gt;Google predicts that machine learning is to be a fundamental feature of business, and so they are looking to become the infrastructure that makes machine learning possible. Metaphorically speaking: If machine learning is electricity, then Google wants to be the pylons carrying it around the country.&lt;/p&gt;

&lt;!-- open source image taken from: https://pixabay.com/en/pylon-sky-electricity-tower-2515429/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/pylon.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Google may not be the only company with such ambitions, but one advantage Google has is the amount of data it possesses. Twenty years of web crawling has given it an unprecedented corpus to train its models.  In addition, its recent moves into voice and video gives it one of the biggest audio and speech datasets, all of which have been used to help create machine learning applications within its products such as search and Gmail. Further investment in machine learning is shown by Google&amp;rsquo;s purchase of &lt;a href=&#34;https://deepmind.com/&#34;&gt;Deepmind&lt;/a&gt;, a UK based A.I. research firm that recently was in the news for defeating the top Go champion with its neural network trained Go bot.  Google has also taken an open-source route with the creation and publication of &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt;, a leading machine learning framework.&lt;/p&gt;

&lt;p&gt;Whilst you can create your own machine learning models, for those users who haven&amp;rsquo;t the expertise, data or time to do so, Google also offers an increasing range of machine learning APIs that are pre-trained, such as image and video recognition or job search.  &lt;code&gt;googleLanguageR&lt;/code&gt; wraps the subset of those machine learning APIs that are language flavoured - Cloud Speech, Translation and Natural Language.&lt;/p&gt;

&lt;p&gt;Since they carry complementary outputs that can be used in each other&amp;rsquo;s input, all three of the APIs are included in one package. For example, you can transcribe a recording of someone speaking in Danish, translate that to English and then identify how positive or negative the writer felt about its content (sentiment analysis) then identify the most important concepts and objects within the content (entity analysis).&lt;/p&gt;

&lt;h3 id=&#34;motivations&#34;&gt;Motivations&lt;/h3&gt;

&lt;h4 id=&#34;fake-news&#34;&gt;Fake news&lt;/h4&gt;

&lt;p&gt;One reason why I started looking at this area was the growth of &amp;lsquo;fake news&amp;rsquo;, and its effect on political discourse on social media. I wondered if there was some way to put metrics on how much a news story fuelled one&amp;rsquo;s own bias within your own filter bubble.  The entity API provides a way to perform entity and sentiment analysis at scale on tweets, and by then comparing different users and news sources preferences the hope is to be able to judge how much they are in agreement with your own bias, views and trusted reputation sources.&lt;/p&gt;

&lt;h4 id=&#34;make-your-own-alexa&#34;&gt;Make your own Alexa&lt;/h4&gt;

&lt;p&gt;Another motivating application is the growth of voice commands that will become the primary way of user interface with technology.  Already, &lt;a href=&#34;https://www.thinkwithgoogle.com/data-gallery/detail/google-app-voice-search/&#34;&gt;Google reports up to 20% of search in its app&lt;/a&gt; is via voice search.  I&amp;rsquo;d like to be able to say &amp;ldquo;R, print me out that report for client X&amp;rdquo;.  A Shiny app that records your voice, uploads to the API then parses the return text into actions gives you a chance to create your very own Alexa-like infrastructure.&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;text-align:center&#34;&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/alexa.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The voice activated internet connected speaker, Amazon&amp;rsquo;s Alexa - image from www.amazon.co.uk&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;translate-everything&#34;&gt;Translate everything&lt;/h4&gt;

&lt;p&gt;Finally, I live and work in Denmark.  As Danish is only spoken by less than 6 million people, applications that work in English may not be available in Danish very quickly, if at all.  The API&amp;rsquo;s translation service is the one that made the news in 2016 for &lt;a href=&#34;https://research.googleblog.com/2016/09/a-neural-network-for-machine.html&#34;&gt;&amp;ldquo;inventing its own language&amp;rdquo;&lt;/a&gt;, and offers much better English to Danish translations that the free web version and may make services available in Denmark sooner.&lt;/p&gt;

&lt;h3 id=&#34;using-the-library&#34;&gt;Using the library&lt;/h3&gt;

&lt;p&gt;To use these APIs within R, you first need to do a one-time setup to create a Google Project, add a credit card and authenticate which is &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/#installation&#34;&gt;detailed on the package website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After that, you feed in the R objects you want to operate upon.  The &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/127&#34;&gt;rOpenSci review&lt;/a&gt; helped to ensure that this can scale up easily, so that you can feed in large character vectors which the library will parse and rate limit as required.  The functions also work within &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; pipe syntax.&lt;/p&gt;

&lt;h4 id=&#34;speech-to-text&#34;&gt;Speech-to-text&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/speech.html&#34;&gt;Cloud Speech API&lt;/a&gt; is exposed via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_speech.html&#34;&gt;&lt;code&gt;gl_speech&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It supports multiple audio formats and languages, and you can either feed a sub-60 second audio file directly, or perform asynchrnous requests for longer audio files.&lt;/p&gt;

&lt;p&gt;Example code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)

my_audio &amp;lt;- &amp;quot;my_audio_file.wav&amp;quot;
gl_speech(my_audio)
#  A tibble: 1 x 3
#  transcript confidence                 words
#* &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;                &amp;lt;list&amp;gt;
#1 Hello Mum  0.9227779 &amp;lt;data.frame [19 x 3]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;translation&#34;&gt;Translation&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/translation.html&#34;&gt;Cloud Translation API&lt;/a&gt; lets you translate text via &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_translate.html&#34;&gt;&lt;code&gt;gl_translate&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you are charged per character, one tip here if you are working with lots of different languages is to perform detection of language offline first using another rOpenSci package, &lt;a href=&#34;https://github.com/ropensci/cld2&#34;&gt;&lt;code&gt;cld2&lt;/code&gt;&lt;/a&gt;.  That way you can avoid charges for text that is already in your target language i.e. English.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
library(cld2)
library(purrr)

my_text &amp;lt;- c(&amp;quot;Katten sidder på måtten&amp;quot;, &amp;quot;The cat sat on the mat&amp;quot;)

## offline detect language via cld2
detected &amp;lt;- map_chr(my_text, detect_language)
# [1] &amp;quot;DANISH&amp;quot;  &amp;quot;ENGLISH&amp;quot;

## get non-English text
translate_me &amp;lt;- my_text[detected != &amp;quot;ENGLISH&amp;quot;]

## translate
gl_translate(translate_me)
## A tibble: 1 x 3
#                 translatedText detectedSourceLanguage                    text
#*                         &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;
#1 The cat is sitting on the mat                     da Katten sidder på måtten
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/nlp.html&#34;&gt;Natural Language API&lt;/a&gt; reveals the structure and meaning of text, accessible via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_nlp.html&#34;&gt;&lt;code&gt;gl_nlp&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It returns several analysis:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Entity analysis&lt;/em&gt; - finds named entities (currently proper names and common nouns) in the text along with entity types, salience, mentions for each entity, and other properties. If possible, will also return metadata about that entity such as a Wikipedia URL.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Syntax&lt;/em&gt; - analyzes the syntax of the text and provides sentence boundaries and tokenization along with part of speech tags, dependency trees, and other properties.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sentiment&lt;/em&gt; - the overall sentiment of the text, represented by a magnitude [0, +inf] and score between -1.0 (negative sentiment) and 1.0 (positive sentiment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all useful to get an understanding of the meaning of a sentence, and has potentially the greatest number of applications of the APIs featured.  With entity analysis, auto categorisation of text is possible; the syntax returns let you pull out nouns and verbs for parsing into other actions; and the sentiment analysis allows you to get a feeling for emotion within text.&lt;/p&gt;

&lt;p&gt;A demonstration is below which gives an idea of what output you can generate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
quote &amp;lt;- &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
nlp &amp;lt;- gl_nlp(quote)

str(nlp)
#List of 6
# $ sentences        :List of 1
#  ..$ :&#39;data.frame&#39;:	1 obs. of  4 variables:
#  .. ..$ content    : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
#  .. ..$ beginOffset: int 0
#  .. ..$ magnitude  : num 0.6
#  .. ..$ score      : num -0.6
# $ tokens           :List of 1
#  ..$ :&#39;data.frame&#39;:	20 obs. of  17 variables:
#  .. ..$ content       : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;things&amp;quot; &amp;quot;are&amp;quot; &amp;quot;infinite&amp;quot; ...
#  .. ..$ beginOffset   : int [1:20] 0 4 11 15 23 25 29 38 42 48 ...
#  .. ..$ tag           : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NOUN&amp;quot; &amp;quot;VERB&amp;quot; &amp;quot;ADJ&amp;quot; ...
#  .. ..$ aspect        : chr [1:20] &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; ...
#  .. ..$ case          : chr [1:20] &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; ...
#  .. ..$ form          : chr [1:20] &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; ...
#  .. ..$ gender        : chr [1:20] &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; ...
#  .. ..$ mood          : chr [1:20] &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;INDICATIVE&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; ...
#  .. ..$ number        : chr [1:20] &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;PLURAL&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; ...
#  .. ..$ person        : chr [1:20] &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; ...
#  .. ..$ proper        : chr [1:20] &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; ...
#  .. ..$ reciprocity   : chr [1:20] &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; ...
#  .. ..$ tense         : chr [1:20] &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;PRESENT&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; ...
#  .. ..$ voice         : chr [1:20] &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; ...
#  .. ..$ headTokenIndex: int [1:20] 1 2 2 2 2 6 2 6 9 6 ...
#  .. ..$ label         : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NSUBJ&amp;quot; &amp;quot;ROOT&amp;quot; &amp;quot;ACOMP&amp;quot; ...
#  .. ..$ value         : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;thing&amp;quot; &amp;quot;be&amp;quot; &amp;quot;infinite&amp;quot; ...
# $ entities         :List of 1
#  ..$ :Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	6 obs. of  9 variables:
#  .. ..$ name         : chr [1:6] &amp;quot;human stupidity&amp;quot; &amp;quot;things&amp;quot; &amp;quot;universe&amp;quot; &amp;quot;universe&amp;quot; ...
#  .. ..$ type         : chr [1:6] &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; ...
#  .. ..$ salience     : num [1:6] 0.1662 0.4771 0.2652 0.2652 0.0915 ...
#  .. ..$ mid          : Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ wikipedia_url: Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ magnitude    : num [1:6] NA NA NA NA NA NA
#  .. ..$ score        : num [1:6] NA NA NA NA NA NA
#  .. ..$ beginOffset  : int [1:6] 42 4 29 86 29 86
#  .. ..$ mention_type : chr [1:6] &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; ...
# $ language         : chr &amp;quot;en&amp;quot;
# $ text             : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
# $ documentSentiment:Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	1 obs. of  2 variables:
#  ..$ magnitude: num 0.6
#  ..$ score    : num -0.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;This package is 10 times better due to the efforts of the rOpenSci reviewers &lt;a href=&#34;http://enpiar.com/&#34;&gt;Neal Richardson&lt;/a&gt; and &lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt;, who have whipped the documentation, outputs and test cases into the form they are today in &lt;code&gt;0.1.0&lt;/code&gt;.  Many thanks to them.&lt;/p&gt;

&lt;p&gt;Hopefully, this is just the beginning and the package can be further improved by its users - if you do give the package a try and find a potential improvement, &lt;a href=&#34;https://github.com/ropensci/googleLanguageR/issues&#34;&gt;raise an issue on GitHub&lt;/a&gt; and we can try to implement it.  I&amp;rsquo;m excited to see what users can do with these powerful tools.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Topic modeling in R</title>
      <link>https://ropensci.org/blog/2014/04/16/topic-modeling-in-r/</link>
      <pubDate>Wed, 16 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2014/04/16/topic-modeling-in-r/</guid>
      <description>
        
        

&lt;p&gt;&lt;em&gt;Editor&amp;rsquo;s note: This is the first in a series of posts from rOpenSci&amp;rsquo;s &lt;a href=&#34;http://ropensci.github.io/hackathon/&#34;&gt;recent hackathon&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I recently had the pleasure of participating in &lt;a href=&#34;https://github.com/ropensci/hackathon/&#34;&gt;rOpenSci&amp;rsquo;s hackathon&lt;/a&gt;. To be honest, I was quite nervous to work among such notables, but I immediately felt welcome thanks to a warm and personable group. &lt;a href=&#34;http://alyssafrazee.com/&#34;&gt;Alyssa Frazee&lt;/a&gt; has a &lt;a href=&#34;http://simplystatistics.org/2014/04/10/the-ropensci-hackathon-ropenhack/&#34;&gt;great post summarizing the event&lt;/a&gt;, so check that out if you haven&amp;rsquo;t already. Once again, many thanks to rOpenSci for making it possible!&lt;/p&gt;

&lt;p&gt;In addition to learning and socializing at the hackathon, I wanted to ensure my time was productive, so I worked on a mini-project related to my research in text mining. rOpenSci has plethora of &lt;a href=&#34;http://ropensci.org/packages/index.html&#34;&gt;R packages&lt;/a&gt; for extracting literary content off the web, including &lt;a href=&#34;https://github.com/ropensci/elife&#34;&gt;elife&lt;/a&gt;, which is a lightweight interface to the &lt;a href=&#34;http://dev.elifesciences.org/&#34;&gt;elife API&lt;/a&gt;. This package is not yet available on CRAN, but we can easily install from GitHub thanks to devtools.&lt;/p&gt;

&lt;h4 id=&#34;installing-the-package&#34;&gt;Installing the package&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;library(devtools)
install_github(&amp;quot;ropensci/elife&amp;quot;)
library(elife)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;brief-overview-of-topic-models&#34;&gt;Brief Overview of Topic Models&lt;/h4&gt;

&lt;p&gt;My research in text mining is focused on a particular type of &lt;a href=&#34;http://en.wikipedia.org/wiki/Topic_model&#34;&gt;topic model&lt;/a&gt; known as &lt;a href=&#34;http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#34;&gt;Latent Dirichlet Allocation&lt;/a&gt; (LDA). In general, a topic model discovers topics (e.g., hidden themes) within a collection of documents. For example, if a given document is generated from a hypothetical &amp;ldquo;statistics topic&amp;rdquo;, there might be a 10% chance a given word in that document is &amp;ldquo;model&amp;rdquo;, a 5% chance that word is &amp;ldquo;probability&amp;rdquo;, a 1% that word is &amp;ldquo;algorithm&amp;rdquo;, etc. Whereas, if a document is generated from a hypothetical &amp;ldquo;computer science topic&amp;rdquo;, there might be a 4% chance a given word in that document is &amp;ldquo;model&amp;rdquo;, a 2% chance that word is &amp;ldquo;probability&amp;rdquo;, a 16% that word is &amp;ldquo;algorithm&amp;rdquo;, etc. In other words, each topic is defined by a probability mass function over each possible word.&lt;/p&gt;

&lt;p&gt;LDA takes this example one step further and allows for each document to be generated from a mixture of topics. For example, a particular document could be 60% statistics, 10% computer science, 20% mathematics, etc. Whereas, a different document could be 30% statistics, 30% computer science, 15% mathematics, etc. Within the LDA literature, fitting models to abstracts of academic articles is quite common, so I thought it would be neat to do the same with abstracts from elife articles.&lt;/p&gt;

&lt;h4 id=&#34;get-all-the-elife-abstracts&#34;&gt;Get all the elife abstracts!&lt;/h4&gt;

&lt;p&gt;In order to grab all the abstracts, first we&amp;rsquo;ll grab all the &lt;a href=&#34;http://en.wikipedia.org/wiki/Digital_object_identifier&#34;&gt;DOIs&lt;/a&gt; that point to currently available articles. Note that we can do more complicated queries of specific articles with &lt;code&gt;searchelife&lt;/code&gt; (the &lt;code&gt;help(searchelife)&lt;/code&gt; page has some nice examples).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dois &amp;lt;- searchelife(terms = &amp;quot;*&amp;quot;, searchin = &amp;quot;article_title&amp;quot;, boolean = &amp;quot;matches&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;dois&lt;/code&gt; can now be used to obtain all sorts of meta data associated with these articles using &lt;code&gt;elife_doi&lt;/code&gt;. In this case, I just want the abstracts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;abs &amp;lt;- sapply(dois, function(x) elife_doi(x, ret = &amp;quot;abstract&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here, we have what we need to fit the topic model. I don&amp;rsquo;t want to focus on technical details here, but if you are interested in the statistics involved, I recommend reading &lt;a href=&#34;http://cpsievert.github.io/xkcd/&#34;&gt;my post on xkcd comics&lt;/a&gt;. This post also covers the method I use to determine an optimal number of topics. I&amp;rsquo;ve provided all the code used to fit the model &lt;a href=&#34;https://github.com/cpsievert/cpsievert.github.com/blob/master/elife/elife.R&#34;&gt;here&lt;/a&gt;, but let&amp;rsquo;s skip to the fun part and jump right into exploring the model output.&lt;/p&gt;

&lt;p&gt;The window below is an interactive visualization of the LDA output derived from elife abstracts. The aim of this visualization is to aid interpretation of topics. Topic interpretation tends to be difficult since each topic is defined by a probability distribution with support over many of words. With this interactive visualization, one can focus on the most &amp;ldquo;relevant&amp;rdquo; words for any topic by hovering/clicking over the appropriate circle. We will define &amp;ldquo;relevance&amp;rdquo; shortly, but for now, go ahead and click on the circle towards the bottom labeled &amp;ldquo;11&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://gallery.shinyapps.io/LDAelife/&#34; target=&#34;_blank&#34;&gt;Go To Application&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;&lt;div class=&#34;col-sm-10 col-lg-10&#34;&gt;
&lt;iframe src=&#34;https://gallery.shinyapps.io/LDAelife/&#34; width=&#34;1200&#34; height = &#34;800&#34;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;div class=&#34;col-sm-8 col-sm-offset-2&#34;&gt;&lt;/p&gt;

&lt;p&gt;Now that topic 11 is selected, in the bar chart to the right, we see &amp;ldquo;relat&amp;rdquo;, &amp;ldquo;evolut&amp;rdquo;, and &amp;ldquo;similar&amp;rdquo; are the top 3 most relevant words. Towards the bottom of the bar chart, we see &amp;ldquo;resid&amp;rdquo; which is the 30th most relevant. Note that before the model was fit &lt;a href=&#34;http://en.wikipedia.org/wiki/Stemming&#34;&gt;stemming&lt;/a&gt; was performed. Thus, a word like &amp;ldquo;relat&amp;rdquo; could stand for &amp;ldquo;relation&amp;rdquo;, &amp;ldquo;relations&amp;rdquo;, &amp;ldquo;relationship&amp;rdquo;, etc. You might now be thinking: &amp;ldquo;That&amp;rsquo;s great, I can see this topic is related to evolutionary biology, but why are these words ranked in this order?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;The topic specific word rankings are determined by a measure known as &lt;em&gt;relevance&lt;/em&gt;. Relevance is a compromise between the probability of a word given the topic (the width of the red bars) and the probability within topic divided by the overall frequency of the word (the ratio of red to gray). Note the &amp;ldquo;Value of lambda&amp;rdquo; slider which controls this compromise. A value of 1 for lambda will rank words solely on the width of the red bars (which tends to over-rank common words). A value of 0 for lambda will rank words solely on the ratio of red to gray (which tends to over-rank rare words). A recent study has shown evidence for an optimal value of lambda around 0.6 which is the default value.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;topic landscape&amp;rdquo; on the left-hand side provides a sense of topic similarity by approximating distances between topics. To produce the point locations, pairwise distances between topic specific word distributions are computed according to the currently selected measure in the &amp;ldquo;Topical Distance Calculation&amp;rdquo; menu. Those distances are next scaled down to two dimensions using the currently select algorithm in the &amp;ldquo;Multidimensional Scaling Method&amp;rdquo; menu. By default, the circle sizes are proportional to the prevalence of each topic in the collection of text.&lt;/p&gt;

&lt;p&gt;Hovering over labels on the bar chart allows us to explore different contexts for the same word. Upon hovering over a word, circles in the topic landscape will change according to the distribution over topics for that given word. For example, if we hover over &amp;ldquo;evolut&amp;rdquo; (the 2nd most relevant word for topic 11 when lambda is 0.6), we see that it has large mass under topic 11 and 20. If we now click on topic 20, the most relevant words suggest this is a &amp;ldquo;population genetics&amp;rdquo; topic. Interestingly, if we hover over &amp;ldquo;adapt&amp;rdquo; (the 29th most relevant word for topic 20 when lambda is 0.6), we see that it has large mass under topic 1 and 20. Now, if we hover over topic 1, we see a &amp;ldquo;general life science&amp;rdquo; topic.&lt;/p&gt;

&lt;p&gt;There are certainly many other things to discover using this interactive visualization. I hope you take the time to explore and leave a comment with findings or questions below. If you are interested in text mining, or open science in general, definitely check out &lt;a href=&#34;http://ropensci.org/packages/index.html&#34;&gt;rOpenSci&amp;rsquo;s R packages&lt;/a&gt;. If you&amp;rsquo;d like to make a similar interactive visualization, check out the &lt;a href=&#34;https://github.com/cpsievert/LDAvis/&#34;&gt;LDAvis&lt;/a&gt; package.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>solr - an R interface to Solr</title>
      <link>https://ropensci.org/blog/2014/01/27/solr/</link>
      <pubDate>Mon, 27 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2014/01/27/solr/</guid>
      <description>
        
        

&lt;p&gt;A number of the APIs we interact with (e.g., PLOS full text API, and USGS&amp;rsquo;s BISON API in &lt;a href=&#34;http://cran.r-project.org/web/packages/rplos/index.html&#34;&gt;rplos&lt;/a&gt; and &lt;a href=&#34;http://cran.r-project.org/web/packages/rbison/index.html&#34;&gt;rbison&lt;/a&gt;, respectively) expose &lt;a href=&#34;http://lucene.apache.org/solr/&#34;&gt;Solr&lt;/a&gt; endpoints. &lt;a href=&#34;http://lucene.apache.org/solr/&#34;&gt;Solr&lt;/a&gt; is an Apache hosted project - it is a powerful search server.  Given that at least two, and possibly more in the future, of the data providers we interact with provide Solr endpoints, it made sense to create an R package to make robust functions to interact with Solr that work across any Solr endpoint. This is then useful to us, and hopefully others.&lt;/p&gt;

&lt;p&gt;The following are a few examples covering some of things you can do in Solr that fall in to six categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Search: via &lt;code&gt;solr_search&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Grouping: via &lt;code&gt;solr_group&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Faceting: via &lt;code&gt;solr_facet&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Highlighting: via &lt;code&gt;solr_highlight&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Stats: via &lt;code&gt;solr_stats&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;More like this: via &lt;code&gt;solr_mlt&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;solr&lt;/code&gt; package generally has two steps for any query: a) send the request given your inputs, and b) parse the output into a useful R data structure. Part a) is quite easy. However, part b) is harder. We are working hard on making parsers that are as general as possible for each of the data formats that are returned by group, facet, highlight, etc., but of course we will still definitely fail in many cases. Please do submit bug reports to &lt;a href=&#34;https://github.com/ropensci/solr/issues?state=open&#34;&gt;our issue tracker&lt;/a&gt; so we can make the parsers work better.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;solr&lt;/code&gt; is on CRAN, so you can install the more stable version there, and some dependencies.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;solr&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can install the development version from Github as follows.  Below we&amp;rsquo;ll use the Github version - most of below is available in the CRAN version too, except &lt;code&gt;solr_group&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;ropensci/solr&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Load the library&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;solr&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;define-url-endpoint-and-key&#34;&gt;Define url endpoint and key&lt;/h2&gt;

&lt;p&gt;As &lt;code&gt;solr&lt;/code&gt; is a general interface to Solr endpoints, you need to define the url. Here, we&amp;rsquo;ll work with the Public Library of Science full text search API (docs &lt;a href=&#34;http://api.plos.org/&#34;&gt;here&lt;/a&gt;). Some Solr endpoints will require authentication - I should note that we don&amp;rsquo;t yet handle authentication schemes other than passing in a key in the url, but that&amp;rsquo;s on the to do list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;url &amp;lt;- &#39;http://api.plos.org/search&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;search&#34;&gt;Search&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_search(q=&#39;*:*&#39;, rows=2, fl=&#39;id&#39;, base=url)
#&amp;gt;                                                              id
#&amp;gt; 1       10.1371/annotation/c313df3a-52bd-4cbe-af14-6676480d1a43
#&amp;gt; 2 10.1371/annotation/c313df3a-52bd-4cbe-af14-6676480d1a43/title
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Search for words &amp;ldquo;sports&amp;rdquo; and &amp;ldquo;alcohol&amp;rdquo; within seven words of each other&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_search(q=&#39;everything:&amp;quot;sports alcohol&amp;quot;~7&#39;, fl=&#39;title&#39;, rows=3, base=url)
#&amp;gt;                                                                                                                                                                         title
#&amp;gt; 1                                      Alcohol Ingestion Impairs Maximal Post-Exercise Rates of Myofibrillar Protein Synthesis following a Single Bout of Concurrent Training
#&amp;gt; 2 “Like Throwing a Bowling Ball at a Battle Ship” Audience Responses to Australian News Stories about Alcohol Pricing and Promotion Policies: A Qualitative Focus Group Study
#&amp;gt; 3                                            Development and Validation of a Risk Score Predicting Substantial Weight Gain over 5 Years in Middle-Aged European Men and Women
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;groups&#34;&gt;Groups&lt;/h2&gt;

&lt;p&gt;Most recent publication by journal&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_group(q=&#39;*:*&#39;, group.field=&#39;journal&#39;, rows=5, group.limit=1, group.sort=&#39;publication_date desc&#39;, fl=&#39;publication_date, score&#39;, base=url)
#&amp;gt;       groupValue numFound start     publication_date score
#&amp;gt; 1       plos one   931323     0 2014-11-24T00:00:00Z     1
#&amp;gt; 2  plos genetics    40603     0 2014-11-20T00:00:00Z     1
#&amp;gt; 3  plos medicine    18514     0 2014-11-18T00:00:00Z     1
#&amp;gt; 4 plos pathogens    35497     0 2014-11-24T00:00:00Z     1
#&amp;gt; 5   plos biology    26133     0 2014-11-18T00:00:00Z     1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First publication by journal&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_group(q=&#39;*:*&#39;, group.field=&#39;journal&#39;, group.limit=1, group.sort=&#39;publication_date asc&#39;, fl=&#39;publication_date, score&#39;, fq=&amp;quot;publication_date:[1900-01-01T00:00:00Z TO *]&amp;quot;, base=url)
#&amp;gt;                          groupValue numFound start     publication_date
#&amp;gt; 1                          plos one   931323     0 2006-12-01T00:00:00Z
#&amp;gt; 2                     plos genetics    40603     0 2005-06-17T00:00:00Z
#&amp;gt; 3                     plos medicine    18514     0 2004-09-07T00:00:00Z
#&amp;gt; 4                    plos pathogens    35497     0 2005-07-22T00:00:00Z
#&amp;gt; 5                      plos biology    26133     0 2003-08-18T00:00:00Z
#&amp;gt; 6                              none    57566     0 2005-08-23T00:00:00Z
#&amp;gt; 7        plos computational biology    29838     0 2005-06-24T00:00:00Z
#&amp;gt; 8  plos neglected tropical diseases    25119     0 2007-08-30T00:00:00Z
#&amp;gt; 9              plos clinical trials      521     0 2006-04-21T00:00:00Z
#&amp;gt; 10                     plos medicin        9     0 2012-04-17T00:00:00Z
#&amp;gt;    score
#&amp;gt; 1      1
#&amp;gt; 2      1
#&amp;gt; 3      1
#&amp;gt; 4      1
#&amp;gt; 5      1
#&amp;gt; 6      1
#&amp;gt; 7      1
#&amp;gt; 8      1
#&amp;gt; 9      1
#&amp;gt; 10     1
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;facet&#34;&gt;Facet&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_facet(q=&#39;*:*&#39;, facet.field=&#39;journal&#39;, facet.query=&#39;cell,bird&#39;, base=url)
#&amp;gt; $facet_queries
#&amp;gt;        term value
#&amp;gt; 1 cell,bird    17
#&amp;gt;
#&amp;gt; $facet_fields
#&amp;gt; $facet_fields$journal
#&amp;gt;                                 X1     X2
#&amp;gt; 1                         plos one 931323
#&amp;gt; 2                    plos genetics  40603
#&amp;gt; 3                   plos pathogens  35497
#&amp;gt; 4       plos computational biology  29838
#&amp;gt; 5                     plos biology  26133
#&amp;gt; 6 plos neglected tropical diseases  25119
#&amp;gt; 7                    plos medicine  18514
#&amp;gt; 8             plos clinical trials    521
#&amp;gt; 9                     plos medicin      9
#&amp;gt;
#&amp;gt;
#&amp;gt; $facet_dates
#&amp;gt; NULL
#&amp;gt;
#&amp;gt; $facet_ranges
#&amp;gt; NULL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Range faceting with &amp;gt; 1 field&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head( solr_facet(q=&#39;*:*&#39;, base=url, facet.range=&#39;alm_twitterCount&#39;, facet.range.start=5, facet.range.end=1000, facet.range.gap=10)$facet_ranges$alm_twitterCount )
#&amp;gt;   X1    X2
#&amp;gt; 1  5 60938
#&amp;gt; 2 15 13668
#&amp;gt; 3 25  6379
#&amp;gt; 4 35  2952
#&amp;gt; 5 45  2297
#&amp;gt; 6 55  1497
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;highlight&#34;&gt;Highlight&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_highlight(q=&#39;alcohol&#39;, hl.fl = &#39;abstract&#39;, rows=2, base = url)
#&amp;gt; $`10.1371/journal.pmed.0040151`
#&amp;gt; $`10.1371/journal.pmed.0040151`$abstract
#&amp;gt; [1] &amp;quot;Background: &amp;lt;em&amp;gt;Alcohol&amp;lt;/em&amp;gt; consumption causes an estimated 4% of the global disease burden, prompting&amp;quot;
#&amp;gt;
#&amp;gt;
#&amp;gt; $`10.1371/journal.pone.0027752`
#&amp;gt; $`10.1371/journal.pone.0027752`$abstract
#&amp;gt; [1] &amp;quot;Background: The negative influences of &amp;lt;em&amp;gt;alcohol&amp;lt;/em&amp;gt; on TB management with regard to delays in seeking&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;stats&#34;&gt;Stats&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_stats(q=&#39;ecology&#39;, stats.field=&#39;alm_twitterCount&#39;, stats.facet=c(&#39;journal&#39;,&#39;volume&#39;), base=url)
#&amp;gt;   min  max count missing    sum sumOfSquares     mean   stddev
#&amp;gt; 1   0 1624 24326       0 113589     19746631 4.669448 28.10656
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;more-like-this&#34;&gt;More like this&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;solr_mlt&lt;/code&gt; is a function to return similar documents to the ones searched for.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out &amp;lt;- solr_mlt(q=&#39;title:&amp;quot;ecology&amp;quot; AND body:&amp;quot;cell&amp;quot;&#39;, mlt.fl=&#39;title&#39;, mlt.mindf=1, mlt.mintf=1, fl=&#39;counter_total_all&#39;, rows=5, base=url)
out$docs
#&amp;gt;                             id counter_total_all
#&amp;gt; 1 10.1371/journal.pbio.1001805             10102
#&amp;gt; 2 10.1371/journal.pbio.0020440             16630
#&amp;gt; 3 10.1371/journal.pone.0087217              2922
#&amp;gt; 4 10.1371/journal.pone.0040117              2514
#&amp;gt; 5 10.1371/journal.pone.0072525              1112
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;raw-data&#34;&gt;Raw data?&lt;/h2&gt;

&lt;p&gt;You can optionally get back raw &lt;code&gt;json&lt;/code&gt; or &lt;code&gt;xml&lt;/code&gt; from all functions by setting parameter &lt;code&gt;raw=TRUE&lt;/code&gt;. You can then parse after the fact with &lt;code&gt;solr_parse&lt;/code&gt;, or just process as you wish. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(out &amp;lt;- solr_highlight(q=&#39;alcohol&#39;, hl.fl = &#39;abstract&#39;, rows=2, base = url, raw=TRUE))
#&amp;gt; [1] &amp;quot;{\&amp;quot;response\&amp;quot;:{\&amp;quot;numFound\&amp;quot;:15301,\&amp;quot;start\&amp;quot;:0,\&amp;quot;docs\&amp;quot;:[{},{}]},\&amp;quot;highlighting\&amp;quot;:{\&amp;quot;10.1371/journal.pmed.0040151\&amp;quot;:{\&amp;quot;abstract\&amp;quot;:[\&amp;quot;Background: &amp;lt;em&amp;gt;Alcohol&amp;lt;/em&amp;gt; consumption causes an estimated 4% of the global disease burden, prompting\&amp;quot;]},\&amp;quot;10.1371/journal.pone.0027752\&amp;quot;:{\&amp;quot;abstract\&amp;quot;:[\&amp;quot;Background: The negative influences of &amp;lt;em&amp;gt;alcohol&amp;lt;/em&amp;gt; on TB management with regard to delays in seeking\&amp;quot;]}}}\n&amp;quot;
#&amp;gt; attr(,&amp;quot;class&amp;quot;)
#&amp;gt; [1] &amp;quot;sr_high&amp;quot;
#&amp;gt; attr(,&amp;quot;wt&amp;quot;)
#&amp;gt; [1] &amp;quot;json&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then parse&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_parse(out, &#39;df&#39;)
#&amp;gt;                          names
#&amp;gt; 1 10.1371/journal.pmed.0040151
#&amp;gt; 2 10.1371/journal.pone.0027752
#&amp;gt;                                                                                                    abstract
#&amp;gt; 1   Background: &amp;lt;em&amp;gt;Alcohol&amp;lt;/em&amp;gt; consumption causes an estimated 4% of the global disease burden, prompting
#&amp;gt; 2 Background: The negative influences of &amp;lt;em&amp;gt;alcohol&amp;lt;/em&amp;gt; on TB management with regard to delays in seeking
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;verbosity&#34;&gt;Verbosity&lt;/h2&gt;

&lt;p&gt;As you have noticed, we include in each function the acutal call to the Solr endpoint made so you know exactly what was submitted to the remote or local Solr instance. You can suppress the message with &lt;code&gt;verbose=FALSE&lt;/code&gt;. This message isn&amp;rsquo;t in the CRAN version.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;advanced-function-queries&#34;&gt;Advanced: Function Queries&lt;/h2&gt;

&lt;p&gt;Function Queries allow you to query on actual numeric fields in the SOLR database, and do addition, multiplication, etc on one or many fields to stort results. For example, here, we search on the product of counter_total_all and alm_twitterCount, using a new temporary field &amp;ldquo;&lt;em&gt;val&lt;/em&gt;&amp;ldquo;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_search(q=&#39;_val_:&amp;quot;product(counter_total_all,alm_twitterCount)&amp;quot;&#39;, rows=5, fl=&#39;id,title&#39;, fq=&#39;doc_type:full&#39;, base=url)
#&amp;gt;                             id
#&amp;gt; 1 10.1371/journal.pmed.0020124
#&amp;gt; 2 10.1371/journal.pone.0105948
#&amp;gt; 3 10.1371/journal.pone.0046362
#&amp;gt; 4 10.1371/journal.pone.0069841
#&amp;gt; 5 10.1371/journal.pbio.1001535
#&amp;gt;                                                                                                title
#&amp;gt; 1                                                     Why Most Published Research Findings Are False
#&amp;gt; 2 Sliding Rocks on Racetrack Playa, Death Valley National Park: First Observation of Rocks in Motion
#&amp;gt; 3 The Power of Kawaii: Viewing Cute Images Promotes a Careful Behavior and Narrows Attentional Focus
#&amp;gt; 4                            Facebook Use Predicts Declines in Subjective Well-Being in Young Adults
#&amp;gt; 5                                                     An Introduction to Social Media for Scientists
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, we search for the papers with the most citations&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_search(q=&#39;_val_:&amp;quot;max(counter_total_all)&amp;quot;&#39;, rows=5, fl=&#39;id,counter_total_all&#39;, fq=&#39;doc_type:full&#39;, base=url)
#&amp;gt;                             id counter_total_all
#&amp;gt; 1 10.1371/journal.pmed.0020124           1002083
#&amp;gt; 2 10.1371/journal.pmed.0050045            324559
#&amp;gt; 3 10.1371/journal.pone.0007595            315117
#&amp;gt; 4 10.1371/journal.pone.0033288            305965
#&amp;gt; 5 10.1371/journal.pone.0069841            277609
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or with the most tweets&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_search(q=&#39;_val_:&amp;quot;max(alm_twitterCount)&amp;quot;&#39;, rows=5, fl=&#39;id,alm_twitterCount&#39;, fq=&#39;doc_type:full&#39;, base=url)
#&amp;gt;                             id alm_twitterCount
#&amp;gt; 1 10.1371/journal.pone.0061981             2298
#&amp;gt; 2 10.1371/journal.pmed.0020124             1700
#&amp;gt; 3 10.1371/journal.pbio.1001535             1624
#&amp;gt; 4 10.1371/journal.pone.0046362             1368
#&amp;gt; 5 10.1371/journal.pmed.1001747             1361
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;further-reading-on-solr&#34;&gt;Further reading on Solr&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://lucene.apache.org/solr/&#34;&gt;Solr home page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.apache.org/solr/HighlightingParameters&#34;&gt;Highlighting help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.apache.org/solr/SimpleFacetParameters&#34;&gt;Faceting help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.apache.org/solr/StatsComponent&#34;&gt;Solr stats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.apache.org/solr/MoreLikeThis&#34;&gt;&amp;lsquo;More like this&amp;rsquo; searches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.apache.org/solr/FieldCollapsing&#34;&gt;Grouping/Feild collapsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ramlev.dk/blog/2012/06/02/install-apache-solr-on-your-mac/&#34;&gt;Installing Solr on Mac using homebrew&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://risnandar.wordpress.com/2013/09/08/how-to-install-and-setup-apache-lucene-solr-in-osx/&#34;&gt;Install and Setup SOLR in OSX, including running Solr&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Highlighting text in text mining</title>
      <link>https://ropensci.org/blog/2013/12/02/rplos-highlights/</link>
      <pubDate>Mon, 02 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2013/12/02/rplos-highlights/</guid>
      <description>
        
        

&lt;p&gt;&lt;code&gt;rplos&lt;/code&gt; is an R package to facilitate easy search and full-text retrieval from all Public Library of Science (PLOS) articles, and we have a little feature which aren&amp;rsquo;t sure if is useful or not. I don&amp;rsquo;t actually do any text-mining for my research, so perhaps text-mining folks can give some feedback.&lt;/p&gt;

&lt;p&gt;You can quickly get a lot of results back using &lt;code&gt;rplos&lt;/code&gt;, so perhaps it is useful to quickly browse what you got. What better tool than a browser to browse? Enter &lt;code&gt;highplos&lt;/code&gt; and &lt;code&gt;highbrow&lt;/code&gt;. &lt;code&gt;highplos&lt;/code&gt; uses the &lt;a href=&#34;http://lucene.apache.org/solr/&#34;&gt;Solr&lt;/a&gt; capabilities of the PLOS search API, and lets you get back a string with the term you searched for highlighted (by default with &lt;code&gt;&amp;lt;em&amp;gt;&lt;/code&gt; tag for italics).&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;devtools&amp;quot;)
library(devtools)
install_github(&amp;quot;rplos&amp;quot;, &amp;quot;ropensci&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rplos)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;search-plos-articles&#34;&gt;Search PLOS articles&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out &amp;lt;- highplos(q = &amp;quot;alcohol&amp;quot;, hl.fl = &amp;quot;abstract&amp;quot;, hl.snippets = 5, rows = 10)
out[[1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $abstract
## [1] &amp;quot;Background: &amp;lt;em&amp;gt;Alcohol&amp;lt;/em&amp;gt; consumption causes an estimated 4% of the global disease burden, prompting&amp;quot;
## [2] &amp;quot; goverments to impose regulations to mitigate the adverse effects of &amp;lt;em&amp;gt;alcohol&amp;lt;/em&amp;gt;. To assist public health leaders&amp;quot;
## [3] &amp;quot; and policymakers, the authors developed a composite indicator—the &amp;lt;em&amp;gt;Alcohol&amp;lt;/em&amp;gt; Policy Index—to gauge the strength&amp;quot;
## [4] &amp;quot; of a country&#39;s &amp;lt;em&amp;gt;alcohol&amp;lt;/em&amp;gt; control policies. Methods and Findings: The Index generates a score based on policies&amp;quot;
## [5] &amp;quot; from five regulatory domains—physical availability of &amp;lt;em&amp;gt;alcohol&amp;lt;/em&amp;gt;, drinking context, &amp;lt;em&amp;gt;alcohol&amp;lt;/em&amp;gt; prices&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;preview-results-in-your-browser&#34;&gt;Preview results in your browser&lt;/h2&gt;

&lt;p&gt;The new function &lt;code&gt;highbrow&lt;/code&gt; (&lt;em&gt;snickers quietly&lt;/em&gt;) automagically creates an easy to digest html page, and opens in your default browser.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;highbrow(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s a screenshot similar to what you should see after the last command&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/rplos_highlights.png&#34;&gt;&lt;/img&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;highbrow&lt;/code&gt; uses the &lt;code&gt;whisker&lt;/code&gt; package to fill in a template for a bootstrap html page to make a somewhat pleasing interface to look at your data. In addition, the DOIs are wrapped in a &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; tag with a &lt;a href=&#34;http://doi.org/&#34;&gt;http://dx.doi.org/&lt;/a&gt; prefix so that you can go directly to the paper if you are so inclined. Also note that the &lt;code&gt;&amp;lt;em&amp;gt;&lt;/code&gt; tags (italicized) are replaced with &lt;code&gt;&amp;lt;strong&amp;gt;&lt;/code&gt; tags (bold) to make the search term pop out from the screen more.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Let us know what you think.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>OA week - A simple use case for programmatic access to PLOS full text</title>
      <link>https://ropensci.org/blog/2013/10/22/oaweek-rplos/</link>
      <pubDate>Tue, 22 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2013/10/22/oaweek-rplos/</guid>
      <description>
        
        

&lt;p&gt;Open access week is here!  We love open access, and think it&amp;rsquo;s extremely important to publish in open access journals. One of the many benefits of open access literature is that we likely can use the text of articles in OA journals for many things, including text-mining.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s even more awesome is some OA publishers provide API (application programming interface) access to their full text articles. Public Library of Science (PLOS) is one of these. We have had an R package for a while now that makes it convenient to search PLOS full text programatically. You can search on specific parts of articles (e.g., just in titles, or just in results sections), and you can return specific parts of articles (e.g., just abstracts). There are additional options for more fine-grained control over searches like facetting.&lt;/p&gt;

&lt;p&gt;What if you want to find similar papers based on their text content?  This can be done using the PLOS search API, with help from the &lt;code&gt;tm&lt;/code&gt; R package. These are basic examples just to demonstrate that you can quickly go from a search of PLOS data to a visualization or analysis.&lt;/p&gt;

&lt;h3 id=&#34;install-rplos-and-other-packages-from-cran&#34;&gt;Install rplos and other packages from CRAN&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(c(&amp;quot;rplos&amp;quot;, &amp;quot;tm&amp;quot;, &amp;quot;wordcloud&amp;quot;, &amp;quot;RColorBrewer&amp;quot;, &amp;quot;proxy&amp;quot;, &amp;quot;plyr&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-some-text&#34;&gt;Get some text&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rplos)
out &amp;lt;- searchplos(&amp;quot;birds&amp;quot;, fields = &amp;quot;id,introduction&amp;quot;, limit = 20, toquery = list(&amp;quot;cross_published_journal_key:PLoSONE&amp;quot;,
    &amp;quot;doc_type:full&amp;quot;))
out$idshort &amp;lt;- sapply(out$id, function(x) strsplit(x, &amp;quot;\\.&amp;quot;)[[1]][length(strsplit(x,
    &amp;quot;\\.&amp;quot;)[[1]])], USE.NAMES = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result is a list of length &lt;code&gt;limit&lt;/code&gt; defined in the previous call.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nrow(out)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[1] 20
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;word-dictionaries&#34;&gt;Word dictionaries.&lt;/h3&gt;

&lt;p&gt;Next, we&amp;rsquo;ll use the tm package to create word dictionaries for each paper.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tm)
library(proxy)
corpus &amp;lt;- Corpus(DataframeSource(out[&amp;quot;introduction&amp;quot;]))

# Clean up corpus
corpus &amp;lt;- tm_map(corpus, function(x) removeWords(x, stopwords(&amp;quot;english&amp;quot;)))
corpus &amp;lt;- tm_map(corpus, function(x) removePunctuation(x))
tdm &amp;lt;- TermDocumentMatrix(corpus)
tdm$dimnames$Docs &amp;lt;- out$idshort

# Comparison among documents in a heatmap
dissmat &amp;lt;- dissimilarity(tdm, method = &amp;quot;Euclidean&amp;quot;)
get_dist_frame &amp;lt;- function(x) {
    temp &amp;lt;- data.frame(subset(data.frame(expand.grid(dimnames(as.matrix(x))),
        expand.grid(lower.tri(as.matrix(x)))), Var1.1 == &amp;quot;TRUE&amp;quot;)[, -3], as.vector(x))
    names(temp) &amp;lt;- c(&amp;quot;one&amp;quot;, &amp;quot;two&amp;quot;, &amp;quot;value&amp;quot;)
    tempout &amp;lt;- temp[!temp[, 1] == temp[, 2], ]
    tempout
}
dissmatdf &amp;lt;- get_dist_frame(dissmat)
ggplot(dissmatdf, aes(one, two)) + geom_tile(aes(fill = value), colour = &amp;quot;white&amp;quot;,
    binwidth = 3) + scale_fill_gradient(low = &amp;quot;white&amp;quot;, high = &amp;quot;steelblue&amp;quot;) +
    theme_grey(base_size = 16) + labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;) + scale_x_discrete(expand = c(0,
    0)) + scale_y_discrete(expand = c(0, 0)) + theme(axis.ticks = theme_blank(),
    axis.text.x = element_text(size = 12, hjust = 0.6, colour = &amp;quot;grey50&amp;quot;, angle = 90),
    panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), panel.border = theme_blank())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-10-22-oaweek-rplos-2/tmit.png&#34; alt=&#34;plot of chunk tmit&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Picking two with low values (=high similarity), dois 10.1371/journal.pone.0000184 and 10.1371/journal.pone.0004148, here&amp;rsquo;s some of the most common terms used (some overlap).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(plyr)
df1 &amp;lt;- sort(termFreq(corpus[[grep(&amp;quot;10.1371/journal.pone.0010997&amp;quot;, out$id)]]))
df1 &amp;lt;- data.frame(terms = names(df1[df1 &amp;gt; 2]), vals = df1[df1 &amp;gt; 2], row.names = NULL)
df2 &amp;lt;- sort(termFreq(corpus[[grep(&amp;quot;10.1371/journal.pone.0004148&amp;quot;, out$id)]]))
df2 &amp;lt;- data.frame(terms = names(df2[df2 &amp;gt; 1]), vals = df2[df2 &amp;gt; 1], row.names = NULL)
df1$terms &amp;lt;- reorder(df1$terms, df1$vals)
df2$terms &amp;lt;- reorder(df2$terms, df2$vals)
dfboth &amp;lt;- ldply(list(`0010997` = df1, `0004148` = df2))
ggplot(dfboth, aes(x = terms, y = vals)) + geom_histogram(stat = &amp;quot;identity&amp;quot;) +
    facet_grid(. ~ .id, scales = &amp;quot;free&amp;quot;) + theme(axis.text.x = element_text(angle = 90))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-10-22-oaweek-rplos-2/words.png&#34; alt=&#34;plot of chunk words&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;determine-similarity-among-papers&#34;&gt;Determine similarity among papers&lt;/h3&gt;

&lt;p&gt;Using a wordcloud&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(wordcloud)
library(RColorBrewer)

m &amp;lt;- as.matrix(tdm)
v &amp;lt;- sort(rowSums(m), decreasing = TRUE)
d &amp;lt;- data.frame(word = names(v), freq = v)
pal &amp;lt;- brewer.pal(9, &amp;quot;Blues&amp;quot;)
pal &amp;lt;- pal[-(1:2)]

# Plot the chart
wordcloud(d$word, d$freq, scale = c(3, 0.1), min.freq = 2, max.words = 250,
    random.order = FALSE, rot.per = 0.2, colors = pal)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2013-10-22-oaweek-rplos-2/wordcloud.png&#34; alt=&#34;plot of chunk wordcloud&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
