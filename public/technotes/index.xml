<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rOpenSci Blog on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/technotes/</link>
    <description>Recent content in rOpenSci Blog on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 13 Jul 2017 22:00:37 -0700</lastBuildDate>
    
        <atom:link href="https://ropensci.org/technotes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>fulltext v0.3: x, y, and z</title>
      <link>https://ropensci.org/technotes/2017/12/12/fulltext-update/</link>
      <pubDate>Tue, 12 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/12/12/fulltext-update/</guid>
      <description>
        
        

&lt;p&gt;Text-mining - the art of answering questions by extracting patterns, data, etc. out of the published literature - is not easy. It&amp;rsquo;s made incredibly difficult because of publishers. Novels, etc. by writers are one thing - but it&amp;rsquo;s hard to swallow the fact that the vast majority of publicly funded research across the globe is published in paywall journals. That is, taxpayers pay twice for research: once for the grant to fund the work, then again to be able to read it.&lt;/p&gt;

&lt;p&gt;Text-mining use cases run from determining the change in use of words through time [ref], to XYZ.&lt;/p&gt;

&lt;h2 id=&#34;the-fulltext-package&#34;&gt;the fulltext package&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;fulltext&lt;/code&gt; is a package to help R users get published literature from the web in it&amp;rsquo;s many forms, and across thousands upon thousands of publishers.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fulltext&lt;/code&gt; tries to make the following use cases as easy as possible:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Search for articles&lt;/li&gt;
&lt;li&gt;Fetch abstracts&lt;/li&gt;
&lt;li&gt;Fetch full text articles&lt;/li&gt;
&lt;li&gt;Get links for full text articles (xml, pdf)&lt;/li&gt;
&lt;li&gt;Extract text from articles / convert formats&lt;/li&gt;
&lt;li&gt;Collect sections of articles that you actually need (e.g., titles)&lt;/li&gt;
&lt;li&gt;Download supplementary materials&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;fulltext&lt;/code&gt; organizes funvctions around use cases, then provides flexiblity to query many data sources within that use case (i.e. function). For example &lt;code&gt;fulltext::ft_search&lt;/code&gt; searches for articles - you can choose among one or more of many data sources to search, passing options to each source as needed.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-does-a-workflow-with-fulltext-look-like&#34;&gt;What does a workflow with fulltext look like?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Search for articles with &lt;code&gt;ft_search()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fetch articles with &lt;code&gt;ft_get()&lt;/code&gt; using the output of the previous step&lt;/li&gt;
&lt;li&gt;Extract sections of articles needed with &lt;code&gt;chunks()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Do some further processing with &lt;code&gt;tm&lt;/code&gt; or &lt;code&gt;?????&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Install &lt;code&gt;fulltext&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;fulltext&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/fulltext&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(fulltext)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;search-ft-search&#34;&gt;Search: ft_search&lt;/h2&gt;

&lt;p&gt;With &lt;code&gt;ft_search&lt;/code&gt; you can search&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PLOS&lt;/li&gt;
&lt;li&gt;BMC&lt;/li&gt;
&lt;li&gt;Crossref&lt;/li&gt;
&lt;li&gt;Entrez&lt;/li&gt;
&lt;li&gt;arxiv&lt;/li&gt;
&lt;li&gt;biorxiv&lt;/li&gt;
&lt;li&gt;Euro&lt;/li&gt;
&lt;li&gt;Scopus&lt;/li&gt;
&lt;li&gt;Microsoft&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- ft_search(query=&#39;ecology&#39;, from=&#39;plos&#39;)
res
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query:
##   [ecology] 
## Found:
##   [PLoS: 40969; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] 
## Returned:
##   [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After running &lt;code&gt;ft_search&lt;/code&gt; you can index to each source that you
selected in the &lt;code&gt;from&lt;/code&gt; parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res$plos
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query: [ecology] 
## Records found, returned: [40969, 10] 
## License: [CC-BY] 
##                              id
## 1  10.1371/journal.pone.0001248
## 2  10.1371/journal.pone.0059813
## 3  10.1371/journal.pone.0155019
## 4  10.1371/journal.pone.0080763
## 5  10.1371/journal.pone.0150648
## 6  10.1371/journal.pcbi.1003594
## 7  10.1371/journal.pone.0102437
## 8  10.1371/journal.pone.0175014
## 9  10.1371/journal.pone.0166559
## 10 10.1371/journal.pone.0054689
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you give many values for the &lt;code&gt;from&lt;/code&gt; parameter you&amp;rsquo;ll get many results, for
example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- ft_search(query=&#39;ecology&#39;, from=c(&#39;plos&#39;, &#39;crossref&#39;))
res$plos
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query: [ecology] 
## Records found, returned: [40969, 10] 
## License: [CC-BY] 
##                              id
## 1  10.1371/journal.pone.0001248
## 2  10.1371/journal.pone.0059813
## 3  10.1371/journal.pone.0155019
## 4  10.1371/journal.pone.0080763
## 5  10.1371/journal.pone.0150648
## 6  10.1371/journal.pcbi.1003594
## 7  10.1371/journal.pone.0102437
## 8  10.1371/journal.pone.0175014
## 9  10.1371/journal.pone.0166559
## 10 10.1371/journal.pone.0054689
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res$crossref
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query: [ecology] 
## Records found, returned: [142207, 10] 
## License: [variable, see individual records] 
##                  container.title    created  deposited
## 1                        Ecology 2006-05-03 2017-04-14
## 2                        Ecology 2006-05-03 2016-03-07
## 3                        Ecology 2006-05-03 2016-03-07
## 4                        Ecology 2006-05-03 2016-03-07
## 5                        Ecology 2006-05-03 2017-04-15
## 6                        Ecology 2006-05-03 2017-04-15
## 7                        Ecology 2006-05-09 2016-03-07
## 8                        Ecology 2017-04-26 2017-07-11
## 9  Trends in Ecology &amp;amp; Evolution 2002-07-25 2017-06-14
## 10 Journal of Industrial Ecology 2017-10-05 2017-11-14
## Variables not shown: doi (chr), indexed (chr), issn (chr), issue (chr),
##      issued (chr), license_date (chr), license_url (chr),
##      license_delay.in.days (chr), license_content.version (chr), member
##      (chr), page (chr), prefix (chr), publisher (chr), reference.count
##      (chr), score (chr), source (chr), subject (chr), title (chr), type
##      (chr), url (chr), volume (chr), author (list), link (list), archive
##      (chr), alternative.id (chr), subtitle (chr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;fetch-abstracts-ft-abstract&#34;&gt;Fetch abstracts: ft_abstract&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;PLOS&lt;/li&gt;
&lt;li&gt;Scopus&lt;/li&gt;
&lt;li&gt;Microsoft&lt;/li&gt;
&lt;li&gt;Crossref&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(res &amp;lt;- ft_search(query = &#39;biology&#39;, from = &#39;plos&#39;, limit = 10, 
   plosopts = list(fq = list(&#39;doc_type:full&#39;, &#39;-article_type:correction&#39;,
                  &#39;-article_type:viewpoints&#39;))))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query:
##   [biology] 
## Found:
##   [PLoS: 170852; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] 
## Returned:
##   [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(out &amp;lt;- ft_abstract(x = res$plos$data$id, from = &amp;quot;plos&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &amp;lt;fulltext abstracts&amp;gt;
## Found:
##   [PLOS: 84; Scopus: 0; Microsoft: 0; Crossref: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;fetch-articles-ft-get&#34;&gt;Fetch articles: ft_get&lt;/h2&gt;

&lt;p&gt;The function &lt;code&gt;ft_get&lt;/code&gt; is the workhorse for getting the namesake of the package:
full text articles.&lt;/p&gt;

&lt;p&gt;Beware that using this function can be tricky depending on where you want to
get articles from. While searching (&lt;code&gt;ft_search&lt;/code&gt;) usually doesn&amp;rsquo;t present any
barriers or stumbling blocks, &lt;code&gt;ft_get&lt;/code&gt; can get frustrating because so many
publishers paywall their articles. The combination of paywalls and their
patchwork of who gets to get through them means that we can&amp;rsquo;t easily predict
who will run into problems with Elsevier, Wiley, etc.&lt;/p&gt;

&lt;p&gt;With this version we&amp;rsquo;ve tried to bulk up the documentation as much as possible
to make jumping over these barriers as easy as possible.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ft_get()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Error in stop(&amp;quot;no &#39;ft_get&#39; method for &amp;quot;, class(x), call. = FALSE): argument &amp;quot;x&amp;quot; is missing, with no default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;get-full-text-links-ft-links&#34;&gt;Get full text links: ft_links&lt;/h2&gt;

&lt;p&gt;In case you want to sort out full text links yourself, and have those links for whatever purpose, &lt;code&gt;ft_links&lt;/code&gt; is your friend. It grabs data from PLOS, Crossref, Entrez, and BMC.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(res2 &amp;lt;- ft_search(query=&#39;ecology&#39;, from=&#39;crossref&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Query:
##   [ecology] 
## Found:
##   [PLoS: 0; BMC: 0; Crossref: 142207; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] 
## Returned:
##   [PLoS: 0; BMC: 0; Crossref: 10; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(out &amp;lt;- ft_links(res2))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &amp;lt;fulltext links&amp;gt;
## [Found] 10 
## [IDs] 10.2307/1929908 10.2307/1931096 10.2307/1934287 10.2307/1943146
##      10.2307/1930158 10.2307/1928969 10.2307/1935066 10.1002/ecy.1807
##      10.1016/s0169-5347(97)89918-1 10.1111/jiec.12669 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out$crossref
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $found
## [1] 10
## 
## $ids
##  [1] &amp;quot;10.2307/1929908&amp;quot;               &amp;quot;10.2307/1931096&amp;quot;              
##  [3] &amp;quot;10.2307/1934287&amp;quot;               &amp;quot;10.2307/1943146&amp;quot;              
##  [5] &amp;quot;10.2307/1930158&amp;quot;               &amp;quot;10.2307/1928969&amp;quot;              
##  [7] &amp;quot;10.2307/1935066&amp;quot;               &amp;quot;10.1002/ecy.1807&amp;quot;             
##  [9] &amp;quot;10.1016/s0169-5347(97)89918-1&amp;quot; &amp;quot;10.1111/jiec.12669&amp;quot;           
## 
## $data
## $data$`10.2307/1929908`
##                                                                     url
## 1 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1929908
##               doi        type member
## 1 10.2307/1929908 unspecified    311
## 
## $data$`10.2307/1931096`
##                                                                    url
## 1 http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1931096
##               doi        type member
## 1 10.2307/1931096 unspecified    311
## 
## $data$`10.2307/1934287`
##                                                                    url
## 1 http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1934287
##               doi        type member
## 1 10.2307/1934287 unspecified    311
## 
## $data$`10.2307/1943146`
##                                                                    url
## 1 http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1943146
##               doi        type member
## 1 10.2307/1943146 unspecified    311
## 
## $data$`10.2307/1930158`
##                                                                     url
## 1  http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1930158
## 2 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1930158
##               doi        type member
## 1 10.2307/1930158         pdf    311
## 2 10.2307/1930158 unspecified    311
## 
## $data$`10.2307/1928969`
##                                                                     url
## 1  http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1928969
## 2 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1928969
##               doi        type member
## 1 10.2307/1928969         pdf    311
## 2 10.2307/1928969 unspecified    311
## 
## $data$`10.2307/1935066`
##                                                                    url
## 1 http://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1935066
##               doi        type member
## 1 10.2307/1935066 unspecified    311
## 
## $data$`10.1002/ecy.1807`
##                                                                      url
## 1 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fecy.1807
## 2       http://onlinelibrary.wiley.com/wol1/doi/10.1002/ecy.1807/fullpdf
##                doi        type member
## 1 10.1002/ecy.1807         pdf    311
## 2 10.1002/ecy.1807 unspecified    311
## 
## $data$`10.1016/s0169-5347(97)89918-1`
##                                                                                    url
## 1   https://api.elsevier.com/content/article/PII:S0169534797899181?httpAccept=text/xml
## 2 https://api.elsevier.com/content/article/PII:S0169534797899181?httpAccept=text/plain
##                             doi  type member
## 1 10.1016/s0169-5347(97)89918-1   xml     78
## 2 10.1016/s0169-5347(97)89918-1 plain     78
## 
## $data$`10.1111/jiec.12669`
##                                                                        url
## 1 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1111%2Fjiec.12669
## 2       http://onlinelibrary.wiley.com/wol1/doi/10.1111/jiec.12669/fullpdf
##                  doi        type member
## 1 10.1111/jiec.12669         pdf    311
## 2 10.1111/jiec.12669 unspecified    311
## 
## 
## $opts
## list()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out$crossref$data[[1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                                                                     url
## 1 https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.2307%2F1929908
##               doi        type member
## 1 10.2307/1929908 unspecified    311
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;extract-text-ft-extract&#34;&gt;Extract text: ft_extract&lt;/h2&gt;

&lt;p&gt;This is a simple wrapper around the &lt;a href=&#34;https://github.com/ropensci/pdftools&#34;&gt;pdftools&lt;/a&gt; package - when dealing with
xml or plain text data, there&amp;rsquo;s no need to parse what you get from &lt;code&gt;ft_get&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;path &amp;lt;- system.file(&amp;quot;examples&amp;quot;, &amp;quot;example1.pdf&amp;quot;, package = &amp;quot;fulltext&amp;quot;)
(res &amp;lt;- ft_extract(path))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &amp;lt;document&amp;gt;/Library/Frameworks/R.framework/Versions/3.4/Resources/library/fulltext/examples/example1.pdf
##   Title: Suffering and mental health among older people living in nursing homes---a mixed-methods study
##   Producer: pdfTeX-1.40.10
##   Creation date: 2015-07-17
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that &lt;code&gt;ft_extract&lt;/code&gt; can now handle both a file path to a pdf file, or
raw bytes.&lt;/p&gt;

&lt;h2 id=&#34;extract-parts-of-documents-you-want-chunk&#34;&gt;Extract parts of documents you want: chunk&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;chunk()&lt;/code&gt; helps you quickly extract sections of articles you want aross many articles and many publishers. This only handles XML (there&amp;rsquo;s no structure in plain text and pdf text) We have internal scripts targeting specific publishers so that we can handle variation in how publishers structure their XML.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;rplos&amp;quot;)
(dois &amp;lt;- searchplos(q=&amp;quot;*:*&amp;quot;, fl=&#39;id&#39;,
   fq=list(&#39;doc_type:full&#39;,&amp;quot;article_type:\&amp;quot;research article\&amp;quot;&amp;quot;),
     limit=5)$data$id)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;10.1371/journal.pone.0074173&amp;quot; &amp;quot;10.1371/journal.pone.0060168&amp;quot;
## [3] &amp;quot;10.1371/journal.pone.0170933&amp;quot; &amp;quot;10.1371/journal.pone.0170932&amp;quot;
## [5] &amp;quot;10.1371/journal.pone.0187293&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- ft_get(dois, from = &amp;quot;plos&amp;quot;)
x %&amp;gt;% chunks(c(&amp;quot;doi&amp;quot;,&amp;quot;history&amp;quot;)) %&amp;gt;% tabularize()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $plos
##                            doi history.received history.accepted
## 1 10.1371/journal.pone.0074173       2013-04-04       2013-07-27
## 2 10.1371/journal.pone.0060168       2012-09-18       2013-02-25
## 3 10.1371/journal.pone.0170933       2016-10-05       2017-01-12
## 4 10.1371/journal.pone.0170932       2016-09-27       2017-01-12
## 5 10.1371/journal.pone.0187293       2016-12-16       2017-10-17
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;

&lt;h3 id=&#34;xxx&#34;&gt;xxx&lt;/h3&gt;

&lt;p&gt;xxx&lt;/p&gt;

&lt;h3 id=&#34;feedback&#34;&gt;Feedback!&lt;/h3&gt;

&lt;p&gt;Please do upgrade/install &lt;code&gt;fulltext&lt;/code&gt;  &lt;code&gt;v0.3&lt;/code&gt; and let us know what you think.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Magick 1.6: clipping, geometries, fonts, fuzz, and a bit of history</title>
      <link>https://ropensci.org/technotes/2017/12/05/magick-16/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/12/05/magick-16/</guid>
      <description>
        
        &lt;img src=&quot;https://i.imgur.com/tTFk7ig.jpg&quot; alt=&quot;cover image&quot;&gt;
        
        

&lt;p&gt;This week &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick&lt;/a&gt; 1.6 appeared on CRAN. This release is a big all-round maintenance update with lots of tweaks and improvements across the package.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/NEWS&#34;&gt;NEWS&lt;/a&gt; file gives an overview of changes in this version. In this post we highlight some changes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)
stopifnot(packageVersion(&#39;magick&#39;) &amp;gt;= 1.6)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you are new to magick, check out the &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;vignette&lt;/a&gt; for a quick introduction.&lt;/p&gt;

&lt;h2 id=&#34;perfect-graphics-rendering&#34;&gt;Perfect Graphics Rendering&lt;/h2&gt;

&lt;p&gt;I have fixed a few small rendering imperfections in the graphics device. The native magick graphics device &lt;code&gt;image_graph()&lt;/code&gt; now renders identical or better quality images as the R-base bitmap devices &lt;code&gt;png&lt;/code&gt;, &lt;code&gt;jpeg&lt;/code&gt;, etc.&lt;/p&gt;

&lt;p&gt;One issue was that sometimes magick graphics would show a 1px black border around the image. It turned out this is caused by rounding of clipping coordinates.&lt;/p&gt;

&lt;p&gt;When R calculates clipping area it often ends up at non-whole values. It is then up to the graphics device to decide what to do with the pixel that is partially clipped. Let&amp;rsquo;s show clipping in action:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;testplot &amp;lt;- function(title = &amp;quot;&amp;quot;){
  plot(1, main = title)
  abline(0, 1, col = &amp;quot;blue&amp;quot;, lwd = 2, lty = &amp;quot;solid&amp;quot;)
  abline(0.1, 1, col = &amp;quot;red&amp;quot;, lwd = 3, lty = &amp;quot;dotted&amp;quot;)
  abline(0.2, 1, col = &amp;quot;green&amp;quot;, lwd = 4, lty = &amp;quot;twodash&amp;quot;)
  abline(0.3, 1, col = &amp;quot;black&amp;quot;, lwd = 5, lty = &amp;quot;dotdash&amp;quot;)
  abline(0.4, 1, col = &amp;quot;purple&amp;quot;, lwd = 6, lty = &amp;quot;dashed&amp;quot;)
  abline(0.5, 1, col = &amp;quot;yellow&amp;quot;, lwd = 7, lty = &amp;quot;longdash&amp;quot;)
  abline(-0.1, 1, col = &amp;quot;blue&amp;quot;, lwd = 10, lend = &amp;quot;round&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
  abline(-0.2, 1, col = &amp;quot;blue&amp;quot;, lwd = 10, lend = &amp;quot;butt&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
  abline(-0.3, 1, col = &amp;quot;blue&amp;quot;, lwd = 10, lend = &amp;quot;square&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we run it with and without clipping:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img2 &amp;lt;- magick::image_graph(clip = FALSE)
testplot(&amp;quot;Without clipping&amp;quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/TtpjlLq.png&#34; alt=&#34;noclip.png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img1 &amp;lt;- magick::image_graph(clip = TRUE)
testplot(&amp;quot;With clipping&amp;quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/JbWMElL.png&#34; alt=&#34;clip.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see the latter image is now perfectly clipped. The colored lines are truncated exactly at the pixel where the axis starts. This is not always the case in base R ;)&lt;/p&gt;

&lt;h2 id=&#34;font-families&#34;&gt;Font Families&lt;/h2&gt;

&lt;p&gt;In magick there are two ways to render text on an image. You can either open the image or graphic in the magick graphics device and then use base R &lt;code&gt;text()&lt;/code&gt; function to print text. Alternatively there is &lt;code&gt;image_annotate()&lt;/code&gt; which is a simpler version to print some text on an image.&lt;/p&gt;

&lt;p&gt;Wherever text rendering is involved, two major headache arise: encoding and fonts. The latter is tricky because different operating systems have different fonts with different names. In addition a font can be specified as a name, or family name, or alias.&lt;/p&gt;

&lt;p&gt;Below is a simple test that I use to quickly inspect if fonts are working on different systems:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img &amp;lt;- image_graph(width = 800, height = 500, pointsize = 20, res = 96)
graphics::plot.new()
graphics::par(mar = c(0,0,3,0))
graphics::plot.window(xlim = c(0, 20), ylim = c(-.5, 8))
title(expression(Gamma %prop% sum(x[alpha], i==1, n) * sqrt(mu)), expression(hat(x)))

# Standard families as supported by other devices
text(0.95, 7, &amp;quot;abcdefg  - Helvetica&amp;quot;, pos = 4, family = &amp;quot;helvetica&amp;quot;)
text(0.95, 6, &amp;quot;abcdefg  - Sans (Arial)&amp;quot;, pos = 4, family = &amp;quot;sans&amp;quot;)
text(0.95, 5, &amp;quot;abcdefg - Serif (Times)&amp;quot;, pos = 4, family = &amp;quot;serif&amp;quot;)
text(0.95, 4, &amp;quot;abcdefg - Monospace (Courier New)&amp;quot;, pos = 4, family = &amp;quot;mono&amp;quot;)
text(0.95, 3, &amp;quot;abcdefg - Symbol Face&amp;quot;, pos = 4, font = 5)
text(0.95, 2, &amp;quot;abcdefg  - Comic Sans&amp;quot;, pos = 4, family = &amp;quot;Comic Sans&amp;quot;)
text(0.95, 1, &amp;quot;abcdefg - Georgia Serif&amp;quot;, pos = 4, family = &amp;quot;Georgia&amp;quot;)
text(0.95, 0, &amp;quot;abcdefg - Courier&amp;quot;, pos = 4, family = &amp;quot;Courier&amp;quot;)
dev.off()
img &amp;lt;- image_border(img, &#39;red&#39;, geometry = &#39;2x2&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/tzIktip.png&#34; alt=&#34;families&#34; /&gt;&lt;/p&gt;

&lt;p&gt;R requires that a graphics device supports at least 4 font types: &lt;code&gt;serif&lt;/code&gt;, &lt;code&gt;sans&lt;/code&gt;, &lt;code&gt;mono&lt;/code&gt; and &lt;code&gt;symbol&lt;/code&gt;. The latter is a special 8bit font with some Greek letters and other characters needed for rendering math. This set of fonts corresponds to the original &lt;strong&gt;13 base fonts&lt;/strong&gt; from the &lt;a href=&#34;https://en.wikipedia.org/wiki/PostScript_fonts#Core_Font_Set&#34;&gt;1984 postscript standard&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;4x Courier (Regular, Oblique, Bold, Bold Oblique)&lt;/li&gt;
&lt;li&gt;4x Helvetica (Regular, Oblique, Bold, Bold Oblique)&lt;/li&gt;
&lt;li&gt;4x Times (Roman, Italic, Bold, Bold Italic)&lt;/li&gt;
&lt;li&gt;Symbol&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below a photo of the 1985 &lt;a href=&#34;https://en.wikipedia.org/wiki/LaserWriter&#34;&gt;Apple Laser Writer&lt;/a&gt; which was &lt;a href=&#34;https://en.wikipedia.org/wiki/PostScript_fonts#History&#34;&gt;the first laser printer&lt;/a&gt; to use the PostScript language and support all these fonts! Not much later PostScript graphics devices were adopted by R&amp;rsquo;s predecessor &lt;a href=&#34;https://en.wikipedia.org/wiki/S_(programming_language)#.22New_S.22&#34;&gt;&amp;ldquo;The New S&amp;rdquo;&lt;/a&gt; (The New S Language, 1988).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://theappletimeline.com/images/color1000.jpg&#34; alt=&#34;printers&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;geometry-helpers&#34;&gt;Geometry Helpers&lt;/h2&gt;

&lt;p&gt;Another major improvement in this release is the introduction of helper functions for geometry and option strings. Many functions in magick require a special geometry syntax to specify a size, area, or point. For example to resize an image you need to specify a size:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;image_resize(img, &amp;quot;50%&amp;quot;)
image_resize(img, &amp;quot;300x300&amp;quot;)
image_resize(img, &amp;quot;300x300!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or to crop you need to specify an area which consists of a size and offset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;image_crop(img, &amp;quot;300x300+100+100&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We added a few handy &lt;code&gt;?geometry&lt;/code&gt; helper functions to generate proper geometry syntax&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/2jivLxi.png&#34; alt=&#34;geometries&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;magick-options&#34;&gt;Magick Options&lt;/h2&gt;

&lt;p&gt;A lot of the power in ImageMagick is contained in the hundreds of built-in filters, colorspaces, compose operators, disposal types, convolution kernels, noise types and what not. These are specified simply as a string in the function.&lt;/p&gt;

&lt;p&gt;For example in our previous &lt;a href=&#34;https://ropensci.org/technotes/2017/11/02/image-convolve/&#34;&gt;post about Image Convolution&lt;/a&gt; we discussed a few kernel types:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Gaussian Kernel
img %&amp;gt;% image_convolve(&#39;Gaussian:0x5&#39;, scaling = &#39;60,40%&#39;)

# Sobel Kernel
img %&amp;gt;% image_convolve(&#39;Sobel&#39;)

# Difference of Gaussians
img %&amp;gt;% image_convolve(&#39;DoG:0,0,2&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Supported values for each option are described in the online ImageMagick documentation. We now have added functions in the magick package that list all values for each option. This should make it a easier to see what is supported and harness the full power of built-in ImageMagick algorithms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/cid6JqU.png&#34; alt=&#34;options&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So we can now easily list e.g. supported kernel types:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; kernel_types()
 [1] &amp;quot;Undefined&amp;quot;     &amp;quot;Unity&amp;quot;         &amp;quot;Gaussian&amp;quot;      &amp;quot;DoG&amp;quot;          
 [5] &amp;quot;LoG&amp;quot;           &amp;quot;Blur&amp;quot;          &amp;quot;Comet&amp;quot;         &amp;quot;Binomial&amp;quot;     
 [9] &amp;quot;Laplacian&amp;quot;     &amp;quot;Sobel&amp;quot;         &amp;quot;FreiChen&amp;quot;      &amp;quot;Roberts&amp;quot;      
[13] &amp;quot;Prewitt&amp;quot;       &amp;quot;Compass&amp;quot;       &amp;quot;Kirsch&amp;quot;        &amp;quot;Diamond&amp;quot;      
[17] &amp;quot;Square&amp;quot;        &amp;quot;Rectangle&amp;quot;     &amp;quot;Disk&amp;quot;          &amp;quot;Octagon&amp;quot;      
[21] &amp;quot;Plus&amp;quot;          &amp;quot;Cross&amp;quot;         &amp;quot;Ring&amp;quot;          &amp;quot;Peaks&amp;quot;        
[25] &amp;quot;Edges&amp;quot;         &amp;quot;Corners&amp;quot;       &amp;quot;Diagonals&amp;quot;     &amp;quot;ThinDiagonals&amp;quot;
[29] &amp;quot;LineEnds&amp;quot;      &amp;quot;LineJunctions&amp;quot; &amp;quot;Ridges&amp;quot;        &amp;quot;ConvexHull&amp;quot;   
[33] &amp;quot;ThinSe&amp;quot;        &amp;quot;Skeleton&amp;quot;      &amp;quot;Chebyshev&amp;quot;     &amp;quot;Manhattan&amp;quot;    
[37] &amp;quot;Octagonal&amp;quot;     &amp;quot;Euclidean&amp;quot;     &amp;quot;User Defined&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a lot of kernels.&lt;/p&gt;

&lt;h2 id=&#34;fuzz-scaling&#34;&gt;Fuzz Scaling&lt;/h2&gt;

&lt;p&gt;Finally one more (breaking) change: several functions in magick use a &lt;code&gt;fuzz&lt;/code&gt; parameter to specify the max distance between two colors to be considered similar.&lt;/p&gt;

&lt;p&gt;For example the flood fill algorithm (the paint-bucket button in ms-paint) changes the color of a given starting pixel, and then recursively all adjacent pixels that have the same color. However sometimes neighboring pixels are not precisely the same color, but nearly the same. The &lt;code&gt;fuzz&lt;/code&gt; parameter allows the fill to continue when pixels are not the same but similar color.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Paint the shirt orange
frink &amp;lt;- image_read(&amp;quot;https://jeroen.github.io/images/frink.png&amp;quot;) %&amp;gt;%
  image_fill(&amp;quot;orange&amp;quot;, point = &amp;quot;+100+200&amp;quot;, fuzz = 25)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/VwlqYWy.png&#34; alt=&#34;frink&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What has changed in this version is that &lt;code&gt;fuzz&lt;/code&gt; parameter been rescaled to a percentage. Hence you should always provide a value between 0 and 100. Previously it was the absolute distance between colors, but this depends on the type and color depth of the image at hand, which was very confusing.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>solrium 1.0: Working with Solr from R</title>
      <link>https://ropensci.org/technotes/2017/11/08/solrium-solr-r/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/11/08/solrium-solr-r/</guid>
      <description>
        
        

&lt;p&gt;Nearly 4 years ago I wrote on this blog about an R package &lt;a href=&#34;https://github.com/ropensci/solr&#34;&gt;solr&lt;/a&gt; for working with the database &lt;a href=&#34;https://lucene.apache.org/solr/&#34;&gt;Solr&lt;/a&gt;. Since then we&amp;rsquo;ve created a refresh of that package in the &lt;a href=&#34;https://github.com/ropensci/solrium&#34;&gt;solrium&lt;/a&gt; package. Since &lt;code&gt;solrium&lt;/code&gt; first hit CRAN about two years ago, users have raised a number of issues that required breaking changes. Thus, this blog post is about a major version bump in &lt;code&gt;solrium&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-is-solr&#34;&gt;What is Solr?&lt;/h2&gt;

&lt;p&gt;Solr is a &amp;ldquo;search platform&amp;rdquo; - a NoSQL database - data is organized by so called documents that are xml/json/etc blobs of text. Documents are nested within either collections or cores (depending on the mode you start Solr in). Solr makes it easy to search for documents, with a huge variety of parameters, and a number of different data formats (json/xml/csv). Solr is similar to &lt;a href=&#34;https://www.elastic.co/products/elasticsearch&#34;&gt;Elasticsearch&lt;/a&gt; (see our Elasticsearch client &lt;a href=&#34;https://github.com/ropensci/elastic&#34;&gt;elastic&lt;/a&gt;) - and was around before it. Solr in my opinion is harder to setup than Elasticsearch, but I don&amp;rsquo;t claim to be an expert on either.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;vignettes&#34;&gt;Vignettes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/solrium/vignettes/search.html&#34;&gt;Solr Search with solrium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/solrium/vignettes/local_setup.html&#34;&gt;Local Solr setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/solrium/vignettes/document_management.html&#34;&gt;Document management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/solrium/vignettes/cores_collections.html&#34;&gt;Cores/collections management&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;noteable-features&#34;&gt;Noteable features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Added in v1, you can now work with many connection objects to different Solr instances.&lt;/li&gt;
&lt;li&gt;Methods for the major search functionalities: search, highlight, stats, mlt, group, and facet. In addition, a catch all function &lt;code&gt;all&lt;/code&gt; to combine all of those.&lt;/li&gt;
&lt;li&gt;Comprehensive coverage of the Solr HTTP API&lt;/li&gt;
&lt;li&gt;Can coerce data from Solr API into data.frame&amp;rsquo;s when possible&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Install &lt;code&gt;solrium&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;solrium&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/solrium&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(solrium)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;initialize-a-client&#34;&gt;Initialize a client&lt;/h2&gt;

&lt;p&gt;A big change in &lt;code&gt;v1&lt;/code&gt; of &lt;code&gt;solrium&lt;/code&gt; is &lt;code&gt;solr_connect&lt;/code&gt; has been replaced by &lt;code&gt;SolrClient&lt;/code&gt;. Now you create an &lt;code&gt;R6&lt;/code&gt; connection object with &lt;code&gt;SolrClient&lt;/code&gt;, then you can call methods on that &lt;code&gt;R6&lt;/code&gt; object, &lt;strong&gt;OR&lt;/strong&gt; you can pass the connection object to functions.&lt;/p&gt;

&lt;p&gt;By default, &lt;code&gt;SolrClient$new()&lt;/code&gt; sets connections details for a Solr instance that&amp;rsquo;s running on &lt;code&gt;localhost&lt;/code&gt;, and on port &lt;code&gt;8983&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(conn &amp;lt;- SolrClient$new())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; &amp;lt;Solr Client&amp;gt;
#&amp;gt;   host: 127.0.0.1
#&amp;gt;   path: 
#&amp;gt;   port: 8983
#&amp;gt;   scheme: http
#&amp;gt;   errors: simple
#&amp;gt;   proxy:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On instantiation, it does not check that the Solr instance is up, but merely sets connection details. You can check if the instance is up by doing for example (assuming you have a collection named &lt;code&gt;gettingstarted&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;conn$ping(&amp;quot;gettingstarted&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; $responseHeader
#&amp;gt; $responseHeader$zkConnected
#&amp;gt; [1] TRUE
#&amp;gt; 
#&amp;gt; $responseHeader$status
#&amp;gt; [1] 0
#&amp;gt; 
#&amp;gt; $responseHeader$QTime
#&amp;gt; [1] 163
#&amp;gt; 
#&amp;gt; $responseHeader$params
#&amp;gt; $responseHeader$params$q
#&amp;gt; [1] &amp;quot;{!lucene}*:*&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$distrib
#&amp;gt; [1] &amp;quot;false&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$df
#&amp;gt; [1] &amp;quot;_text_&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$rows
#&amp;gt; [1] &amp;quot;10&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$wt
#&amp;gt; [1] &amp;quot;json&amp;quot;
#&amp;gt; 
#&amp;gt; $responseHeader$params$echoParams
#&amp;gt; [1] &amp;quot;all&amp;quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $status
#&amp;gt; [1] &amp;quot;OK&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A good hint when connecting to a publicly exposed Solr instance is that you likely don&amp;rsquo;t need to specify a port, so a pattern like this should work to connect to a URL like &lt;code&gt;http://foobar.com/search&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SolrClient$new(host = &amp;quot;foobar.com&amp;quot;, path = &amp;quot;search&amp;quot;, port = NULL)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the instance uses SSL, simply specify that like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;SolrClient$new(host = &amp;quot;foobar.com&amp;quot;, path = &amp;quot;search&amp;quot;, port = NULL, scheme = &amp;quot;https&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;query-and-body-parameters&#34;&gt;Query and body parameters&lt;/h2&gt;

&lt;p&gt;Another big change in the package is that we wanted to make it easy to determine whether your Solr query gets passed as query parameters in a &lt;code&gt;GET&lt;/code&gt; request or as body in a &lt;code&gt;POST&lt;/code&gt; request. Solr clients in some other languages do this, and it made sense to port over that idea here. Now you pass your key-value pairs to either &lt;code&gt;params&lt;/code&gt; or &lt;code&gt;body&lt;/code&gt;. If nothing is passed to &lt;code&gt;body&lt;/code&gt;, we do a &lt;code&gt;GET&lt;/code&gt; request. If something is passed to &lt;code&gt;body&lt;/code&gt; we do a &lt;code&gt;POST&lt;/code&gt; request, even if there&amp;rsquo;s also key-value pairs passed to &lt;code&gt;params&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This change does break the interface we had in the old version, but we think it&amp;rsquo;s worth it.&lt;/p&gt;

&lt;p&gt;For example, to do a search you have to pass the collection name and a list of named parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;conn$search(name = &amp;quot;gettingstarted&amp;quot;, params = list(q = &amp;quot;*:*&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 5 x 5
#&amp;gt;      id   title title_str  `_version_` price
#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
#&amp;gt; 1    10 adfadsf   adfadsf 1.582913e+18    NA
#&amp;gt; 2    12  though    though 1.582913e+18    NA
#&amp;gt; 3    14 animals   animals 1.582913e+18    NA
#&amp;gt; 4     1    &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt; 1.582913e+18   100
#&amp;gt; 5     2    &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt; 1.582913e+18   500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can instead pass the connection object to &lt;code&gt;solr_search&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;solr_search(conn, name = &amp;quot;gettingstarted&amp;quot;, params = list(q = &amp;quot;*:*&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; # A tibble: 5 x 5
#&amp;gt;      id   title title_str  `_version_` price
#&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
#&amp;gt; 1    10 adfadsf   adfadsf 1.582913e+18    NA
#&amp;gt; 2    12  though    though 1.582913e+18    NA
#&amp;gt; 3    14 animals   animals 1.582913e+18    NA
#&amp;gt; 4     1    &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt; 1.582913e+18   100
#&amp;gt; 5     2    &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt; 1.582913e+18   500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the same pattern applies for the other functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;solr_facet&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_group&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_mlt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_highlight&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_stats&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;solr_all&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;new-functions-for-atomic-updates&#34;&gt;New functions for atomic updates&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ropensci/solrium/issues/97&#34;&gt;A user requested&lt;/a&gt; the ability to do &lt;a href=&#34;https://lucene.apache.org/solr/guide/7_0/updating-parts-of-documents.html&#34;&gt;atomic updates&lt;/a&gt; - partial updates to documents without having to re-index the entire document.&lt;/p&gt;

&lt;p&gt;Two functions were added: &lt;code&gt;update_atomic_json&lt;/code&gt; and &lt;code&gt;update_atomic_xml&lt;/code&gt; for JSON and XML based updates. Check out their help pages for usage.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;search-results-as-attributes&#34;&gt;Search results as attributes&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;solr_search&lt;/code&gt; and &lt;code&gt;solr_all&lt;/code&gt; in &lt;code&gt;v1&lt;/code&gt; gain attributes that include &lt;code&gt;numFound&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, and &lt;code&gt;maxScore&lt;/code&gt;. That is, you can get to these three values after data is returned. Note that some Solr instances may not return all three values.&lt;/p&gt;

&lt;p&gt;For example, let&amp;rsquo;s use the Public Library of Science Solr search instance at &lt;a href=&#34;http://api.plos.org/search&#34;&gt;http://api.plos.org/search&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plos &amp;lt;- SolrClient$new(host = &amp;quot;api.plos.org&amp;quot;, path = &amp;quot;search&amp;quot;, port = NULL)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Search&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- plos$search(params = list(q = &amp;quot;*:*&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get attributes&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;attr(res, &amp;quot;numFound&amp;quot;)
#&amp;gt; [1] 1902279
attr(res, &amp;quot;start&amp;quot;)
#&amp;gt; [1] 0
attr(res, &amp;quot;maxScore&amp;quot;)
#&amp;gt; [1] 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;automatically-adjust-rows-parameter&#34;&gt;Automatically adjust rows parameter&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ropensci/solrium/pull/102&#34;&gt;A user higlighted&lt;/a&gt; that &lt;a href=&#34;https://wiki.apache.org/solr/SolrPerformanceProblems#Asking_for_too_many_rows&#34;&gt;there&amp;rsquo;s a performance penalty when asking for too many rows&lt;/a&gt;. The resulting change in &lt;code&gt;solrium&lt;/code&gt; is that in some search functions we automatically adjust the &lt;code&gt;rows&lt;/code&gt; parameter to avoid the performance penalty.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;usage-in-other-packages&#34;&gt;Usage in other packages&lt;/h2&gt;

&lt;p&gt;I maintain 4 other packages that use &lt;code&gt;solrium&lt;/code&gt;: &lt;a href=&#34;https://github.com/ropensci/rplos&#34;&gt;rplos&lt;/a&gt;, &lt;a href=&#34;https://github.com/ropensci/ritis&#34;&gt;ritis&lt;/a&gt;, &lt;a href=&#34;https://github.com/ropensci/rdatacite&#34;&gt;rdatacite&lt;/a&gt;, and &lt;a href=&#34;https://github.com/ropensci/rdryad&#34;&gt;rdryad&lt;/a&gt;. If you are interested in using &lt;code&gt;solrium&lt;/code&gt; in your package, looking at any of those four packages will give a good sense of how to do it.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;

&lt;h3 id=&#34;solr-pkg&#34;&gt;solr pkg&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;solr&lt;/code&gt; package will soon be archived on CRAN. We&amp;rsquo;ve moved all packages depending on it to &lt;code&gt;solrium&lt;/code&gt;. Let me know ASAP if you have any complaints about archiving it on CRAN.&lt;/p&gt;

&lt;h3 id=&#34;feedback&#34;&gt;Feedback!&lt;/h3&gt;

&lt;p&gt;Please do upgrade/install &lt;code&gt;solrium&lt;/code&gt;  &lt;code&gt;v1&lt;/code&gt; and let us know what you think.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Using Magick with RMarkdown and Shiny</title>
      <link>https://ropensci.org/technotes/2017/11/07/magick-knitr/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/11/07/magick-knitr/</guid>
      <description>
        
        &lt;img src=&quot;https://i.imgur.com/tTFk7ig.jpg&quot; alt=&quot;cover image&quot;&gt;
        
        

&lt;p&gt;This week &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick&lt;/a&gt; 1.5 appeared on CRAN. The latest update adds support for using images in knitr documents and shiny apps. In this post we show how this nicely ties together a reproducible image workflow in R, from source image or plot directly into your report or application.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)
stopifnot(packageVersion(&#39;magick&#39;) &amp;gt;= 1.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also the magick &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;intro vignette&lt;/a&gt; has been updated in this version to cover the latest features available in the package.&lt;/p&gt;

&lt;h2 id=&#34;magick-in-knitr-rmarkdown-documents&#34;&gt;Magick in Knitr / RMarkdown Documents&lt;/h2&gt;

&lt;p&gt;Magick 1.5 is now fully compatible with knitr. To embed magick images in your rmarkdown report, simply use standard code chunk syntax in your &lt;code&gt;Rmd&lt;/code&gt; file. No special options or packages are required; the image automatically appears in your documents when printed!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Example from our post last week
image_read(&#39;logo:&#39;) %&amp;gt;%
  image_convolve(&#39;DoG:0,0,2&#39;) %&amp;gt;%
  image_negate() %&amp;gt;%
  image_resize(&amp;quot;400x400&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/PhwCJ4k.gif&#34; alt=&#34;fig1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can also combine this with the magick graphics device to post process or animate your plots and figures directly in knitr. Again no special packages or system dependencies are required.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Produce graphic
fig &amp;lt;- image_graph(width = 800, height = 600, res = 96)
ggplot2::qplot(factor(cyl), data = mtcars, fill = factor(gear))
invisible(dev.off())

print(fig)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/zFLcHws.png&#34; alt=&#34;fig2&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
# Some post-processing
frink &amp;lt;- image_read(&amp;quot;https://jeroen.github.io/images/frink.png&amp;quot;)

fig %&amp;gt;%
  image_rotate(10) %&amp;gt;%
  image_implode(.6) %&amp;gt;%
  image_composite(frink, offset = &amp;quot;+140+70&amp;quot;) %&amp;gt;%
  image_annotate(&amp;quot;Very usefull stuff&amp;quot;, size = 40, location = &amp;quot;+300+100&amp;quot;, color = &amp;quot;navy&amp;quot;, boxcolor = &amp;quot;pink&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/0E5cqaz.png&#34; alt=&#34;fig3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Same works for animation with &lt;code&gt;image_animate()&lt;/code&gt;; the figure shows automatically up in the report as a gif image:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;image_read(&amp;quot;https://jeroen.github.io/images/banana.gif&amp;quot;) %&amp;gt;%
  image_apply( function(banana){
    image_composite(fig, banana, offset = &amp;quot;+200+200&amp;quot;)
  }) %&amp;gt;%
  image_resize(&amp;quot;50%&amp;quot;) %&amp;gt;%
  image_animate()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/mi67gjt.gif&#34; alt=&#34;fig4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The magick vignette &lt;a href=&#34;https://raw.githubusercontent.com/ropensci/magick/master/vignettes/intro.Rmd&#34;&gt;source code&lt;/a&gt; is itself written in Rmarkdown, so it&amp;rsquo;s great example to see this in action. Try rendering it in RStudio to see how easy it is!&lt;/p&gt;

&lt;h2 id=&#34;magick-in-shiny-apps&#34;&gt;Magick in Shiny Apps&lt;/h2&gt;

&lt;p&gt;While we&amp;rsquo;re at it, several people had asked how to use magick images in shiny apps. The easiest way is to write the image to a &lt;code&gt;tempfile()&lt;/code&gt; within the &lt;code&gt;renderImage()&lt;/code&gt; callback function. For example the server part could look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;output$img &amp;lt;- renderImage({
    tmpfile &amp;lt;- image %&amp;gt;%
      image_resize(input$size) %&amp;gt;%
      image_implode(input$implode) %&amp;gt;%
      image_blur(input$blur, input$blur) %&amp;gt;%
      image_rotate(input$rotation) %&amp;gt;%
      image_write(tempfile(fileext=&#39;jpg&#39;), format = &#39;jpg&#39;)

  # Return a list
  list(src = tmpfile, contentType = &amp;quot;image/jpeg&amp;quot;)
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is a simple shiny app that demonstrates this. Have a look at the &lt;a href=&#34;https://github.com/jeroen/shinymagick/blob/master/app.R&#34;&gt;source code&lt;/a&gt; or just run it in R:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(shiny)
library(magick)
runGitHub(&amp;quot;jeroen/shinymagick&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://jeroen.shinyapps.io/shinymagick&#34;&gt;&lt;img src=&#34;https://i.imgur.com/tTFk7ig.jpg&#34; alt=&#34;tigrou&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Perhaps there&amp;rsquo;s an even better way to make this work by wrapping magick images into an htmlwidget but I have not figured this out yet.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Image Convolution in R using Magick</title>
      <link>https://ropensci.org/technotes/2017/11/02/image-convolve/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/11/02/image-convolve/</guid>
      <description>
        
        

&lt;p&gt;Release 1.4 of the &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick package&lt;/a&gt; introduces
a new feature called &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_(image_processing)#Convolution&#34;&gt;image convolution&lt;/a&gt; that
was requested by Thomas L. Pedersen. In this post we explain what this is all about.&lt;/p&gt;

&lt;h2 id=&#34;kernel-matrix&#34;&gt;Kernel Matrix&lt;/h2&gt;

&lt;p&gt;The new &lt;code&gt;image_convolve()&lt;/code&gt; function applies a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_(image_processing)&#34;&gt;kernel&lt;/a&gt; over the image. Kernel convolution means that each pixel value is recalculated using the &lt;em&gt;weighted neighborhood sum&lt;/em&gt; defined in the kernel matrix. For example lets look at this simple kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)

kern &amp;lt;- matrix(0, ncol = 3, nrow = 3)
kern[1, 2] &amp;lt;- 0.25
kern[2, c(1, 3)] &amp;lt;- 0.25
kern[3, 2] &amp;lt;- 0.25
kern
##      [,1] [,2] [,3]
## [1,] 0.00 0.25 0.00
## [2,] 0.25 0.00 0.25
## [3,] 0.00 0.25 0.00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This kernel changes each pixel to the mean of its horizontal and vertical neighboring pixels, which results in a slight blurring effect in the right-hand image below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img &amp;lt;- image_read(&#39;logo:&#39;)
img_blurred &amp;lt;- image_convolve(img, kern)
image_append(c(img, img_blurred))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Y6xByUL.gif&#34; alt=&#34;image_appended&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;standard-kernels&#34;&gt;Standard Kernels&lt;/h2&gt;

&lt;p&gt;Many operations in &lt;code&gt;magick&lt;/code&gt;  such as blurring, sharpening, and edge detection are
actually special cases of image convolution. The benefit of explicitly using
&lt;code&gt;image_convolve()&lt;/code&gt; is more control. For example, we can blur an image and then blend
it together with the original image in one step by mixing a blurring kernel with the
unit kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;Gaussian:0x5&#39;, scaling = &#39;60,40%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/6Vf6c2hl.gif&#34; alt=&#34;mixed&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The above requires a bit of explanation. ImageMagick defines several common
&lt;a href=&#34;http://www.imagemagick.org/Usage/convolve/&#34;&gt;standard kernels&lt;/a&gt; such as the
gaussian kernel. Most of the standard kernels take one or more parameters,
e.g. the example above used a gaussian kernel with 0 &lt;em&gt;radius&lt;/em&gt; and 5 &lt;em&gt;sigma&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In addition, &lt;code&gt;scaling&lt;/code&gt; argument defines the magnitude of the kernel, and possibly
how much of the original picture should be mixed in. Here we mix 60% of the
blurring with 40% of the original picture in order to get a diffused lightning effect.&lt;/p&gt;

&lt;h2 id=&#34;edge-detection&#34;&gt;Edge Detection&lt;/h2&gt;

&lt;p&gt;Another area where kernels are of use is in edge detection. A simple example of
a direction-aware edge detection kernel is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sobel_operator&#34;&gt;&lt;em&gt;Sobel&lt;/em&gt;&lt;/a&gt; kernel.
As can be seen below, vertical edges are detected while horizontals are not.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;Sobel&#39;) %&amp;gt;% image_negate()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/i8ndfCu.gif&#34; alt=&#34;edges&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Something less apparent is that the result of the edge detection is truncated.
Edge detection kernels can result in negative color values which get truncated to zero.
To combat this it is possible to add a &lt;code&gt;bias&lt;/code&gt; to the result. Often you&amp;rsquo;ll end up with
scaling the kernel to 50% and adding 50% bias to move the midpoint of the result to 50%
grey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;Sobel&#39;, scaling = &#39;50%&#39;, bias = &#39;50%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/llUawrg.gif&#34; alt=&#34;50pct&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sharpening&#34;&gt;Sharpening&lt;/h2&gt;

&lt;p&gt;ImageMagick has many more edge detection kernels, some of which are insensitive to
the direction of the edge. To emulate a classic high-pass filter from photoshop use
&lt;a href=&#34;https://en.wikipedia.org/wiki/Difference_of_Gaussians&#34;&gt;difference of gaussians&lt;/a&gt; kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;DoG:0,0,2&#39;) %&amp;gt;% image_negate()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/o5kODpc.gif&#34; alt=&#34;dog&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As with the blurring, the original image can be blended in with the transformed one, effectively sharpening the image along edges.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;img %&amp;gt;% image_convolve(&#39;DoG:0,0,2&#39;, scaling = &#39;100, 100%&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/MtcMSn7.gif&#34; alt=&#34;combination&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://www.imagemagick.org/Usage/convolve/&#34;&gt;ImageMagick documentation&lt;/a&gt; has more examples of convolve with various avaiable kernels.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Changes to Internet Connectivity in R on Windows</title>
      <link>https://ropensci.org/technotes/2017/10/10/curl-30/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/10/10/curl-30/</guid>
      <description>
        
        

&lt;p&gt;This week we released version 3.0 of the &lt;a href=&#34;https://cran.r-project.org/web/packages/curl/vignettes/intro.html&#34;&gt;curl&lt;/a&gt; R package to CRAN. You may have never used this package directly, but &lt;code&gt;curl&lt;/code&gt; provides the foundation for most HTTP infrastructure in R, including &lt;code&gt;httr&lt;/code&gt;, &lt;code&gt;rvest&lt;/code&gt;, and all packages that build on it. If R packages need to go online, chances are traffic is going via curl.&lt;/p&gt;

&lt;p&gt;This release introduces an important change for Windows users: we are switching from OpenSSL to Secure Channel on Windows 7 / 2008-R2 and up. Let me explain this in a bit more detail.&lt;/p&gt;

&lt;h2 id=&#34;why-switching-ssl-backends&#34;&gt;Why Switching SSL backends&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://curl.haxx.se/libcurl/&#34;&gt;libcurl&lt;/a&gt; C library requires an external crypto library to provide the SSL layer (the S in HTTPS). On Linux / MacOS, libcurl is included with the OS so we don&amp;rsquo;t worry about this. However on Windows we ship our own build of libcurl so we can choose if we want to build against &lt;a href=&#34;https://www.openssl.org/&#34;&gt;OpenSSL&lt;/a&gt; or Windows native SSL api called &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/aa380123(v=vs.85).aspx&#34;&gt;Secure Channel&lt;/a&gt;, also referred to as just &amp;ldquo;WinSSL&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Thus far we have always used libcurl with OpenSSL, which works consistently on all versions of Windows. However OpenSSL requires that we provide our own CA bundle, which is not ideal. In particular users on corporate / government networks have reported difficulty connecting to the internet in R. The reason is often that their enterprise gateway / proxy uses custom certificates which are installed in the Windows certificate manager, but are not present in R&amp;rsquo;s bundle.&lt;/p&gt;

&lt;p&gt;Moreover shipping our own CA bundle can be a security risk. If a CA gets hacked, the corresponding certificate needs to be revoked immediately. Operating systems can quickly push a security update to all users, but we cannot do this in R.&lt;/p&gt;

&lt;h2 id=&#34;switching-to-winssl&#34;&gt;Switching to WinSSL&lt;/h2&gt;

&lt;p&gt;If we build libcurl against Windows native &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/aa380123(v=vs.85).aspx&#34;&gt;Secure Channel&lt;/a&gt;, it automatically uses the same SSL certificates as Internet Explorer. Hence we do not have to ship and maintain a custom CA bundle. Earlier this year I tried to switch the &lt;code&gt;curl&lt;/code&gt; package to WinSSL, and everything seemed to work great on my machine.&lt;/p&gt;

&lt;p&gt;However when we started checking reverse dependecies on CRAN WinBuilder, many packages depending on curl started to fail! It turned out Windows versions before Windows 7 do not natively support TLS 1.1 and 1.2 by default. Because TLS 1.2 is used by the majority of HTTPS servers today, WinSSL is basically useless on these machines. Unfortunately this also includes CRAN WinBuilder which runs Windows 2008 (the server edition of Vista).&lt;/p&gt;

&lt;p&gt;So we had no choice but to roll back to OpenSSL in order to keep everything working properly on CRAN. Bummer.&lt;/p&gt;

&lt;h2 id=&#34;towards-dual-ssl&#34;&gt;Towards Dual SSL&lt;/h2&gt;

&lt;p&gt;I had almost given up on this when a few weeks ago Daniel Stenberg posted the following &lt;a href=&#34;https://curl.haxx.se/mail/lib-2017-08/0118.html&#34;&gt;announcement&lt;/a&gt; on the libcurl mailing list:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hi friends!
As of minutes ago, libcurl has the ability to change SSL backend dynamically
at run-time - if built with the support enabled. That means that the choice
does no longer only have to happen at build-time.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This new feature gives us exactly the flexibility we need. We can take advantage of native Secure Channel on Windows 7 and up which are almost all users. However we can keep things working in legacy servers by falling back on OpenSSL on these machines, including the CRAN win builder.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cran/curl/blob/3.0/src/ssl.c#L11-L17&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/curl30.png&#34; alt=&#34;code&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So this is where we are. Version 3.0 of the curl R package uses the latest &lt;a href=&#34;https://github.com/rwinlib/libcurl/releases&#34;&gt;libcurl 7.56.0&lt;/a&gt; and automatically switches to native SSL on Windows 7 and up. If all goes well, nobody should not notice any changes, except those people on enterprise networks where things will, hopefully, magically start working.&lt;/p&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;Because each Windows network seems to have a different setup, testing and debugging these things is often difficult. We are interested to hear from Windows users if updating to curl 3.0 has improved the situation, or if any unexpected side effects arise. Please &lt;a href=&#34;https://github.com/jeroen/curl/issues&#34;&gt;open an issue&lt;/a&gt; on Github if you run into problems.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>The writexl package: zero dependency xlsx writer for R</title>
      <link>https://ropensci.org/technotes/2017/09/08/writexl-release/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/09/08/writexl-release/</guid>
      <description>
        
        

&lt;p&gt;We have started working on a new rOpenSci package called &lt;a href=&#34;https://github.com/ropensci/writexl#readme&#34;&gt;writexl&lt;/a&gt;. This package wraps the very powerful &lt;a href=&#34;https://libxlsxwriter.github.io/&#34;&gt;libxlsxwriter&lt;/a&gt; library which allows for exporting data to Microsoft Excel format.&lt;/p&gt;

&lt;p&gt;The major benefit of writexl over other packages is that it is completely written in C and has absolutely zero dependencies. No Java, Perl or Rtools are required.&lt;/p&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;write_xlsx&lt;/code&gt; function writes a data frame to an xlsx file. You can test that data roundtrips properly by reading it back using the readxl package. Columns containing dates and factors get automatically coerced to character strings.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(writexl)
library(readxl)
write_xlsx(iris, &amp;quot;iris.xlsx&amp;quot;)

# read it back
out &amp;lt;- read_xlsx(&amp;quot;iris.xlsx&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also give it a named list of data frames, in which case each data frame becomes a sheet in the xlsx file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;write_xlsx(list(iris = iris, cars = cars, mtcars = mtcars), &amp;quot;mydata.xlsx&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance is good too; in our benchmarks writexl is about twice as fast as openxlsx:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(microbenchmark)
library(nycflights13)
microbenchmark(
  writexl = writexl::write_xlsx(flights, tempfile()),
  openxlsx = openxlsx::write.xlsx(flights, tempfile()),
  times = 5
)
### Unit: seconds
###      expr       min        lq      mean    median        uq       max neval
###   writexl  8.884712  8.904431  9.103419  8.965643  9.041565  9.720743     5
###  openxlsx 17.166818 18.072527 19.171003 18.669805 18.756661 23.189206     5
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;roadmap&#34;&gt;Roadmap&lt;/h2&gt;

&lt;p&gt;The initial version of writexl implements the most important functionality for R users: exporting data frames. However the underlying &lt;a href=&#34;https://libxlsxwriter.github.io/&#34;&gt;libxlsxwriter&lt;/a&gt; library actually provides far more sophisticated functionality such as custom formatting, writing complex objects, formulas, etc.&lt;/p&gt;

&lt;p&gt;Most of this probably won&amp;rsquo;t be useful to R users. But if you have a well defined use case for exposing some specific features from the library in writexl, &lt;a href=&#34;https://github.com/ropensci/writexl/issues&#34;&gt;open an issue&lt;/a&gt; on Github and we&amp;rsquo;ll look into it!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Spelling 1.0: quick and effective spell checking in R</title>
      <link>https://ropensci.org/technotes/2017/09/07/spelling-release/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/09/07/spelling-release/</guid>
      <description>
        
        

&lt;p&gt;The new rOpenSci &lt;a href=&#34;https://cran.r-project.org/web/packages/spelling/index.html&#34;&gt;spelling&lt;/a&gt; package provides utilities for spell checking common document formats including latex, markdown, manual pages, and DESCRIPTION files. It also includes tools especially for package authors to automate spell checking of R documentation and vignettes.&lt;/p&gt;

&lt;h2 id=&#34;spell-checking-packages&#34;&gt;Spell Checking Packages&lt;/h2&gt;

&lt;p&gt;The main purpose of this package is to quickly find spelling errors in R packages. The &lt;code&gt;spell_check_package()&lt;/code&gt; function extracts all text from your package manual pages and vignettes, compares it against a language (e.g. en_US or en_GB), and lists potential errors in a nice tidy format:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; spelling::spell_check_package(&amp;quot;~/workspace/writexl&amp;quot;)
  WORD       FOUND IN
booleans   write_xlsx.Rd:21
xlsx       write_xlsx.Rd:6,18
           title:1
           description:1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results may contain false positives, i.e. names or technical jargon which does not appear in the English dictionary. Therefore you can create a WORDLIST file, which serves as a package-specific dictionary of allowed words:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; spelling::update_wordlist(&amp;quot;~/workspace/writexl&amp;quot;)
The following words will be added to the wordlist:
 - booleans
 - xlsx
Are you sure you want to update the wordlist?
1: Yes
2: No
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Words added to this file are ignored in the spell check, making it easier to catch actual spelling errors:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; spell_check_package(&amp;quot;~/workspace/writexl&amp;quot;)
No spelling errors found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The package also includes a cool function &lt;code&gt;spell_check_setup()&lt;/code&gt; which adds a unit test to your package that automatically runs the spell check.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; spelling::spell_check_setup(&amp;quot;~/workspace/writexl&amp;quot;)
No changes required to /Users/jeroen/workspace/writexl/inst/WORDLIST
Updated /Users/jeroen/workspace/writexl/tests/spelling.R
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By default this unit test will never actually fail; it merely displays potential spelling errors at the end of a &lt;code&gt;R CMD check&lt;/code&gt;. But you can configure it to fail if you&amp;rsquo;d like, which can be useful to automatically highlight spelling errors on e.g. Travis CI.&lt;/p&gt;

&lt;h2 id=&#34;under-the-hood&#34;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;The spelling package builds on &lt;a href=&#34;https://ropensci.org/blog/technotes/2016/09/12/hunspell-release-20&#34;&gt;hunspell&lt;/a&gt; which has a fully customizable spell checking engine. Most of the code in the spelling package is dedicated to parsing and extracting text from documents before feeding it to the spell checker.
For example, when spell checking an rmarkdown file, we first extract words from headers and paragraphs (but not urls or R syntax).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Spell check this post
&amp;gt; spelling::spell_check_files(&amp;quot;~/workspace/roweb/_posts/2017-09-07-spelling-release.md&amp;quot;, lang = &#39;en_US&#39;)
  WORD         FOUND IN
blog         2017-09-07-spelling-release.md:7
commonmark   2017-09-07-spelling-release.md:88
hunspell     2017-09-07-spelling-release.md:69
Jeroen       2017-09-07-spelling-release.md:7
knitr        2017-09-07-spelling-release.md:88
Ooms         2017-09-07-spelling-release.md:7
rmarkdown    2017-09-07-spelling-release.md:88
rOpenSci     2017-09-07-spelling-release.md:18
urls         2017-09-07-spelling-release.md:88
wordlist     2017-09-07-spelling-release.md:49
WORDLIST     2017-09-07-spelling-release.md:34
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To accomplish this, we use knitr to drop code chunks, and subsequently parse markdown using &lt;a href=&#34;https://ropensci.org/blog/blog/2016/12/02/commonmark&#34;&gt;commonmark&lt;/a&gt; and xml2, which gives us the text nodes and approximate line numbers in the source document.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>FedData - Getting assorted geospatial data into R</title>
      <link>https://ropensci.org/technotes/2017/08/24/feddata-release/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/08/24/feddata-release/</guid>
      <description>
        
        

&lt;p&gt;The package &lt;a href=&#34;https://github.com/ropensci/FedData&#34;&gt;&lt;code&gt;FedData&lt;/code&gt;&lt;/a&gt; has gone through software review and is now part of &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt;. &lt;code&gt;FedData&lt;/code&gt; includes functions to automate downloading geospatial data available from several federated data sources (mainly sources maintained by the US Federal government).&lt;/p&gt;

&lt;p&gt;Currently, the package enables extraction from six datasets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;http://ned.usgs.gov&#34;&gt;National Elevation Dataset (NED)&lt;/a&gt; digital elevation models (1 and &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; arc-second; USGS)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://nhd.usgs.gov&#34;&gt;National Hydrography Dataset (NHD)&lt;/a&gt; (USGS)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://websoilsurvey.sc.egov.usda.gov/&#34;&gt;Soil Survey Geographic (SSURGO) database&lt;/a&gt; from the National Cooperative Soil Survey (NCSS), which is led by the Natural Resources Conservation Service (NRCS) under the USDA,&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn&#34;&gt;Global Historical Climatology Network (GHCN)&lt;/a&gt;, coordinated by National Climatic Data Center at NOAA,&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://daymet.ornl.gov/&#34;&gt;Daymet&lt;/a&gt; gridded estimates of daily weather parameters for North America, version 3, available from the Oak Ridge National Laboratory&amp;rsquo;s Distributed Active Archive Center (DAAC), and&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://www.ncdc.noaa.gov/data-access/paleoclimatology-data/datasets/tree-ring&#34;&gt;International Tree Ring Data Bank (ITRDB)&lt;/a&gt;, coordinated by National Climatic Data Center at NOAA.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; is designed with the large-scale geographic information system (GIS) use-case in mind: cases where the use of dynamic web-services is impractical due to the scale (spatial and/or temporal) of analysis. It functions primarily as a means of downloading tiled or otherwise spatially-defined datasets; additionally, it can preprocess those datasets by extracting data within an area of interest (AoI), defined spatially. It relies heavily on the &lt;a href=&#34;https://cran.r-project.org/package=sp&#34;&gt;&lt;code&gt;sp&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/package=raster&#34;&gt;&lt;code&gt;raster&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://cran.r-project.org/package=rgdal&#34;&gt;&lt;code&gt;rgdal&lt;/code&gt;&lt;/a&gt; packages.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Load &lt;code&gt;FedData&lt;/code&gt; and define a study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# FedData Tester
library(FedData)
library(magrittr)

# Extract data for the Village Ecodynamics Project &amp;quot;VEPIIN&amp;quot; study area:
# http://veparchaeology.org
vepPolygon &amp;lt;- polygon_from_extent(raster::extent(672800, 740000, 4102000, 4170000),
                                  proj4string = &amp;quot;+proj=utm +datum=NAD83 +zone=12&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get and plot the National Elevation Dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NED (USA ONLY)
# Returns a raster
NED &amp;lt;- get_ned(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;)
# Plot with raster::plot
raster::plot(NED)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the Daymet dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the DAYMET (North America only)
# Returns a raster
DAYMET &amp;lt;- get_daymet(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;,
               elements = c(&amp;quot;prcp&amp;quot;,&amp;quot;tmax&amp;quot;),
               years = 1980:1985)
# Plot with raster::plot
raster::plot(DAYMET$tmax$X1985.10.23)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the daily GHCN precipitation data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the daily GHCN data (GLOBAL)
# Returns a list: the first element is the spatial locations of stations,
# and the second is a list of the stations and their daily data
GHCN.prcp &amp;lt;- get_ghcn_daily(template = vepPolygon,
                            label = &amp;quot;VEPIIN&amp;quot;,
                            elements = c(&#39;prcp&#39;))
# Plot the NED again
raster::plot(NED)
# Plot the spatial locations
sp::plot(GHCN.prcp$spatial,
         pch = 1,
         add = TRUE)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend=&amp;quot;GHCN Precipitation Records&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the daily GHCN temperature data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Elements for which you require the same data
# (i.e., minimum and maximum temperature for the same days)
# can be standardized using standardize==T
GHCN.temp &amp;lt;- get_ghcn_daily(template = vepPolygon,
                            label = &amp;quot;VEPIIN&amp;quot;,
                            elements = c(&#39;tmin&#39;,&#39;tmax&#39;),
                            years = 1980:1985,
                            standardize = TRUE)
# Plot the NED again
raster::plot(NED)
# Plot the spatial locations
sp::plot(GHCN.temp$spatial,
         add = TRUE,
         pch = 1)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend = &amp;quot;GHCN Temperature Records&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the National Hydrography Dataset for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NHD (USA ONLY)
NHD &amp;lt;- get_nhd(template = vepPolygon,
               label = &amp;quot;VEPIIN&amp;quot;)
# Plot the NED again
raster::plot(NED)
# Plot the NHD data
NHD %&amp;gt;%
  lapply(sp::plot,
         col = &#39;black&#39;,
         add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the NRCS SSURGO data for the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the NRCS SSURGO data (USA ONLY)
SSURGO.VEPIIN &amp;lt;- get_ssurgo(template = vepPolygon,
                     label = &amp;quot;VEPIIN&amp;quot;)
#&amp;gt; Warning: 1 parsing failure.
#&amp;gt; row # A tibble: 1 x 5 col     row     col               expected actual expected   &amp;lt;int&amp;gt;   &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; actual 1  1276 slope.r no trailing characters     .5 file # ... with 1 more variables: file &amp;lt;chr&amp;gt;
# Plot the NED again
raster::plot(NED)
# Plot the SSURGO mapunit polygons
plot(SSURGO.VEPIIN$spatial,
     lwd = 0.1,
     add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the NRCS SSURGO data for particular soil survey areas&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Or, download by Soil Survey Area names
SSURGO.areas &amp;lt;- get_ssurgo(template = c(&amp;quot;CO670&amp;quot;,&amp;quot;CO075&amp;quot;),
                           label = &amp;quot;CO_TEST&amp;quot;)

# Let&#39;s just look at spatial data for CO675
SSURGO.areas.CO675 &amp;lt;- SSURGO.areas$spatial[SSURGO.areas$spatial$AREASYMBOL==&amp;quot;CO075&amp;quot;,]

# And get the NED data under them for pretty plotting
NED.CO675 &amp;lt;- get_ned(template = SSURGO.areas.CO675,
                            label = &amp;quot;SSURGO_CO675&amp;quot;)

# Plot the SSURGO mapunit polygons, but only for CO675
plot(NED.CO675)
plot(SSURGO.areas.CO675,
     lwd = 0.1,
     add = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Get and plot the ITRDB chronology locations in the study area&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the ITRDB records
ITRDB &amp;lt;- get_itrdb(template = vepPolygon,
                        label = &amp;quot;VEPIIN&amp;quot;,
                        makeSpatial = TRUE)
# Plot the NED again
raster::plot(NED)
# Map the locations of the tree ring chronologies
plot(ITRDB$metadata,
     pch = 1,
     add = TRUE)
legend(&#39;bottomleft&#39;,
       pch = 1,
       legend = &amp;quot;ITRDB chronologies&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/ropensci/FedData/raw/master/inst/image/README-unnamed-chunk-13-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;todo&#34;&gt;TODO&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The current CRAN version of &lt;code&gt;FedData&lt;/code&gt;, v2.4.6, will (hopefully) be the final CRAN release of &lt;code&gt;FedData&lt;/code&gt; 2. &lt;code&gt;FedData&lt;/code&gt; 3 will be released in the coming months, but some code built on &lt;code&gt;FedData&lt;/code&gt; 2 will not be compatible with FedData 3.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; was initially developed prior to widespread use of modern web mapping services and RESTful APIs by many Federal data-holders. Future releases of &lt;code&gt;FedData&lt;/code&gt; will limit data transfer by utilizing server-side geospatial and data queries. We will also implement &lt;a href=&#34;https://github.com/hadley/dplyr&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/a&gt; verbs, tidy data structures, (&lt;a href=&#34;https://github.com/tidyverse/magrittr&#34;&gt;&lt;code&gt;magrittr&lt;/code&gt;&lt;/a&gt;) piping, functional programming using &lt;a href=&#34;https://github.com/hadley/purrr&#34;&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/a&gt;, simple features for spatial data from &lt;a href=&#34;https://github.com/edzer/sfr&#34;&gt;&lt;code&gt;sf&lt;/code&gt;&lt;/a&gt;, and local data storage in OGC-compliant data formats (probably GeoJSON and NetCDF). I am also aiming for 100% testing coverage.&lt;/p&gt;

&lt;p&gt;All that being said, much of the functionality of the &lt;code&gt;FedData&lt;/code&gt; package could be spun off into more domain-specific packages. For example, ITRDB download functions could be part of the &lt;a href=&#34;https://r-forge.r-project.org/projects/dplr/&#34;&gt;&lt;code&gt;dplR&lt;/code&gt;&lt;/a&gt; dendrochronology package; concepts/functions having to do with the GHCN data integrated into &lt;a href=&#34;https://github.com/ropensci/rnoaa&#34;&gt;&lt;code&gt;rnoaa&lt;/code&gt;&lt;/a&gt;; and Daymet concepts integrated into &lt;a href=&#34;https://github.com/khufkens/daymetr&#34;&gt;&lt;code&gt;daymetr&lt;/code&gt;&lt;/a&gt;. I welcome any and all suggestions about how to improve the utility of FedData; please &lt;a href=&#34;https://github.com/ropensci/FedData/issues&#34;&gt;submit an issue&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; is a product of SKOPE (&lt;a href=&#34;http://www.openskope.org&#34;&gt;Synthesizing Knowledge of Past Environments&lt;/a&gt;) and the &lt;a href=&#34;http://veparchaeology.org/&#34;&gt;Village Ecodynamics Project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;FedData&lt;/code&gt; was reviewed for &lt;a href=&#34;https://ropensci.org&#34;&gt;rOpenSci&lt;/a&gt; by &lt;a href=&#34;https://github.com/jooolia&#34;&gt;@jooolia&lt;/a&gt;, with &lt;a href=&#34;https://github.com/sckott&#34;&gt;@sckott&lt;/a&gt; as onboarding editor, and was greatly improved as a result.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Tesseract and Magick: High Quality OCR in R</title>
      <link>https://ropensci.org/technotes/2017/08/17/tesseract-16/</link>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/08/17/tesseract-16/</guid>
      <description>
        
        

&lt;p&gt;Last week we released an update of the tesseract package to CRAN. This package provides R bindings to Google&amp;rsquo;s OCR library &lt;a href=&#34;https://en.wikipedia.org/wiki/Tesseract_(software)&#34;&gt;Tesseract&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;tesseract&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The new version ships with the latest libtesseract 3.05.01 on Windows and MacOS. Furthermore it includes enhancements for managing language data and using tesseract together with the magick package.&lt;/p&gt;

&lt;h2 id=&#34;installing-language-data&#34;&gt;Installing Language Data&lt;/h2&gt;

&lt;p&gt;The new version has several improvements for installing additional language data. On Windows and MacOS you use the &lt;code&gt;tesseract_download()&lt;/code&gt; function to install additional languages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tesseract_download(&amp;quot;fra&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Language data are now stored in &lt;code&gt;rappdirs::user_data_dir(&#39;tesseract&#39;)&lt;/code&gt; which makes it persist across updates of the package. To OCR french text:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;french &amp;lt;- tesseract(&amp;quot;fra&amp;quot;)
text &amp;lt;- ocr(&amp;quot;https://jeroen.github.io/images/french_text.png&amp;quot;, engine = french)
cat(text)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Très Bien! Note that on Linux you should not use &lt;code&gt;tesseract_download&lt;/code&gt; but instead install languages using apt-get (e.g. &lt;a href=&#34;https://packages.debian.org/testing/tesseract-ocr-fra&#34;&gt;tesseract-ocr-fra&lt;/a&gt;) or yum (e.g. &lt;a href=&#34;https://apps.fedoraproject.org/packages/tesseract-langpack-fra&#34;&gt;tesseract-langpack-fra&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;tesseract-and-magick&#34;&gt;Tesseract and Magick&lt;/h2&gt;

&lt;p&gt;The tesseract developers &lt;a href=&#34;https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality&#34;&gt;recommend&lt;/a&gt; to clean up the image before OCR&amp;rsquo;ing it to improve the quality of the output. This involves things like cropping out the text area, rescaling, increasing contrast, etc.&lt;/p&gt;

&lt;p&gt;The rOpenSci &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/15/magick-10&#34;&gt;magick&lt;/a&gt; package is perfectly suitable for this task. The latest version contains a convenient wrapper &lt;code&gt;image_ocr()&lt;/code&gt; that works with pipes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/magick&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s give it a try on some &lt;a href=&#34;https://courses.cs.vt.edu/csonline/AI/Lessons/VisualProcessing/OCRscans.html&#34;&gt;example scans&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://courses.cs.vt.edu/csonline/AI/Lessons/VisualProcessing/OCRscans_files/bowers.jpg&#34; alt=&#34;example&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Requires devel version of magick
# devtools::install_github(&amp;quot;ropensci/magick&amp;quot;)

# Test it
library(magick)
library(magrittr)

text &amp;lt;- image_read(&amp;quot;https://courses.cs.vt.edu/csonline/AI/Lessons/VisualProcessing/OCRscans_files/bowers.jpg&amp;quot;) %&amp;gt;%
  image_resize(&amp;quot;2000&amp;quot;) %&amp;gt;%
  image_convert(colorspace = &#39;gray&#39;) %&amp;gt;%
  image_trim() %&amp;gt;%
  image_ocr()

cat(text)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The Llfe and Work of
Fredson Bowers
by
G. THOMAS TANSELLE

N EVERY FIELD OF ENDEAVOR THERE ARE A FEW FIGURES WHOSE ACCOM-
plishment and inﬂuence cause them to be the symbols of their age;
their careers and oeuvres become the touchstones by which the
ﬁeld is measured and its history told. In the related pursuits of
analytical and descriptive bibliography, textual criticism, and scholarly
editing, Fredson Bowers was such a ﬁgure, dominating the four decades
after 1949, when his Principles of Bibliographical Description was pub-
lished. By 1973 the period was already being called “the age of Bowers”:
in that year Norman Sanders, writing the chapter on textual scholarship
for Stanley Wells&#39;s Shakespeare: Select Bibliographies, gave this title to
a section of his essay. For most people, it would be achievement enough
to rise to such a position in a ﬁeld as complex as Shakespearean textual
studies; but Bowers played an equally important role in other areas.
Editors of ninetcemh-cemury American authors, for example, would
also have to call the recent past “the age of Bowers,&amp;quot; as would the writers
of descriptive bibliographies of authors and presses. His ubiquity in
the broad ﬁeld of bibliographical and textual study, his seemingly com-
plete possession of it, distinguished him from his illustrious predeces-
sors and made him the personiﬁcation of bibliographical scholarship in

his time.

\Vhen in 1969 Bowers was awarded the Gold Medal of the Biblio-
graphical Society in London, John Carter’s citation referred to the
Principles as “majestic,&amp;quot; called Bowers&#39;s current projects “formidable,&amp;quot;
said that he had “imposed critical discipline&amp;quot; on the texts of several
authors, described Studies in Bibliography as a “great and continuing
achievement,&amp;quot; and included among his characteristics &amp;quot;uncompromising
seriousness of purpose” and “professional intensity.&amp;quot; Bowers was not
unaccustomed to such encomia, but he had also experienced his share of
attacks: his scholarly positions were not universally popular, and he
expressed them with an aggressiveness that almost seemed calculated to
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not bad but not perfect. Can you do a better job?&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>elastic - Elasticsearch for R</title>
      <link>https://ropensci.org/technotes/2017/08/02/elasticsearch-client/</link>
      <pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/08/02/elasticsearch-client/</guid>
      <description>
        
        

&lt;p&gt;&lt;strong&gt;elastic&lt;/strong&gt; is an R client for &lt;a href=&#34;https://www.elastic.co/products/elasticsearch&#34;&gt;Elasticsearch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;elastic&lt;/code&gt; has been around since 2013, with the first commit in &lt;a href=&#34;https://github.com/ropensci/elastic/commit/f7b04589b2cb711a21223bb4f20b34bc9330ef8d&#34;&gt;November, 2013&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;sidebar - &amp;lsquo;elastic&amp;rsquo; was picked as a package named before the company now known as Elastic
changed their name to Elastic.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;what-is-elasticsearch&#34;&gt;What is Elasticsearch?&lt;/h3&gt;

&lt;p&gt;If you aren&amp;rsquo;t familiar with Elasticsearch, it is a distributed, RESTful search and analytics engine.
It&amp;rsquo;s similar to &lt;a href=&#34;https://lucene.apache.org/solr/&#34;&gt;Solr&lt;/a&gt;. It falls in the NoSQL bin of databases, holding data in JSON documents, instead
of rows and columns. Elasticsearch has a concept of &lt;strong&gt;index&lt;/strong&gt;, similar to a database in SQL-land.
You can hold many documents of similar type within a single index. There is powerful search
capabilities, including lots of different types of queries that can be done separately
or combined. And best of all it&amp;rsquo;s super fast.&lt;/p&gt;

&lt;h3 id=&#34;other-clients&#34;&gt;Other clients&lt;/h3&gt;

&lt;p&gt;The Elastic company maintains some official clients, including the Python client
&lt;a href=&#34;http://elasticsearch-py.readthedocs.io/en/master/&#34;&gt;elasticsearch-py&lt;/a&gt;, and it&amp;rsquo;s higher
level DSL client &lt;a href=&#34;https://elasticsearch-dsl.readthedocs.io/en/latest/&#34;&gt;elasticsearch-dsl&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I won&amp;rsquo;t talk much about it, but we have slowly been working on an R equivalent of the
Python DSL client, called &lt;a href=&#34;https://github.com/ropensci/elasticdsl&#34;&gt;elasticdsl&lt;/a&gt;, for
a human friendly way to compose Elasticsearch queries.&lt;/p&gt;

&lt;h3 id=&#34;vignettes&#34;&gt;Vignettes&lt;/h3&gt;

&lt;p&gt;Check out the &lt;a href=&#34;https://cran.rstudio.com/web/packages/elastic/vignettes/elastic_intro.html&#34;&gt;elastic introduction vignette&lt;/a&gt;
and the &lt;a href=&#34;https://cran.rstudio.com/web/packages/elastic/vignettes/search.html&#34;&gt;search vignette&lt;/a&gt; to get started.&lt;/p&gt;

&lt;h3 id=&#34;noteable-features&#34;&gt;Noteable features&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;elastic&lt;/code&gt; has nearly complete coverage of the Elasticsearch HTTP API. If there&amp;rsquo;s
anything missing you need in this client, let us know! Check out the
&lt;a href=&#34;https://github.com/ropensci/elastic/issues?q=is%3Aissue+is%3Aopen+label%3Afeatures&#34;&gt;features label&lt;/a&gt;
for features we plan to add to the package.&lt;/li&gt;
&lt;li&gt;We fail well. This is important to us. We allow the user to choose simple errors
to just give e.g., 404 HTTP error, or complex errors, including full stack trace
from Elasticsearch in addition to the HTTP errror. We strive to fail well when
users give the wrong type of input, etc. as well. Let us know if &lt;code&gt;elastic&lt;/code&gt; is not
failing well!&lt;/li&gt;
&lt;li&gt;We strive to allow R centric ways of interacting with Elasticsearch. For example,
in the function &lt;code&gt;docs_bulk&lt;/code&gt;, our interface to the Elasticsearch &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html&#34;&gt;bulk API&lt;/a&gt;
we make it easy to create documents in your Elasticsearch instance from R lists,
data.frame&amp;rsquo;s and from bulk format files on disk.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;elastic&lt;/code&gt; works with most versions of Elasticsearch. We run the test suite on 11
versions of Elasticsearch, from &lt;code&gt;v1.0.0&lt;/code&gt; up to &lt;code&gt;v5.5.0&lt;/code&gt;. We strive to fail well
with useful messages when there is a feature no longer available or one that is
a new feature and not available in previous Elasticsearch versions.&lt;/li&gt;
&lt;li&gt;Search inputs are flexible: lists and JSON strings both work.&lt;/li&gt;
&lt;li&gt;Arguably, a noteable feature is that this client has been around nearly 4 years,
so we&amp;rsquo;ve surfaced and squashed many bugs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;getting-help&#34;&gt;Getting help&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;If you have a bug or a feature request, post it in the repo at &lt;a href=&#34;https://github.com/ropensci/elastic/issues&#34;&gt;https://github.com/ropensci/elastic/issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stackoverflow: Check out combination of the tags &lt;code&gt;[elasticsearch]&lt;/code&gt; and &lt;code&gt;[r]&lt;/code&gt; &lt;a href=&#34;https://stackoverflow.com/questions/tagged/elasticsearch+r&#34;&gt;https://stackoverflow.com/questions/tagged/elasticsearch+r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reach out to me on Twitter at &lt;a href=&#34;https://twitter.com/sckottie&#34;&gt;@sckottie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Post question/problem in the &lt;a href=&#34;https://discuss.ropensci.org/&#34;&gt;rOpenSci discussion forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Email me &lt;a href=&#34;mailto:myrmecocystus@gmail.com&#34;&gt;directly&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;setup&#34;&gt;Setup&lt;/h3&gt;

&lt;p&gt;Install &lt;code&gt;elastic&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;elastic&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/elastic&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(elastic)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;m running Elasticsearch version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ping()$version$number
#&amp;gt; [1] &amp;quot;5.4.0&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;

&lt;h4 id=&#34;initialize-a-client&#34;&gt;Initialize a client&lt;/h4&gt;

&lt;p&gt;Using &lt;code&gt;connect()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;elastic::connect()
#&amp;gt; transport:  http
#&amp;gt; host:       127.0.0.1
#&amp;gt; port:       9200
#&amp;gt; path:       NULL
#&amp;gt; username:   NULL
#&amp;gt; password:   &amp;lt;secret&amp;gt;
#&amp;gt; errors:     simple
#&amp;gt; headers (names):  NULL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By default, you connect to &lt;code&gt;localhost&lt;/code&gt; and port &lt;code&gt;9200&lt;/code&gt;. There&amp;rsquo;s paramaters
for setting transport schema, username, password, and base search path (e.g.,
&lt;code&gt;_search&lt;/code&gt; or something else).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;See bottom of post about possible changes in connections.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;get-some-data&#34;&gt;Get some data&lt;/h4&gt;

&lt;p&gt;Elasticsearch has a bulk load API to load data in fast. The format is pretty weird
though. It&amp;rsquo;s sort of JSON, but would pass no JSON linter. I include a few data
sets in &lt;code&gt;elastic&lt;/code&gt; so it&amp;rsquo;s easy to get up and running, and so when you run examples
in this package they&amp;rsquo;ll actually run the same way (hopefully).&lt;/p&gt;

&lt;h4 id=&#34;public-library-of-science-plos-data&#34;&gt;Public Library of Science (PLOS) data&lt;/h4&gt;

&lt;p&gt;A dataset inluded in the &lt;code&gt;elastic&lt;/code&gt; package is metadata for PLOS scholarly articles.
Get the file path, then load:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plosdat &amp;lt;- system.file(&amp;quot;examples&amp;quot;, &amp;quot;plos_data.json&amp;quot;, package = &amp;quot;elastic&amp;quot;)
invisible(docs_bulk(plosdat))
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;search&#34;&gt;Search&lt;/h4&gt;

&lt;p&gt;The main search function is &lt;code&gt;Search()&lt;/code&gt;. Running it without any inputs searches
across all indices - in this case only the &lt;code&gt;plos&lt;/code&gt; index.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Search()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; $took
#&amp;gt; [1] 1
#&amp;gt;
#&amp;gt; $timed_out
#&amp;gt; [1] FALSE
#&amp;gt;
#&amp;gt; $`_shards`
#&amp;gt; $`_shards`$total
#&amp;gt; [1] 5
#&amp;gt;
#&amp;gt; $`_shards`$successful
#&amp;gt; [1] 5
#&amp;gt;
#&amp;gt; $`_shards`$failed
#&amp;gt; [1] 0
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Search just the &lt;code&gt;plos&lt;/code&gt; index and only return 1 result&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Search(index = &amp;quot;plos&amp;quot;, size = 1)$hits$hits
#&amp;gt; [[1]]
#&amp;gt; [[1]]$`_index`
#&amp;gt; [1] &amp;quot;plos&amp;quot;
#&amp;gt;
#&amp;gt; [[1]]$`_type`
#&amp;gt; [1] &amp;quot;article&amp;quot;
#&amp;gt;
#&amp;gt; [[1]]$`_id`
#&amp;gt; [1] &amp;quot;0&amp;quot;
#&amp;gt;
#&amp;gt; [[1]]$`_score`
#&amp;gt; [1] 1
#&amp;gt;
#&amp;gt; [[1]]$`_source`
#&amp;gt; [[1]]$`_source`$id
#&amp;gt; [1] &amp;quot;10.1371/journal.pone.0007737&amp;quot;
#&amp;gt;
#&amp;gt; [[1]]$`_source`$title
#&amp;gt; [1] &amp;quot;Phospholipase C-β4 Is Essential for the Progression of the Normal Sleep Sequence and Ultradian Body Temperature Rhythms in Mice&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Search the &lt;code&gt;plos&lt;/code&gt; index, and the &lt;code&gt;article&lt;/code&gt; document type, sort by title, and query for &lt;em&gt;antibody&lt;/em&gt;, limit to 1 result.&lt;/p&gt;

&lt;p&gt;First, with Elasticsearch &lt;code&gt;v5&lt;/code&gt; and greater, we need to set &lt;code&gt;fielddata = true&lt;/code&gt; if we want to search on or sort on a text field.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mapping_create(&amp;quot;plos&amp;quot;, &amp;quot;article&amp;quot;, update_all_types = TRUE, body = &#39;{
   &amp;quot;properties&amp;quot;: {
     &amp;quot;title&amp;quot;: {
     &amp;quot;type&amp;quot;:     &amp;quot;text&amp;quot;,
     &amp;quot;fielddata&amp;quot;: true
   }
 }
}&#39;)
#&amp;gt; $acknowledged
#&amp;gt; [1] TRUE
Search(index = &amp;quot;plos&amp;quot;, type = &amp;quot;article&amp;quot;, sort = &amp;quot;title&amp;quot;, q = &amp;quot;antibody&amp;quot;, size = 1)$hits$hits
#&amp;gt; [[1]]
#&amp;gt; [[1]]$`_index`
#&amp;gt; [1] &amp;quot;plos&amp;quot;
#&amp;gt;
#&amp;gt; [[1]]$`_type`
#&amp;gt; [1] &amp;quot;article&amp;quot;
#&amp;gt;
#&amp;gt; [[1]]$`_id`
#&amp;gt; [1] &amp;quot;568&amp;quot;
#&amp;gt;
#&amp;gt; [[1]]$`_score`
#&amp;gt; NULL
#&amp;gt;
#&amp;gt; [[1]]$`_source`
#&amp;gt; [[1]]$`_source`$id
#&amp;gt; [1] &amp;quot;10.1371/journal.pone.0085002&amp;quot;
#&amp;gt;
#&amp;gt; [[1]]$`_source`$title
#&amp;gt; [1] &amp;quot;Evaluation of 131I-Anti-Angiotensin II Type 1 Receptor Monoclonal Antibody as a Reporter for Hepatocellular Carcinoma&amp;quot;
#&amp;gt;
#&amp;gt;
#&amp;gt; [[1]]$sort
#&amp;gt; [[1]]$sort[[1]]
#&amp;gt; [1] &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;get-documents&#34;&gt;Get documents&lt;/h4&gt;

&lt;p&gt;Get document with &lt;code&gt;id=1&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;docs_get(index = &#39;plos&#39;, type = &#39;article&#39;, id = 1)
#&amp;gt; $`_index`
#&amp;gt; [1] &amp;quot;plos&amp;quot;
#&amp;gt;
#&amp;gt; $`_type`
#&amp;gt; [1] &amp;quot;article&amp;quot;
#&amp;gt;
#&amp;gt; $`_id`
#&amp;gt; [1] &amp;quot;1&amp;quot;
#&amp;gt;
#&amp;gt; $`_version`
#&amp;gt; [1] 1
#&amp;gt;
#&amp;gt; $found
#&amp;gt; [1] TRUE
#&amp;gt;
#&amp;gt; $`_source`
#&amp;gt; $`_source`$id
#&amp;gt; [1] &amp;quot;10.1371/journal.pone.0098602&amp;quot;
#&amp;gt;
#&amp;gt; $`_source`$title
#&amp;gt; [1] &amp;quot;Population Genetic Structure of a Sandstone Specialist and a Generalist Heath Species at Two Levels of Sandstone Patchiness across the Strait of Gibraltar&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get certain fields&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;docs_get(index = &#39;plos&#39;, type = &#39;article&#39;, id = 1, fields = &#39;id&#39;)
#&amp;gt; $`_index`
#&amp;gt; [1] &amp;quot;plos&amp;quot;
#&amp;gt;
#&amp;gt; $`_type`
#&amp;gt; [1] &amp;quot;article&amp;quot;
#&amp;gt;
#&amp;gt; $`_id`
#&amp;gt; [1] &amp;quot;1&amp;quot;
#&amp;gt;
#&amp;gt; $`_version`
#&amp;gt; [1] 1
#&amp;gt;
#&amp;gt; $found
#&amp;gt; [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;raw-json-data&#34;&gt;Raw JSON data&lt;/h4&gt;

&lt;p&gt;You can optionally get back raw JSON from many functions by setting parameter &lt;code&gt;raw=TRUE&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example, get raw JSON, then parse with &lt;code&gt;jsonlite&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(out &amp;lt;- docs_mget(index = &amp;quot;plos&amp;quot;, type = &amp;quot;article&amp;quot;, id = 5:6, raw = TRUE))
#&amp;gt; [1] &amp;quot;{\&amp;quot;docs\&amp;quot;:[{\&amp;quot;_index\&amp;quot;:\&amp;quot;plos\&amp;quot;,\&amp;quot;_type\&amp;quot;:\&amp;quot;article\&amp;quot;,\&amp;quot;_id\&amp;quot;:\&amp;quot;5\&amp;quot;,\&amp;quot;_version\&amp;quot;:1,\&amp;quot;found\&amp;quot;:true,\&amp;quot;_source\&amp;quot;:{\&amp;quot;id\&amp;quot;:\&amp;quot;10.1371/journal.pone.0085123\&amp;quot;,\&amp;quot;title\&amp;quot;:\&amp;quot;MiR-21 Is under Control of STAT5 but Is Dispensable for Mammary Development and Lactation\&amp;quot;}},{\&amp;quot;_index\&amp;quot;:\&amp;quot;plos\&amp;quot;,\&amp;quot;_type\&amp;quot;:\&amp;quot;article\&amp;quot;,\&amp;quot;_id\&amp;quot;:\&amp;quot;6\&amp;quot;,\&amp;quot;_version\&amp;quot;:1,\&amp;quot;found\&amp;quot;:true,\&amp;quot;_source\&amp;quot;:{\&amp;quot;id\&amp;quot;:\&amp;quot;10.1371/journal.pone.0098600\&amp;quot;,\&amp;quot;title\&amp;quot;:\&amp;quot;Correction: Designing Mixed Species Tree Plantations for the Tropics: Balancing Ecological Attributes of Species with Landholder Preferences in the Philippines\&amp;quot;}}]}&amp;quot;
#&amp;gt; attr(,&amp;quot;class&amp;quot;)
#&amp;gt; [1] &amp;quot;elastic_mget&amp;quot;
jsonlite::fromJSON(out)
#&amp;gt; $docs
#&amp;gt;   _index   _type _id _version found                   _source.id
#&amp;gt; 1   plos article   5        1  TRUE 10.1371/journal.pone.0085123
#&amp;gt; 2   plos article   6        1  TRUE 10.1371/journal.pone.0098600
#&amp;gt;                                                                                                                                                     _source.title
#&amp;gt; 1                                                                       MiR-21 Is under Control of STAT5 but Is Dispensable for Mammary Development and Lactation
#&amp;gt; 2 Correction: Designing Mixed Species Tree Plantations for the Tropics: Balancing Ecological Attributes of Species with Landholder Preferences in the Philippines
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;aggregation-search&#34;&gt;Aggregation search&lt;/h4&gt;

&lt;p&gt;Here, we&amp;rsquo;ll use another dataset that comes with the package on Shakespeare plays.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gbifdat &amp;lt;- system.file(&amp;quot;examples&amp;quot;, &amp;quot;gbif_data.json&amp;quot;, package = &amp;quot;elastic&amp;quot;)
invisible(docs_bulk(gbifdat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Define an aggregation query:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;aggs &amp;lt;- &#39;{
    &amp;quot;aggs&amp;quot;: {
        &amp;quot;latbuckets&amp;quot; : {
           &amp;quot;histogram&amp;quot; : {
               &amp;quot;field&amp;quot; : &amp;quot;decimalLatitude&amp;quot;,
               &amp;quot;interval&amp;quot; : 5
           }
        }
    }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Search the &lt;code&gt;gbif&lt;/code&gt; index&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- Search(index = &amp;quot;gbif&amp;quot;, body = aggs, size = 0)$aggregations$latbuckets$buckets
do.call(&amp;quot;rbind.data.frame&amp;quot;, res)
#&amp;gt;    key doc_count
#&amp;gt; 2  -35         1
#&amp;gt; 22 -30         0
#&amp;gt; 3  -25         0
#&amp;gt; 4  -20         0
#&amp;gt; 5  -15         0
#&amp;gt; 6  -10         0
#&amp;gt; 7   -5         1
#&amp;gt; 8    0         0
#&amp;gt; 9    5         0
#&amp;gt; 10  10         0
#&amp;gt; 11  15         0
#&amp;gt; 12  20         0
#&amp;gt; 13  25         4
#&amp;gt; 14  30         2
#&amp;gt; 15  35         3
#&amp;gt; 16  40         2
#&amp;gt; 17  45        66
#&amp;gt; 18  50       183
#&amp;gt; 19  55       487
#&amp;gt; 20  60       130
#&amp;gt; 21  65        20
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;scrolling-search-instead-of-paging&#34;&gt;Scrolling search - instead of paging&lt;/h4&gt;

&lt;p&gt;When you want all the documents, your best bet is likely to be &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html&#34;&gt;scrolling search&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example. First, use &lt;code&gt;Search()&lt;/code&gt;, setting a value for the &lt;code&gt;scroll&lt;/code&gt; parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res1 &amp;lt;- Search(index = &#39;shakespeare&#39;, scroll = &amp;quot;1m&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You get a scroll ID back when setting the &lt;code&gt;scroll&lt;/code&gt; parameter&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res1$`_scroll_id`
#&amp;gt; [1] &amp;quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAElFnZ2X3FJVWEyUU1HQjl2cFpWUFl0cXcAAAAAAAABJBZ2dl9xSVVhMlFNR0I5dnBaVlBZdHF3AAAAAAAAAScWdnZfcUlVYTJRTUdCOXZwWlZQWXRxdwAAAAAAAAEmFnZ2X3FJVWEyUU1HQjl2cFpWUFl0cXcAAAAAAAABIxZ2dl9xSVVhMlFNR0I5dnBaVlBZdHF3&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Use a while loop to get all results&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out1 &amp;lt;- list()
hits &amp;lt;- 1
while (hits != 0) {
  tmp1 &amp;lt;- scroll(scroll_id = res1$`_scroll_id`)
  hits &amp;lt;- length(tmp1$hits$hits)
  if (hits &amp;gt; 0) {
   out1 &amp;lt;- c(out1, tmp1$hits$hits)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Woohoo! Collected all 1 documents in very little time.&lt;/p&gt;

&lt;p&gt;Now, get &lt;code&gt;_source&lt;/code&gt; from each document:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;docs &amp;lt;- lapply(out1, &amp;quot;[[&amp;quot;, &amp;quot;_source&amp;quot;)
length(docs)
#&amp;gt; [1] 4988
vapply(docs[1:10], &amp;quot;[[&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;text_entry&amp;quot;)
#&amp;gt;  [1] &amp;quot;Without much shame retold or spoken of.&amp;quot;
#&amp;gt;  [2] &amp;quot;For more uneven and unwelcome news&amp;quot;
#&amp;gt;  [3] &amp;quot;And shape of likelihood, the news was told;&amp;quot;
#&amp;gt;  [4] &amp;quot;Mordake the Earl of Fife, and eldest son&amp;quot;
#&amp;gt;  [5] &amp;quot;It is a conquest for a prince to boast of.&amp;quot;
#&amp;gt;  [6] &amp;quot;Amongst a grove, the very straightest plant;&amp;quot;
#&amp;gt;  [7] &amp;quot;That some night-tripping fairy had exchanged&amp;quot;
#&amp;gt;  [8] &amp;quot;Then would I have his Harry, and he mine.&amp;quot;
#&amp;gt;  [9] &amp;quot;This is his uncles teaching; this is Worcester,&amp;quot;
#&amp;gt; [10] &amp;quot;Malevolent to you in all aspects;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;bulk-documents&#34;&gt;Bulk documents&lt;/h4&gt;

&lt;p&gt;You&amp;rsquo;ve already seen the bulk docs API in action above. Above though, we were
using &lt;code&gt;docs_bulk.character&lt;/code&gt; - where the input is a character string that&amp;rsquo;s a
file path.&lt;/p&gt;

&lt;p&gt;Here, I&amp;rsquo;ll describe briefly how you can insert any data.frame as documents in your
Elasticsearch instance. We&amp;rsquo;ll use the diamonds dataset from the ~54K row &lt;code&gt;ggplot2&lt;/code&gt;
package.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#&amp;gt; $acknowledged
#&amp;gt; [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)
invisible(docs_bulk(diamonds, &amp;quot;diam&amp;quot;))
#&amp;gt; |==================================| 100%
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Search(&amp;quot;diam&amp;quot;)$hits$total
#&amp;gt; [1] 47375
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s pretty easy! This function is used a lot, particularly with data.frame&amp;rsquo;s - so
we get many questions/feedback on this so it will just keep getting better/faster.&lt;/p&gt;

&lt;h3 id=&#34;to-do&#34;&gt;TO DO&lt;/h3&gt;

&lt;h4 id=&#34;connections&#34;&gt;Connections&lt;/h4&gt;

&lt;p&gt;We&amp;rsquo;re planning to roll out changes in how you connect to Elasticsearch from &lt;code&gt;elastic&lt;/code&gt;.
Right now, you can only connect to one Elasticsearch instance per R session -
your details are set and then recalled internally in each function. We plan to change
this to instantiate a client and then you either call functions on the client
(e.g., using &lt;code&gt;R6&lt;/code&gt;) or pass the client object onto functions.&lt;/p&gt;

&lt;p&gt;Checkout &lt;a href=&#34;https://github.com/ropensci/elastic/issues/87&#34;&gt;issue #87&lt;/a&gt; to follow
progress or discuss.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h4 id=&#34;move-to-using-crul-for-http&#34;&gt;Move to using crul for http&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;crul&lt;/code&gt; is a relatively new R http client - and has async baked in - as well as mocking.
Development should be easier with it as I can mock requests for test suites, and
allow users to toggle async more easily.&lt;/p&gt;

&lt;h3 id=&#34;call-to-action&#34;&gt;Call to action&lt;/h3&gt;

&lt;p&gt;We can use your help! Elasticsearch development moves pretty fast - we&amp;rsquo;d love this client to
work with every single Elasticsearch version to the extent possible - and we&amp;rsquo;d love to
squash every bug and solve every feature request fast.&lt;/p&gt;

&lt;p&gt;If you need to use Elasticsearch from R, please try out &lt;code&gt;elastic&lt;/code&gt;!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Report bugs!&lt;/li&gt;
&lt;li&gt;File feature requests!&lt;/li&gt;
&lt;li&gt;Send PR&amp;rsquo;s!&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>All the fake data that&#39;s fit to print</title>
      <link>https://ropensci.org/technotes/2017/06/22/charlatan/</link>
      <pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/06/22/charlatan/</guid>
      <description>
        
        

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;charlatan&lt;/strong&gt; makes fake data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Excited to annonunce a new package called &lt;code&gt;charlatan&lt;/code&gt;. While perusing
packages from other programming languages, I saw a neat Python library
called &lt;code&gt;faker&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;charlatan&lt;/code&gt; is inspired from and ports many things from Python&amp;rsquo;s
&lt;a href=&#34;https://github.com/joke2k/faker&#34;&gt;https://github.com/joke2k/faker&lt;/a&gt; library. In turn, &lt;code&gt;faker&lt;/code&gt; was inspired from
&lt;a href=&#34;https://github.com/fzaninotto/Faker&#34;&gt;PHP&amp;rsquo;s faker&lt;/a&gt;,
&lt;a href=&#34;http://search.cpan.org/~jasonk/Data-Faker-0.07/&#34;&gt;Perl&amp;rsquo;s Faker&lt;/a&gt;, and
&lt;a href=&#34;https://rubygems.org/gems/faker&#34;&gt;Ruby&amp;rsquo;s faker&lt;/a&gt;. It appears that the PHP
library was the original - nice work PHP.&lt;/p&gt;

&lt;h2 id=&#34;use-cases&#34;&gt;Use cases&lt;/h2&gt;

&lt;p&gt;What could you do with this package? Here&amp;rsquo;s some use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Students in a classroom setting learning any task that needs a dataset.&lt;/li&gt;
&lt;li&gt;People doing simulations/modeling that need some fake data&lt;/li&gt;
&lt;li&gt;Generate fake dataset of users for a database before actual users exist&lt;/li&gt;
&lt;li&gt;Complete missing spots in a dataset&lt;/li&gt;
&lt;li&gt;Generate fake data to replace sensitive real data with before public release&lt;/li&gt;
&lt;li&gt;Create a random set of colors for visualization&lt;/li&gt;
&lt;li&gt;Generate random coordinates for a map&lt;/li&gt;
&lt;li&gt;Get a set of randomly generated DOIs (Digial Object Identifiers) to
assign to fake scholarly artifacts&lt;/li&gt;
&lt;li&gt;Generate fake taxonomic names for a biological dataset&lt;/li&gt;
&lt;li&gt;Get a set of fake sequences to use to test code/software that uses
sequence data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Language support: A huge selling point of &lt;code&gt;charlatan&lt;/code&gt; is language support.
Of course for some data types (numbers), languages don&amp;rsquo;t come into play, but
for many they do. That means you can create fake datasets specific to a
language, or a dataset with a mix of languages, etc. For the variables
in this package, we have not yet ported over all languages for those
variables that Python&amp;rsquo;s &lt;code&gt;faker&lt;/code&gt; has.&lt;/li&gt;
&lt;li&gt;Lite weight: We&amp;rsquo;ve tried to make this package as lite as possible so
that it&amp;rsquo;s just generally easy to install, but also can be used in
other packages or workflows while bringing along as little baggage
as possible.&lt;/li&gt;
&lt;li&gt;Reviewed: it&amp;rsquo;s been reviewed! See reviews by &lt;a href=&#34;reviewba&#34;&gt;Brooke Anderson&lt;/a&gt; and
&lt;a href=&#34;(https://github.com/ropensci/onboarding/issues/94#issuecomment-283799109)&#34;&gt;Tristan Mahr&lt;/a&gt;, and handling editor &lt;a href=&#34;https://github.com/noamross&#34;&gt;Noam Ross&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;R specific features such as methods to create data.frame&amp;rsquo;s (so the
user doesn’t have to do the extra step of putting vectors together)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;status&#34;&gt;Status&lt;/h2&gt;

&lt;p&gt;We have not ported every variable, or every language yet in those variables.
We have added some variables to &lt;code&gt;charlatan&lt;/code&gt; that are not in &lt;code&gt;faker&lt;/code&gt; (e.g.,
taxonomy, gene sequences). Check out the &lt;a href=&#34;https://github.com/ropensci/charlatan/issues&#34;&gt;issues&lt;/a&gt;
to follow progress.&lt;/p&gt;

&lt;h2 id=&#34;package-api&#34;&gt;Package API&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ch_generate&lt;/code&gt;: generate a data.frame with fake data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fraudster&lt;/code&gt;: single interface to all fake data methods&lt;/li&gt;
&lt;li&gt;High level interfaces: There are high level functions prefixed with
&lt;code&gt;ch_&lt;/code&gt; that wrap low level interfaces, and are meant to be easier
to use and provide easy way to make many instances of a thing.&lt;/li&gt;
&lt;li&gt;Low level interfaces: All of these are R6 objects that a user can
initialize and then call methods on the them.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;other-r-work-in-this-space&#34;&gt;Other R work in this space:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/paulhendricks/generator&#34;&gt;generator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/fakeR/&#34;&gt;fakeR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/karthik/randNames&#34;&gt;randNames&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;vignette&#34;&gt;Vignette&lt;/h2&gt;

&lt;p&gt;Check out the &lt;a href=&#34;https://cran.rstudio.com/web/packages/charlatan/vignettes/charlatan_vignette.html&#34;&gt;package vignette&lt;/a&gt; to get started.&lt;/p&gt;

&lt;h2 id=&#34;setup&#34;&gt;setup&lt;/h2&gt;

&lt;p&gt;Install &lt;code&gt;charlatan&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;charlatan&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/charlatan&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(charlatan)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;h3 id=&#34;high-level-interface&#34;&gt;high level interface&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;fraudster&lt;/code&gt; is an interface for all fake data variables (and locales):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- fraudster()
x$job()
#&amp;gt; [1] &amp;quot;Textile designer&amp;quot;
x$name()
#&amp;gt; [1] &amp;quot;Cris Johnston-Tremblay&amp;quot;
x$job()
#&amp;gt; [1] &amp;quot;Database administrator&amp;quot;
x$color_name()
#&amp;gt; [1] &amp;quot;SaddleBrown&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to set locale, do so like &lt;code&gt;fraudster(locale = &amp;quot;{locale}&amp;quot;)&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;locale-support&#34;&gt;locale support&lt;/h3&gt;

&lt;p&gt;The locales that are supported vary by data variable. We&amp;rsquo;re adding more
locales through time, so do check in from time to time - or even better,
send a pull request adding support for the locale you want for the
variable(s) you want.&lt;/p&gt;

&lt;p&gt;As an example, you can set locale for job data to any number of supported
locales.&lt;/p&gt;

&lt;p&gt;For jobs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_job(locale = &amp;quot;en_US&amp;quot;, n = 3)
#&amp;gt; [1] &amp;quot;Charity officer&amp;quot;   &amp;quot;Financial adviser&amp;quot; &amp;quot;Buyer, industrial&amp;quot;
ch_job(locale = &amp;quot;fr_FR&amp;quot;, n = 3)
#&amp;gt; [1] &amp;quot;Illustrateur&amp;quot;                 &amp;quot;Guichetier&amp;quot;
#&amp;gt; [3] &amp;quot;Responsable d&#39;ordonnancement&amp;quot;
ch_job(locale = &amp;quot;hr_HR&amp;quot;, n = 3)
#&amp;gt; [1] &amp;quot;Pomoćnik strojovođe&amp;quot;
#&amp;gt; [2] &amp;quot;Pećar&amp;quot;
#&amp;gt; [3] &amp;quot;Konzervator – restaurator savjetnik&amp;quot;
ch_job(locale = &amp;quot;uk_UA&amp;quot;, n = 3)
#&amp;gt; [1] &amp;quot;Фрілансер&amp;quot;  &amp;quot;Астрофізик&amp;quot; &amp;quot;Доцент&amp;quot;
ch_job(locale = &amp;quot;zh_TW&amp;quot;, n = 3)
#&amp;gt; [1] &amp;quot;包裝設計&amp;quot;         &amp;quot;空調冷凍技術人員&amp;quot; &amp;quot;鍋爐操作技術人員&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For colors:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_color_name(locale = &amp;quot;en_US&amp;quot;, n = 3)
#&amp;gt; [1] &amp;quot;DarkMagenta&amp;quot; &amp;quot;Navy&amp;quot;        &amp;quot;LightGray&amp;quot;
ch_color_name(locale = &amp;quot;uk_UA&amp;quot;, n = 3)
#&amp;gt; [1] &amp;quot;Синій ВПС&amp;quot;          &amp;quot;Темно-зелений хакі&amp;quot; &amp;quot;Берлінська лазур&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;charlatan&lt;/code&gt; will tell you when a locale is not supported&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_job(locale = &amp;quot;cv_MN&amp;quot;)
#&amp;gt; Error: cv_MN not in set of available locales
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;generate-a-dataset&#34;&gt;generate a dataset&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;ch_generate()&lt;/code&gt; helps you create data.frame&amp;rsquo;s with whatever variables
you want that &lt;code&gt;charlatan&lt;/code&gt; supports. Then you&amp;rsquo;re ready to use the
data.frame immediately in whatever your application is.&lt;/p&gt;

&lt;p&gt;By default, you get back a certain set of variables. Right now, that is:
&lt;code&gt;name&lt;/code&gt;, &lt;code&gt;job&lt;/code&gt;, and &lt;code&gt;phone_number&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_generate()
#&amp;gt; # A tibble: 10 x 3
#&amp;gt;                          name                       job
#&amp;gt;                         &amp;lt;chr&amp;gt;                     &amp;lt;chr&amp;gt;
#&amp;gt;  1                  Coy Davis     Geneticist, molecular
#&amp;gt;  2               Artis Senger                 Press sub
#&amp;gt;  3                 Tal Rogahn              Town planner
#&amp;gt;  4             Nikolas Carter         Barrister&#39;s clerk
#&amp;gt;  5            Sharlene Kemmer Insurance account manager
#&amp;gt;  6            Babyboy Volkman           Quality manager
#&amp;gt;  7 Dr. Josephus Marquardt DVM                  Best boy
#&amp;gt;  8                Vernal Dare            Engineer, site
#&amp;gt;  9              Emilia Hessel       Administrator, arts
#&amp;gt; 10              Urijah Beatty     Editor, commissioning
#&amp;gt; # ... with 1 more variables: phone_number &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can select just the variables you want:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_generate(&#39;job&#39;, &#39;phone_number&#39;, n = 30)
#&amp;gt; # A tibble: 30 x 2
#&amp;gt;                           job         phone_number
#&amp;gt;                         &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;
#&amp;gt;  1        Call centre manager  1-670-715-3079x9104
#&amp;gt;  2 Nurse, learning disability 1-502-781-3386x33524
#&amp;gt;  3           Network engineer       1-692-089-3060
#&amp;gt;  4           Industrial buyer       1-517-855-8517
#&amp;gt;  5     Database administrator  (999)474-9975x89650
#&amp;gt;  6       Operations geologist          06150655769
#&amp;gt;  7             Engineer, land     360-043-3630x592
#&amp;gt;  8     Pension scheme manager        (374)429-6821
#&amp;gt;  9          Personnel officer   1-189-574-3348x338
#&amp;gt; 10         Editor, film/video       1-698-135-1664
#&amp;gt; # ... with 20 more rows
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;data-types&#34;&gt;Data types&lt;/h3&gt;

&lt;p&gt;A sampling of the data types available in &lt;code&gt;charlatan&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;person name&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_name()
#&amp;gt; [1] &amp;quot;Jefferey West-O&#39;Connell&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_name(10)
#&amp;gt;  [1] &amp;quot;Dylon Hintz&amp;quot;          &amp;quot;Dr. Billy Willms DDS&amp;quot; &amp;quot;Captain Bednar III&amp;quot;
#&amp;gt;  [4] &amp;quot;Carli Torp&amp;quot;           &amp;quot;Price Strosin III&amp;quot;    &amp;quot;Grady Mayert&amp;quot;
#&amp;gt;  [7] &amp;quot;Nat Herman-Kuvalis&amp;quot;   &amp;quot;Noelle Funk&amp;quot;          &amp;quot;Dr. Jaycie Herzog MD&amp;quot;
#&amp;gt; [10] &amp;quot;Ms. Andrea Zemlak&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;phone number&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_phone_number()
#&amp;gt; [1] &amp;quot;643.993.1958&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_phone_number(10)
#&amp;gt;  [1] &amp;quot;+06(6)6080789632&amp;quot;    &amp;quot;05108334280&amp;quot;         &amp;quot;447-126-9775&amp;quot;
#&amp;gt;  [4] &amp;quot;+96(7)2112213020&amp;quot;    &amp;quot;495-425-1506&amp;quot;        &amp;quot;1-210-372-3188x514&amp;quot;
#&amp;gt;  [7] &amp;quot;(300)951-5115&amp;quot;       &amp;quot;680.567.5321&amp;quot;        &amp;quot;1-947-805-4758x8167&amp;quot;
#&amp;gt; [10] &amp;quot;888-998-5511x554&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;job&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_job()
#&amp;gt; [1] &amp;quot;Scientist, water quality&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_job(10)
#&amp;gt;  [1] &amp;quot;Engineer, production&amp;quot;
#&amp;gt;  [2] &amp;quot;Architect&amp;quot;
#&amp;gt;  [3] &amp;quot;Exhibitions officer, museum/gallery&amp;quot;
#&amp;gt;  [4] &amp;quot;Patent attorney&amp;quot;
#&amp;gt;  [5] &amp;quot;Surveyor, minerals&amp;quot;
#&amp;gt;  [6] &amp;quot;Electronics engineer&amp;quot;
#&amp;gt;  [7] &amp;quot;Secondary school teacher&amp;quot;
#&amp;gt;  [8] &amp;quot;Intelligence analyst&amp;quot;
#&amp;gt;  [9] &amp;quot;Nutritional therapist&amp;quot;
#&amp;gt; [10] &amp;quot;Information officer&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;messy-data&#34;&gt;Messy data&lt;/h3&gt;

&lt;p&gt;Real data is messy!  &lt;code&gt;charlatan&lt;/code&gt; makes it easy to create
messy data. This is still in the early stages so is not available
across most data types and languages, but we&amp;rsquo;re working on it.&lt;/p&gt;

&lt;p&gt;For example, create messy names:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ch_name(50, messy = TRUE)
#&amp;gt;  [1] &amp;quot;Mr. Vernell Hoppe Jr.&amp;quot;     &amp;quot;Annika Considine d.d.s.&amp;quot;
#&amp;gt;  [3] &amp;quot;Dr. Jose Kunde DDS&amp;quot;        &amp;quot;Karol Leuschke-Runte&amp;quot;
#&amp;gt;  [5] &amp;quot;Kayleen Kutch-Hintz&amp;quot;       &amp;quot;Jahir Green&amp;quot;
#&amp;gt;  [7] &amp;quot;Stuart Emmerich&amp;quot;           &amp;quot;Hillard Schaden&amp;quot;
#&amp;gt;  [9] &amp;quot;Mr. Caden Braun&amp;quot;           &amp;quot;Willie Ebert&amp;quot;
#&amp;gt; [11] &amp;quot;Meg Abbott PhD&amp;quot;            &amp;quot;Dr Rahn Huel&amp;quot;
#&amp;gt; [13] &amp;quot;Kristina Crooks d.d.s.&amp;quot;    &amp;quot;Lizbeth Hansen&amp;quot;
#&amp;gt; [15] &amp;quot;Mrs. Peyton Kuhn&amp;quot;          &amp;quot;Hayley Bernier&amp;quot;
#&amp;gt; [17] &amp;quot;Dr. Lavon Schimmel d.d.s.&amp;quot; &amp;quot;Iridian Murray&amp;quot;
#&amp;gt; [19] &amp;quot;Cary Romaguera&amp;quot;            &amp;quot;Tristan Windler&amp;quot;
#&amp;gt; [21] &amp;quot;Marlana Schroeder md&amp;quot;      &amp;quot;Mr. Treyton Nitzsche&amp;quot;
#&amp;gt; [23] &amp;quot;Hilmer Nitzsche-Glover&amp;quot;    &amp;quot;Marius Dietrich md&amp;quot;
#&amp;gt; [25] &amp;quot;Len Mertz&amp;quot;                 &amp;quot;Mrs Adyson Wunsch DVM&amp;quot;
#&amp;gt; [27] &amp;quot;Dr. Clytie Feest DDS&amp;quot;      &amp;quot;Mr. Wong Lebsack I&amp;quot;
#&amp;gt; [29] &amp;quot;Arland Kessler&amp;quot;            &amp;quot;Mrs Billy O&#39;Connell m.d.&amp;quot;
#&amp;gt; [31] &amp;quot;Stephen Gerlach&amp;quot;           &amp;quot;Jolette Lueilwitz&amp;quot;
#&amp;gt; [33] &amp;quot;Mrs Torie Green d.d.s.&amp;quot;    &amp;quot;Mona Denesik&amp;quot;
#&amp;gt; [35] &amp;quot;Mitchell Auer&amp;quot;             &amp;quot;Miss. Fae Price m.d.&amp;quot;
#&amp;gt; [37] &amp;quot;Todd Lehner&amp;quot;               &amp;quot;Elva Lesch&amp;quot;
#&amp;gt; [39] &amp;quot;Miss. Gustie Rempel DVM&amp;quot;   &amp;quot;Lexie Parisian-Stark&amp;quot;
#&amp;gt; [41] &amp;quot;Beaulah Cremin-Rice&amp;quot;       &amp;quot;Parrish Schinner&amp;quot;
#&amp;gt; [43] &amp;quot;Latrell Beier&amp;quot;             &amp;quot;Garry Wolff Sr&amp;quot;
#&amp;gt; [45] &amp;quot;Bernhard Vandervort&amp;quot;       &amp;quot;Stevie Johnston&amp;quot;
#&amp;gt; [47] &amp;quot;Dawson Gaylord&amp;quot;            &amp;quot;Ivie Labadie&amp;quot;
#&amp;gt; [49] &amp;quot;Ronal Parker&amp;quot;              &amp;quot;Mr Willy O&#39;Conner Sr.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right now only suffixes and prefixes for names in &lt;code&gt;en_US&lt;/code&gt; locale
are supported. Notice above some variation in prefixes and suffixes.&lt;/p&gt;

&lt;h3 id=&#34;to-do&#34;&gt;TO DO&lt;/h3&gt;

&lt;p&gt;We have lots ot do still. Some of those things include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Locales: For existing data variables in the package, we need to fill in
locales for which Python&amp;rsquo;s &lt;code&gt;faker&lt;/code&gt; has the data, but we need to port it over
still.&lt;/li&gt;
&lt;li&gt;Data variables: there&amp;rsquo;s more we can port over from Python&amp;rsquo;s &lt;code&gt;faker&lt;/code&gt;.
In addition, we may find inspiration from faker libraries in other
programming languages.&lt;/li&gt;
&lt;li&gt;Messy data: we want to make messy data support more available throughout
the package. Watch &lt;a href=&#34;https://github.com/ropensci/charlatan/issues/41&#34;&gt;issue #41&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If you have ideas for potential data variables, &lt;a href=&#34;https://github.com/ropensci/charlatan/issues/11&#34;&gt;issue #11&lt;/a&gt; is a good place for those.
Or open a new issue, either way.&lt;/li&gt;
&lt;li&gt;One reviewer brought up whether data should be within bounds of reality (
see &lt;a href=&#34;https://github.com/ropensci/charlatan/issues/40&#34;&gt;issue #40&lt;/a&gt;). The first
question for me is should we do this - if the answer is yes or at least sometimes,
then we can explore how. It&amp;rsquo;s not yet clear if it&amp;rsquo;s the right thing to do.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Random GeoJSON and WKT with randgeo</title>
      <link>https://ropensci.org/technotes/2017/04/20/randgeo/</link>
      <pubDate>Thu, 20 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/04/20/randgeo/</guid>
      <description>
        
        

&lt;p&gt;&lt;strong&gt;randgeo&lt;/strong&gt; generates random points and shapes in GeoJSON and WKT formats for
use in examples, teaching, or statistical applications.&lt;/p&gt;

&lt;p&gt;Points and shapes are generated in the long/lat coordinate system and with
appropriate spherical geometry; random points are distributed evenly across
the globe, and random shapes are sized according to a maximum great-circle
distance from the center of the shape.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;randgeo&lt;/strong&gt; was adapted from &lt;a href=&#34;https://github.com/tmcw/geojson-random&#34;&gt;https://github.com/tmcw/geojson-random&lt;/a&gt; to have
a pure R implementation without any dependencies as well as appropriate
geometry. Data generated by &lt;strong&gt;randgeo&lt;/strong&gt; may be processed or displayed of with
packages such as &lt;a href=&#34;https://cran.r-project.org/package=sf&#34;&gt;&lt;strong&gt;sf&lt;/strong&gt;&lt;/a&gt;,
&lt;a href=&#34;https://cran.r-project.org/package=wicket&#34;&gt;&lt;strong&gt;wicket&lt;/strong&gt;&lt;/a&gt;,
&lt;a href=&#34;https://cran.r-project.org/package=geojson&#34;&gt;&lt;strong&gt;geojson&lt;/strong&gt;&lt;/a&gt;,
&lt;a href=&#34;https://cran.r-project.org/package=wellknown&#34;&gt;&lt;strong&gt;wellknown&lt;/strong&gt;&lt;/a&gt;,
&lt;a href=&#34;https://cran.r-project.org/package=geojsonio&#34;&gt;&lt;strong&gt;geojsonio&lt;/strong&gt;&lt;/a&gt;, or
&lt;a href=&#34;https://cran.r-project.org/package=lawn&#34;&gt;&lt;strong&gt;lawn&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Package API:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rg_position&lt;/code&gt; - random position (lon, lat)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geo_point&lt;/code&gt; - random GeoJSON point&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geo_polygon&lt;/code&gt; - random GeoJSON polygon&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wkt_point&lt;/code&gt; - random WKT point&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wkt_polygon&lt;/code&gt; - random WKT polygon&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Install &lt;code&gt;randgeo&lt;/code&gt; - and we&amp;rsquo;ll need a few other packages for examples below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;randgeo&amp;quot;)
install.packages(c(&#39;leaflet&#39;, &#39;lawn&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(randgeo)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Functions that start with &lt;code&gt;geo&lt;/code&gt; are for creating GeoJSON data in JSON format.
If you want to create an R list or data.frame, you can use &lt;code&gt;jsonlite::fromJSON&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;random&#34;&gt;Random&lt;/h2&gt;

&lt;p&gt;Evenly distributed across the sphere.  The &lt;code&gt;bbox&lt;/code&gt; option allows
you to limit points to within long/lat bounds.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;geo_point()
#&amp;gt; $type
#&amp;gt; [1] &amp;quot;FeatureCollection&amp;quot;
#&amp;gt;
#&amp;gt; $features
#&amp;gt; $features[[1]]
#&amp;gt; $features[[1]]$type
#&amp;gt; [1] &amp;quot;Feature&amp;quot;
#&amp;gt;
#&amp;gt; $features[[1]]$geometry
#&amp;gt; $features[[1]]$geometry$type
#&amp;gt; [1] &amp;quot;Point&amp;quot;
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates
#&amp;gt; [1] 105.95999 -46.58477
#&amp;gt;
#&amp;gt;
#&amp;gt; $features[[1]]$properties
#&amp;gt; NULL
#&amp;gt;
#&amp;gt;
#&amp;gt;
#&amp;gt; attr(,&amp;quot;class&amp;quot;)
#&amp;gt; [1] &amp;quot;geo_list&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Centered on a random point, with default maximum size&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;geo_polygon()
#&amp;gt; $type
#&amp;gt; [1] &amp;quot;FeatureCollection&amp;quot;
#&amp;gt;
#&amp;gt; $features
#&amp;gt; $features[[1]]
#&amp;gt; $features[[1]]$type
#&amp;gt; [1] &amp;quot;Feature&amp;quot;
#&amp;gt;
#&amp;gt; $features[[1]]$geometry
#&amp;gt; $features[[1]]$geometry$type
#&amp;gt; [1] &amp;quot;Polygon&amp;quot;
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates
#&amp;gt; $features[[1]]$geometry$coordinates[[1]]
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[1]]
#&amp;gt; [1] -138.49434  -25.11895
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[2]]
#&amp;gt; [1] -145.95566  -28.17623
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[3]]
#&amp;gt; [1] -145.87817  -28.74364
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[4]]
#&amp;gt; [1] -146.61325  -28.59748
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[5]]
#&amp;gt; [1] -139.18167  -31.07703
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[6]]
#&amp;gt; [1] -140.88748  -31.24708
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[7]]
#&amp;gt; [1] -143.50402  -33.93551
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[8]]
#&amp;gt; [1] -146.48114  -30.43185
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[9]]
#&amp;gt; [1] -144.68315  -35.45465
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[10]]
#&amp;gt; [1] -157.58084  -24.52897
#&amp;gt;
#&amp;gt; $features[[1]]$geometry$coordinates[[1]][[11]]
#&amp;gt; [1] -138.49434  -25.11895
#&amp;gt;
#&amp;gt;
#&amp;gt;
#&amp;gt;
#&amp;gt; $features[[1]]$properties
#&amp;gt; NULL
#&amp;gt;
#&amp;gt;
#&amp;gt;
#&amp;gt; attr(,&amp;quot;class&amp;quot;)
#&amp;gt; [1] &amp;quot;geo_list&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visualize your shapes with &lt;strong&gt;lawn&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lawn::view(jsonlite::toJSON(unclass(geo_polygon(count = 4)), auto_unbox = TRUE))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-04-20-randgeo/plot1.png&#34; alt=&#34;map&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;wkt&#34;&gt;WKT&lt;/h2&gt;

&lt;p&gt;Functions prefixed with &lt;code&gt;wkt&lt;/code&gt; create random Well-Known Text (WKT) data. These functions
wrap the GeoJSON versions, but then convert the data to WKT.&lt;/p&gt;

&lt;p&gt;Random point:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;wkt_point()
#&amp;gt; [1] &amp;quot;POINT (179.8795330 -29.1106238)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Random polygon:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;wkt_polygon()
#&amp;gt; [1] &amp;quot;POLYGON ((-60.0870329 -12.9315478, -61.5073816 -25.3204334, -62.6987366 -24.5766272, -64.1853669 -24.0497260, -67.7152546 -27.4752321, -68.4190340 -26.9510818, -67.6018452 -21.5489551, -64.3083560 -21.6772242, -63.1471630 -21.9415438, -64.1137279 -14.2398013, -60.0870329 -12.9315478))&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;use-case&#34;&gt;Use case&lt;/h2&gt;

&lt;p&gt;Example of geospatial data manipulation, using &lt;code&gt;randgeo&lt;/code&gt;, &lt;code&gt;leaflet&lt;/code&gt; and
&lt;code&gt;lawn&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Generate random overlapping polygons&lt;/li&gt;
&lt;li&gt;Calculate a single polygon from overlapping polygons&lt;/li&gt;
&lt;li&gt;Map polygon&lt;/li&gt;
&lt;li&gt;Generate random locaitons (points)&lt;/li&gt;
&lt;li&gt;Clip locations to the polygon&lt;/li&gt;
&lt;li&gt;Overlay locations (more random points) on the polygon&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(randgeo)
library(lawn)
library(leaflet)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;generate random data&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(5)
polys &amp;lt;- randgeo::geo_polygon(count = 2, num_vertices = 4, bbox = c(-120, 40, -100, 50))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get intersection of polygons&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;polysinter &amp;lt;- lawn::lawn_intersect(polys$features[[1]], polys$features[[2]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Map polygons&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;polysinter %&amp;gt;% lawn::view()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-04-20-randgeo/plot2.png&#34; alt=&#34;map&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Generate random points - clip points to polygon&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pts &amp;lt;- randgeo::geo_point(count = 500, bbox = c(-120, 40, -100, 50))
pts &amp;lt;- lawn::lawn_within(
  points = lawn_featurecollection(pts),
  polygons = lawn_featurecollection(polysinter)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Draw polygon + points on map&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;polysinter %&amp;gt;%
  view() %&amp;gt;%
  addGeoJSON(geojson = jsonlite::toJSON(unclass(pts)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-04-20-randgeo/plot3.png&#34; alt=&#34;map&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;Let us know what you think!  &lt;code&gt;randgeo&lt;/code&gt; doesn&amp;rsquo;t have any revdep&amp;rsquo;s on CRAN yet, but
is being &lt;a href=&#34;https://github.com/search?utf8=%E2%9C%93&amp;amp;q=%22randgeo%22+language%3AR+-user%3Acran+-user%3Aropensci&amp;amp;type=Code&#34;&gt;used in one package on GitHub&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>ccafs - client for CCAFS General Circulation Models data</title>
      <link>https://ropensci.org/technotes/2017/03/01/ccafs-release/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/03/01/ccafs-release/</guid>
      <description>
        
        

&lt;p&gt;I&amp;rsquo;ve recently released the new package &lt;a href=&#34;https://github.com/ropensci/ccafs&#34;&gt;ccafs&lt;/a&gt;, which provides access
to data from Climate Change, Agriculture and Food Security
(CCAFS; &lt;a href=&#34;http://ccafs-climate.org/&#34;&gt;http://ccafs-climate.org/&lt;/a&gt;) General Circulation Models (GCM) data.
GCM&amp;rsquo;s are a particular type of climate model, used for weather forecasting,
and climate change forecasting - read more at
&lt;a href=&#34;https://en.wikipedia.org/wiki/General_circulation_model&#34;&gt;https://en.wikipedia.org/wiki/General_circulation_model&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ccafs&lt;/code&gt; falls in the data client camp - its focus is on getting users
data - many &lt;a href=&#34;https://ropensci.org/packages/#data_access&#34;&gt;rOpenSci packages&lt;/a&gt;
fall into this area. These kinds of packages are important so that
scientists don&amp;rsquo;t have to recreate the wheel themselves every time, but
instead use one client that everyone else uses.&lt;/p&gt;

&lt;p&gt;CCAFS GCM data files are &lt;code&gt;.zip&lt;/code&gt; files with a bunch of files inside. The
individual files are in ARC ASCII format (&lt;a href=&#34;https://en.wikipedia.org/wiki/Esri_grid#ASCII&#34;&gt;https://en.wikipedia.org/wiki/Esri_grid#ASCII&lt;/a&gt;) -
a plain text data format, but still require painful manipulation/wrangling to
get into an easily consumable format. The files have a &lt;code&gt;.asc&lt;/code&gt; file extension.&lt;/p&gt;

&lt;p&gt;For each &lt;code&gt;.asc&lt;/code&gt; file, the first 6 lines of each file indicate the reference of
the grid (number of columns and rows, corner coordinates, cellsize, and missing
data value), followed by the actual data values, delimited with single
space characters.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a related binary format - but its proprietary, so nevermind.&lt;/p&gt;

&lt;p&gt;The workflow with &lt;code&gt;ccafs&lt;/code&gt; for most users will likely be as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Search for data they want: &lt;code&gt;cc_search()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fetch/download data: &lt;code&gt;cc_data_fetch()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Reaad data: &lt;code&gt;cc_data_read()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;ll dive into more details below.&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;First, install the package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;ccafs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then load &lt;code&gt;ccafs&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;ccafs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;search-for-data&#34;&gt;Search for data&lt;/h2&gt;

&lt;p&gt;Searching CCAF&amp;rsquo;s data holdings is not as easy as it could be as they don&amp;rsquo;t
provide any programmatic way to do so. However, we provide a way to search
using their web interface from R.&lt;/p&gt;

&lt;p&gt;You can search by the numbers representing each possible value for
each parameter. See the &lt;code&gt;?&#39;ccafs-search&#39;&lt;/code&gt; for help on what the numbers
refer to.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(result1 &amp;lt;- cc_search(file_set = 4, scenario = 6, model = 2, extent = &amp;quot;global&amp;quot;,
  format = &amp;quot;ascii&amp;quot;, period = 5, variable = 2, resolution = 3))
#&amp;gt; [1] &amp;quot;http://gisweb.ciat.cgiar.org/ccafs_climate/files/data/ipcc_4ar_ciat/sres_b1/2040s/bccr_bcm2_0/5min/bccr_bcm2_0_sres_b1_2040s_prec_5min_no_tile_asc.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, you can use the helper list &lt;code&gt;cc_params&lt;/code&gt; where you can reference
options by name; the downside is that this leads to very verbose code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(result2 &amp;lt;- cc_search(file_set = cc_params$file_set$`Delta method IPCC AR4`,
                  scenario = cc_params$scenario$`SRES B1`,
                  model = cc_params$model$bccr_bcm2_0,
                  extent = cc_params$extent$global,
                  format = cc_params$format$ascii,
                  period = cc_params$period$`2040s`,
                  variable = cc_params$variable$Precipitation,
                  resolution = cc_params$resolution$`5 minutes`))
#&amp;gt; [1] &amp;quot;http://gisweb.ciat.cgiar.org/ccafs_climate/files/data/ipcc_4ar_ciat/sres_b1/2040s/bccr_bcm2_0/5min/bccr_bcm2_0_sres_b1_2040s_prec_5min_no_tile_asc.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you already know what you want in terms of file paths, you can query
Amazon S3 directly with &lt;code&gt;cc_list_keys()&lt;/code&gt; (the data file come from Amazon S3):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cc_list_keys(max = 3)
#&amp;gt; # A tibble: 3 × 5
#&amp;gt;                                              Key             LastModified
#&amp;gt;                                            &amp;lt;chr&amp;gt;                    &amp;lt;chr&amp;gt;
#&amp;gt; 1                                         ccafs/ 2014-02-28T15:15:45.000Z
#&amp;gt; 2 ccafs/2014-05-24-01-19-33-3A0DFF1F86F3E7F7.txt 2014-07-01T02:15:51.000Z
#&amp;gt; 3                                 ccafs/amzn.csv 2014-02-28T15:21:32.000Z
#&amp;gt; # ... with 3 more variables: ETag &amp;lt;chr&amp;gt;, Size &amp;lt;chr&amp;gt;, StorageClass &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When using &lt;code&gt;cc_list_keys()&lt;/code&gt;, you&amp;rsquo;ll get not just &lt;code&gt;.zip&lt;/code&gt; files that can be
downloaded, but also directories. So beware that if you&amp;rsquo;re going after grabbing
&amp;ldquo;keys&amp;rdquo; for files that can be downloaded, you&amp;rsquo;re looking for &lt;code&gt;.zip&lt;/code&gt; files.&lt;/p&gt;

&lt;h2 id=&#34;fetch-and-read-data&#34;&gt;Fetch and read data&lt;/h2&gt;

&lt;p&gt;Once you get links from &lt;code&gt;cc_search()&lt;/code&gt; or &amp;ldquo;keys&amp;rdquo; from &lt;code&gt;cc_list_keys()&lt;/code&gt;, you
can pass either to &lt;code&gt;cc_data_fetch()&lt;/code&gt; - which normalizes the input - so it
doesn&amp;rsquo;t matter whether you pass in e.g.,&lt;/p&gt;

&lt;p&gt;&lt;code&gt;http://gisweb.ciat.cgiar.org/ccafs_climate/files/data/ipcc_4ar_ciat/&lt;/code&gt;
&lt;code&gt;sres_b1/2040s/bccr_bcm2_0/5min/bccr_bcm2_0_sres_b1_2040s_prec_5min_no_tile_asc.zip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ccafs_climate/files/data/ipcc_4ar_ciat/sres_b1/2040s/bccr_bcm2_0/5min/&lt;/code&gt;
&lt;code&gt;bccr_bcm2_0_sres_b1_2040s_prec_5min_no_tile_asc.zip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s download data with &lt;code&gt;cc_data_fetch()&lt;/code&gt; using the result we got above
using &lt;code&gt;cc_search()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xx &amp;lt;- cc_data_fetch(result2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can read data with &lt;code&gt;cc_data_read()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(dat &amp;lt;- cc_data_read(xx))
#&amp;gt; class       : RasterStack
#&amp;gt; dimensions  : 1800, 4320, 7776000, 12  (nrow, ncol, ncell, nlayers)
#&amp;gt; resolution  : 0.08333333, 0.08333333  (x, y)
#&amp;gt; extent      : -180, 180, -60, 90  (xmin, xmax, ymin, ymax)
#&amp;gt; coord. ref. : NA
#&amp;gt; names       :      prec_1,     prec_10,     prec_11,     prec_12,      prec_2,      prec_3,      prec_4,      prec_5,      prec_6,      prec_7,      prec_8,      prec_9
#&amp;gt; min values  : -2147483648, -2147483648, -2147483648, -2147483648, -2147483648, -2147483648, -2147483648, -2147483648, -2147483648, -2147483648, -2147483648, -2147483648
#&amp;gt; max values  :  2147483647,  2147483647,  2147483647,  2147483647,  2147483647,  2147483647,  2147483647,  2147483647,  2147483647,  2147483647,  2147483647,  2147483647
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which gives a &lt;code&gt;raster&lt;/code&gt; class object, which you are likely familiar with - which
opens up all the tools that deal with &lt;code&gt;raster&lt;/code&gt; class objects, yay!&lt;/p&gt;

&lt;p&gt;You can easily plot the data with the &lt;code&gt;plot&lt;/code&gt; method from the  &lt;code&gt;raster&lt;/code&gt; package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;raster&amp;quot;)
plot(dat)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-03-01-ccafs-release/unnamed-chunk-9-1.png&#34; alt=&#34;plot&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;caching&#34;&gt;Caching&lt;/h2&gt;

&lt;p&gt;For a better user experience, we cache files for you. That means
when we download data, we put the files in a known location. When a
user tries to download the same data again, we look to see if it&amp;rsquo;s already
been downloaded, and use the cached version - if we don&amp;rsquo;t have it
already, we download it.&lt;/p&gt;

&lt;p&gt;Of course, CCAFS may change their files, so you may not want the cached
version, but the new version from them. We provide tools to inspect your
cached files, and delete them.&lt;/p&gt;

&lt;p&gt;List your cached files:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cc_cache_list()
#&amp;gt;   [1] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc&amp;quot;
#&amp;gt;   [2] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc.zip&amp;quot;
#&amp;gt;   [3] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc/bio_1.asc&amp;quot;
#&amp;gt;   [4] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc/bio_10.asc&amp;quot;
#&amp;gt;   [5] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc/bio_11.asc&amp;quot;
#&amp;gt;   [6] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc/bio_12.asc&amp;quot;
#&amp;gt;   [7] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc/bio_13.asc&amp;quot;
#&amp;gt;   [8] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc/bio_14.asc&amp;quot;
#&amp;gt;   [9] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc/bio_15.asc&amp;quot;
#&amp;gt;  [10] &amp;quot;/Users/sacmac/Library/Caches/ccafs/bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc/bio_16.asc&amp;quot;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get details on all files or a specific file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# cc_cache_details() # details for all files
cc_cache_details(cc_cache_list()[1])
#&amp;gt; &amp;lt;ccafs cached files&amp;gt;
#&amp;gt;   directory: /Users/sacmac/Library/Caches/ccafs
#&amp;gt;
#&amp;gt;   file: /bcc_csm1_1_m_rcp2_6_2030s_bio_10min_r1i1p1_no_tile_asc
#&amp;gt;   size: 0.001 mb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Be careful with &lt;code&gt;cc_cache_delete_all()&lt;/code&gt; as you will delete all your cached
files.&lt;/p&gt;

&lt;h2 id=&#34;ccafs-software-review&#34;&gt;ccafs software review&lt;/h2&gt;

&lt;p&gt;I want to touch briefly on the software review for this package. The reviews
for &lt;code&gt;ccafs&lt;/code&gt; were great, and I think the package was greatly improved via the
review process.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/mikoontz&#34;&gt;Michael Koontz&lt;/a&gt; and &lt;a href=&#34;https://github.com/manuramon&#34;&gt;Manuel Ramon&lt;/a&gt;
did reviews for &lt;code&gt;ccafs&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One thing in particular that improved about &lt;code&gt;ccafs&lt;/code&gt; was the user interface -
that is, the programmatic interface. One feature about the interface was
adding the &lt;code&gt;cc_search()&lt;/code&gt; function. When I started developing &lt;code&gt;ccafs&lt;/code&gt;, I didn&amp;rsquo;t
see a way to programmatically search CCAFS data - other than the Amazon S3
data, which isn&amp;rsquo;t really search, but more like listing files in a directory -
so I just left it at that. During the reviews, reviewers wanted a clear workflow
for potential users - the package as submitted for review didn&amp;rsquo;t really have a
clear workflow; it was&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Know what you want already (&lt;code&gt;cc_list_keys&lt;/code&gt; helped get real paths at least)&lt;/li&gt;
&lt;li&gt;Download data&lt;/li&gt;
&lt;li&gt;Read data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Which is not ideal. There should be a discovery portion to the workflow. So
I decided to dig into possibly querying the CCAFS web portal itself. That panned
out, and the workflow we have now is much better:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Search for data with all the same variables you would on the CCAFS website&lt;/li&gt;
&lt;li&gt;Download data&lt;/li&gt;
&lt;li&gt;Read data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is much better!&lt;/p&gt;

&lt;p&gt;As always, reviews improved the documentation a lot by pointing out areas that
could use improvement - which all users will greatly benefit from.&lt;/p&gt;

&lt;p&gt;A new vignette (&lt;a href=&#34;https://cran.rstudio.com/web/packages/ccafs/vignettes/amazon_s3_keys.html&#34;&gt;https://cran.rstudio.com/web/packages/ccafs/vignettes/amazon_s3_keys.html&lt;/a&gt;)
was added in the review process to explain how to get a &amp;ldquo;key&amp;rdquo;, a URL for CCAFS data.&lt;/p&gt;

&lt;h2 id=&#34;to-do-and-feedback&#34;&gt;To Do and Feedback&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s probably lots of improvements that can be made - I&amp;rsquo;m looking forward
to getting feedback from users on any bugs or feature requests. One immediate
thing is to &lt;a href=&#34;https://github.com/ropensci/ccafs/issues/22&#34;&gt;make the cache details more compact&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Package evolution - changing stuff in your package</title>
      <link>https://ropensci.org/technotes/2017/01/05/package-evolution/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/01/05/package-evolution/</guid>
      <description>
        
        

&lt;p&gt;Making packages is a great way to organize R code, whether it’s a set of scripts for personal use, a set of functions for internal company use or a lab group, or to distribute your new cool framework &lt;code&gt;foobar&lt;/code&gt; to the masses. There&amp;rsquo;s a number of guides to writing packages, including &lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;http://r-pkgs.had.co.nz/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As you develop packages there&amp;rsquo;s a number of issues that don&amp;rsquo;t often get much air time. I&amp;rsquo;ll cover some of them here.&lt;/p&gt;

&lt;h2 id=&#34;philosophy-of-changes&#34;&gt;Philosophy of changes&lt;/h2&gt;

&lt;p&gt;Everyone&amp;rsquo;s free to have their own opinion about how freely parameters/functions/etc. are changed in a library - rules about package changes are not enforced by CRAN or otherwise. Generally, as a library gets more mature, changes to user facing methods (i.e., exported functions in an R package) should become very rare. Libraries that are depended on by many other libraries are likely to be more careful, and should be, about changes.&lt;/p&gt;

&lt;h2 id=&#34;parameters-changing-parameter-names&#34;&gt;Parameters: changing parameter names&lt;/h2&gt;

&lt;p&gt;Sometimes parameter names must be changed for clarity, or some other reason.&lt;/p&gt;

&lt;p&gt;An approach I often use is to catch all parameters passed in to the function and check against some list of parameters, and stop or warn with a meaningful message.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foo_bar &amp;lt;- function(x, y) {
    calls &amp;lt;- names(sapply(match.call(), deparse))[-1]
    if(any(&amp;quot;x&amp;quot; %in% calls)) {
        stop(&amp;quot;use &#39;y&#39; instead of &#39;x&#39;&amp;quot;)
    }
    y^2
}

foo_bar(x = 5)
#&amp;gt; Error in foo_bar(x = 5) : use &#39;y&#39; instead of &#39;x&#39; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or instead of stopping with error, you could check for use of &lt;code&gt;x&lt;/code&gt; parameter and set it to &lt;code&gt;y&lt;/code&gt; internally.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foo_bar &amp;lt;- function(x, y) {
    calls &amp;lt;- names(sapply(match.call(), deparse))[-1]
    if(any(&amp;quot;x&amp;quot; %in% calls)) {
        y &amp;lt;- x
    }
    y^2
}

foo_bar(x = 5)
#&amp;gt; 25
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Be aware of the parameter &lt;code&gt;...&lt;/code&gt;. If your function has &lt;code&gt;...&lt;/code&gt;, and you have already removed a parameter (lets call it &lt;code&gt;z&lt;/code&gt;), a user may have older code that uses &lt;code&gt;z&lt;/code&gt;. When they pass in &lt;code&gt;z&lt;/code&gt;, it&amp;rsquo;s not a parameter in the function definition, and will likely be silently ignored - not what you want. So do make sure to always check for removed parameters moving forward since you can&amp;rsquo;t force users to upgrade.&lt;/p&gt;

&lt;h2 id=&#34;functions-changing-function-names&#34;&gt;Functions: changing function names&lt;/h2&gt;

&lt;p&gt;If you must change a function name, do it gradually, as with any package changes.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you have a function &lt;code&gt;foo&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foo &amp;lt;- function(x) x + 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, you want to change the function name to &lt;code&gt;bar&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Instead of simply changing the function name and &lt;code&gt;foo&lt;/code&gt; no longer existing straight away, in the first version of the package that &lt;code&gt;bar&lt;/code&gt; appears, make an alias like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; foo - add 1 to an input
#&#39; @export
foo &amp;lt;- function(x) x + 1

#&#39; @export
#&#39; @rdname foo
bar &amp;lt;- foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the above, the user can use either &lt;code&gt;foo()&lt;/code&gt; or &lt;code&gt;bar()&lt;/code&gt; - either will do the same thing, as they are the same function.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also useful to have a message but then you&amp;rsquo;ll only want to throw that message when they use the old function name, e.g.,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; foo - add 1 to an input
#&#39; @export
foo &amp;lt;- function(x) {
    if (as.character(match.call()[[1]]) == &amp;quot;foo&amp;quot;) {
        warning(&amp;quot;please use bar() instead of foo()&amp;quot;, call. = FALSE)
    }
    x + 1
}

#&#39; @export
#&#39; @rdname foo
bar &amp;lt;- foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After users have used the package version for a while (with both &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt;), in the next version you can remove the old function name (&lt;code&gt;foo&lt;/code&gt;), and only have &lt;code&gt;bar&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; bar - add 1 to an input
#&#39; @export
bar &amp;lt;- function(x) x + 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;functions-deprecate-defunct&#34;&gt;Functions: deprecate &amp;amp; defunct&lt;/h2&gt;

&lt;p&gt;To remove a function from a package (let&amp;rsquo;s say your package name is &lt;code&gt;helloworld&lt;/code&gt;), I use the following protocol:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mark the function as deprecated in package version &lt;code&gt;x&lt;/code&gt; (e.g., &lt;code&gt;v0.2.0&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the function itself, use &lt;code&gt;.Deprecated()&lt;/code&gt; like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foo &amp;lt;- function() {
    .Deprecated(msg = &amp;quot;&#39;foo&#39; will be removed in the next version&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s options in &lt;code&gt;.Deprecated&lt;/code&gt; for specifying a new function name, as well as a new package name, which I do use when moving functions into different packages.&lt;/p&gt;

&lt;p&gt;The message that&amp;rsquo;s given by &lt;code&gt;.Deprecated&lt;/code&gt; is a warning, so can be suppressed by users with &lt;code&gt;suppressWarnings()&lt;/code&gt; if desired.&lt;/p&gt;

&lt;p&gt;Make a man page for deprecated functions like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Deprecated functions in helloworld
#&#39; 
#&#39; These functions still work but will be removed (defunct) in the next version.
#&#39; 
#&#39; \itemize{
#&#39;  \item \code{\link{foo}}: This function is deprecated, and will
#&#39;  be removed in the next version of this package.
#&#39; }
#&#39; 
#&#39; @name helloworld-deprecated
NULL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This creates a man page that users can access like &lt;code&gt;?helloworld-deprecated&lt;/code&gt; and they&amp;rsquo;ll see in the documentation index. Add any functions to this page as needed, and take away as a function moves to defunct (see below).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the next version (&lt;code&gt;v0.3.0&lt;/code&gt;) you can make the function defunct (that is, completely gone from the package, except for a man page with a note about it).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the function itself, use &lt;code&gt;.Defunct()&lt;/code&gt; like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foo &amp;lt;- function() {
    .Defunct(msg = &amp;quot;&#39;foo&#39; has been removed from this package&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the message in &lt;code&gt;.Defunct&lt;/code&gt; is an error, so the function stops - whereas &lt;code&gt;.Deprecated&lt;/code&gt; returned a warning, letting the function proceed.&lt;/p&gt;

&lt;p&gt;In addition, I like to add &lt;code&gt;...&lt;/code&gt; to all defunct functions so that if users pass in any parameters they&amp;rsquo;ll get the same defunct message instead of a &lt;code&gt;unused argument&lt;/code&gt; message, so like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foo &amp;lt;- function(...) {
    .Defunct(msg = &amp;quot;&#39;foo&#39; has been removed from this package&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Without &lt;code&gt;...&lt;/code&gt; gives:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foo(x = 5)
#&amp;gt; Error in foo(x = 5) : unused argument (x = 5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And with &lt;code&gt;...&lt;/code&gt; gives:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;foo(x = 5)
#&amp;gt; Error: &#39;foo&#39; has been removed from this package
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make a man page for defunct functions like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Defunct functions in helloworld
#&#39; 
#&#39; These functions are gone, no longer available.
#&#39; 
#&#39; \itemize{
#&#39;  \item \code{\link{foo}}: This function is defunct.
#&#39; }
#&#39; 
#&#39; @name helloworld-defunct
NULL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This creates a man page that users can access like &lt;code&gt;?helloworld-defunct&lt;/code&gt; and they&amp;rsquo;ll see in the documentation index. Add any functions to this page as needed. You&amp;rsquo;ll likely want to keep this man page indefinitely.&lt;/p&gt;

&lt;h2 id=&#34;others&#34;&gt;Others?&lt;/h2&gt;

&lt;p&gt;What are some other less discussed aspects of how to make changes in your packages?&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Update jsonlite 1.2</title>
      <link>https://ropensci.org/technotes/2017/01/04/jsonlite-12/</link>
      <pubDate>Wed, 04 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2017/01/04/jsonlite-12/</guid>
      <description>
        
        

&lt;p&gt;A new version of &lt;a href=&#34;https://cran.r-project.org/web/packages/jsonlite/index.html&#34;&gt;jsonlite&lt;/a&gt; package to CRAN. This is a maintenance release with enhancements and bug fixes. A summary of changes in v1.2 from the &lt;a href=&#34;https://cran.r-project.org/web/packages/jsonlite/NEWS&#34;&gt;NEWS&lt;/a&gt; file:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Add &lt;code&gt;read_json&lt;/code&gt; and &lt;code&gt;write_json&lt;/code&gt; convenience wrappers, &lt;a href=&#34;https://github.com/jeroen/jsonlite/issues/161&#34;&gt;#161&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Update &lt;code&gt;modp_numtoa&lt;/code&gt; from upstream, fixes a rounding issue in &lt;a href=&#34;https://github.com/jeroen/jsonlite/issues/148&#34;&gt;#148&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Ensure &lt;code&gt;asJSON.POSIXt&lt;/code&gt; does not use sci notation for negative values, &lt;a href=&#34;https://github.com/jeroen/jsonlite/issues/155&#34;&gt;#155&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tweak &lt;code&gt;num_to_char&lt;/code&gt; to properly print large negative numbers&lt;/li&gt;
&lt;li&gt;Performance optimization for simplyfing data frames (see below)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Use the &lt;em&gt;Github compare&lt;/em&gt; page to see the full diff on &lt;a href=&#34;https://github.com/cran/jsonlite/compare/1.1...1.2&#34;&gt;metacran&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;new-read-write-api&#34;&gt;New read/write API&lt;/h2&gt;

&lt;p&gt;The package has gained new high level functions &lt;code&gt;read_json&lt;/code&gt; and &lt;code&gt;write_json&lt;/code&gt;. These are &lt;a href=&#34;https://github.com/cran/jsonlite/blob/1.2/R/read_json.R#L18-L29&#34;&gt;wrappers&lt;/a&gt; for &lt;code&gt;fromJSON&lt;/code&gt; and &lt;code&gt;toJSON&lt;/code&gt; which read/write json directly from/to disk. This API is consistent with tidyverse packages like readr, readxl and haven (see &lt;a href=&#34;https://github.com/jeroen/jsonlite/issues/161&#34;&gt;#161&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The only thing to note is that &lt;code&gt;read_json&lt;/code&gt; does not simplify by default, as is done by &lt;code&gt;fromJSON&lt;/code&gt;. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Write Data frame to a temp file
tmp &amp;lt;- tempfile()
write_json(iris, tmp)

# Nested lists
read_json(tmp)

# A data frame
read_json(tmp, simplifyVector = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice how &lt;code&gt;read_json&lt;/code&gt; only returns a data frame when &lt;code&gt;simplifyVector&lt;/code&gt; is explicitly set to &lt;code&gt;TRUE&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;performance-enhancements&#34;&gt;Performance enhancements&lt;/h2&gt;

&lt;p&gt;We have ported a bit of C code to optimize simplification for data frame structures. This script compares performance for both versions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# example json
json &amp;lt;- jsonlite::toJSON(ggplot2::diamonds)

# Test with jsonlite 1.1
devtools::install_github(&amp;quot;cran/jsonlite@1.1&amp;quot;)
microbenchmark::microbenchmark(jsonlite::fromJSON(json), times = 50)

# Unload jsonlite 1.1 (might need restart R on windows)
unloadNamespace(&amp;quot;jsonlite&amp;quot;)
library.dynam.unload(&#39;jsonlite&#39;, find.package(&#39;jsonlite&#39;))

# Test with jsonlite 1.2
devtools::install_github(&amp;quot;cran/jsonlite@1.2&amp;quot;)
microbenchmark::microbenchmark(jsonlite::fromJSON(json), times = 50)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On my Macbook this has reduced the median time from approx 0.91s to 0.76s.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>finch - parse Darwin Core files</title>
      <link>https://ropensci.org/technotes/2016/12/23/finch-release/</link>
      <pubDate>Fri, 23 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2016/12/23/finch-release/</guid>
      <description>
        
        

&lt;p&gt;&lt;code&gt;finch&lt;/code&gt; has just been released to CRAN (binaries should be up soon).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;finch&lt;/code&gt; is a package to parse Darwin Core files. &lt;a href=&#34;http://rs.tdwg.org/dwc/&#34;&gt;Darwin Core&lt;/a&gt; (&lt;code&gt;DwC&lt;/code&gt;) is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;a body of standards. It includes a glossary of terms (in other contexts these might be called properties, elements, fields, columns, attributes, or concepts) intended to facilitate the sharing of information about biological diversity by providing reference definitions, examples, and commentaries. The Darwin Core is primarily based on taxa, their occurrence in nature as documented by observations, specimens, samples, and related information. &amp;hellip; The Simple Darwin Core [SIMPLEDWC] is a specification for one particular way to use the terms - to share data about taxa and their occurrences in a simply structured way - and is probably what is meant if someone suggests to &amp;ldquo;format your data according to the Darwin Core&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GBIF (Global Biodiversity Information Facility) is the biggest holder of biodiversity data. When you request
data in bulk format from GBIF they call give it to you in what&amp;rsquo;s called a Darwin Core Archive, or
&lt;code&gt;DwC-A&lt;/code&gt;. GBIF has a validator for DwC-A files as well: &lt;a href=&#34;http://tools.gbif.org/dwca-validator/&#34;&gt;http://tools.gbif.org/dwca-validator/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One of our most used packages is probably &lt;code&gt;rgbif&lt;/code&gt;, a client to interact with GBIF&amp;rsquo;s web services.
There&amp;rsquo;s a series of functions in &lt;code&gt;rgbif&lt;/code&gt; to request data in bulk format (see functions starting
with &lt;code&gt;occ_download&lt;/code&gt;), and from this you get a DwC-A file. This is where &lt;code&gt;finch&lt;/code&gt; comes in:
it can parse these DwC-A files into something useable inside R.&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;finch&amp;quot;)
# or from source if binary not available yet
install.packages(&amp;quot;finch&amp;quot;, type = &amp;quot;source&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;finch&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To parse a simple darwin core file like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;SimpleDarwinRecordSet
 xmlns=&amp;quot;http://rs.tdwg.org/dwc/xsd/simpledarwincore/&amp;quot;
 xmlns:dc=&amp;quot;http://purl.org/dc/terms/&amp;quot;
 xmlns:dwc=&amp;quot;http://rs.tdwg.org/dwc/terms/&amp;quot;
 xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;
 xsi:schemaLocation=&amp;quot;http://rs.tdwg.org/dwc/xsd/simpledarwincore/ ../../xsd/tdwg_dwc_simple.xsd&amp;quot;&amp;gt;
 &amp;lt;SimpleDarwinRecord&amp;gt;
  &amp;lt;dwc:occurrenceID&amp;gt;urn:catalog:YPM:VP.057488&amp;lt;/dwc:occurrenceID&amp;gt;
  &amp;lt;dc:type&amp;gt;PhysicalObject&amp;lt;/dc:type&amp;gt;
  &amp;lt;dc:modified&amp;gt;2009-02-12T12:43:31&amp;lt;/dc:modified&amp;gt;
  &amp;lt;dc:language&amp;gt;en&amp;lt;/dc:language&amp;gt;
  &amp;lt;dwc:basisOfRecord&amp;gt;FossilSpecimen&amp;lt;/dwc:basisOfRecord&amp;gt;
  &amp;lt;dwc:institutionCode&amp;gt;YPM&amp;lt;/dwc:institutionCode&amp;gt;
  &amp;lt;dwc:collectionCode&amp;gt;VP&amp;lt;/dwc:collectionCode&amp;gt;
  &amp;lt;dwc:catalogNumber&amp;gt;VP.057488&amp;lt;/dwc:catalogNumber&amp;gt;
  &amp;lt;dwc:individualCount&amp;gt;1&amp;lt;/dwc:individualCount&amp;gt;
  &amp;lt;dwc:locationID xsi:nil=&amp;quot;true&amp;quot;/&amp;gt;
  &amp;lt;dwc:continent&amp;gt;North America&amp;lt;/dwc:continent&amp;gt;
  &amp;lt;dwc:country&amp;gt;United States&amp;lt;/dwc:country&amp;gt;
  &amp;lt;dwc:countryCode&amp;gt;US&amp;lt;/dwc:countryCode&amp;gt;
  &amp;lt;dwc:stateProvince&amp;gt;Montana&amp;lt;/dwc:stateProvince&amp;gt;
  &amp;lt;dwc:county&amp;gt;Garfield&amp;lt;/dwc:county&amp;gt;
  &amp;lt;dwc:scientificName&amp;gt;Tyrannosourus rex&amp;lt;/dwc:scientificName&amp;gt;
  &amp;lt;dwc:genus&amp;gt;Tyrannosourus&amp;lt;/dwc:genus&amp;gt;
  &amp;lt;dwc:specificEpithet&amp;gt;rex&amp;lt;/dwc:specificEpithet&amp;gt;
  &amp;lt;dwc:earliestPeriodOrHighestSystem&amp;gt;Creataceous&amp;lt;/dwc:earliestPeriodOrHighestSystem&amp;gt;
  &amp;lt;dwc:latestPeriodOrHighestSystem&amp;gt;Creataceous&amp;lt;/dwc:latestPeriodOrHighestSystem&amp;gt;
  &amp;lt;dwc:earliestEonOrHighestEonothem&amp;gt;Late Cretaceous&amp;lt;/dwc:earliestEonOrHighestEonothem&amp;gt;
  &amp;lt;dwc:latestEonOrHighestEonothem&amp;gt;Late Cretaceous&amp;lt;/dwc:latestEonOrHighestEonothem&amp;gt;
 &amp;lt;/SimpleDarwinRecord&amp;gt;
&amp;lt;/SimpleDarwinRecordSet&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This file is in this package as an example file, get the file, then &lt;code&gt;simple()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;file &amp;lt;- system.file(&amp;quot;examples&amp;quot;, &amp;quot;example_simple_fossil.xml&amp;quot;, package = &amp;quot;finch&amp;quot;)
out &amp;lt;- simple_read(file)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Index to &lt;code&gt;meta&lt;/code&gt;, &lt;code&gt;dc&lt;/code&gt; or &lt;code&gt;dwc&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out$dc
#&amp;gt; [[1]]
#&amp;gt; [[1]]$type
#&amp;gt; [1] &amp;quot;PhysicalObject&amp;quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; [[2]]
#&amp;gt; [[2]]$modified
#&amp;gt; [1] &amp;quot;2009-02-12T12:43:31&amp;quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; [[3]]
#&amp;gt; [[3]]$language
#&amp;gt; [1] &amp;quot;en&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;parse-darwin-core-archive&#34;&gt;Parse Darwin Core Archive&lt;/h2&gt;

&lt;p&gt;To parse a Darwin Core Archive like can be gotten from GBIF use &lt;code&gt;dwca_read()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;dwca_read()&lt;/code&gt; can parse a DwC-A file as a directory, zipped file, or from a URL.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s an example Darwin Core Archive:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;file &amp;lt;- system.file(&amp;quot;examples&amp;quot;, &amp;quot;0000154-150116162929234.zip&amp;quot;, package = &amp;quot;finch&amp;quot;)
(out &amp;lt;- dwca_read(file, read = TRUE))
#&amp;gt; &amp;lt;gbif dwca&amp;gt;
#&amp;gt;   Package ID: 6cfaaf9c-d518-4ca3-8dc5-f5aadddc0390
#&amp;gt;   No. data sources: 10
#&amp;gt;   No. datasets: 3
#&amp;gt;   Dataset occurrence.txt: [225 X 443]
#&amp;gt;   Dataset multimedia.txt: [15 X 1]
#&amp;gt;   Dataset verbatim.txt: [209 X 443]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;List files in the archive&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out$files
#&amp;gt; $xml_files
#&amp;gt; [1] &amp;quot;/Library/Frameworks/R.framework/Versions/3.3/Resources/library/finch/examples/0000154-150116162929234/meta.xml&amp;quot;    
#&amp;gt; [2] &amp;quot;/Library/Frameworks/R.framework/Versions/3.3/Resources/library/finch/examples/0000154-150116162929234/metadata.xml&amp;quot;
#&amp;gt; 
#&amp;gt; $txt_files
#&amp;gt; [1] &amp;quot;/Library/Frameworks/R.framework/Versions/3.3/Resources/library/finch/examples/0000154-150116162929234/citations.txt&amp;quot; 
#&amp;gt; [2] &amp;quot;/Library/Frameworks/R.framework/Versions/3.3/Resources/library/finch/examples/0000154-150116162929234/multimedia.txt&amp;quot;
#&amp;gt; [3] &amp;quot;/Library/Frameworks/R.framework/Versions/3.3/Resources/library/finch/examples/0000154-150116162929234/occurrence.txt&amp;quot;
#&amp;gt; [4] &amp;quot;/Library/Frameworks/R.framework/Versions/3.3/Resources/library/finch/examples/0000154-150116162929234/rights.txt&amp;quot;    
#&amp;gt; [5] &amp;quot;/Library/Frameworks/R.framework/Versions/3.3/Resources/library/finch/examples/0000154-150116162929234/verbatim.txt&amp;quot;  
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;High level metadata for the whole archive (printing a subset for brevity)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out$emlmeta
#&amp;gt; &amp;lt;eml packageId=&amp;quot;6cfaaf9c-d518-4ca3-8dc5-f5aadddc0390&amp;quot; system=&amp;quot;http://gbif.org&amp;quot; scope=&amp;quot;system&amp;quot; xml:lang=&amp;quot;en&amp;quot; xsi:schemaLocation=&amp;quot;eml://ecoinformatics.org/eml-2.1.1 http://rs.gbif.org/schema/eml-gbif-profile/1.0.2/eml.xsd&amp;quot;&amp;gt;
#&amp;gt;   &amp;lt;dataset&amp;gt;
#&amp;gt;     &amp;lt;title&amp;gt;GBIF Occurrence Download 0000154-150116162929234&amp;lt;/title&amp;gt;
#&amp;gt;     &amp;lt;creator&amp;gt;
#&amp;gt;       &amp;lt;individualName&amp;gt;
#&amp;gt;         &amp;lt;surName&amp;gt;GBIF Download Service&amp;lt;/surName&amp;gt;
#&amp;gt;       &amp;lt;/individualName&amp;gt;
#&amp;gt;     &amp;lt;/creator&amp;gt;
#&amp;gt;     &amp;lt;metadataProvider&amp;gt;
#&amp;gt;       &amp;lt;individualName&amp;gt;
#&amp;gt;         &amp;lt;surName&amp;gt;GBIF Download Service&amp;lt;/surName&amp;gt;
#&amp;gt;       &amp;lt;/individualName&amp;gt;
#&amp;gt;     &amp;lt;/metadataProvider&amp;gt;
#&amp;gt;     &amp;lt;associatedParty&amp;gt;
#&amp;gt;       &amp;lt;organizationName&amp;gt;OZCAM (Online Zoological Collections of Australian Museums) Provider&amp;lt;/organizationName&amp;gt;
#&amp;gt;       &amp;lt;onlineUrl&amp;gt;http://www.ozcam.org.au/&amp;lt;/onlineUrl&amp;gt;
#&amp;gt;       &amp;lt;role&amp;gt;CONTENT_PROVIDER&amp;lt;/role&amp;gt;
#&amp;gt;     &amp;lt;/associatedParty&amp;gt;
#&amp;gt;     &amp;lt;associatedParty&amp;gt;
#&amp;gt;       &amp;lt;individualName&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;High level metadata for each data file, there&amp;rsquo;s many files, but we&amp;rsquo;ll just look at one&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hm &amp;lt;- out$highmeta
head( hm$occurrence.txt )
#&amp;gt;   index                                        term delimitedBy
#&amp;gt; 1     0         http://rs.gbif.org/terms/1.0/gbifID        &amp;lt;NA&amp;gt;
#&amp;gt; 2     1           http://purl.org/dc/terms/abstract        &amp;lt;NA&amp;gt;
#&amp;gt; 3     2       http://purl.org/dc/terms/accessRights        &amp;lt;NA&amp;gt;
#&amp;gt; 4     3      http://purl.org/dc/terms/accrualMethod        &amp;lt;NA&amp;gt;
#&amp;gt; 5     4 http://purl.org/dc/terms/accrualPeriodicity        &amp;lt;NA&amp;gt;
#&amp;gt; 6     5      http://purl.org/dc/terms/accrualPolicy        &amp;lt;NA&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can get the same metadata as above for each dataset that went into the tabular dataset downloaded&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out$dataset_meta[[1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;View one of the datasets, brief overview.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(out$data[[1]][,c(1:5)])
#&amp;gt;      gbifID abstract accessRights accrualMethod accrualPeriodicity
#&amp;gt; 1  50280003       NA                         NA                 NA
#&amp;gt; 2 477550574       NA                         NA                 NA
#&amp;gt; 3 239703844       NA                         NA                 NA
#&amp;gt; 4 239703843       NA                         NA                 NA
#&amp;gt; 5 239703833       NA                         NA                 NA
#&amp;gt; 6 477550692       NA                         NA                 NA
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(out$data[[1]])[1:20]
#&amp;gt;  [1] &amp;quot;gbifID&amp;quot;                &amp;quot;abstract&amp;quot;             
#&amp;gt;  [3] &amp;quot;accessRights&amp;quot;          &amp;quot;accrualMethod&amp;quot;        
#&amp;gt;  [5] &amp;quot;accrualPeriodicity&amp;quot;    &amp;quot;accrualPolicy&amp;quot;        
#&amp;gt;  [7] &amp;quot;alternative&amp;quot;           &amp;quot;audience&amp;quot;             
#&amp;gt;  [9] &amp;quot;available&amp;quot;             &amp;quot;bibliographicCitation&amp;quot;
#&amp;gt; [11] &amp;quot;conformsTo&amp;quot;            &amp;quot;contributor&amp;quot;          
#&amp;gt; [13] &amp;quot;coverage&amp;quot;              &amp;quot;created&amp;quot;              
#&amp;gt; [15] &amp;quot;creator&amp;quot;               &amp;quot;date&amp;quot;                 
#&amp;gt; [17] &amp;quot;dateAccepted&amp;quot;          &amp;quot;dateCopyrighted&amp;quot;      
#&amp;gt; [19] &amp;quot;dateSubmitted&amp;quot;         &amp;quot;description&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;using-with-rgbif&#34;&gt;Using with rgbif&lt;/h2&gt;

&lt;p&gt;Now that we have &lt;code&gt;finch&lt;/code&gt; we can make working with GBIF bulk downloads from R a
richer experience.&lt;/p&gt;

&lt;p&gt;Right now, we make it easy to import just the occurrence data from DwC-A files
via &lt;code&gt;occ_download_import()&lt;/code&gt;, e.g.,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rgbif)
res &amp;lt;- occ_download(&#39;taxonKey = 7264332&#39;, &#39;hasCoordinate = TRUE&#39;)
dat &amp;lt;- occ_download_get(res)
occ_download_import(dat)
#&amp;gt; Download file size: 0.26 MB
#&amp;gt;
#&amp;gt;        gbifID abstract accessRights accrualMethod accrualPeriodicity accrualPolicy
#&amp;gt; 1  1269880600       NA                         NA                 NA            NA
#&amp;gt; 2  1269861719       NA                         NA                 NA            NA
#&amp;gt; 3  1269850111       NA                         NA                 NA            NA
#&amp;gt; 4  1265524086       NA                         NA                 NA            NA
#&amp;gt; 5  1257400209       NA                         NA                 NA            NA
#&amp;gt; 6  1257396860       NA                         NA                 NA            NA
#&amp;gt; 7  1257391874       NA                         NA                 NA            NA
#&amp;gt; 8  1257390731       NA                         NA                 NA            NA
#&amp;gt; 9  1257383844       NA                         NA                 NA            NA
#&amp;gt; 10 1257375500       NA                         NA                 NA            NA
#&amp;gt; ..        ...      ...          ...           ...                ...           ...
#&amp;gt; Variables not shown: accrualPolicy (lgl), alternative (lgl), audience (lgl),
#&amp;gt;      available (lgl), bibliographicCitation (lgl), conformsTo (lgl),
#&amp;gt;      contributor (lgl), coverage (lgl), created (lgl), creator (lgl), date
#&amp;gt;      (lgl), dateAccepted (lgl), dateCopyrighted (lgl), dateSubmitted (lgl),
#&amp;gt;      description (lgl), educationLevel (lgl), extent (lgl), format (lgl),
#&amp;gt;      hasFormat (lgl), hasPart (lgl), hasVersion (lgl), identifier (chr),
#&amp;gt;      instructionalMethod (lgl), isFormatOf (lgl), isPartOf (lgl),
#&amp;gt;      isReferencedBy (lgl), isReplacedBy (lgl), isRequiredBy (lgl), ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With &lt;code&gt;finch&lt;/code&gt;, you can access all the data in the DwC-A file. &lt;code&gt;finch&lt;/code&gt; is not integrated
into &lt;code&gt;rgbif&lt;/code&gt;, though we may in the future.&lt;/p&gt;

&lt;p&gt;The object returned from &lt;code&gt;occ_download_get&lt;/code&gt; is just a path, so we can use that
with &lt;code&gt;finch&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(finch)
(out &amp;lt;- dwca_read(dat[1], read = TRUE))
#&amp;gt; &amp;lt;gbif dwca&amp;gt;
#&amp;gt;   Package ID: 10.15468/dl.mmecqc
#&amp;gt;   No. data sources: 8
#&amp;gt;   No. datasets: 3
#&amp;gt;   Dataset occurrence.txt: [235 X 1371]
#&amp;gt;   Dataset multimedia.txt: [15 X 0]
#&amp;gt;   Dataset verbatim.txt: [217 X 1371]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have access to not just the occurrence data&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(out$data$occurrence.txt)[,1:5]
#&amp;gt;       gbifID abstract accessRights accrualMethod accrualPeriodicity
#&amp;gt; 1 1269880600       NA                         NA                 NA
#&amp;gt; 2 1269861719       NA                         NA                 NA
#&amp;gt; 3 1269850111       NA                         NA                 NA
#&amp;gt; 4 1265524086       NA                         NA                 NA
#&amp;gt; 5 1257400209       NA                         NA                 NA
#&amp;gt; 6 1257396860       NA                         NA                 NA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But all the rich metadata in the other files. Yay!&lt;/p&gt;

&lt;h2 id=&#34;future-work&#34;&gt;Future work&lt;/h2&gt;

&lt;p&gt;DwC-A files can be very large - This is for sure going to be a pain point for some.
We&amp;rsquo;ll continue to test and refine on big data files.&lt;/p&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback?&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;d love to know what people think about this package.&lt;/p&gt;

&lt;p&gt;Documentation can be better, e.g., there&amp;rsquo;s no vignette yet (but adding
that soon).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Announcing pdftools 1.0</title>
      <link>https://ropensci.org/technotes/2016/12/09/pdftools-10/</link>
      <pubDate>Fri, 09 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2016/12/09/pdftools-10/</guid>
      <description>
        
        

&lt;p&gt;This week we released version &lt;code&gt;1.0&lt;/code&gt; of the ropensci &lt;a href=&#34;https://cran.r-project.org/web/packages/pdftools/index.html&#34;&gt;pdftools&lt;/a&gt; package to CRAN. Pdftools provides utilities for extracting text, fonts, attachments and other data from PDF files. It also supports rendering of PDF files into bitmap images.&lt;/p&gt;

&lt;p&gt;This release has a few internal enhancements and fixes an annoying bug for landscape PDF pages. The version bump to &lt;code&gt;1.0&lt;/code&gt; signifies that the package has undergone sufficient testing and the API is stable.&lt;/p&gt;

&lt;h2 id=&#34;extracting-text&#34;&gt;Extracting Text&lt;/h2&gt;

&lt;p&gt;As described in our previous &lt;a href=&#34;https://ropensci.org/blog/blog/2016/03/01/pdftools-and-jeroen&#34;&gt;post&lt;/a&gt;, the most common use of &lt;code&gt;pdftools&lt;/code&gt; is extracting text from (scientific) articles for searching / indexing. But let&amp;rsquo;s try a somewhat more unusual PDF file this time: a poster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(pdftools)
url &amp;lt;- &amp;quot;https://www.rstudio.com/wp-content/uploads/2016/02/advancedR.pdf&amp;quot;

# Display author, editor
pdf_info(url)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;pdf_info&lt;/code&gt; file returns all kind of metadata from the pdf file. For example we can read that this PDF was created on 2016-02-12 by Arianne Colton using Acrobat PDFMaker 11 for PowerPoint.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# extract text vector
text &amp;lt;- pdf_text(url)

# Print text from page 1
cat(text[1])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;pdf_text&lt;/code&gt; function extracts text into an R character vector if length equal to the number of pages in the PDF.
Note how the text is spaced to match the position in the PDF page.&lt;/p&gt;

&lt;h2 id=&#34;rendering-pdf&#34;&gt;Rendering PDF&lt;/h2&gt;

&lt;p&gt;Recent versions of pdftools allow rendering of PDF pages into bitmap images. The &lt;code&gt;pdf_render_page&lt;/code&gt; function returns the bitmap as a raw vector array of size channels * width * height (in pixels).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(pdftools)
bitmap &amp;lt;- pdf_render_page(url, page = 1, dpi = 72)
dim(bitmap)
### 4 1100  850
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here we can use for example the rOpenSci &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;magick&lt;/a&gt; package to read the bitmap and manipulate/export it to various formats.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)
poster &amp;lt;- image_read(bitmap)
print(poster)
image_write(poster, &amp;quot;out.png&amp;quot;, format = &amp;quot;png&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or have some fun with the other magick tools :)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Download dancing banana
banana &amp;lt;- image_read(&amp;quot;https://jeroen.github.io/images/banana.gif&amp;quot;)
banana &amp;lt;- image_scale(banana, &amp;quot;300&amp;quot;)

# Combine and flatten frames
frames &amp;lt;- lapply(banana, function(frame) {
  image_composite(poster, frame, offset = &amp;quot;+70+30&amp;quot;)
})

# Turn frames into animation
animation &amp;lt;- image_animate(image_join(frames))
print(animation)

# Save as gif
image_write(animation, &amp;quot;output.gif&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

      </description>
    </item>
    
    <item>
      <title>Tesseract Update: Options and Languages</title>
      <link>https://ropensci.org/technotes/2016/12/08/tesseract-13/</link>
      <pubDate>Thu, 08 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2016/12/08/tesseract-13/</guid>
      <description>
        
        

&lt;p&gt;A few weeks ago we &lt;a href=&#34;https://ropensci.org/blog/blog/2016/11/16/tesseract&#34;&gt;announced&lt;/a&gt; the first release of the &lt;a href=&#34;https://cran.r-project.org/web/packages/tesseract/index.html&#34;&gt;tesseract&lt;/a&gt; package: a high quality OCR engine in R. We have now released an update with extra features.&lt;/p&gt;

&lt;h2 id=&#34;installing-training-data&#34;&gt;Installing Training Data&lt;/h2&gt;

&lt;p&gt;As explained in the &lt;a href=&#34;https://ropensci.org/blog/blog/2016/11/16/tesseract&#34;&gt;first post&lt;/a&gt;, the tesseract system is powered by language specific training data. By default only English training data is installed. &lt;a href=&#34;https://cran.r-project.org/web/packages/tesseract/index.html&#34;&gt;Version 1.3&lt;/a&gt; adds utilities to make it easier to install additional training data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Download French training data
tesseract_download(&amp;quot;fra&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that this function is not needed on Linux. Here you should install training data via your system package manager instead. For example on Debian/Ubuntu:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt-get install tesseract-ocr-fra
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And on Fedora/CentOS you use:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo yum install tesseract-langpack-fra
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Use &lt;code&gt;tesseract_info()&lt;/code&gt; to see which training data are currently installed.&lt;/p&gt;

&lt;h2 id=&#34;ocr-engine-parameters&#34;&gt;OCR Engine Parameters&lt;/h2&gt;

&lt;p&gt;Tesseract supports many &lt;a href=&#34;http://www.sk-spell.sk.cx/tesseract-ocr-parameters-in-302-version&#34;&gt;parameters&lt;/a&gt; to fine tune the OCR engine. For example you can limit the possible characters that can be recognized.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;engine &amp;lt;- tesseract(options = list(tessedit_char_whitelist = &amp;quot;0123456789&amp;quot;))
ocr(&amp;quot;image.png&amp;quot;, engine = engine)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the example above, Tesseract will only consider numeric characters. If you know in advance the data is numeric (for example an accounting spreadsheet) such options can tremendously improve the accuracy.&lt;/p&gt;

&lt;h2 id=&#34;magick-images&#34;&gt;Magick Images&lt;/h2&gt;

&lt;p&gt;Tesseract now automatically recognizes images from the awesome &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/index.html&#34;&gt;magick&lt;/a&gt; package (our &lt;a href=&#34;https://ropensci.org/blog/blog/2016/08/23/z-magick-release&#34;&gt;R wrapper to ImageMagick&lt;/a&gt;). This can be useful to preprocess images before feeding to tesseract.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(magick)
library(tesseract)
image &amp;lt;- image_read(&amp;quot;http://jeroen.github.io/files/dog_hq.png&amp;quot;)
image &amp;lt;- image_crop(image, &amp;quot;1700x100+50+150&amp;quot;)
cat(ocr(image))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We plan to more integration between Magick and Tesseract in future versions.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>fauxpas - HTTP conditions package</title>
      <link>https://ropensci.org/technotes/2016/11/18/fauxpas-release/</link>
      <pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/technotes/2016/11/18/fauxpas-release/</guid>
      <description>
        
        

&lt;p&gt;HTTP, or &lt;a href=&#34;https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol&#34;&gt;Hypertext Transfer Protocol&lt;/a&gt; is a protocol by which most
of us interact with the web. When we do requests to a website in a browser
on desktop or mobile, or get some data from a server in R, all of that is
using HTTP.&lt;/p&gt;

&lt;p&gt;HTTP has a rich suite of &lt;a href=&#34;https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html&#34;&gt;status codes&lt;/a&gt; describing different HTTP
conditions, ranging from &lt;code&gt;Success&lt;/code&gt; to various client errors, to server errors.
R has a few HTTP client libraries - &lt;a href=&#34;https://cran.rstudio.com/web/packages/crul&#34;&gt;crul&lt;/a&gt;, &lt;a href=&#34;https://cran.rstudio.com/web/packages/curl&#34;&gt;curl&lt;/a&gt;, &lt;a href=&#34;https://cran.rstudio.com/web/packages/httr&#34;&gt;httr&lt;/a&gt;,
and &lt;a href=&#34;https://cran.rstudio.com/web/packages/RCurl&#34;&gt;RCurl&lt;/a&gt; - each of which is slightly different. I thought it would
be nice if there was a single way to do HTTP exception handling across these
libraries.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fauxpas&lt;/code&gt; is a package to handle HTTP conditions. Methods are included for
general purpose HTTP error handling, as well as individual methods for every
HTTP status code, both via status code numbers as well as their descriptive
names. &lt;code&gt;fauxpas&lt;/code&gt; allows flexibility to have stop, message or warning behavior.&lt;/p&gt;

&lt;p&gt;In addition, you can use custom &lt;a href=&#34;https://cran.rstudio.com/web/packages/whisker&#34;&gt;whisker&lt;/a&gt; template to have any
configuration of status code, short description, and verbose message.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fauxpas&lt;/code&gt; currently supports integration with &lt;code&gt;crul&lt;/code&gt;, &lt;code&gt;curl&lt;/code&gt;, and &lt;code&gt;httr&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It will be interesting to see how people use &lt;code&gt;fauxpas&lt;/code&gt;. It may be that
users really like being able to handle individual HTTP conditions separately,
or it may be that most users simply want a general purpose handler.&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;fauxpas&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;fauxpas&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;generic-http-condition-handler&#34;&gt;Generic http condition handler&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;Error()&lt;/code&gt; is the most generic handler in &lt;code&gt;fuaxpas&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s make an HTTP request, here with &lt;code&gt;crul&lt;/code&gt;, using the wonderful
&lt;a href=&#34;https://httpbin.org&#34;&gt;https://httpbin.org&lt;/a&gt; site.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;crul&amp;quot;)
res &amp;lt;- HttpClient$new(&amp;quot;https://httpbin.org/status/418&amp;quot;)$get()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we&amp;rsquo;ll constrct an error object&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(x &amp;lt;- Error$new(behavior = &amp;quot;stop&amp;quot;))
#&amp;gt; &amp;lt;HTTP Error&amp;gt;
#&amp;gt;   behavior: stop
#&amp;gt;   message_template: {{reason}} (HTTP {{status}}).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The default behavior is &lt;code&gt;stop&lt;/code&gt; - you can also choose &lt;code&gt;warning&lt;/code&gt; or &lt;code&gt;message&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The object is of class &lt;code&gt;Error&lt;/code&gt; and is an &lt;code&gt;R6&lt;/code&gt; object. You can inspect parts
of the object&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x$behavior
#&amp;gt; [1] &amp;quot;stop&amp;quot;
x$behavior_type
#&amp;gt; [1] &amp;quot;error&amp;quot;
x$call.
#&amp;gt; [1] FALSE
x$message_template
#&amp;gt; [1] &amp;quot;{{reason}} (HTTP {{status}}).&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the response&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x$do(res)
#&amp;gt; Error: I&#39;m a teapot (HTTP 418).
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;http-r6-methods&#34;&gt;HTTP* R6 methods&lt;/h2&gt;

&lt;p&gt;In addition to &lt;code&gt;Error()&lt;/code&gt;, there&amp;rsquo;s a method for every HTTP code - they start with
&lt;code&gt;HTTP&lt;/code&gt;, followed by the http code name. For example, &lt;code&gt;HTTPBadGateway&lt;/code&gt; for
the 502 code.&lt;/p&gt;

&lt;p&gt;These are R6 methods as well, but inherit from the &lt;code&gt;Error&lt;/code&gt; class. So they implement
&lt;code&gt;do()&lt;/code&gt;, but also &lt;code&gt;do_verbose()&lt;/code&gt; to include a verbose explanation of the http
condition.&lt;/p&gt;

&lt;p&gt;Make an HTTP request:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;res &amp;lt;- HttpClient$new(&amp;quot;https://httpbin.org/status/414&amp;quot;)$get()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- HTTPRequestURITooLong$new()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x$do(res)
#&amp;gt; Error: Request-URI Too Long (HTTP 414).
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x$do_verbose(res)
#&amp;gt; Error: Request-URI Too Long (HTTP 414).
#&amp;gt;  - The server is refusing to service the request because the Request-URI is longer 
#&amp;gt; than the server is willing to interpret. This rare condition is only likely to occur 
#&amp;gt; when a client has improperly converted a POST request to a GET request with long 
#&amp;gt; query information, when the client has descended into a URI &#39;black hole&#39; of 
#&amp;gt; redirection (e.g., a redirected URI prefix that points to a suffix of itself), or 
#&amp;gt; when the server is under attack by a client attempting to exploit security holes 
#&amp;gt; present in some servers using fixed-length buffers for reading or manipulating
#&amp;gt; the Request-URI.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;http-methods&#34;&gt;http* methods&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s a series of simpler to use functions for every http condition that wrap
the above &lt;code&gt;HTTP*&lt;/code&gt; methods that follow the form &lt;code&gt;http*&lt;/code&gt;, where the &lt;code&gt;*&lt;/code&gt; is the
status code number. For example, &lt;code&gt;http404&lt;/code&gt; for the 404 code.&lt;/p&gt;

&lt;p&gt;Make an HTTP request:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;crul&amp;quot;)
res &amp;lt;- HttpClient$new(&amp;quot;https://httpbin.org/status/418&amp;quot;)$get()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Default behavior is to &lt;code&gt;stop()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;http(res)
#&amp;gt; Error: I&#39;m a teapot (HTTP 418).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But you can easily do &lt;code&gt;warning()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;http(res, behavior = &amp;quot;warning&amp;quot;)
#&amp;gt; Warning message:
#&amp;gt; I&#39;m a teapot (HTTP 418). 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or &lt;code&gt;message()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;http(res, behavior = &amp;quot;message&amp;quot;)
#&amp;gt; I&#39;m a teapot (HTTP 418).
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;custom-templates&#34;&gt;Custom templates&lt;/h2&gt;

&lt;p&gt;The default way that conditions are handled is with the template:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;quot;{{reason}} (HTTP {{status}}).&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Named elements that are used for &lt;code&gt;do()&lt;/code&gt; are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;reason&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;status&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Named elements that are used for &lt;code&gt;do_verbose()&lt;/code&gt; are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;reason&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;status&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;message&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All other named elements are ignored.&lt;/p&gt;

&lt;p&gt;First, make an HTTP request:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;crul&amp;quot;)
res &amp;lt;- HttpClient$new(&amp;quot;https://httpbin.org/status/418&amp;quot;)$get()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, run a handler with a custom template:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;http418(res, message_template = &amp;quot;{{status}}\n  --&amp;gt; {{reason}}&amp;quot;)
#&amp;gt; Error: 418
#&amp;gt;   --&amp;gt; I&#39;m a teapot
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;future-work&#34;&gt;Future work&lt;/h2&gt;

&lt;h3 id=&#34;even-more-customizeable-messages&#34;&gt;Even more customizeable messages&lt;/h3&gt;

&lt;p&gt;Right now, you can use a hack to customize the &lt;code&gt;do_verbose()&lt;/code&gt; method to replace
the &lt;code&gt;mssg&lt;/code&gt; string (which holds the verbose explanation of the HTTP condition
from the HTTP spec) with your own message. For example, servers often return custom
messages explaining why a request failed (e.g., &lt;em&gt;page must be a number&lt;/em&gt;). Ideally,
you&amp;rsquo;d want to return that message along with the HTTP code (presumably &lt;code&gt;400&lt;/code&gt; or
similar) and the HTTP name (&lt;code&gt;Bad Request&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m going to make this easier, so you don&amp;rsquo;t have to replace the verbose HTTP
condition explanation, so you can still expose that if you want.&lt;/p&gt;

&lt;h3 id=&#34;support-rcurl&#34;&gt;Support RCurl&lt;/h3&gt;

&lt;p&gt;I hope to support &lt;code&gt;RCurl&lt;/code&gt; at some point.&lt;/p&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback?&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;d love to know what people think about this package.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Does the package API make sense?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Any additional exported methods you&amp;rsquo;d like?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I know documentation can be better, e.g., there&amp;rsquo;s no vignette yet (but adding
that soon).&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
