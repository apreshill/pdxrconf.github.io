<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rOpenSci Blog on rOpenSci - open tools for open science</title>
    <link>https://ropensci.org/blog/</link>
    <description>Recent content in rOpenSci Blog on rOpenSci - open tools for open science</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 13 Jul 2017 22:00:37 -0700</lastBuildDate>
    
        <atom:link href="https://ropensci.org/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>.rprofile: Jenny Bryan</title>
      <link>https://ropensci.org/blog/2017/12/08/rprofile-jenny-bryan/</link>
      <pubDate>Fri, 08 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/12/08/rprofile-jenny-bryan/</guid>
      <description>
        
        &lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-08-rprofile-jenny-bryan/jenny_bryan_lowres.jpg&#34; alt=&#34;Jenny Bryan&#34; style=&#34;margin: 0px 20px; width: 250px;&#34; align=&#34;left&#34;&gt;
&lt;em&gt;Jenny Bryan @JennyBryan is a Software Engineer at RStudio and is on leave from being an Associate Professor at the University of British Columbia. Jenny serves in leadership positions with rOpenSci and &lt;a href=&#34;https://forwards.github.io/&#34;&gt;Forwards&lt;/a&gt; and as an Ordinary member of &lt;a href=&#34;https://www.r-project.org/foundation/&#34;&gt;The R Foundation&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;KO: What is your name, your title, and how many years have you worked in R?&lt;/p&gt;

&lt;p&gt;JB: I’m Jenny Bryan, I am a software engineer at RStudio (still getting used to that title)., And I am on leave from being an Associate Professor at the University of British Columbia. I’ve been working with R or it’s predecessors since 1996. I switched to R from S in the early 2000s.&lt;/p&gt;

&lt;p&gt;KO: Why did you make the switch to R from S?&lt;/p&gt;

&lt;p&gt;JB: It just seemed like the community was switching over to R and I didn’t have a specific reason to do otherwise, I was just following the communal path of least resistance.&lt;/p&gt;

&lt;p&gt;KO: You have a huge following from all the stuff you post about your course. Did you always want to be a teacher? How did you get into teaching?&lt;/p&gt;

&lt;p&gt;JB: No, I wouldn’t say that I always wanted to be a teacher, but I think I’ve enjoyed that above average compared to other professors. But it was more that I realized several years ago that I could have a bigger impact on what people did by improving data analysis workflows, thinking, and tooling instead of trying to make incremental progress on statistical methodology. It is a reflection of where I have a comparative advantage with respect to interest and aptitude, so it’s not really a knock on statistical methodology. But I feel we could use more people working on this side of the field &amp;ndash; working on knowledge translation.&lt;/p&gt;

&lt;p&gt;I was also reacting to what I saw in my collaborative work. I would work with people in genomics and if I’m completely honest with myself, often my biggest contribution to the paper would be getting all the datasets and analyses organized. I didn’t necessarily do some highly sophisticated statistical analysis. It would often boil down to just doing millions of t-tests or something. But the reason I had an impact on the project would be that I got everything organized so that we could re-run it and have more confidence in our results. And I was like, I have a PhD in stats, why is this my main contribution? Why do the postdocs, grad students, and bioinformaticians not know how to do these things? So then I started to make that more and more the focus of my course, instead of squeezing in more statistical methods. Then the teaching sort of changed who I was and what I allowed myself to think about and spend time on. I used to not let myself spend time on those things. Or if I did, I would feel guilty about it because I thought, I can’t get any professional credit for this! It’s not statistically profound, but it seems to be what the world needs me to do, and needs other people to be doing.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You don’t always have to be proving a theorem, you don’t always have to be writing a package, there’s still a lot of space for worthwhile activity in between all of those things.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Do you feel proud of what you’ve accomplished?&lt;/p&gt;

&lt;p&gt;JB: I finally in some sense gave myself permission to start teaching what I thought people actually needed to know. And then after spending lots of time on it in the classroom, you realize what gaps there are, you become increasingly familiar with the tooling that you’re teaching and you’re like, hey I could actually improve that. Or no one really talks about how you get the output of this step to flow nicely as the input into the following step, i.e. how to create workflows. It really helped open my mind to different forms of work that are still valuable. You don’t always have to be proving a theorem, you don’t always have to be writing a package, there’s still a lot of space for worthwhile activity in between all of those things. However because we don’t have names for all of it, it can be difficult from a career point of view. But so many people see it, use it, and are grateful for it.&lt;/p&gt;

&lt;p&gt;KO: Can you talk about your transition into working for RStudio and what that will look like on a day-to-day basis?&lt;/p&gt;

&lt;p&gt;JB: In many ways it looks a lot like my life already did because I had, especially in the last two to three years, decided if I want to work on R packages or on exposition, I’m going to do that. That’s what I think tenure is for! So I had decided to stop worrying about how to sell myself in a framework set up to reward traditional work in statistical methodology. That freed up a lot of mental energy, to pursue these other activities, unapologetically. Which lead to other opportunities, such as RStudio. I was already working mostly from home. The Statistics department is by no means a negative environment for me, but the internet helped me find virtual colleagues around the globe who really share my interests. The physical comfort of home is very appealing. RStudio is also very light on meetings, which is a beautiful thing.&lt;/p&gt;

&lt;p&gt;KO: What is your team like at RStudio? How many projects are you juggling at any given time? Do you have an idea of what you want to accomplish while you’re there?&lt;/p&gt;

&lt;p&gt;JB: The person I interact with most is &lt;a href=&#34;http://hadley.nz/&#34;&gt;Hadley Wickham&lt;/a&gt; and he now has a team of five. There’s a fair amount of back and forth with other team members. I might seek their advice on, e.g., development practices, or just put questions out there for everyone. This team is pretty new and the formalization of the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; is pretty new, so everyone has different packages that they’re working on, either from scratch or shifting some of the maintenance burden off of Hadley. There’s a concerted effort to figure out “what does it mean to be an ecosystem of packages that work together?&amp;ldquo;.&lt;/p&gt;

&lt;p&gt;KO: Do you have a well defined road map at this point on the team?&lt;/p&gt;

&lt;p&gt;JB: I’ve been on that team since January and before that we had queued up &lt;a href=&#34;http://readxl.tidyverse.org/&#34;&gt;readxl&lt;/a&gt; as a good project for me. It was also overdue for maintenance! I was already a “Spreadsheet Lady”, very familiar with the underlying objects, and with the problem space. It was a good opportunity for me to write compiled code which I hadn’t done in a really long time. I had never written C++ so it was a way to kill at least three birds with one stone. So that was an easy selection for the first thing to work on. And even before that was done, it was clear that going back and doing another project in the Google arena made sense. We knew we would do some work with interns. Wrapping the Google Drive API was going to be useful (in general and for a future update of &lt;a href=&#34;https://github.com/jennybc/googlesheets&#34;&gt;googlesheets&lt;/a&gt;) and I knew our intern &lt;a href=&#34;http://www.lucymcgowan.com/&#34;&gt;Lucy McGowan&lt;/a&gt; would be a great person to work with on it.&lt;/p&gt;

&lt;p&gt;So no, there’s not some detailed 18-month roadmap stretching ahead of me. I think it will cycle between doing something that’s mine or new and doing maintenance on something that already exists. I also continue to do a lot of exposition, training, and speaking.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It actually pisses me off when people criticize “when” people work - like that’s a signifier of a poor work-life balance … their heart is in the right place to encourage balance, but I have a certain amount of work I want to get done.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Day-to-day, do you have regular standups? How do you like your day to be structured?&lt;/p&gt;

&lt;p&gt;JB: Oh there&amp;rsquo;s how I wish my day was structured and how it&amp;rsquo;s actually structured. I wish I could get up and just work because that’s when I feel by far my most productive. Unfortunately, this coincides with the morning chaos of a household with three kids, who, despite the fact that we’re trying to get them more independent with lunches and getting to school, you cannot completely tune out through this part of the day. So I do not really get up and just work, I sort of work until everyone gets out the door. Then I usually go exercise at that point, get that taken care of. I get more work done in the afternoon until the children all arrive home. I do a lot of work between 9 or 10 at night and 1 in the morning. Not because I love working at that time, but that’s just what I have.&lt;/p&gt;

&lt;p&gt;Given that I have this platform, it actually pisses me off when people criticize “when” people work - like that’s a signifier of a poor work-life balance, though it is possible that I have a poor work-life balance, but I feel like it’s usually coming from people who don’t have the same constraints in their life. “You shouldn’t work on the weekends, You shouldn’t work in the evenings”. I’m like, when the heck else do you think I would work? I feel like sometimes people are - their heart is in the right place to encourage balance, but I have a certain amount of work I want to get done. And I have a family and it means that I work when my children are asleep.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;They’re happy years but the tension between all the things you want to do is unbelievable because you will not do all of them. You cannot do it all.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: This topic is very interesting and personal to me. As I get older I’ve been thinking (nebulously) about starting a family, and I don’t know what that looks like. It’s scary to me, to not want to give up this lifestyle and this career that I’ve started for myself.&lt;/p&gt;

&lt;p&gt;JB: My pivoting of thinking about myself as an applied statistician to more of a data scientist, coincided with me reemerging from having little kids. I had all of them pre-tenure and at some point we had “three under three”. I was trying to get tenure, just barely getting it all done and I was kind of in my own little world, just surviving. Then the tenure process completed successfully, the kids got older, they were all in school, and eventually they didn’t need any out of school care. So me being able to string multiple abstract thoughts together and carve out hours at a time to do thought work coincided with me also freeing myself to work on stuff that I found more interesting.&lt;/p&gt;

&lt;p&gt;I don’t know how this all would have worked out if the conventional academic statistical work had suited me better. The time where I was most conflicted between doing a decent job parenting and doing decent work was also when I was doing work I wasn’t passionate about. I can’t tell if having more enthusiasm about the work would have made that period harder or easier! I really thought about ditching it all more than a few times.&lt;/p&gt;

&lt;p&gt;The reinvigoration that coincided with switching emphasis also coincided with the reinvigoration that comes from the kids becoming more independent. It does eventually happen! There are some very tough years - they’re not dark years, they’re happy years but the tension between all the things you want to do is unbelievable because you will not do all of them. You cannot do it all.&lt;/p&gt;

&lt;p&gt;KO: What are your favorite tools for managing your workflow?&lt;/p&gt;

&lt;p&gt;JB: In terms of working with R I’ve completely standardized on working with RStudio. Before that I was an Emacs-ESS zealot and I still have more accumulated years in that sphere. But once RStudio really existed and was viable, I started teaching with it. I hate doing R one way when I’m in front of students and another when I’m alone. It got very confusing and mixing up the keyboard shortcuts would create chaos. So now I’ve fully embraced RStudio and have never looked back.&lt;/p&gt;

&lt;p&gt;I’m also a &lt;a href=&#34;http://happygitwithr.com/&#34;&gt;git evangelist&lt;/a&gt;. Everything I do is in git, everything is on &lt;a href=&#34;https://github.com/jennybc&#34;&gt;Github&lt;/a&gt; and at this point, almost everything is public because I’ve gotten unselfconscious enough to put it up there. Plus there’s enough volume now that no one could be looking at any particular one thing. It’s so much easier for me to find it again later. I just put everything in a public place rather than trying to have this granular access control; it simplifies things greatly. Working in the open has simplified a lot of decisions, that’s nice.&lt;/p&gt;

&lt;p&gt;Otherwise I feel like my workflow is very primitive. I have thousands of email in my inbox. I’ve completely given up on managing email and I’m mostly okay with that. It’s out of my control and I can’t commit to a system where I’m forced to get to inbox zero. I’ve just given up on it. And twitter and slack are important ways to feel connected when I’m sitting at home on my sofa.&lt;/p&gt;

&lt;p&gt;KO: Do you have any online blogs, personalities or podcasts that you particularly enjoy? It doesn’t have to be R related.&lt;/p&gt;

&lt;p&gt;JB: I do follow people on twitter and the &lt;a href=&#34;https://twitter.com/hashtag/rstats&#34;&gt;rstats hashtag&lt;/a&gt;, so that often results in serendipitous one-off links that I enjoy. I don’t follow certain blogs regularly, but there are certain places that I end up at regularly. I like the Not So Standard Deviations podcast. In the end I always listen to every episode, but it’s what I do on an airplane or car drive.&lt;/p&gt;

&lt;p&gt;KO: You build up a backlog?&lt;/p&gt;

&lt;p&gt;JB: Exactly. Then the next time I need to drive to Seattle in traffic, I’ll power through four episodes.&lt;/p&gt;

&lt;p&gt;KO: What are some of your favorite R packages - do you have some that you think are funny, or love?&lt;/p&gt;

&lt;p&gt;JB: I live entirely in the tidyverse. I’m not doing primary data analysis on projects anymore. It’s weird that the more involved you become in honing the tools, the less time you spend wielding them. So I’m increasingly focused on the data prep, data wrangling, data input part of the cycle and not on modeling. I did a lot more of that when I was a statistician and now it’s not where my comparative interest and advantage seems to lie. There’s plenty to do on the other end. And also not that many people who like it. I actually do enjoy it. I don’t have to force myself to enjoy it - this is really important, and it pleases me. Given how important I think the work is, it’s a relatively uncrowded field. Whereas machine learning, it seems like everyone wants to make a contribution there. I’m like, you go for it - I’m going to be over here getting data out of Excel spreadsheets.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Exploratory Data Analysis of Ancient Texts with rperseus</title>
      <link>https://ropensci.org/blog/2017/12/05/rperseus/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/12/05/rperseus/</guid>
      <description>
        
        

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;When I was in grad school at Emory, I had a favorite desk in the library. The desk wasn’t particularly cozy or private, but what it lacked in comfort it made up for in real estate. My books and I needed room to operate. Students of the ancient world require many tools, and when jumping between commentaries, lexicons, and interlinears, additional clutter is additional “friction”, i.e., lapses in thought due to frustration. Technical solutions to this clutter exist, but the best ones are proprietary and expensive. Furthermore, they are somewhat inflexible, and you may have to shoehorn your thoughts into their framework. More friction.&lt;/p&gt;

&lt;p&gt;Interfacing with &lt;a href=&#34;http://www.perseus.tufts.edu/hopper/&#34;&gt;the Perseus Digital Library&lt;/a&gt; was a popular online alternative. The library includes a catalog of classical texts, a Greek and Latin lexicon, and a word study tool for appearances and references in other literature. If the university library’s reference copies of BDAG&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;em&gt;Synopsis Quattuor Evangeliorum&lt;/em&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; were unavailable, Perseus was our next best thing.&lt;/p&gt;

&lt;p&gt;Fast forward several years, and I’ve abandoned my quest to become a biblical scholar. Much to my father’s dismay, I’ve learned writing code is more fun than writing exegesis papers. Still, I enjoy dabbling with dead languages, and it was the desire to wed my two loves, biblical studies and R, that birthed my latest package, &lt;code&gt;rperseus&lt;/code&gt;. The goal of this package is to furnish classicists with texts of the ancient world and a toolkit to unpack them.&lt;/p&gt;

&lt;h3 id=&#34;exploratory-data-analysis-in-biblical-studies&#34;&gt;Exploratory Data Analysis in Biblical Studies&lt;/h3&gt;

&lt;p&gt;Working with the Perseus Digital Library was already a trip down memory lane, but here’s an example of how I would have leveraged &lt;code&gt;rperseus&lt;/code&gt; many years ago.&lt;/p&gt;

&lt;p&gt;My best papers often sprung from the outer margins of my &lt;a href=&#34;https://en.wikipedia.org/wiki/Novum_Testamentum_Graece&#34;&gt;&lt;em&gt;Nestle-Aland Novum Testamentum Graece.&lt;/em&gt;&lt;/a&gt; Here the editors inserted cross references to parallel vocabulary, themes, and even grammatical constructions. Given the intertextuality of biblical literature, the margins are a rich source of questions: Where else does the author use similar vocabulary? How is the source material used differently? Does the literary context affect our interpretation of a particular word? This is exploratory data analysis in biblical studies.&lt;/p&gt;

&lt;p&gt;Unfortunately the excitement of your questions is incommensurate with the tedium of the process&amp;ndash;EDA continues by flipping back and forth between books, dog-earring pages, and avoiding paper cuts. &lt;code&gt;rperseus&lt;/code&gt; aims to streamline this process with two functions: &lt;code&gt;get_perseus_text&lt;/code&gt; and &lt;code&gt;perseus_parallel&lt;/code&gt;. The former returns a data frame containing the text from any work in the Perseus Digital Library, and the latter renders a parallel in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Suppose I am writing a paper on different expressions of love in Paul’s letters. Naturally, I start in 1 Corinthians 13, the famed “Love Chapter” often heard at weddings and seen on bumper stickers. I finish the chapter and turn to the margins. In the image below, I see references to Colossians 1:4, 1 Thessalonians 1:3, 5:8, Hebrews 10:22-24, and Romans 8:35-39.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/nantg.png&#34; alt=&#34;&#34; /&gt;
&lt;em&gt;1 Corinithians 13 in Nestle-Aland Novum Testamentum Graece&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ignoring that some scholars exclude Colossians from the “authentic” letters, let’s see the references alongside each other:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rperseus) #devtools::install_github(“ropensci/rperseus”)
library(tidyverse)

tribble(
  ~label, ~excerpt,
  &amp;quot;Colossians&amp;quot;, &amp;quot;1.4&amp;quot;,
  &amp;quot;1 Thessalonians&amp;quot;, &amp;quot;1.3&amp;quot;,
  &amp;quot;1 Thessalonians&amp;quot;, &amp;quot;5.8&amp;quot;,
  &amp;quot;Romans&amp;quot;, &amp;quot;8.35-8.39&amp;quot;
  ) %&amp;gt;% 
  left_join(perseus_catalog) %&amp;gt;%
  filter(language == &amp;quot;grc&amp;quot;) %&amp;gt;%
  select(urn, excerpt) %&amp;gt;%
  pmap_df(get_perseus_text) %&amp;gt;%
  perseus_parallel(words_per_row = 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/Parallel1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A brief explanation: First, I specify the labels and excerpts within a tibble. Second, I join the lazily loaded &lt;code&gt;perseus_catalog&lt;/code&gt; onto the data frame. Third, I filter for the Greek&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; and select the columns containing the arguments required for &lt;code&gt;get_perseus_text&lt;/code&gt;. Fourth, I map over each urn and excerpt, returning another data frame. Finally, I pipe the output into &lt;code&gt;perseus_parallel&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The key word shared by each passage is &lt;em&gt;agape&lt;/em&gt; (“love”). Without going into detail, it might be fruitful to consider the references alongside each other, pondering how the semantic range of &lt;em&gt;agape&lt;/em&gt; expands or contracts within the Pauline corpus. Paul had a penchant for appropriating and recasting old ideas&amp;ndash;often in slippery and unexpected ways&amp;ndash;and your Greek lexicon provides a mere approximation. In other words, how can we move from the dictionary definition of &lt;em&gt;agape&lt;/em&gt; towards Paul&amp;rsquo;s unique vision?&lt;/p&gt;

&lt;p&gt;If your Greek is rusty, you can parse each word with &lt;code&gt;parse_excerpt&lt;/code&gt; by locating the text&amp;rsquo;s urn within the &lt;code&gt;perseus_catalog&lt;/code&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;parse_excerpt(urn = &amp;quot;urn:cts:greekLit:tlg0031.tlg012.perseus-grc2&amp;quot;, excerpt = &amp;quot;1.4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;form&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;verse&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;part_of_speech&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;person&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;number&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;tense&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;mood&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;voice&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;gender&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;case&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;degree&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ἀκούω&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ἀκούσαντες&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;verb&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;plural&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aorist&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;participle&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;active&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;masculine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;nominative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ὁ&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;τὴν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;article&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;singular&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;feminine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;accusative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;πίστις&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;πίστιν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;noun&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;singular&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;feminine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;accusative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ὑμός&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ὑμῶν&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;pronoun&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;plural&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;masculine&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;genative&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If your Greek is &lt;em&gt;really&lt;/em&gt; rusty, you can also flip the &lt;code&gt;language&lt;/code&gt; filter to “eng” to view an older English translation.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; And if the margin references a text from the Old Testament, you can call the Septuagint as well as the original Hebrew.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tribble(
  ~label, ~excerpt,
  &amp;quot;Genesis&amp;quot;, &amp;quot;32.31&amp;quot;,
  &amp;quot;Genesis, pointed&amp;quot;, &amp;quot;32.31&amp;quot;,
  &amp;quot;Numeri&amp;quot;, &amp;quot;12.8&amp;quot;,
  &amp;quot;Numbers, pointed&amp;quot;, &amp;quot;12.8&amp;quot;
  ) %&amp;gt;% 
  left_join(perseus_catalog) %&amp;gt;%
  filter(language %in% c(&amp;quot;grc&amp;quot;, &amp;quot;hpt&amp;quot;)) %&amp;gt;%
  select(urn, excerpt) %&amp;gt;%
  pmap_df(get_perseus_text) %&amp;gt;%
  perseus_parallel()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-05-rperseus/Parallel2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Admittedly, there is some “friction” here in joining the &lt;code&gt;perseus_catalog&lt;/code&gt; onto the initial tibble. There is a learning curve with getting acquainted with the idiosyncrasies of the catalog object. A later release will aim to streamline this workflow.&lt;/p&gt;

&lt;h3 id=&#34;future-work&#34;&gt;Future Work&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://ropensci.github.io/rperseus/articles/rperseus-vignette.html&#34;&gt;Check the vignette&lt;/a&gt; for a more general overview of &lt;code&gt;rperseus&lt;/code&gt;. In the meantime, I look forward to getting more intimately acquainted with the Perseus Digital Library. Tentative plans to extend &lt;code&gt;rperseus&lt;/code&gt; a Shiny interface to further reduce “friction” and a method of creating a “book” of custom parallels with &lt;code&gt;bookdown&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;I want to thank my two rOpenSci reviewers, &lt;a href=&#34;https://www.ildiczeller.com/&#34;&gt;Ildikó Czeller&lt;/a&gt; and &lt;a href=&#34;https://francoismichonneau.net/&#34;&gt;François Michonneau,&lt;/a&gt; for coaching me through the review process. They were the first two individuals to ever scrutinize my code, and I was lucky to hear their feedback. rOpenSci onboarding is truly a wonderful process.&lt;/p&gt;

&lt;!-- references --&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Bauer, Walter. &lt;em&gt;A Greek-English Lexicon of the New Testament and Other Early Christian Literature.&lt;/em&gt; Edited by Frederick W. Danker. 3rd ed. Chicago: University of Chicago Press, 2000.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Aland, Kurt. &lt;em&gt;Synopsis Quattuor Evangeliorum.&lt;/em&gt; Deutsche Bibelgesellschaft, 1997.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;The Greek text from the Perseus Digital Library is from 1885 standards. The advancement of textual criticism in the 20th century led to a more stable text you would find in current editions of the Greek New Testament.&lt;br /&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;The English translation is from Rainbow Missions, Inc. &lt;em&gt;World English Bible.&lt;/em&gt; Rainbow Missions, Inc.; revision of the American Standard Version of 1901. I’ve toyed with the idea of incorporating more modern translations, but that would require require resources beyond the Perseus Digital Library.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;&amp;ldquo;hpt&amp;rdquo; is the pointed Hebrew text from &lt;em&gt;Codex Leningradensis.&lt;/em&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>The Value of Welcome, part 2: How to prepare 40 new community members for an unconference</title>
      <link>https://ropensci.org/blog/2017/12/01/unconf-welcome/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/12/01/unconf-welcome/</guid>
      <description>
        
        &lt;p&gt;I’ve raved about the value of &lt;a href=&#34;https://ropensci.org/blog/2017/07/18/value-of-welcome/&#34;&gt;extending a personalized welcome&lt;/a&gt; to new community members and I recently shared &lt;a href=&#34;https://ropensci.org/blog/2017/11/17/unconf-sixtips&#34;&gt;six tips for running a successful hackathon-flavoured unconference&lt;/a&gt;. Building on these, I’d like to share the specific approach and (free!) tools I used to help prepare new rOpenSci community members to be productive at our &lt;a href=&#34;http://unconf17.ropensci.org/&#34;&gt;unconference&lt;/a&gt;. My approach was inspired directly by my &lt;a href=&#34;https://blog.trelliscience.com/introducing-the-2017-community-engagement-fellows/&#34;&gt;AAAS Community Engagement Fellowship Program&lt;/a&gt; (AAAS-CEFP) training. Specifically, 1) one mentor said that the most successful conference they ever ran involved having one-to-one meetings with all participants prior to the event, and 2) prior to our in-person AAAS-CEFP training, we completed an intake questionnaire that forced us to consider things like “what do you hope to get out of this” and “what do you hope to contribute”.&lt;/p&gt;

&lt;p&gt;A challenge of this year’s unconference was the fact that we were inviting 70 people to participate. As a rule, one third of the crowd will have participated in one of our previous unconferences and two-thirds would be first-time participants. With only two days together, these people need to quickly self-sort into project groups and get working.&lt;/p&gt;

&lt;p&gt;So I sent this email to 45 first-time participants:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-01-unconf-welcome/pre-unconf-email.png&#34; alt=&#34;pre-unconf-email&#34;&gt;&lt;/p&gt;

&lt;p&gt;Arranging meetings is one of my least favorite activities, but the free &lt;a href=&#34;https://calendly.com/&#34;&gt;Calendly tool&lt;/a&gt; made this process relatively painless. When a person clicks on the calendar link in the email above, it reveals only times that I am available in my Google Calendar, the time slot they choose shows up in my calendar, and I receive a confirmation email indicating who booked a meeting with me. In my busiest week, I had 19 meetings, but that meant the bulk of them were done!&lt;/p&gt;

&lt;p&gt;To make the meeting time most effective I followed AAAS-CEFP program director &lt;a href=&#34;https://twitter.com/LouWoodley&#34;&gt;Lou Woodley’s&lt;/a&gt; model for onboarding our AAAS CEFP cohort by sending a set of questions to be answered in advance. I took the model to the next level by creating a free &lt;a href=&#34;https://www.google.ca/forms/about/&#34;&gt;Google Form&lt;/a&gt; questionnaire so that all answers were automatically collected and could be viewed per individual or collectively, and automatically exported to a spreadsheet.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-01-unconf-welcome/pre-unconf-google-form.png&#34; alt=&#34;pre-unconf-google-form&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Questions included:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;List three things you hope to get from the unconference&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;connect with people working in a similar domain&lt;/em&gt;, &lt;em&gt;learn about best practices in data science with R&lt;/em&gt;, or &lt;em&gt;develop a new package that does X&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;List three things you hope to contribute to the unconference&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;expertise or experience in X&lt;/em&gt;, &lt;em&gt;mentoring skills&lt;/em&gt;, &lt;em&gt;write all the docs!&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Have you had any previous interactions with the rOpenSci community?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;I read the rOpenSci blog&lt;/em&gt;, or &lt;em&gt;I submitted software for rOpenSci open peer review&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Do you have any concerns about your readiness to participate?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;I’ve never developed an R package&lt;/em&gt; or &lt;em&gt;How do I decide what project to work on?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Would you be interested in writing a blog post about your unconference project or your unconference experience?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Do you  have a preferred working style or anything you would like to let us know about how you work best?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Examples: &lt;em&gt;I’m an introvert who likes to take my lunch alone sometimes to recover from group activities – it doesn’t mean I’m not having fun.&lt;/em&gt; or &lt;em&gt;I know I have a tendency to dominate in group discussions – it’s totally fine to ask me to step back and let others contribute. I won’t be offended.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These questions encouraged participants to reflect in advance. The example answer snippets we provided gave them ideas from which to seed their answers and in some cases gave them permission to show some vulnerability. Individual’s answers gave me cues for things to address in our chat and freed both of us to spend our time talking about the most important issues.&lt;/p&gt;

&lt;p&gt;The answers to the question “List three things you hope to get from the unconf” were so heartening:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-12-01-unconf-welcome/unconf-three-things.png&#34; alt=&#34;unconf-three-things&#34;&gt;&lt;/p&gt;

&lt;p&gt;Beautiful, but in a different way, were answers to the question, “Do you have any concerns about your readiness to participate?”. People expressed real concerns about impostor syndrome, their perceived ability to contribute &amp;ldquo;as much or as well&amp;rdquo; as others, and feeling &amp;ldquo;outclassed by all the geniuses present&amp;rdquo;. These responses prompted me to reassure people that they were 100% qualified to participate, and opened an opportunity to listen to and address specific concerns.&lt;/p&gt;

&lt;p&gt;To conduct the pre-unconference chats, I used video conferencing via &lt;a href=&#34;https://appear.in/&#34;&gt;appear.in&lt;/a&gt;, a free, browser-based application that does not require plugins or user accounts. Rather than being exhausted from these calls, I felt energized and optimistic and I experienced many direct positive outcomes. These conversations enabled me to prime people to connect on day-one of the unconference with others with similar interests or from related work sectors. Frequently, I noticed that immediately after our conversation, first-time participants would join the online discussion of existing project ideas, or they themselves proposed new ideas. My conversations with two first-time participants led directly to their proposing community-focussed projects - a &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/63&#34;&gt;group discussion&lt;/a&gt; and a new &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/64&#34;&gt;blog series of interviews&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;An unexpected benefit was that questions people asked me during the video chats led to actions I could take to improve the unconference. For example, when someone wanted to know what previous participants wished they knew beforehand, I asked for and &lt;a href=&#34;https://twitter.com/rOpenSci/status/855531572081991680&#34;&gt;shared example resources&lt;/a&gt;. One wise person asked me what my plan was for having project teams report out at the end of the unconference and this led directly to a streamlined plan (See &lt;a href=&#34;https://ropensci.org/blog/2017-11-17-unconf-sixtips&#34;&gt;Six tips for running a successful unconference&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Big thanks to AAAS CEFP training for giving me the confidence to try this community experiment! Arranging and carrying out these pre-unconference questionnaires and video chats took a big investment of my time and energy and yet, I consider this effort to be one of the biggest contributors to participants’ satisfaction with the unconference. Will 100% do this again!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Announcing a New rOpenSci Software Review Collaboration</title>
      <link>https://ropensci.org/blog/2017/11/29/review-collaboration-mee/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/29/review-collaboration-mee/</guid>
      <description>
        
        

&lt;p&gt;rOpenSci is pleased to announce a new collaboration with the &lt;a href=&#34;http://besjournals.onlinelibrary.wiley.com/hub/journal/10.1111/(ISSN)2041-210X/&#34;&gt;Methods in Ecology and Evolution (MEE)&lt;/a&gt;, a journal of the &lt;a href=&#34;http://www.britishecologicalsociety.org/&#34;&gt;British Ecological Society&lt;/a&gt;, published by Wiley press &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Publications destined for MEE that include the development of a scientific R package will now have the option of a joint review process whereby the R package is reviewed by rOpenSci, followed by fast-tracked review of the manuscript by MEE. Authors opting for this process will be recognized via a mark on both web and print versions of their paper.&lt;/p&gt;

&lt;p&gt;We are very excited for this partnership to improve the rigor of both scientific software and software publications and to provide greater recognition to developers in the fields of ecology and evolution.  It is a natural outgrowth of our interest in supporting scientists in developing and maintaining software, and of MEE&amp;rsquo;s mission of vetting and disseminating tools and methods for the research community. The collaboration formalizes and eases a path already pursued by researchers: The &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12593/abstract&#34;&gt;&lt;strong&gt;rotl&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12469/abstract&#34;&gt;&lt;strong&gt;RNexML&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00247.x/abstract&#34;&gt;&lt;strong&gt;treebase&lt;/strong&gt;&lt;/a&gt; packages were all developed or reviewed by rOpenSci and subsequently had associated manuscripts published in MEE.&lt;/p&gt;

&lt;h3 id=&#34;about-ropensci-software-review&#34;&gt;About rOpenSci software review&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; is a diverse community of researchers from academia, non-profit, government, and industry who collaborate to develop and maintain tools and practices around open data and reproducible research. The rOpenSci suite of tools is made of core infrastructure software developed and maintained by the &lt;a href=&#34;https://ropensci.org/about#team&#34;&gt;project staff&lt;/a&gt;. The suite also contains numerous packages that are contributed by members of the broader R community. The volume of community submissions has grown considerably over the years necessitating a formal system of review quite analogous to that of a peer reviewed academic journal.&lt;/p&gt;

&lt;p&gt;rOpenSci welcomes full software submissions that fit within our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;aims and scope&lt;/a&gt;, with the option of a pre-submission inquiry in cases when the scope of a submission is not immediately obvious. This software peer review framework, known as the rOpenSci Onboarding process, operates with three editors and one editor in chief who carefully vet all incoming submissions. After an editorial review, editors solicit detailed, public and signed reviews from two reviewers, and the path to acceptance from then on is similar to a standard journal review process. Details about the system are described in &lt;a href=&#34;https://ropensci.org/blog/2016/03/28/software-review/&#34;&gt;various&lt;/a&gt; &lt;a href=&#34;https://ropensci.org/blog/2017/09/01/nf-softwarereview/&#34;&gt;blog&lt;/a&gt; &lt;a href=&#34;https://ropensci.org/blog/2017/09/11/software-review-update/&#34;&gt;posts&lt;/a&gt; by the editorial team.&lt;/p&gt;

&lt;h3 id=&#34;collaboration-with-journals&#34;&gt;Collaboration with journals&lt;/h3&gt;

&lt;p&gt;This is our second collaboration with a journal. Since late 2015, rOpenSci has partnered with the &lt;a href=&#34;http://joss.theoj.org/&#34;&gt;Journal of Open Source software (JOSS)&lt;/a&gt;, an open access journal that publishes brief articles on research software. Packages accepted to rOpenSci can be submitted for fast-track publication at JOSS, in which JOSS editors may evaluate based on rOpenSci&amp;rsquo;s reviews alone. As rOpenSci&amp;rsquo;s review criteria is significantly more stringent and designed to be compatible with JOSS, these packages are generally accepted without additional review. We have had great success with this partnership providing rOpenSci authors with an additional venue to publicize and archive their work. Given this success, we are keen on expanding to other journals and fields where there is potential for software reviewed and created by rOpenSci to play a significant role in supporting scientific findings.&lt;/p&gt;

&lt;h3 id=&#34;the-details&#34;&gt;The details&lt;/h3&gt;

&lt;p&gt;Our new partnership with MEE broadly resembles that with JOSS, with the major difference that MEE, rather than rOpenSci, leads review of the manuscript component.  Authors with R packages and associated manuscripts that fit the Aims and Scope for both &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;rOpenSci&lt;/a&gt; and &lt;a href=&#34;http://www.methodsinecologyandevolution.org/view/0/aimsAndScope.html&#34;&gt;MEE&lt;/a&gt; are encouraged to first submit to rOpenSci. The &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12593/abstract&#34;&gt;&lt;strong&gt;rotl&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12469/abstract&#34;&gt;&lt;strong&gt;RNexML&lt;/strong&gt;&lt;/a&gt;, and &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00247.x/abstract&#34;&gt;&lt;strong&gt;treebase&lt;/strong&gt;&lt;/a&gt; packages are all great examples of such packages. MEE editors may also refer authors to this option if authors submit an appropriate manuscript to MEE first.&lt;/p&gt;

&lt;p&gt;On submission to rOpenSci, authors can use our updated &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/issue_template.md&#34;&gt;submission template&lt;/a&gt; to choose MEE as a publication venue. Following acceptance by rOpenSci, the associated manuscript will be reviewed by an expedited process at MEE, with reviewers and editors having the knowledge that the software has already been reviewed and the public reviews available to them.&lt;/p&gt;

&lt;p&gt;Should the manuscript be accepted, a footnote will appear in the web version and the first page of the print version of the MEE article indicating that the software as well as the manuscript has been peer-reviewed, with a link to the rOpenSci open reviews.&lt;/p&gt;

&lt;p&gt;As with any collaboration, there may be a few hiccups early on and we welcome ideas to make the process more streamlined and efficient. We look forward to the community&amp;rsquo;s submissions and to your participation in this process.&lt;/p&gt;

&lt;p&gt;Many thanks to MEE&amp;rsquo;s Assistant Editor Chris Grieves and Senior Editor Bob O&amp;rsquo;Hara for working with us on this collaboration.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;See also MEE&amp;rsquo;s post from today at &lt;a href=&#34;https://methodsblog.wordpress.com/2017/11/29/software-review/&#34;&gt;https://methodsblog.wordpress.com/2017/11/29/software-review/&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>changes: easy Git-based version control from R</title>
      <link>https://ropensci.org/blog/2017/11/28/ropensci-changes/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/28/ropensci-changes/</guid>
      <description>
        
        

&lt;p&gt;Are you new to version control and always running into trouble with Git?
Or are you a seasoned user, haunted by the traumas of learning Git and reliving them whilst trying to teach it to others?
Yeah, us too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-28-ropensci-changes/monkeys.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Git is a version control tool designed for software development, and it is extraordinarily powerful. It didn’t actually dawn on me quite how amazing Git is until I spent a weekend in Melbourne with a group of Git whizzes using Git to write a package targeted toward Git beginners. Whew, talk about total Git immersion! I was taking part in the 2017 &lt;a href=&#34;https://ozunconf17.ropensci.org/&#34;&gt;rOpenSci ozunconf&lt;/a&gt;, in which forty-odd  developers, scientists, researchers, nerds, teachers, starving students, cat ladies, and R users of all descriptions form teams to create new R packages fulfilling some new and useful function. Many of the groups used Git for their collaborative workflows all weekend.&lt;/p&gt;

&lt;p&gt;Unfortunately, just like many a programming framework, Git can often be a teensy bit (read: extremely, prohibitively) intimidating, especially for beginners who don&amp;rsquo;t need all of Git&amp;rsquo;s numerous and baffling features.
It’s one of those platforms that makes your life a million times better once you know how to use it, but if you’re trying to teach yourself the basics using the internet, or—heaven forbid—trying to untangle yourself from some Git-branch tangle that you’ve unwittingly become snarled in… (definitely done that one…) well, let’s just say using your knuckles to break a brick wall can sometimes seem preferable.
Just ask the Git whizzes.
They laugh, because they’ve been there, done that.&lt;/p&gt;

&lt;p&gt;The funny thing is, doing basic version control in Git only requires a few commands.
After browsing through the available project ideas and settling into teams, a group of eight of us made a list of the commands that we use on a daily basis, and the list came to about a dozen.
We looked up our Git histories and compiled a Git vocabulary, which came out to less than 50 commands, including combination commands.&lt;/p&gt;

&lt;p&gt;As &lt;a href=&#34;https://github.com/goldingn&#34;&gt;Nick Golding&lt;/a&gt; so shrewdly recognized in the lead up to this year’s unconference, the real obstacle for new Git users is not the syntax, it&amp;rsquo;s actually (a) the scary, scary terminal window and (b) the fact that Git terminology was apparently chosen by randomly opening a verb dictionary and blindly pointing to a spot on the page.
(Ok, I’m exaggerating, but the point is that the terminology is pretty confusing).
We decided to address these two problems by making a package that uses the R console and reimagining the version control vocabulary and workflow for people who are new to version control and only need some of its many features.&lt;/p&gt;

&lt;p&gt;Somewhat ironically, nine people worked for two days on a dozen branches, using Git and GitHub to seamlessly merge our workflows.
It was wonderful to see how so many people’s various talents can be combined to make something that no group members could have done all on their own.&lt;/p&gt;

&lt;p&gt;Enter, &lt;code&gt;changes&lt;/code&gt; ( &lt;a href=&#34;https://github.com/ropenscilabs/ozrepro&#34;&gt;repo&lt;/a&gt;, &lt;a href=&#34;https://ropenscilabs.github.io/changes/&#34;&gt;website&lt;/a&gt; – made using &lt;a href=&#34;https://github.com/hadley/pkgdown&#34;&gt;pkgdown&lt;/a&gt;), our new R package to do version control with a few simple commands.
It uses Git and &lt;a href=&#34;https://cran.r-project.org/web/packages/git2r/index.html&#34;&gt;Git2r&lt;/a&gt; under the hood, but new users don’t need to know any Git to begin using version control with &lt;code&gt;changes&lt;/code&gt;.
Best of all, it works seamlessly with regular Git. So if a user thinks they&amp;rsquo;re ready to expand their horizons they can start using git commands via the &lt;a href=&#34;https://GitHub.com/jennybc/Githug&#34;&gt;Githug&lt;/a&gt; package, RStudio&amp;rsquo;s git interface, or on the command line.&lt;/p&gt;

&lt;p&gt;Here is an overview of some of the ways we’ve made simple version control easy with &lt;code&gt;changes&lt;/code&gt;:&lt;/p&gt;

&lt;h4 id=&#34;simple-terminology&#34;&gt;Simple terminology&lt;/h4&gt;

&lt;p&gt;It uses simple and deliberately un-git-like terminology:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You start a new version control project with &lt;code&gt;create_repo()&lt;/code&gt;, which is like &lt;code&gt;git init&lt;/code&gt; but it can set up a nice project directory structure for you, automatically ignoring things like output folders.&lt;/li&gt;
&lt;li&gt;All of the steps involved in commiting edits have been compressed into one function: &lt;code&gt;record()&lt;/code&gt;. All files that aren&amp;rsquo;t ignored will be committed, so users don&amp;rsquo;t need to know the difference between tracking, staging and committing files.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s easy to set which files to omit from version control with &lt;code&gt;ignore()&lt;/code&gt;, and to change your mind with &lt;code&gt;unignore()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;changes()&lt;/code&gt; lets you know which files have changed since the last record, like a hybrid of &lt;code&gt;git status&lt;/code&gt; and &lt;code&gt;git diff&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;You can look back in history with &lt;code&gt;timeline()&lt;/code&gt; (a simplified version of &lt;code&gt;git log&lt;/code&gt;), &lt;code&gt;go_to()&lt;/code&gt; a previous record (like &lt;code&gt;git checkout&lt;/code&gt;), and &lt;code&gt;scrub()&lt;/code&gt; any unwanted changes since the last record (like &lt;code&gt;git reset --hard&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;it-s-linear&#34;&gt;It&amp;rsquo;s linear&lt;/h4&gt;

&lt;p&gt;After a long discussion, we decided that changes won&amp;rsquo;t provide an interface to Git branches (at least not yet), as the merge conflicts it leads to are one of the scariest things about version control for beginners.
With linear version control, users can can easily &lt;code&gt;go_to()&lt;/code&gt; a past record with a version number, rather than unfamiliar &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Internals-Git-Objects&#34;&gt;SHA&amp;rsquo;s&lt;/a&gt;. These numbers appear in the a lovely visual representation of their &lt;code&gt;timeline()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      (1) initial commit
       |  2017-11-18 02:55
       |
      (2) set up project structure
       |  2017-11-18 02:55
       |
      (3) added stuff to readme
          2017-11-18 02:55
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to roll your project back to a previous record, you can &lt;code&gt;retrieve()&lt;/code&gt; it, and changes will simply append that record at the top of your timeline (storing all the later records, just in case).&lt;/p&gt;

&lt;h4 id=&#34;readable-messages-and-automatic-reminders&#34;&gt;Readable messages and automatic reminders&lt;/h4&gt;

&lt;p&gt;Some of Git&amp;rsquo;s messages and helpfiles are totally cryptic to all but the most hardened computer scientists.
Having been confronted with our fair share of &lt;a href=&#34;https://www.git-tower.com/learn/git/faq/detached-head-when-checkout-commit&#34;&gt;&lt;code&gt;detached HEAD&lt;/code&gt;&lt;/a&gt;s and offers to &lt;a href=&#34;https://git-scm.com/docs/git-push&#34;&gt;&lt;code&gt;update remote refs along with associated objects&lt;/code&gt;&lt;/a&gt;, we were keen to make sure all the error messages and helpfiles in changes are as intuitive and understandable as possible.&lt;/p&gt;

&lt;p&gt;It can also be hard to get into the swing of recording edits, so changes will give you reminders to encourage you to use &lt;code&gt;record()&lt;/code&gt; regularly. You can change the time interval for reminders, or switch them off, using &lt;code&gt;remind_me()&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;coming-soon&#34;&gt;Coming soon&lt;/h4&gt;

&lt;p&gt;We made a lot of progress in two days, but there&amp;rsquo;s plenty more we&amp;rsquo;re planning to add soon:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Simplified access to GitHub with a &lt;code&gt;sync()&lt;/code&gt; command to automagically handle most uses of &lt;code&gt;git fetch&lt;/code&gt;, &lt;code&gt;git merge&lt;/code&gt;, and &lt;code&gt;git push&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A Git training-wheels mode, so that people who want to move use Git can view the Git commands &lt;code&gt;changes&lt;/code&gt; is using under the hood.&lt;/li&gt;
&lt;li&gt;Added flexibility – we are working on adding functionality to handle simple deviations from the defaults, such as recording changes only to named files, or to all except some excluded files.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;rsquo;d be really keen to hear your suggestions too, so please let us know your ideas via the &lt;a href=&#34;https://github.com/ropenscilabs/changes/issues&#34;&gt;changes issue tracker&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;I have only recently started using Git and GitHub, and this year’s &lt;a href=&#34;https://ozunconf17.ropensci.org/&#34;&gt;rOpenSci ozunconf&lt;/a&gt; was a big eye-opener for me, in several ways.
Beyond finally understanding to power of proper version control, I met a group of wonderful people dedicated to participating in the R community.
Now as it turns out, R users take the word “community” very seriously.
Each and every person I met during the event was open and friendly.
Each person had ideas for attracting new users to R, making it easier to learn, making methods and data more readily available, and creating innovative new functionality.
Even before the workshop began, dozens of ideas for advancement circulated on &lt;a href=&#34;https://github.com/ropensci/ozunconf17/issues&#34;&gt;GitHub Issues&lt;/a&gt;.
Throughout the conference, it was a pleasure to be a part of the ongoing conversation and dialogue about growing and improving the R community.
That’s right, you can delete any lingering ‘introverted computer geek’ stereotypes you might still be harbouring in a cobwebbed attic of your mind.
In today’s day and age, programming is as much about helping each other, communicating, learning, and networking as it is about solving problems.
And building the community is a group effort.&lt;/p&gt;

&lt;p&gt;R users come from all sorts of backgrounds, but I was gratified to see scientists and researchers well-represented at the unconference.
Gone are the days when I need to feel like the ugly duckling for being the only R user in my biology lab!
If you still find yourself isolated, join the blooming online R users community, or any one of a number of meetups and clubs that are popping up everywhere.
I have dipped my toe in those waters, and boy am I glad I did!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>ochRe - Australia themed colour palettes</title>
      <link>https://ropensci.org/blog/2017/11/21/ochre/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/21/ochre/</guid>
      <description>
        
        

&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;The second rOpenSci &lt;a href=&#34;http://ozunconf17.ropensci.org/&#34;&gt;OzUnConf&lt;/a&gt; was held in Melbourne Australia a few weeks ago. A diverse range of &lt;a href=&#34;https://ropensci.org/blog/2017/10/31/ozunconf2017/&#34;&gt;scientists, developers and general good-eggs&lt;/a&gt; came together to make some R-magic happen and also learn a lot along the way. Before the conference began, a huge stack of projects were suggested on the unconf  &lt;a href=&#34;https://github.com/ropensci/ozunconf17/issues&#34;&gt;GitHub repo&lt;/a&gt;. For six data-visualisation enthusiasts, one issue in particular caught their eye, and the &lt;code&gt;ochRe&lt;/code&gt; package was born.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/AusElevationExamplePalettes.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/ropenscilabs/ochRe&#34;&gt;&lt;code&gt;ochRe&lt;/code&gt;&lt;/a&gt; package contains colour palettes influenced by the Australian landscape, iconic Australian artists and images. OchRe is originally the brain-child of &lt;a href=&#34;https://twitter.com/visnut&#34;&gt;Di Cook&lt;/a&gt;, who was inspired by Karthik Ram&amp;rsquo;s &lt;a href=&#34;https://github.com/karthik/wesanderson&#34;&gt;&lt;code&gt;wesanderson&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;h3 id=&#34;why-ochre&#34;&gt;Why &amp;ldquo;ochre&amp;rdquo;?&lt;/h3&gt;

&lt;p&gt;Naming our package was the &amp;ldquo;most important&amp;rdquo; task facing us after all jumping on board the project. Fuelled by a selection of pastries we opened the discussions, fully expecting this to take some time. Fortunately, we all agreed on the name in less than 5 minutes, which meant plenty of pastries were left for the serious business of package building.&lt;/p&gt;

&lt;p&gt;Ochre is naturally occuring, brownish-yellow &lt;a href=&#34;https://en.wikipedia.org/wiki/Ochre&#34;&gt;pigment&lt;/a&gt; found in many parts of Australia, so frequently in fact, that Australia is sometimes referred to as the &amp;ldquo;land of ochre soil&amp;rdquo;. Additionally, ochre pigment has been used for thousands of years by Aboriginal people in Australia, with many culturally important uses from artwork to the preservation of animal skins.&lt;/p&gt;

&lt;h3 id=&#34;building-ochre&#34;&gt;Building ochRe&lt;/h3&gt;

&lt;p&gt;We started our package building journey by each picking an iconic Australian artwork (this took longer than you might think). Once we had selected our images, we used the online &lt;a href=&#34;http://www.coolphptools.com/color_extract&#34;&gt;Image Color Extract PHP&lt;/a&gt; demo tool to extract the hex code for the main colours within each image. Some images required a more selective approach, so where needed the colour code extraction was done using the eyedropper tool (in macOS) or the Google Chrome colourPick extension.&lt;/p&gt;

&lt;p&gt;Once we were happy with the colours, codes and order for each palette we loaded this information into &lt;code&gt;ochRe&lt;/code&gt; as lists of hex codes associated with the palette name. We adopted &lt;code&gt;scales&lt;/code&gt; to improve the fucntionality of the packages when using &lt;code&gt;ggplot&lt;/code&gt;, in particular to allow manipulation of colour ramping and transparency. The package also contains a few simple functions for displaying the different palettes.&lt;/p&gt;

&lt;p&gt;Below are some examples of original art work and their associated palettes:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;namatjira_qual&lt;/code&gt; and &lt;code&gt;namatjira_div&lt;/code&gt; are both inspired by the watercolour painting &lt;a href=&#34;http://www.menziesartbrands.com/items/twin-ghosts&#34;&gt;&amp;ldquo;Twin Ghosts&amp;rdquo;&lt;/a&gt;, by Aboriginal artist Albert Namatjira. &lt;code&gt;namatjira_div&lt;/code&gt; is ordered for plotting divergent datasets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/TwinGhosts_AlbertNamatjira.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/namatjira_qual.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;nolan_ned&lt;/code&gt; palette is inspired by the famous paintings of the outlaw &lt;a href=&#34;https://cs.nga.gov.au/detail.cfm?irn=28926&#34;&gt;Ned Kelly&lt;/a&gt; by Sidney Nolan.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/nedKelly_sidneyNolan.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/nolan_ned.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;olsen_seq&lt;/code&gt; has been designed for plotting sequential data, such as a heat map or landscape layers. The colours come from the abstract piece &lt;a href=&#34;https://artsearch.nga.gov.au/Detail-LRG.cfm?IRN=26102&#34;&gt;&amp;ldquo;Sydney Sun&amp;rdquo;&lt;/a&gt;, 1965, by John Olsen.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/sydneySun_johnOlsen.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/olsen_seq.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There was a high proportion of ecologists at the #ozunconf, which inspired the somewhat pessimistic &lt;code&gt;healthy_reef&lt;/code&gt; and &lt;code&gt;dead_reef&lt;/code&gt; palettes, with the colours taken from recent underwater photgraphs of the Great Barrier Reef.&lt;/p&gt;

&lt;h3 id=&#34;introducing-ochre&#34;&gt;Introducing: &lt;code&gt;ochRe&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;Our package currently contains 16 colour palettes, each one inspired by either an Australian landscape, an artwork or image by an Australian artist, or an Australian animal. Some of the palettes are more suited to displaying continuous data (such as in the Australian elevation maps above). Other palettes will perform best plotting discrete data (as in the &lt;code&gt;parliament&lt;/code&gt; example below).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ochRe&lt;/code&gt; can be currently be installed from GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# You need to install the &#39;devtools&#39; package first
devtools::install_github(&amp;quot;ropenscilabs/ochRe&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can visualise all 16 palettes using the following code snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pal_names &amp;lt;- names(ochre_palettes)

par(mfrow=c(length(ochre_palettes)/2, 2), lheight = 2, mar=rep(1, 4), adj = 0)
for (i in 1:length(ochre_palettes)){
    viz_palette(ochre_palettes[[i]], pal_names[i])
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/ochrePalettes.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here are some worked examples, showing how to use the palettes for different types of data visualisation, including both &lt;code&gt;ggplot&lt;/code&gt; and base plotting in R.&lt;/p&gt;

&lt;p&gt;An example using base R and the &lt;code&gt;winmar&lt;/code&gt; palette, this is based on an iconic photograph by Wayne Ludbey, &amp;ldquo;Nicky Winmar St Kilda Footballer&amp;rdquo;, 1993. In the photo, Aboriginal AFL player Nicky Winmar is baring his skin in response to racial abuse during an AFL game.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## basic example code
pal &amp;lt;- colorRampPalette(ochre_palettes[[&amp;quot;winmar&amp;quot;]])
image(volcano, col = pal(20))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/VolcanoWithWinmar.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Paired scatter plot using the &lt;code&gt;emu_Woman_paired&lt;/code&gt; palette, inspired by &amp;ldquo;Emu Woman&amp;rdquo;, 1988-89, by Emily Kame Kngwarreye.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(ochRe)
library(naniar)

# Exploring missing values benefits from a paired palette, like the emu women
# Here missing status on air temperature is shown in a plot of the two wind variables
data(oceanbuoys)
oceanbuoys &amp;lt;- oceanbuoys %&amp;gt;% add_shadow(humidity, air_temp_c) 
ggplot(oceanbuoys, aes(x=wind_ew, y=wind_ns, colour=air_temp_c_NA)) + 
    geom_point(alpha=0.8) + 
    scale_colour_ochre(palette=&amp;quot;emu_woman_paired&amp;quot;) +
    theme_bw() + theme(aspect.ratio=1)

# Slightly more complicated, forcing the pairs
clrs &amp;lt;- ochre_palettes$emu_woman_paired[11:12]
ggplot(oceanbuoys, aes(x=wind_ew, y=wind_ns, colour=air_temp_c_NA)) + 
    geom_point(alpha=0.8) + 
    scale_colour_manual(values=clrs) +
    theme_bw() + theme(aspect.ratio=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/Ochre_Example1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A map of the Australian electoral boundaries, using the &lt;code&gt;galah&lt;/code&gt; palette. &lt;a href=&#34;https://en.wikipedia.org/wiki/Galah&#34;&gt;Galahs&lt;/a&gt; are a common species of cockatoo found throughout mainland Australia.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Map of the 2016 Australian electoral boundaries
# with the galah palette
library(eechidna)
library(ggthemes)
data(nat_map_2016)
data(nat_data_2016)
ggplot(aes(map_id=id), data=nat_data_2016) +
    geom_map(aes(fill=Area_SqKm), map=nat_map_2016) +
    expand_limits(x=nat_map$long, y=nat_map$lat) + 
    scale_fill_ochre(palette=&amp;quot;galah&amp;quot;, discrete=FALSE) +
    theme_map()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/Ochre_Example2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Results of the 2016 Australian election to the senate, coloured by political party using the &lt;code&gt;parliament&lt;/code&gt; palette. The colours for this palette were taken from the &lt;a href=&#34;https://www.aph.gov.au/~/media/06%20Visit%20Parliament/66%20Parl%20House%20Art%20Collection/661%20five%20treasures/five%20treasures%20detail%20pics/M19840057UntitledBOYDunframed.png?la=en&#34;&gt;tapestry&lt;/a&gt; by Arthur Boyd found in the Great Hall of Parliament House.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Election results
senate &amp;lt;- read_csv(&amp;quot;http://results.aec.gov.au/20499/Website/Downloads/SenateSenatorsElectedDownload-20499.csv&amp;quot;, 
                   skip = 1)
coalition &amp;lt;- c(&amp;quot;Country Liberals (NT)&amp;quot;, &amp;quot;Liberal&amp;quot;, &amp;quot;Liberal National Party of Queensland&amp;quot;, 
               &amp;quot;The Nationals&amp;quot;)
labor &amp;lt;- c(&amp;quot;Australian Labor Party&amp;quot;, &amp;quot;Australian Labor Party (Northern Territory) Branch&amp;quot;, 
           &amp;quot;Labor&amp;quot;)
greens &amp;lt;- c(&amp;quot;The Greens&amp;quot;, &amp;quot;Australian Greens&amp;quot;, &amp;quot;The Greens (WA)&amp;quot;)

senate &amp;lt;- senate %&amp;gt;% mutate(PartyNm = ifelse(as.character(PartyNm) %in% coalition, 
                                             &amp;quot;Liberal National Coalition&amp;quot;, PartyNm))

senate &amp;lt;- senate %&amp;gt;% mutate(PartyNm = ifelse(as.character(PartyNm) %in% labor, 
                                             &amp;quot;Australian Labor Party&amp;quot;, PartyNm))

senate &amp;lt;- senate %&amp;gt;% mutate(PartyNm = ifelse(as.character(PartyNm) %in% greens, 
                                             &amp;quot;Australian Greens&amp;quot;, PartyNm))

senate$PartyNm &amp;lt;- factor(senate$PartyNm, 
                         levels = names(sort(table(senate$PartyNm), 
                            decreasing = T)))

ggplot(data = senate, aes(x = PartyNm, fill = PartyNm)) + 
    geom_bar() + xlab(&amp;quot;&amp;quot;) + 
    ylab(&amp;quot;&amp;quot;) + scale_fill_ochre(palette=&amp;quot;parliament&amp;quot;) + coord_flip() + 
    theme_bw() + theme(legend.position = &amp;quot;None&amp;quot;) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-21-ochRe/Ochre_Example3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For more information about the individual palettes available in &lt;code&gt;ochRe&lt;/code&gt; visit our &lt;a href=&#34;https://github.com/ropenscilabs/ochRe/tree/master/vignettes&#34;&gt;vignette&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All of the &lt;code&gt;ochRe&lt;/code&gt; team had a great time at #ozunconf, Thank you to the organisers for a brilliant event. Special Thanks to &lt;a href=&#34;https://github.com/mdsumner&#34;&gt;Michael Sumner&lt;/a&gt; for providing code to access the Australian elevation map you see at the start of this post.&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The Oz colour palette gang soakin it up outside &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; &lt;a href=&#34;https://t.co/XiLkhwZwTv&#34;&gt;pic.twitter.com/XiLkhwZwTv&lt;/a&gt;&lt;/p&gt;&amp;mdash; Miles McBain (@MilesMcBain) &lt;a href=&#34;https://twitter.com/MilesMcBain/status/923682409400250368?ref_src=twsrc%5Etfw&#34;&gt;October 26, 2017&lt;/a&gt;&lt;/blockquote&gt;

      </description>
    </item>
    
    <item>
      <title>Six tips for running a successful unconference</title>
      <link>https://ropensci.org/blog/2017/11/17/unconf-sixtips/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/17/unconf-sixtips/</guid>
      <description>
        
        

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-17-unconf-sixtips/ropensci-unconf17-community-nistara-randawa.jpg&#34; alt=&#34;Attendees at the May 2017 rOpenSci unconference. Photo credit: Nistara Randhawa&#34;&gt;
&lt;em&gt;Attendees at the May 2017 rOpenSci unconference. Photo credit: Nistara Randhawa&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In May 2017, I helped run a wildly successful “unconference” that had a huge positive impact on the community I serve. rOpenSci is a non-profit initiative enabling open and reproducible research by creating technical infrastructure in the form of staff- and community-contributed software tools in the R programming language that lower barriers to working with scientific data sources on the web, and creating social infrastructure through a welcoming and diverse community of software users and developers. Our 4th annual unconference brought together 70 people to hack on projects they dreamed up and to give them opportunities to meet and work together in person. One third of the participants had attended before, and two thirds were first-timers, selected from an open call for applications. We paid all costs up front for anyone who requested this in order to lower barriers to participation.&lt;/p&gt;

&lt;p&gt;It’s called an “unconference” because there is no schedule set before the event – participants discuss project ideas online in advance and projects are selected by participant-voting at the start. I’m sharing some tips here on how to do this well for this particular flavour of unconference.&lt;/p&gt;

&lt;h4 id=&#34;1-have-a-code-of-conduct&#34;&gt;1. Have a code of conduct&lt;/h4&gt;

&lt;p&gt;Having a &lt;a href=&#34;https://ropensci.org/blog/blog/2016/12/21/commcallv12-review-coc&#34;&gt;code of conduct&lt;/a&gt; that the organizers promote in the welcome goes a long way to creating a welcoming and safe environment and preventing violations in the first place.&lt;/p&gt;

&lt;h4 id=&#34;2-host-online-discussion-of-project-ideas-before-the-unconference&#34;&gt;2. Host online discussion of project ideas before the unconference&lt;/h4&gt;

&lt;p&gt;Our unconference centered on teams working on programming projects, rather than discussions, so prior to the unconference, we asked all participants to &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/&#34;&gt;suggest project ideas&lt;/a&gt; using an open online system, called GitHub, that allows everyone to see and comment on ideas or just share enthusiastic emoji to show support.&lt;/p&gt;

&lt;h4 id=&#34;3-have-a-pre-unconference-video-chat-with-first-time-participants&#34;&gt;3. Have a pre-unconference video-chat with first-time participants&lt;/h4&gt;

&lt;p&gt;Our AAAS CEFP training emphasizes the importance of extending a personalized welcome to community members, so I was inspired to make the bold move of talking with more than 40 first-time participants prior to the unconference. I asked each person to complete a short questionnaire to get them to consider the roles they anticipated playing prior to our chat. Frequently, within an hour of a video-chat, without prompting, I would see the person post a new project idea or share their thoughts about someone else’s. Other times, our conversation gave the person an opportunity to say “this idea maybe isn’t relevant but…” and I would help them talk through it, inevitably leading to “oh my gosh this is such a cool idea”. I got a huge return on my investment. People’s questions like “how do you plan to have 20 different projects present their work?” led to better planning on my part. Specific ideas for improvements came from me responding with “well…how would YOU do it?”&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Between the emails, slack channel, issues on GitHub, and personal video chats, I felt completely at ease going into the unconf (where I knew next to no one!).&lt;/p&gt;

&lt;p&gt;-rOpenSci unconf17 participant&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;4-run-an-effective-ice-breaker&#34;&gt;4. Run an effective ice breaker&lt;/h4&gt;

&lt;p&gt;I adapted the “&lt;a href=&#34;https://www.facinghistory.org/resource-library/teaching-strategies/barometer-taking-stand-controversial-issues&#34;&gt;Human Barometer&lt;/a&gt;” ice breaker to enable 70 people to share opinions across all perceived levels of who a participant is and introduce themselves to the entire group within a 1 hour period. Success depended on creating questions that were relevant to the unconference crowd, and on visually keeping track of who had spoken up in order to call on those who had not. Ice breakers and the rOpenSci version of the Human Barometer will be the subject of a future CEFP blog post.&lt;/p&gt;

&lt;h4 id=&#34;5-have-a-plan-to-capture-content&#34;&gt;5. Have a plan to capture content&lt;/h4&gt;

&lt;p&gt;So much work and money go into running a great unconference, you can’t afford to do it without a plan to capture and disseminate stories about the people and the products. Harness the brain work that went into the ideas! I used the concept of &lt;a href=&#34;http://www.socialfish.org/2016/11/you-have-more-content-than-you-realize/&#34;&gt;content pillars&lt;/a&gt; to come up with a plan. Every project group was given a public repository on GitHub to house their code and documentation. In a 2-day unconference with 70 people in 20 projects, how do people present their results?! We told everyone that they had just three minutes to present, and that the only presentation material they could use was their project README (the page of documentation in their code repository). No slides allowed! This kept their focus on great documentation for the project rather than an ephemeral set of pretty slides. Practically speaking, this meant that all presentations were accessible from a single laptop connected to the projector and that to access their presentation, a speaker had only to &lt;a href=&#34;https://ropenscilabs.github.io/runconf17-projects/&#34;&gt;click on the link to their repo&lt;/a&gt;. Where did the essence of this great idea come from? From a pre-unconference chat of course!&lt;/p&gt;

&lt;p&gt;In the week following the unconference, we used the projects’ README documentation to create a series of five posts released Monday through Friday, noting every one of the 20 projects with links to their code repositories. To get more in-depth stories about people and projects, I let participants know we were keen to host community-contributed blog posts and that accepted posts would be tweeted to rOpenSci’s &amp;gt;13,000 followers. Immediately after the unconference, we invited selected projects to contribute longer form narrative blog posts and posted these once a week. The series ended with &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/08/unconfroadsnottaken&#34;&gt;Unconf 2017: The Roads Not Taken&lt;/a&gt;, about all the great ideas that were not yet pursued and inviting people to contribute to these.&lt;/p&gt;

&lt;p&gt;All of this content is tied together in &lt;a href=&#34;https://ropensci.org/blog/blog/2017/06/02/unconf2017&#34;&gt;one blog post to summarize&lt;/a&gt; the unconference and link to all staff- and community-contributed posts in the unconference projects series as well as to posts of warm personal and career-focussed reflections by some participants.&lt;/p&gt;

&lt;h4 id=&#34;6-care-about-other-people-s-success&#34;&gt;6. Care about other people’s success&lt;/h4&gt;

&lt;p&gt;Community managers do a lot of “emotional work”. In all of this, my #1 key to running a successful unconference is to genuinely and deeply care that participants arrive feeling ready to hit the ground running and leave feeling like they got what they wanted out of the experience. Ultimately, the success of our unconference is more about the human legacy – building people’s capacity as software users and developers, and the in-person relationships they develop within our online community – than it is about the projects.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“I’ve steered clear of ‘hackathons’ because they feel intimidating and the ‘bad’ kind of competitive. But, this….this was something totally different.”&lt;/p&gt;

&lt;p&gt;– rOpenSci unconf17 participant&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Additional Resources&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.trelliscience.com/introducing-the-2017-community-engagement-fellows/&#34;&gt;AAAS Community Engagement Fellows Program&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://socialinsilico.wordpress.com/2014/11/07/all-together-now-event-formats-for-more-practical-sessions/&#34;&gt;All together now: Event formats for more practical sessions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Budd A, Dinkel H, Corpas M, Fuller JC, Rubinat L, Devos DP, et al. (2015) Ten Simple Rules for Organizing an Unconference. PLoS Comput Biol11(1): e1003905. &lt;a href=&#34;https://doi.org/10.1371/journal.pcbi.1003905&#34;&gt;https://doi.org/10.1371/journal.pcbi.1003905&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>2017 rOpenSci ozunconf :: Reflections and the realtime Package</title>
      <link>https://ropensci.org/blog/2017/11/14/realtime/</link>
      <pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/14/realtime/</guid>
      <description>
        
        &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This year&amp;rsquo;s &lt;a href=&#34;https://ozunconf17.ropensci.org/&#34;&gt;rOpenSci ozunconf&lt;/a&gt; was held in Melbourne, bringing together over 45 R enthusiasts from around the country and beyond. As is customary, ideas for projects were discussed in &lt;a href=&#34;https://github.com/ropensci/ozunconf17/issues&#34;&gt;GitHub Issues&lt;/a&gt; (41 of them by the time the unconf rolled around!) and there was no shortage of enthusiasm, interesting concepts, and varied experience.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been to a few unconfs now and I treasure the time I get to spend with new people, new ideas, new backgrounds, new approaches, and new insights. That&amp;rsquo;s not to take away from the time I get to spend with people I met at previous unconfs; I&amp;rsquo;ve gained great friendships and started collaborations on side projects with these wonderful people.&lt;/p&gt;

&lt;p&gt;When the call for nominations came around this year it was an easy decision. I don&amp;rsquo;t have employer support to attend these things so I take time off work and pay my own way. This is my networking time, my development time, and my skill-building time. I wasn&amp;rsquo;t sure what sort of project I&amp;rsquo;d be interested in but I had no doubts something would come up that sounded interesting.&lt;/p&gt;

&lt;p&gt;As it happened, I had been playing around with a bit of code, purely out of interest and hoping to learn how &lt;a href=&#34;https://www.htmlwidgets.org/&#34;&gt;&lt;code&gt;htmlwidgets&lt;/code&gt;&lt;/a&gt; work. The idea I had was to make a classic graphic equaliser visualisation like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/graphiceq.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;using R.&lt;/p&gt;

&lt;p&gt;This presents several challenges; how can I get live audio into R, and how fast can I plot the signal? I had doubts about both parts, partly because of the way that R calls tie up the session (&lt;a href=&#34;https://appsilondatascience.com/blog/rstats/2017/11/01/r-promises-hands-on.html&#34;&gt;for now&amp;hellip;&lt;/a&gt;) and partly because constructing a &lt;code&gt;ggplot2&lt;/code&gt; object is somewhat slow (in terms of raw audio speeds). I&amp;rsquo;d heard about &lt;code&gt;htmlwidgets&lt;/code&gt; and thought there must be a way to leverage that towards my goal.&lt;/p&gt;

&lt;p&gt;I searched for a graphic equaliser javascript library to work with and didn&amp;rsquo;t find much that aligned with what I had in my head. Eventually I stumbled on &lt;a href=&#34;https://p5js.org/&#34;&gt;&lt;code&gt;p5.js&lt;/code&gt;&lt;/a&gt; and its examples page which has an &lt;a href=&#34;https://p5js.org/examples/sound-frequency-spectrum.html&#34;&gt;audio-input plot with a live demo&lt;/a&gt;. It&amp;rsquo;s a frequency spectrum, but I figured that&amp;rsquo;s just a bit of binning away from what I need. Running the example there looks like&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/p5sound_optimised.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This seemed to be worth a go. I managed to follow enough of &lt;a href=&#34;https://www.htmlwidgets.org/develop_intro.html&#34;&gt;this tutorial&lt;/a&gt; to have the library called from R. I modified the javascript canvas code to look a little more familiar, and the first iteration of &lt;code&gt;geom_realtime()&lt;/code&gt; was born&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/3mdiCUbgxi0&#34; frameborder=&#34;0&#34; gesture=&#34;media&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;This seemed like enough of an idea that I proposed it in the GitHub Issues for the unconf. It got a bit of attention, which was worrying, because I had no idea what to do with this next. &lt;a href=&#34;https://github.com/petehaitch&#34;&gt;Peter Hickey&lt;/a&gt; pointed out that &lt;a href=&#34;https://github.com/seankross&#34;&gt;Sean Kross&lt;/a&gt; had &lt;a href=&#34;https://seankross.com/2017/08/11/Beyond-Axes-Simulating-Systems-with-Interactive-Graphics.html&#34;&gt;already wrapped some of the &lt;code&gt;p5.js&lt;/code&gt; calls into R calls&lt;/a&gt; with his &lt;code&gt;p5&lt;/code&gt; package, so this seemed like a great place to start. It&amp;rsquo;s quite a clever way of doing it too; it involves re-writing the &lt;code&gt;javascript&lt;/code&gt; which &lt;code&gt;htmlwidgets&lt;/code&gt; calls on each time you want to do something.&lt;/p&gt;

&lt;p&gt;Fast forward to the unconf and a decent number of people gathered around a little slip of paper with &lt;code&gt;geom_realtime()&lt;/code&gt; written on it. I had to admit to everyone that the &lt;code&gt;ggplot2&lt;/code&gt; aspect of my demo was a sham (it&amp;rsquo;s surprisingly easy to draw a canvas in just the right shade of grey with white gridlines), but people stayed, and we got to work seeing what else we could do with the idea. We came up with some suggestions for input sources, some different plot types we might like to support, and set about trying to understand what Sean&amp;rsquo;s package actually did.&lt;/p&gt;

&lt;p&gt;As it tends to work out, we had a great mix of people with different experience levels in different aspects of the project; some who knew how to make a package, some who knew how to work with &lt;code&gt;javascript&lt;/code&gt;, some who knew how to work with &lt;code&gt;websockets&lt;/code&gt;, some who knew about realtime data sources, and some who knew about nearly none of these things (✋ that would be me). If everyone knew every aspect about how to go about an unconf project I suspect the endeavor would be a bit boring. I love these events because I get to learn so much about so many different topics.&lt;/p&gt;

&lt;p&gt;I shared my demo script and we deconstructed the pieces. We dug into the inner workings of the &lt;code&gt;p5&lt;/code&gt; package and started determining which parts we could siphon off to meet our own needs. One of the aspects that we wanted to figure out was how to simulate realtime data. This could be useful both for testing, and also in the situation where one might want to &amp;rsquo;re-cast&amp;rsquo; some time-coded data. We were thankful that &lt;a href=&#34;https://github.com/kcf-jackson&#34;&gt;Jackson Kwok&lt;/a&gt; had gone deep-dive into &lt;code&gt;websockets&lt;/code&gt; and pretty soon (surprisingly soon, perhaps; within the first day) we had examples of (albeit, constructed) real-time (every 100ms) data streaming from a server and being plotted at-speed&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/realtime_runif_optimised.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Best of all, running the plot code didn&amp;rsquo;t tie up the session; it uses a listener written into the &lt;code&gt;javascript&lt;/code&gt; so it just waits for input on a particular port.&lt;/p&gt;

&lt;p&gt;With the core goal well underway, people started branching out into aspects they found most interesting. We had some people work on finding and connecting actual data sources, such as the bitcoin exchange rate&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/realtime_btc_optimised.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and a live-stream of binary-encoded data from the &lt;a href=&#34;http://qrng.anu.edu.au/index.php&#34;&gt;Australian National University (ANU) Quantum Random Numbers Server&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/realtime_bin_optimised.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Others formalised the code so that it can be piped into different &amp;lsquo;themes&amp;rsquo;, and retain the &lt;code&gt;p5&lt;/code&gt; structure for adding more components&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/realtime_bw.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These were still toy examples of course, but they highlight what&amp;rsquo;s possible. They were each constructed using an offshoot of the &lt;code&gt;p5&lt;/code&gt; package whereby the &lt;code&gt;javascript&lt;/code&gt; is re-written to include various features each time the plot is generated.&lt;/p&gt;

&lt;p&gt;Another route we took is to use the direct &lt;code&gt;javascript&lt;/code&gt; binding API with factory functions. This had less flexibility in terms of adding modular components, but meant that the &lt;code&gt;javascript&lt;/code&gt; could be modified without worrying about how it needed to interact with &lt;code&gt;p5&lt;/code&gt; so much. This resulted in some outstanding features such as side-scrolling and date-time stamps. We also managed to pipe the data off to another thread for additional processing (in R) before being sent to the plot.&lt;/p&gt;

&lt;p&gt;The example we ended up with reads the live-feed of Twitter posts under a given hashtag, computes a sentiment analysis on the words with R, and live-plots the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/auspol.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Overall I was amazed at the progress we made over just two days. Starting from a silly idea/demo, we built a package which can plot realtime data, and can even serve up some data to be plotted. I have no expectations that this will be the way of the future, but it&amp;rsquo;s been a fantastic learning experience for me (and hopefully others too). It&amp;rsquo;s highlighted that there&amp;rsquo;s ways to achieve realtime plots, even if we&amp;rsquo;ve used a library built for drawing rather than one built for plotting per se.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s even inspired offshoots in the form of some R packages;  &lt;a href=&#34;https://github.com/ropenscilabs/tRainspotting&#34;&gt;&lt;code&gt;tRainspotting&lt;/code&gt;&lt;/a&gt; which shows realtime data on New South Wales public transport using &lt;code&gt;leaflet&lt;/code&gt; as the canvas&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-14-realtime/tRainspotting.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and &lt;a href=&#34;https://github.com/kcf-jackson/jsReact/&#34;&gt;&lt;code&gt;jsReact&lt;/code&gt;&lt;/a&gt; which explores the interaction between R and Javascript&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/kcf-jackson/jsReact/raw/master/inst/example_5.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/kcf-jackson/jsReact/raw/master/inst/example_6.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The possibilities are truly astounding. My list of &amp;lsquo;things to learn&amp;rsquo; has grown significantly since the unconf, and projects are still starting up/continuing to develop. The &lt;a href=&#34;https://github.com/jonocarroll/ggeasy&#34;&gt;&lt;code&gt;ggeasy&lt;/code&gt;&lt;/a&gt; package isn&amp;rsquo;t related, but it was spawned from another unconf Github Issue idea. Again; ideas and collaborations starting and developing.&lt;/p&gt;

&lt;p&gt;I had a great time at the unconf, and I can&amp;rsquo;t wait until the next one. My hand will be going up to help out, attend, and help start something new.&lt;/p&gt;

&lt;p&gt;My thanks and congratulations go out to each of the &lt;a href=&#34;https://github.com/ropenscilabs/realtime&#34;&gt;&lt;code&gt;realtime&lt;/code&gt;&lt;/a&gt; developers: &lt;a href=&#34;https://github.com/richardbeare&#34;&gt;Richard Beare&lt;/a&gt;, &lt;a href=&#34;https://github.com/jonocarroll&#34;&gt;Jonathan Carroll&lt;/a&gt;, &lt;a href=&#34;https://github.com/kimnewzealand&#34;&gt;Kim Fitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/softloud&#34;&gt;Charles Gray&lt;/a&gt;, &lt;a href=&#34;https://github.com/jeffreyhanson&#34;&gt;Jeffrey O Hanson&lt;/a&gt;, &lt;a href=&#34;https://github.com/holtzy&#34;&gt;Yan Holtz&lt;/a&gt;, &lt;a href=&#34;https://github.com/kcf-jackson&#34;&gt;Jackson Kwok&lt;/a&gt;, &lt;a href=&#34;https://github.com/MilesMcBain&#34;&gt;Miles McBain&lt;/a&gt; and the entire cohort of &lt;a href=&#34;https://ozunconf17.ropensci.org&#34;&gt;2017 rOpenSci ozunconf attendees&lt;/a&gt;. In particular, my thanks go to the organisers of such a wonderful event; &lt;a href=&#34;https://github.com/njtierney&#34;&gt;Nick Tierney&lt;/a&gt;, &lt;a href=&#34;https://github.com/robjhyndman&#34;&gt;Rob Hyndman&lt;/a&gt;, &lt;a href=&#34;https://github.com/dicook&#34;&gt;Di Cook&lt;/a&gt;, and &lt;a href=&#34;https://github.com/MilesMcBain&#34;&gt;Miles McBain&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>.rprofile: Mara Averick</title>
      <link>https://ropensci.org/blog/2017/11/10/rprofile-mara-averick/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/11/10/rprofile-mara-averick/</guid>
      <description>
        
        &lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-10-rprofile-mara-averick/mara_averick.png&#34; alt=&#34;Mara Averick, Data Nerd At Large&#34; style=&#34;margin: 0px 20px; width: 250px;&#34; align=&#34;left&#34;&gt;
&lt;em&gt;Mara Averick is a non-profit data nerd, NBA stats junkie, and most recently, tidyverse developer advocate at RStudio. She is the voice behind two very popular Twitter accounts, @dataandme and @batpigandme. Mara and I discussed sports analytics, how attending a cool conference can change the approach to your career, and how she uses Twitter as a mechanism for self-imposed forced learning.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;KO: What is your name, job title, and how long have you been using R? [Note: This interview took place in May 2017. Mara joined RStudio as their tidyverse developer advocate in November 2017.]&lt;/p&gt;

&lt;p&gt;MA: My name is Mara Averick, I do consulting, data science, I just say “data nerd at large” because I’ve seen those Venn diagrams and I’m definitely not a data scientist. I used R in high school for fantasy basketball. I graduated from high school in 2003, and then in college used SPSS, and I didn’t use R for a long time. And then I was working with a company that does grant proposals for non-profits, doing all of the demand- and outcome-analysis and it all was in Excel and I thought, we could do better - R might also be helpful for this. It turns out there’s a package for American Community Survey data in R (&lt;a href=&#34;https://cran.r-project.org/web/packages/acs/index.html&#34;&gt;acs&lt;/a&gt;), so that was how I got back into R.&lt;/p&gt;

&lt;p&gt;KO: How did you find out about R when you first started using it in high school?&lt;/p&gt;

&lt;p&gt;MA: I honestly don’t remember. I didn’t even use RStudio until two years ago. I think it was probably from other fantasy nerds?&lt;/p&gt;

&lt;p&gt;KO: Is there an underground R fantasy basketball culture?&lt;/p&gt;

&lt;p&gt;MA: Well R for fantasy football is legit. &lt;a href=&#34;http://fantasyfootballanalytics.net/&#34;&gt;Fantasy Football Analytics&lt;/a&gt; is all R modeling.&lt;/p&gt;

&lt;p&gt;KO: That’s awesome - so now, do you work with sports analytics? Or is that your personal project/passion?&lt;/p&gt;

&lt;p&gt;MA: A little bit of both, I worked for this startup called Stattleship (&lt;a href=&#34;https://twitter.com/stattleship&#34;&gt;@stattleship&lt;/a&gt;). Because I’ll get involved with anything if there’s a good pun involved… and so we were doing sports analytics work that kind of ended up shifting more in a marketing direction. I still do consulting with the head data scientist [&lt;a href=&#34;https://twitter.com/tanyacash21/&#34;&gt;Tanya Cashorali&lt;/a&gt;] for that [at &lt;a href=&#34;http://tcbanalytics.com/&#34;&gt;TCB Analytics&lt;/a&gt;]. Some of the analysis/consulting will be with companies who are doing either consumer products for sports or data journalism stuff around sports analytics.&lt;/p&gt;

&lt;p&gt;KO: How often do you use R now?&lt;/p&gt;

&lt;p&gt;MA: Oh, I use R like every day. I use it… I don’t use Word any more. [Laughter] Yeah so one of the things about basketball is that there are times of the year where there are games every day. So that’s been my morning workflow for a while - scraping basketball data.&lt;/p&gt;

&lt;p&gt;KO: So you get up every morning and scrape what’s new in Basketball?&lt;/p&gt;

&lt;p&gt;MA: Yeah! So I end up in RStudio bright and early (often late, as well).&lt;/p&gt;

&lt;p&gt;KO: So is that literally what the first half hour of your day looks like?&lt;/p&gt;

&lt;p&gt;MA: No, so incidentally that’s kind of how this Twitter thing got started. My dog has long preceded me on Twitter and the internet at large, he’s kind of an internet famous dog &lt;a href=&#34;https://twitter.com/batpigandme&#34;&gt;@batpigandme&lt;/a&gt;. There’s an application called &lt;a href=&#34;https://buffer.com/&#34;&gt;Buffer&lt;/a&gt; which allows you to schedule tweets and facebook page posts, which was most of Batpig’s traffic - &lt;a href=&#34;https://www.facebook.com/batpigandme&#34;&gt;facebook page&lt;/a&gt; visits from Japan. And so I had this morning routine (started in the winter when I had one of those light things you sit in front of for a certain number of minutes) where I would wake up and schedule batpig posts while I’m sitting there and read emails. And that ended up being a nice morning workflow thing.&lt;/p&gt;

&lt;p&gt;I went to a &lt;a href=&#34;http://www.dogooddata.com/&#34;&gt;Do Good Data&lt;/a&gt; conference, which is a &lt;a href=&#34;http://www.dataanalystsforsocialgood.com/&#34;&gt;Data Analysts for Social Good&lt;/a&gt; (&lt;a href=&#34;https://twitter.com/DA4SG&#34;&gt;@DA4SG&lt;/a&gt;) event, just over two years ago, and everyone there was giving out their twitter handles, and I was like, oh - maybe people who aren’t dogs also use Twitter? [Laughter] So that was how I ended up creating my own account &lt;a href=&#34;https://twitter.com/dataandme&#34;&gt;@dataandme&lt;/a&gt; independent from Batpig.&lt;/p&gt;

&lt;p&gt;KO: What happened after you went to this conference? Was it awesome, did it inspire you?&lt;/p&gt;

&lt;p&gt;MA: Yeah so, I was the stats person at the company I was working at. And I didn’t realize there was all this really awesome work being done with really rigorous evaluation that wasn’t necessarily federal grant proposal stuff.  So I was really inspired by that and started learning more about what other people were doing, some of it in R, some of it not. I kept in touch with some of the people from that conference. And then NBA Twitter is also a thing it turns out, and NBA, R/Statistics is also a really big thing so that was kind of what pulled me in. And it was really fun. A lot of interesting projects and people that I work with were all through that [Twitter] which still surprises me - that I can read a book and tell the author something and they care? It’s weird.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I like to make arbitrary rules for myself, one of the things is I don’t tweet stuff that I haven’t read.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Everyone loves your twitter account. How do you find and curate the things you end up posting about?&lt;/p&gt;

&lt;p&gt;MA: I like to make arbitrary rules for myself, one of the things is I don’t tweet stuff that I haven’t read. I like to learn new things and/or I have to learn new things every day so I basically started scheduling [tweets] as a way to make myself read the things that I want to read and get back to.&lt;/p&gt;

&lt;p&gt;KO: Wait, so you schedule a tweet and then you’re like, okay well this is my deadline to read this thing - or I’ll be a liar.&lt;/p&gt;

&lt;p&gt;MA: Totally.&lt;/p&gt;

&lt;p&gt;KO: Whoa that’s awesome.&lt;/p&gt;

&lt;p&gt;MA: I’ve also never not finished a book in my life. It’s one of my rules, I’m really strict about it.&lt;/p&gt;

&lt;p&gt;KO: That’s a lot of pressure!&lt;/p&gt;

&lt;p&gt;MA: So that was kind of how it started out - especially because I didn’t even know all the stuff I didn’t know. Then, as I’ve used R more and more, there’s stuff that I’ve just happened to read because I don’t know what I’m doing.&lt;/p&gt;

&lt;p&gt;KO: The more you learn the more you can learn.&lt;/p&gt;

&lt;p&gt;MA: Yeah so now a lot of the stuff [tweets] is stuff I end up reading over the course of the day and then add it [to the queue]. Or it’s just stuff I’ve already read when I feel like being lazy.&lt;/p&gt;

&lt;p&gt;KO: Do you have side projects other than the basketball/sports stuff?&lt;/p&gt;

&lt;p&gt;MA: I actually majored in science and technology studies, which means I was randomly trained in ethical/legal/social implications of science. So I’m working on some data ethics projects which unfortunately I can’t talk about. And then my big side project for total amusement was this &lt;a href=&#34;https://archervisualization.herokuapp.com/&#34;&gt;D3.js in Action analysis of Archer&lt;/a&gt; which is a cartoon that I watch. But that’s also how I learned really how to use &lt;a href=&#34;https://github.com/juliasilge/tidytext&#34;&gt;tidytext&lt;/a&gt;. So then I ended up doing a technical review for David [Robinson] and Julia&amp;rsquo;s [Silge] book &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;Text Mining with R: A Tidy Approach&lt;/a&gt;. It was super fun. So yeah, I always have a bunch of random side projects going on.&lt;/p&gt;

&lt;p&gt;KO: How is your work-life balance?&lt;/p&gt;

&lt;p&gt;MA: It’s funny because I like what I do. So I don’t always know where that starts and ends. And I’m really bad at capitalism. It never occurs to me that I should be paid for doing some things. Especially if it involves open data and open source - surely you can’t charge for that? But I read a lot of stuff that’s not R too. I think I’m getting sort of a balance, but I’m not sure.&lt;/p&gt;

&lt;p&gt;KO: Switching back to your job-job now. Are you on a team, are you remote, are you in an office, what are the logistics like?&lt;/p&gt;

&lt;p&gt;MA: Kind of all of the above. In my old job I was on a team but I was the only person doing anything data related. And I developed some really lazy habits from that - really ugly code and committing terrible stuff to git. But with this NBA project I end up working with a lot of different people (who are also basketball-stat nerds).&lt;/p&gt;

&lt;p&gt;KO: Do you work with people who are employed by the actual NBA teams, or just people who are really interested in the subject?&lt;/p&gt;

&lt;p&gt;MA: No, so there is an unfortunate attrition of people whom I work with when they get hired by teams - which is not unfortunate, it’s awesome, but then they can no longer do anything with us. So that’s collaborative work but I don’t work on a team anymore.&lt;/p&gt;

&lt;p&gt;KO: So you don’t have daily stand-ups or anything.&lt;/p&gt;

&lt;p&gt;MA: No, no. I could probably benefit from that, but my goal is never to be 100% remote. After I went to that first data conference, I felt like being around all these people who are so much smarter than I am, and know so much more than I do is intimidating, but I also learned so much. And I learned so many things I was doing, not wrong, but inefficiently. I still learn about 80 things I’m doing inefficiently every day.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My goal right now - stop holding on to all of my projects that are not as done as I want them to be, and will never be done.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Do you have set beginnings and endings to projects? How many projects are you juggling at a given time?&lt;/p&gt;

&lt;p&gt;MA: After doing federal grant proposals, it doesn’t feel like anything is a deadline compared to that. They don’t care if your house burned down if it’s not in at the right time. So nothing feels as hard and fast as that. There are certain things like the NBA that —&lt;/p&gt;

&lt;p&gt;KO: There are timely things.&lt;/p&gt;

&lt;p&gt;MA: Yeah, and then sometimes we’ll just set arbitrary deadlines, just to kind of get out of a cycle of trying to perfect it, which I fall deeply into. Yeah so that’s kind of a little bit of my goal right now - stop holding on to all of my projects that are not as done as I want them to be, and will never be done. With the first iteration of this Archer thing I literally spent three days trying to get this faceted bar chart thing to sort in multiple ways and was super frustrated and then I tweeted something about it and immediately David Robinson responded with precisely what I needed and would have never figured out. So I’m working on doing that more. And also because it’s so helpful to me when other people do that.&lt;/p&gt;

&lt;p&gt;KO: How did you get hooked up with Julia and David, just through Twitter?&lt;/p&gt;

&lt;p&gt;MA: Yeah! So Julia I’d met at Open Vis Conf, David I’d read his &lt;a href=&#34;http://varianceexplained.org/programming/bad-code/&#34;&gt;blog about a million lines of bad code&lt;/a&gt; - it was open on my iPad for like years because I loved it so much, and still do. And yeah so again as this super random twitter-human that I feel like I am, I do end up meeting and doing things with cool people who are super smart and do really cool things.&lt;/p&gt;

&lt;p&gt;KO: It’s impressive how much you post and not just that, but it’s really evident that you care. People can tell that this isn’t just someone who reposts a million things day.&lt;/p&gt;

&lt;p&gt;MA: I mean it’s totally selfish, don’t get me wrong. But I’m super glad that it’s helpful to other people too. It gives me so much anxiety to think that people might think I know how to do all the things that I post, which I don’t, that’s why I had to read them - but even when I read them, sometimes I don’t know. The R community is pretty awesome, at least the parts of it that I know; which is not universally true of any community of any group of scientists. R Twitter is super-super helpful. And that was evident really quickly, at least to me.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My plea to everyone who has a blog is to put their Twitter handle somewhere on it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: What are some of your favorite things on the internet? Blogs, Twitter Accounts, Podcasts…&lt;/p&gt;

&lt;p&gt;MA: I have never skipped one of &lt;a href=&#34;https://juliasilge.com/&#34;&gt;Julia Silge’s blog&lt;/a&gt; posts. Her posts are always something that I know I should learn how to do. Both she and D-Rob [David Robinson] know their stuff and they write really well. So those are two blogs and follows that I love. &lt;a href=&#34;https://rud.is/b/&#34;&gt;Bob Rudis&lt;/a&gt; - almost daily, I can’t believe how quickly he churns stuff out. R-Bloggers is a great way to discover new stuff. Dr. Simon J [&lt;a href=&#34;https://drsimonj.svbtle.com/&#34;&gt;Simon Jackson&lt;/a&gt;] - I literally think of people by their twitter handles [&lt;a href=&#34;https://twitter.com/drsimonj&#34;&gt;@drsimonj&lt;/a&gt;], and there are so many others.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/img/blog-images/2017-11-10-rprofile-mara-averick/put-a-bird-on-it.png&#34; alt=&#34;PUT A BIRD ON IT!&#34; align=&#34;right&#34; style=&#34;margin: 0px 20px&#34;, width=&#34;350px&#34;&gt;&lt;/p&gt;

&lt;p&gt;Every day I’m amazed by all the stuff I didn’t know existed. And also there’s stuff that people wrote three or four years ago. A lot of the data vis stuff I end up finding from weird angles. So those are some of my favorites - I’m sure there are more. Oh! Thomas Lin Pedersen, &lt;a href=&#34;http://www.data-imaginist.com&#34;&gt;Data Imaginist is his blog&lt;/a&gt;. There are so many good blogs. My plea to everyone who has a blog is to put their twitter handle somewhere on it. I actually try really hard to find attribution stuff. Every now and then I get it really wrong and it’ll be someone who has nothing to do with it but who has the same name. There’s a bikini model who has the same name as someone who I said wrote a thing - which I vetted it too! I was like, well she’s multi-faceted, good for her! And then somebody was like, I don’t think that’s the right one. Oops! I have to say that that’s the one thing that Medium nailed - when you click share it gives you their twitter handle. If you have a blog, put your twitter handle there so I don’t end up attributing it to a bikini model.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Building Communities Together at ozunconf, 2017</title>
      <link>https://ropensci.org/blog/2017/10/31/ozunconf2017/</link>
      <pubDate>Tue, 31 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/31/ozunconf2017/</guid>
      <description>
        
        

&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-hex-cookies.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Just last week we organised the 2nd rOpenSci &lt;a href=&#34;http://ozunconf17.ropensci.org&#34;&gt;ozunconference&lt;/a&gt;, the sibling rOpenSci unconference, held in Australia. Last year it was &lt;a href=&#34;http://auunconf.ropensci.org&#34;&gt;held in Brisbane&lt;/a&gt;, this time around, the ozunconf was hosted in Melbourne, from October 26-27, 2017.&lt;/p&gt;

&lt;p&gt;At the ozunconf, we brought together 45 R-software users and developers, scientists, and open data enthusiasts from academia, industry, government, and non-profits. Participants travelled from far and wide, with people coming from 6 cities around Australia, 2 cities in New Zealand, and one city in the USA. Before the ozunconf we discussed and dreamt up projects to work on for a few days, then met up and brought about a bakers dozen of them into reality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-womens-data-discuss.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;upskilling-participants&#34;&gt;Upskilling participants&lt;/h3&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Day 0.5 of &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; and an excellent intro to writing packages and using Git from &lt;a href=&#34;https://twitter.com/rdpeng?ref_src=twsrc%5Etfw&#34;&gt;@rdpeng&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/nj_tierney?ref_src=twsrc%5Etfw&#34;&gt;@nj_tierney&lt;/a&gt;. I enjoyed the post-it note system! &lt;a href=&#34;https://t.co/iKTbWkeCA0&#34;&gt;pic.twitter.com/iKTbWkeCA0&lt;/a&gt;&lt;/p&gt;&amp;mdash; Holly Kirk (@HollyKirk) &lt;a href=&#34;https://twitter.com/HollyKirk/status/923065587915415552?ref_src=twsrc%5Etfw&#34;&gt;October 25, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;On Day 0, one day before the ozunconf, &lt;a href=&#34;https://twitter.com/rdpeng&#34;&gt;Roger Peng&lt;/a&gt; and I ran a half day training session on how to develop R packages and share them on GitHub. The participants picked things up really quickly, and by the end of the session, everyone could make an R package, and push it to GitHub. We also introduced them to &lt;a href=&#34;https://www.youtube.com/watch?v=s3JldKoA0zw&amp;amp;feature=youtu.be&#34;&gt;the wonders of RMarkdown&lt;/a&gt;. The event then kicked on to the &lt;a href=&#34;https://www.meetup.com/R-Ladies-Melbourne/events/244102535/&#34;&gt;R-Ladies Melbourne special one year anniversary event&lt;/a&gt;, which featured a great talk and introduction to Random Forests by Elisabeth Vogel.&lt;/p&gt;

&lt;h3 id=&#34;bringing-people-together&#34;&gt;Bringing people together&lt;/h3&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Biscuit decorating at &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; ! &lt;a href=&#34;https://t.co/M8PxOyRUJI&#34;&gt;pic.twitter.com/M8PxOyRUJI&lt;/a&gt;&lt;/p&gt;&amp;mdash; Nikeisha Caruana (@bluebirdi) &lt;a href=&#34;https://twitter.com/bluebirdi/status/923305923208036352?ref_src=twsrc%5Etfw&#34;&gt;October 25, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;Before the ozunconf, we discussed various ideas for projects in &lt;a href=&#34;https://github.com/ropensci/ozunconf17/issues&#34;&gt;the GitHub issues&lt;/a&gt;. Things really started to pick up in the last week and we ended up at 41 issues - almost as many issues as participants.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-oz-data-discuss.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Day one kicked off with decorating some hex cookies, baked by &lt;a href=&#34;https://twitter.com/visnut&#34;&gt;Di Cook&lt;/a&gt;. This uncovered a fun fact that &lt;a href=&#34;http://stefanbache.dk/&#34;&gt;Stefan Milton Bache&lt;/a&gt; - creator of the beloved pipe operator (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) from the &lt;a href=&#34;https://github.com/tidyverse/magrittr&#34;&gt;&lt;code&gt;magrittr&lt;/code&gt;&lt;/a&gt; package, apparently also created the first #rstats hex sticker.&lt;/p&gt;

&lt;p&gt;We then stuck the various projects that had been discussed throughout the week around the room and participants sticker voted on projects that they were interested in working on. Introductions were made, and quotes like these (from &lt;a href=&#34;https://twitter.com/stephdesilva&#34;&gt;Steph de Silva&lt;/a&gt;) led to entertaining discussions around data:&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;quot;I&amp;#39;m 50% data hazmat, 50% data grief counselling&amp;quot; best Intro ever at &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt;&lt;/p&gt;&amp;mdash; Steve Bennett (@stevage1) &lt;a href=&#34;https://twitter.com/stevage1/status/923314625428336641?ref_src=twsrc%5Etfw&#34;&gt;October 25, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;We were really lucky to be in the beautiful Monash City Campus, a place that almost seems to have been designed for an unconf, with some classroom style space, as well as plenty of nooks and crannies to sit in, including an outdoor astroturfed garden complete with bean bags and native flora.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-earo.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The Oz colour palette gang soakin it up outside &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; &lt;a href=&#34;https://t.co/XiLkhwZwTv&#34;&gt;pic.twitter.com/XiLkhwZwTv&lt;/a&gt;&lt;/p&gt;&amp;mdash; Miles McBain (@MilesMcBain) &lt;a href=&#34;https://twitter.com/MilesMcBain/status/923682409400250368?ref_src=twsrc%5Etfw&#34;&gt;October 26, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;The venue even seemed to reflect our love of hex stickers, providing a nice hex sticker themed carpet:&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Appropriately shaped carpet pattern for the &lt;a href=&#34;https://twitter.com/hashtag/ozunconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ozunconf&lt;/a&gt; venue &lt;a href=&#34;https://twitter.com/hashtag/hex?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#hex&lt;/a&gt; &lt;a href=&#34;https://t.co/QsbVWhCQyb&#34;&gt;pic.twitter.com/QsbVWhCQyb&lt;/a&gt;&lt;/p&gt;&amp;mdash; Holly Kirk (@HollyKirk) &lt;a href=&#34;https://twitter.com/HollyKirk/status/923420699900997632?ref_src=twsrc%5Etfw&#34;&gt;October 26, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;We had some great sponsors for this event, including &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt;, &lt;a href=&#34;http://rstudio.com/&#34;&gt;RStudio&lt;/a&gt; &lt;a href=&#34;http://r-consortium.org/&#34;&gt;The RConsortium&lt;/a&gt;, &lt;a href=&#34;https://inghaminstitute.org.au/&#34;&gt;The Ingham Institute&lt;/a&gt;, and &lt;a href=&#34;http://www.monash.edu/business&#34;&gt;Monash Business School&lt;/a&gt;. The event was also organised by &lt;a href=&#34;https://twitter.com/nj_tierney&#34;&gt;myself&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/visnut&#34;&gt;Di Cook&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/robjhyndman&#34;&gt;Rob Hyndman&lt;/a&gt;, and also &lt;a href=&#34;https://twitter.com/milesmcbain&#34;&gt;Miles McBain&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-hex-mat.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We wrapped up at the end of day 2, giving each projects group three minutes to debrief on their projects, using the unconf style - only the README.md (mostly!). You can &lt;a href=&#34;https://ropenscilabs.github.io/ozunconf-projects/&#34;&gt;check out all the ozunconf projects here&lt;/a&gt;, thanks to a template from &lt;a href=&#34;http://seankross.com/&#34;&gt;Sean Kross&lt;/a&gt;. Soon we will publish a series of short posts covering some of these great fun projects.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a quick taster:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/realtime&#34;&gt;&lt;code&gt;realtime&lt;/code&gt;&lt;/a&gt;. Realtime streamingplots built on the p5.js library.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/ozrepro&#34;&gt;&lt;code&gt;stow&lt;/code&gt;&lt;/a&gt;. A simplified version control interface to git, from within R.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/icon&#34;&gt;&lt;code&gt;icon&lt;/code&gt;&lt;/a&gt;. Easily access and insert web icons into HTML and PDF documents.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/ropenscilabs/ochRe&#34;&gt;&lt;code&gt;ochRe&lt;/code&gt;&lt;/a&gt;. Provide Australia-themed Colour Palettes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;ll share a quick summary of all of the projects over the coming weeks.&lt;/p&gt;

&lt;h3 id=&#34;what-s-your-story&#34;&gt;What&amp;rsquo;s Your Story?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/StephdeSilva/status/923875737102200832&#34;&gt;Some key #ozunconf lessons from Steph de Silva&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The opportunity to try but not succeed is a luxury to be savoured&lt;/li&gt;
&lt;li&gt;Mistakes make you a better programmer&lt;/li&gt;
&lt;li&gt;The best thing about R isn&amp;rsquo;t the language, it&amp;rsquo;s the number of people around who want to help&lt;/li&gt;
&lt;li&gt;Your skills are valuable, so your productivity is too. Investing in the tools that maximise it is worthwhile.&lt;/li&gt;
&lt;li&gt;git really is out to get you.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A few people have already written about their unconf17 experience. Have you? Share the link in the comments below and we&amp;rsquo;ll add it here.&lt;/p&gt;

&lt;h4 id=&#34;projects-posts&#34;&gt;Projects posts&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.healthintersections.com.au/?p=2740&#34;&gt;FHIR and R: &lt;code&gt;ronfhir&lt;/code&gt;&lt;/a&gt;, by &lt;a href=&#34;http://www.healthintersections.com.au/&#34;&gt;Grahame Grieve&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/blog/2017/11/14/realtime/&#34;&gt;2017 rOpenSci ozunconf :: Reflections and the &lt;code&gt;realtime&lt;/code&gt; Package&lt;/a&gt;, by &lt;a href=&#34;https://jcarroll.com.au/&#34;&gt;Jonathan Carroll&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropensci.org/blog/2017/11/21/ochre/&#34;&gt;&lt;code&gt;ochRe&lt;/code&gt; - Australia themed colour palettes&lt;/a&gt;, by &lt;a href=&#34;https://twitter.com/HollyKirk&#34;&gt;Holly Kirk&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/visnut&#34;&gt;Di Cook&lt;/a&gt;, &lt;a href=&#34;https://github.com/alicia-a&#34;&gt;Alicia Allan&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/ross_gayler&#34;&gt;Ross Gayler&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/rdpeng&#34;&gt;Roger Peng&lt;/a&gt;, &lt;a href=&#34;https://github.com/ellesaber&#34;&gt;Elle Saber&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;community-posts&#34;&gt;Community posts&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://simplystatistics.org/2017/10/30/how-do-you-convince-others-to-use-r/&#34;&gt;How do you convince others to use R?&lt;/a&gt;, by &lt;a href=&#34;http://www.biostat.jhsph.edu/~rpeng/&#34;&gt;Roger Peng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rex-analytics.com/failure-is-an-option/&#34;&gt;Failure Is An Option&lt;/a&gt;, by &lt;a href=&#34;https://twitter.com/StephdeSilva&#34;&gt;Steph da Silva&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;introducing-people-to-the-ropensci-community&#34;&gt;Introducing people to the rOpenSci community&lt;/h3&gt;

&lt;p&gt;rOpenSci has had a profound impact on me and my work. At the end of 2015 I got in touch with them to discuss arranging an unconference in Australia, and they welcomed me and my friends. Today, I am proud to be welcoming those from the ozunconf to this big, kind, wonderful community, and say, as Shannon Ellis summed up: &lt;a href=&#34;https://ropensci.org/blog/2017/06/23/community/&#34;&gt;&amp;ldquo;Hey! You there! You are welcome here&amp;rdquo;&lt;/a&gt;. It was also really great to have a diverse group of participants at the ozunconf, and in particular, that 40% of participants were women or other underrepresented genders.&lt;/p&gt;

&lt;h3 id=&#34;starts-not-ends&#34;&gt;Starts, not ends&lt;/h3&gt;

&lt;p&gt;One thing that I&amp;rsquo;ve realised in my involvement with organising and attending these events is that when the unconf ends, it feels a bit sad, sure, to say goodbye to the environment, the community, the friends, and the projects. At the last unconf in LA, we were sending out a stream of tweets, &amp;ldquo;&lt;a href=&#34;https://twitter.com/nj_tierney/status/868572134548713472&#34;&gt;it&amp;rsquo;s not&lt;/a&gt; &lt;a href=&#34;https://twitter.com/MilesMcBain/status/868590677843599360&#34;&gt;over&lt;/a&gt; &lt;a href=&#34;https://twitter.com/AmeliaMN/status/868605633435533312&#34;&gt;until&lt;/a&gt; &lt;a href=&#34;https://twitter.com/MilesMcBain/status/869044724086185985&#34;&gt;it&amp;rsquo;s&lt;/a&gt; &lt;a href=&#34;https://twitter.com/dataandme/status/869664700606406656&#34;&gt;over&lt;/a&gt;&amp;rdquo;. But, in reflection, standing back, taking it all in, the unconference doesn&amp;rsquo;t really end - it just starts. It starts many new things - projects, ideas, collaborations, and friendships.&lt;/p&gt;

&lt;p&gt;The ozunconf comes to an end. Now, let&amp;rsquo;s get started.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://njtierney.updog.co/img/ozunconf-group-photo.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Data from Public Bicycle Hire Systems</title>
      <link>https://ropensci.org/blog/2017/10/17/bikedata/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/17/bikedata/</guid>
      <description>
        
        

&lt;p&gt;A new rOpenSci package provides access to data to which users may already have directly contributed, and for which contribution is fun, keeps you fit, and &lt;a href=&#34;http://www.bmj.com/content/357/bmj.j1456&#34;&gt;helps make the world a better place&lt;/a&gt;. The data come from using public bicycle hire schemes, and the package is called &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt;. Public bicycle hire systems operate in many cities throughout the world, and most systems collect (generally anonymous) data, minimally consisting of the times and locations at which every single bicycle trip starts and ends. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package provides access to data from all cities which openly publish these data, currently including &lt;a href=&#34;https://tfl.gov.uk/modes/cycling/santander-cycles&#34;&gt;London, U.K.&lt;/a&gt;, and in the U.S.A., &lt;a href=&#34;https://www.citibikenyc.com&#34;&gt;New York&lt;/a&gt;, &lt;a href=&#34;https://bikeshare.metro.net&#34;&gt;Los Angeles&lt;/a&gt;, &lt;a href=&#34;https://www.rideindego.com&#34;&gt;Philadelphia&lt;/a&gt;, &lt;a href=&#34;https://www.divvybikes.com&#34;&gt;Chicago&lt;/a&gt;, &lt;a href=&#34;https://www.thehubway.com&#34;&gt;Boston&lt;/a&gt;, and &lt;a href=&#34;https://www.capitalbikeshare.com&#34;&gt;Washington DC&lt;/a&gt;. The package will expand as more cities openly publish their data (with the newly enormously expanded San Francisco system &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/2&#34;&gt;next on the list&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;why-bikedata&#34;&gt;Why bikedata?&lt;/h3&gt;

&lt;p&gt;The short answer to that question is that the package provides access to what is arguably one of the most spatially and temporally detailed databases of finely-scaled human movement throughout several of the world&amp;rsquo;s most important cities. Such data are likely to prove invaluable in the increasingly active and well-funded attempt to develop a science of cities. Such a science does not yet exist in any way comparable to most other well-established scientific disciplines, but the importance of developing a science of cities is indisputable, and reflected in such enterprises as the NYU-based &lt;a href=&#34;http://cusp.nyu.edu&#34;&gt;Center for Urban Science and Progress&lt;/a&gt;, or the UCL-based &lt;a href=&#34;https://www.ucl.ac.uk/bartlett/casa/&#34;&gt;Centre for Advanced Spatial Analysis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;People move through cities, yet at present anyone faced with the seemingly fundamental question of how, when, and where people do so would likely have to draw on some form of private data (typically operators of transport systems or mobile phone providers). There are very few open, public data providing insight into this question. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package aims to be one contribution towards filling this gap. The data accessed by the package are entirely open, and are constantly updated, typically on a monthly basis. The package thus provides ongoing insight into the dynamic changes and reconfigurations of these cities. Data currently available via the package amounts to several tens of Gigabytes, and will expand rapidly both with time, and with the inclusion of more cities.&lt;/p&gt;

&lt;h3 id=&#34;why-are-these-data-published&#34;&gt;Why are these data published?&lt;/h3&gt;

&lt;p&gt;In answer to that question, all credit must rightfully go to &lt;a href=&#34;http://www.theregister.co.uk/2011/01/11/transport_for_london_foi/&#34;&gt;Adrian Short&lt;/a&gt;, who submitted a Freedom of Information request in 2011 to Transport for London for usage statistics from the relatively new, and largely publicly-funded, bicycle scheme. This request from one individual ultimately resulted in the data being openly published on an ongoing basis. All U.S. systems included in &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; commenced operation subsequent to that point in time, and many of them have openly published their data from the very beginning. The majority of the world&amp;rsquo;s public bicycle hire systems (&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_bicycle-sharing_systems&#34;&gt;see list here&lt;/a&gt;) nevertheless do not openly publish data, notably including very large systems in China, France, and Spain. One important aspiration of the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package is to demonstrate the positive benefit for the cities themselves of openly and easily facilitating complex analyses of usage data, which brings us to &amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;what-s-important-about-these-data&#34;&gt;What&amp;rsquo;s important about these data?&lt;/h3&gt;

&lt;p&gt;As mentioned, the data really do provide uniquely valuable insights into the movement patterns and behaviour of people within some of the world&amp;rsquo;s major cities. While the more detailed explorations below demonstrate the kinds of things that can be done with the package, the variety of insights these data facilitate is best demonstrated through considering the work of other people, exemplified by &lt;a href=&#34;http://toddwschneider.com/posts/a-tale-of-twenty-two-million-citi-bikes-analyzing-the-nyc-bike-share-system/&#34;&gt;Todd Schneider&amp;rsquo;s high-profile blog piece&lt;/a&gt; on the New York City system. Todd&amp;rsquo;s analyses clearly demonstrate how these data can provide insight into where and when people move, into inter-relationships between various forms of transport, and into relationships with broader environmental factors such as weather. As cities evolve, and public bicycle hire schemes along with them, data from these systems can play a vital role in informing and guiding the ongoing processes of urban development. The &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package greatly facilitates analysing such processes, not only through making data access and aggregation enormously easier, but through enabling analyses from any one system to be immediately applied to, and compared with, any other systems.&lt;/p&gt;

&lt;h3 id=&#34;how-it-works&#34;&gt;How it works&lt;/h3&gt;

&lt;p&gt;The package currently focusses on the data alone, and provides functionality for downloading, storage, and aggregation. The data are stored in an &lt;code&gt;SQLite3&lt;/code&gt; database, enabling newly published data to be continually added, generally with one simple line of code. It&amp;rsquo;s as easy as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;store_bikedata (city = &amp;quot;chicago&amp;quot;, bikedb = &amp;quot;bikedb&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the nominated database (&lt;code&gt;bikedb&lt;/code&gt;) already holds data for Chicago, only new data will be added, otherwise all historical data will be downloaded and added. All bicycle hire systems accessed by &lt;code&gt;bikedata&lt;/code&gt; have fixed docking stations, and the primary means of aggregation is in terms of &amp;ldquo;trip matrices&amp;rdquo;, which are square matrices of numbers of trips between all pairs of stations, extracted with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chi&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that most parameters are highly flexible in terms of formatting, so pretty much anything starting with &lt;code&gt;&amp;quot;ch&amp;quot;&lt;/code&gt; will be recognised as Chicago. Of course, if the database only contains data for Chicago, the &lt;code&gt;city&lt;/code&gt; parameter may be omitted entirely. Trip matrices may be filtered by time, through combinations of year, month, day, hour, minute, or even second, as well as by demographic characteristics such as gender or date of birth for those systems which provide such data. (These latter data are freely provided by users of the systems, and there can be no guarantee of their accuracy.) These can all be combined in calls like the following, which further demonstrates the highly flexible ways of specifying the various parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trips &amp;lt;- bike_tripmat (&amp;quot;bikedb&amp;quot;, city = &amp;quot;london, innit&amp;quot;,
                       start_date = 20160101, end_date = &amp;quot;16,02,28&amp;quot;,
                       start_time = 6, end_time = 24,
                       birth_year = 1980:1990, gender = &amp;quot;f&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The second mode of aggregation is as daily time series, via the &lt;code&gt;bike_daily_trips()&lt;/code&gt; function. See &lt;a href=&#34;https://ropensci.github.io/bikedata/articles/bikedata.html&#34;&gt;the vignette&lt;/a&gt; for further details.&lt;/p&gt;

&lt;h3 id=&#34;what-can-be-done-with-these-data&#34;&gt;What can be done with these data?&lt;/h3&gt;

&lt;p&gt;Lots of things. How about examining how far people ride. This requires getting the distances between all pairs of docking stations as routed through the street network, to yield a distance matrix corresponding to the trip matrix. The latest version of &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; has a brand new function to perform exactly that task, so it&amp;rsquo;s as easy as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;ropensci/bikedata&amp;quot;) # to install latest version
dists &amp;lt;- bike_distmat (bikedb = bikedb, city = &amp;quot;chicago&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are distances as routed through the underlying street network, with street types prioritised for bicycle travel. The network is extracted from OpenStreetMap using the &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;rOpenSci &lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;, and the distances are calculated using a brand new package called &lt;a href=&#34;https://cran.r-project.org/package=dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; (Distances on Directed Graphs). (Disclaimer: It&amp;rsquo;s my package, and this is a shameless plug for it - please use it!)&lt;/p&gt;

&lt;p&gt;The distance matrix extracted with &lt;code&gt;bike_distmat&lt;/code&gt; is between all stations listed for a given system, which &lt;code&gt;bike_tripmat&lt;/code&gt; will return trip matrices only between those stations in operation over a specified time period. Because systems expand over time, the two matrices will generally not be directly comparable, so it is necessary to submit both to the &lt;code&gt;bikedata&lt;/code&gt; function &lt;code&gt;match_matrices()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 636 636
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mats &amp;lt;- match_matrices (trips, dists)
trips &amp;lt;- mats$trip
dists &amp;lt;- mats$dist
dim (trips); dim (dists)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 581 581

## [1] 581 581
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;identical (rownames (trips), rownames (dists))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Distances can then be visually related to trip numbers to reveal their distributional form. These matrices contain too many values to plot directly, so the &lt;code&gt;hexbin&lt;/code&gt; package is used here to aggregate in a &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library (hexbin)
library (ggplot2)
dat &amp;lt;- data.frame (distance = as.vector (dmat),
                   number = as.vector (trips))
ggplot (dat, aes (x = distance, y = number)) +
    stat_binhex(aes(fill = log (..count..))) +
    scale_x_log10 (breaks = c (0.1, 0.5, 1, 2, 5, 10, 20),
                   labels = c (&amp;quot;0.1&amp;quot;, &amp;quot;0.5&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;20&amp;quot;)) +
    scale_y_log10 (breaks = c (10, 100, 1000)) +
    scale_fill_gradientn(colours = c(&amp;quot;seagreen&amp;quot;,&amp;quot;goldenrod1&amp;quot;),
                         name = &amp;quot;Frequency&amp;quot;, na.value = NA) +
    guides (fill = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/chicago.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The central region of the graph (yellow hexagons) reveals that numbers of trips generally decrease roughly exponentially with increasing distance (noting that scales are logarithmic), with most trip distances lying below 5km. What is the &amp;ldquo;average&amp;rdquo; distance travelled in Chicago? The easiest way to calculate this is as a weighted mean,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum (as.vector (dmat) * as.vector (trips) / sum (trips), na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 2.510285
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;giving a value of just over 2.5 kilometres. We could also compare differences in mean distances between cyclists who are registered with a system and causal users. These two categories may be loosely considered to reflect &amp;ldquo;residents&amp;rdquo; and &amp;ldquo;non-residents&amp;rdquo;. Let&amp;rsquo;s wrap this in a function so we can use it for even cooler stuff in a moment.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean &amp;lt;- function (bikedb = &amp;quot;bikedb&amp;quot;, city = &amp;quot;chicago&amp;quot;)
{
    tm &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
    tm_memb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = TRUE)
    tm_nomemb &amp;lt;- bike_tripmat (bikedb = bikedb, city = city, member = FALSE)
    stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
    dists &amp;lt;- bike_distmat (bikedb = bikedb, city = city)
    mats &amp;lt;- match_mats (dists, tm_memb)
    tm_memb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm_nomemb)
    tm_nomemb &amp;lt;- mats$trip
    mats &amp;lt;- match_mats (dists, tm)
    tm &amp;lt;- mats$trip
    dists &amp;lt;- mats$dists

    d0 &amp;lt;- sum (as.vector (dists) * as.vector (tm) / sum (tm), na.rm = TRUE)
    dmemb &amp;lt;- sum (as.vector (dists) * as.vector (tmemb) / sum (t_memb), na.rm = TRUE)
    dnomemb &amp;lt;- sum (as.vector (dists) * as.vector (tm_nomemb) / sum (tm_nomemb), na.rm = TRUE)
    res &amp;lt;- c (d0, dmemb / dnomemb)
    names (res) &amp;lt;- c (&amp;quot;dmean&amp;quot;, &amp;quot;ratio_memb_non&amp;quot;)
    return (res)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Differences in distances ridden between &amp;ldquo;resident&amp;rdquo; and &amp;ldquo;non-resident&amp;rdquo; cyclists can then be calculated with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dmean (bikedb = bikedb, city = &amp;quot;ch&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##          dmean ratio_memb_non
##       2.510698       1.023225
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And system members cycle slightly longer distances than non-members. (Do not at this point ask about statistical tests - these comparisons are made between millions&amp;ndash;often tens of millions&amp;ndash;of points, and statistical significance may always be assumed to be negligibly small.) Whatever the reason for this difference between &amp;ldquo;residents&amp;rdquo; and others, we can use this exact same code to compare equivalent distances for all cities which record whether users are members or not (which is all cities except London and Washington DC).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cities &amp;lt;- c (&amp;quot;ny&amp;quot;, &amp;quot;ch&amp;quot;, &amp;quot;bo&amp;quot;, &amp;quot;la&amp;quot;, &amp;quot;ph&amp;quot;) # NYC, Chicago, Boston, LA, Philadelphia
sapply (cities, function (i) dmean (bikedb = bikedb, city = i))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                       ny       ch       bo       la       ph
## dmean          2.8519131 2.510285 2.153918 2.156919 1.702372
## ratio_memb_non 0.9833729 1.023385 1.000635 1.360099 1.130929
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we thus discover that Boston manifests the greatest equality in terms of distances cycled between residents and non-residents, while LA manifests the greatest difference. New York City is the only one of these five in which non-members of the system actually cycle further than members. (And note that these two measures can&amp;rsquo;t be statistically compared in any direct way, because mean distances are also affected by relative numbers of member to non-member trips.) These results likely reflect a host of (scientifically) interesting cultural and geo-spatial differences between these cities, and demonstrate how the &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; package (combined with &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt;&lt;/a&gt;) can provide unique insight into differences in human behaviour between some of the most important cities in the U.S.&lt;/p&gt;

&lt;h3 id=&#34;visualisation&#34;&gt;Visualisation&lt;/h3&gt;

&lt;p&gt;Many users are likely to want to visualise how people use a given bicycle system, and in particular are likely to want to produce maps. This is also readily done with the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt;, which can route and aggregate transit flows for a particular mode of transport throughout a street network. Let&amp;rsquo;s plot bicycle flows for the Indego System of Philadelphia PA. First get the trip matrix, along with the coordinates of all bicycle stations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github (&amp;quot;gmost/dodgr&amp;quot;) # to install latest version
city &amp;lt;- &amp;quot;ph&amp;quot;
# store_bikedata (bikedb = bikedb, city = city) # if not already done
trips &amp;lt;- bike_tripmat (bikedb = bikedb, city = city)
stns &amp;lt;- bike_stations (bikedb = bikedb, city = city)
xy &amp;lt;- stns [, which (names (stns) %in% c (&amp;quot;longitude&amp;quot;, &amp;quot;latitude&amp;quot;))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows of cyclists are calculated between those &lt;code&gt;xy&lt;/code&gt;points, so the &lt;code&gt;trips&lt;/code&gt; table has to match the &lt;code&gt;stns&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;indx &amp;lt;- match (stns$stn_id, rownames (trips))
trips &amp;lt;- trips [indx, indx]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; can be used to extract the underlying street network surrounding those &lt;code&gt;xy&lt;/code&gt; points (expanded here by 50%):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;net &amp;lt;- dodgr_streetnet (pts = xy, expand = 0.5) %&amp;gt;%
    weight_streetnet (wt_profile = &amp;quot;bicycle&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then need to align the bicycle station coordinates in &lt;code&gt;xy&lt;/code&gt; to the nearest points (or &amp;ldquo;vertices&amp;rdquo;) in the street network:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;verts &amp;lt;- dodgr_vertices (net)
pts &amp;lt;- verts$id [match_pts_to_graph (verts, xy)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flows between these points can then be mapped onto the underlying street network with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;flow &amp;lt;- dodgr_flows (net, from = pts, to = pts, flow = trips) %&amp;gt;%
    merge_directed_flows ()
net &amp;lt;- net [flow$edge_id, ]
net$flow &amp;lt;- flow$flow
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; documentation&lt;/a&gt; for further details of how this works. We&amp;rsquo;re now ready to plot those flows, but before we do, let&amp;rsquo;s overlay them on top of the rivers of Philadelphia, extracted with rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmdata&#34;&gt;&lt;code&gt;osmdata&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;q &amp;lt;- opq (&amp;quot;Philadelphia pa&amp;quot;)
rivers1 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;waterway&amp;quot;, value = &amp;quot;river&amp;quot;, value_exact = FALSE) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers2 &amp;lt;- q %&amp;gt;%
    add_osm_feature (key = &amp;quot;natural&amp;quot;, value = &amp;quot;water&amp;quot;) %&amp;gt;%
    osmdata_sf (quiet = FALSE)
rivers &amp;lt;- c (rivers1, rivers2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally plot the map, using rOpenSci&amp;rsquo;s &lt;a href=&#34;https://github.com/ropensci/osmplotr&#34;&gt;&lt;code&gt;osmplotr&lt;/code&gt; package&lt;/a&gt; to prepare a base map with the underlying rivers, and the &lt;code&gt;ggplot2::geom_segment()&lt;/code&gt; function to add the line segments with colours and widths weighted by bicycle flows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#gtlibrary (osmplotr)
require (ggplot2)
bb &amp;lt;- get_bbox (c (-75.22, 39.91, -75.10, 39.98))
cols &amp;lt;- colorRampPalette (c (&amp;quot;lawngreen&amp;quot;, &amp;quot;red&amp;quot;)) (30)
map &amp;lt;- osm_basemap (bb, bg = &amp;quot;gray10&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_multipolygons, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_osm_objects (rivers$osm_lines, col = &amp;quot;gray20&amp;quot;) %&amp;gt;%
    add_colourbar (zlims = range (net$flow / 1000), col = cols)
map &amp;lt;- map + geom_segment (data = net, size = net$flow / 50000,
                           aes (x = from_lon, y = from_lat, xend = to_lon, yend = to_lat,
                                colour = flow, size = flow)) +
    scale_colour_gradient (low = &amp;quot;lawngreen&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;none&amp;quot;)
print_osm_map (map)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-17-bikedata/ph.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The colour bar on the right shows thousands of trips, with the map revealing the relatively enormous numbers crossing the South Street Bridge over the Schuylkill River, leaving most other flows coloured in the lower range of green or yellows. This map thus reveals that anyone wanting to see Philadelphia&amp;rsquo;s Indego bikes in action without braving the saddle themselves would be best advised to head straight for the South Street Bridge.&lt;/p&gt;

&lt;h3 id=&#34;future-plans&#34;&gt;Future plans&lt;/h3&gt;

&lt;p&gt;Although the &lt;a href=&#34;https://github.com/gmost/dodgr&#34;&gt;&lt;code&gt;dodgr&lt;/code&gt; package&lt;/a&gt; greatly facilitates the production of such maps, the code is nevertheless rather protracted, and it would probably be very useful to convert much of the code in the preceding section to an internal &lt;a href=&#34;https://github.com/ropensci/bikedata&#34;&gt;&lt;code&gt;bikedata&lt;/code&gt;&lt;/a&gt; function to map trips between pairs of stations onto corresponding flows through the underlying street networks.&lt;/p&gt;

&lt;p&gt;Beyond that point, and the list of currently open issues awaiting development on the &lt;a href=&#34;https://github.com/ropensci/bikedata/issues&#34;&gt;github repository&lt;/a&gt;, future development is likely to depend very much on how users use the package, and on what extra features people might want. How can you help? A great place to start might be the official &lt;a href=&#34;https://ropensci.org/blog/blog/2017/10/02/hacktoberfest&#34;&gt;Hacktoberfest issue&lt;/a&gt;, helping to import the next lot of data from &lt;a href=&#34;https://github.com/ropensci/bikedata/issues/34&#34;&gt;San Francisco&lt;/a&gt;. Or just use the package, and open up a new issue in response to any ideas that might pop up, no matter how minor they might seem. See the &lt;a href=&#34;https://github.com/ropensci/bikedata/blob/master/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for general advice.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Finally, this package wouldn&amp;rsquo;t be what it is without my co-author &lt;a href=&#34;https://github.com/richardellison&#34;&gt;Richard Ellison&lt;/a&gt;, who greatly accelerated development through encouraging C rather than C++ code for the SQL interfaces. &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maëlle Salmon&lt;/a&gt; majestically guided the entire review process, and made the transformation of the package to its current polished form a joy and a pleasure. I remain indebted to both &lt;a href=&#34;https://github.com/chucheria&#34;&gt;Bea Hernández&lt;/a&gt; and &lt;a href=&#34;https://github.com/eamcvey&#34;&gt;Elaine McVey&lt;/a&gt; for offering their time to extensively test and &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/116&#34;&gt;review the package&lt;/a&gt; as part of rOpenSci&amp;rsquo;s onboarding process. The review process has made the package what it is, and for that I am grateful to all involved!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>.rprofile: David Smith</title>
      <link>https://ropensci.org/blog/2017/10/13/rprofile-david-smith/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/13/rprofile-david-smith/</guid>
      <description>
        
        &lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-13-rprofile_david_smith/david-smith.jpg&#34; alt=&#34;David Smith, R Community Lead at Microsoft&#34; style=&#34;margin: 0px 20px; width: 250px;&#34; align=&#34;left&#34;&gt;
&lt;em&gt;David Smith is a Blogger and Community Lead at Microsoft. I had the chance to interview David last May at rOpenSci unconf17. We spoke about his career, the process of working remote within a team, community development/outreach and his personal methods for discovering great content to share and write about.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;KO: What is your name, job title, and how long have you been using R?&lt;/p&gt;

&lt;p&gt;DS: My name is David Smith. I work at Microsoft and my self-imposed title is ‘R Community Lead’. I’ve been working with R specifically for about 10 years, but I’d been working with S since the early 90s.&lt;/p&gt;

&lt;p&gt;KO: How did you transition into using R?&lt;/p&gt;

&lt;p&gt;DS: I was using S for a long, long time, and I worked for the company that commercialized S, where I was a project manager for S-PLUS. And I got out of that company, and then worked for a startup in a different industry for a couple of years. But while I was there, the people that founded Revolution Analytics approached me because they were setting up a company to build a commercial business around R, and reached out to me because of my connections with the S community.&lt;/p&gt;

&lt;p&gt;KO: So you came to Microsoft through Revolution?&lt;/p&gt;

&lt;p&gt;DS: That’s correct. I was with Revolution Analytics and then Microsoft bought that company, so I’ve been with Microsoft since then.&lt;/p&gt;

&lt;p&gt;KO: How has that transition gone, is there a Revolution team inside of Microsoft, or has it become more spread out?&lt;/p&gt;

&lt;p&gt;DS: It’s been more spread out. It got split up into the engineering people and the marketing people, sales people all got distributed around. When I first went to Microsoft I started off in the engineering group doing product management. But I was also doing the community role, and it just wasn’t a very good fit just time-wise, between doing community stuff and doing code or product management. So then I switched to a different group called the Ecosystem team, and so now I’m 100% focused on community within a group that’s focused on the ecosystem in general.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The one piece of advice I could give anyone starting out in their careers is - write what you do, write it in public, and make it so that other people can reproduce what you did.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: What does it mean to be 100% community focused, do you do a lot of training?&lt;/p&gt;

&lt;p&gt;DS: I don’t do a lot of training myself, but I work with a lot of other people on the team who do training. We’re focused mainly on building up the ecosystem of people that ultimately add value to the products that Microsoft has. And we’re specifically involved in the products that Microsoft has that now incorporate R by building up the value of the R ecosystem.&lt;/p&gt;

&lt;p&gt;KO: What does your day-to-day look like, are you in an office, do you work remote?&lt;/p&gt;

&lt;p&gt;DS: I work from home. I had moved from Seattle where Microsoft is headquartered to Chicago a couple of months before the acquisition happened, so I wasn’t about to move back to Seattle. But they let me work from home in Chicago, which has worked out great because most of my job is communicating with the community at large. So I do the &lt;a href=&#34;http://blog.revolutionanalytics.com/&#34;&gt;Revolutions Blog&lt;/a&gt;, which I’ve been writing for eight or nine years now, writing stories about people using R, and applications of R packages. All as a way of communicating to the wider world the cool stuff that people can do with R, and also to the R community occasionally, what kind of things you can do with R in the Microsoft environment.&lt;/p&gt;

&lt;p&gt;KO: Have you always been a writer or interested in writing and communications?&lt;/p&gt;

&lt;p&gt;DS: No, no. I have a mathematics and computer science degree. I’m not trained as a writer. But it’s actually been useful having the perspective of statistics and mathematics and programming, and to bring that to a broader audience through writing. I’ve learned a lot about the whole writing and blogging and journalism process through that, but I’m certainly not trained in that way.&lt;/p&gt;

&lt;p&gt;KO: How does your Ecosystems team at Microsoft function and collaborate?&lt;/p&gt;

&lt;p&gt;DS: Unlike many teams at Microsoft, our team is very distributed. We have people working remotely from Denver, I’m in Chicago, Seattle, we’re all kind of distributed all around the place. So we meet virtually through Skype, have video meetings once a week and communicate a lot online.&lt;/p&gt;

&lt;p&gt;KO: What kind of tools are you using?&lt;/p&gt;

&lt;p&gt;DS: Traditionally, as in Microsoft, mainly email and Skype for the meetings. I set up an internal team focused around community more broadly around Microsoft and we use Microsoft Teams for that,
which is a little bit like Slack. But a lot of the stuff that I do is more out in the open, so I use a lot of Twitter and Github for the code that I point to and stuff like that.&lt;/p&gt;

&lt;p&gt;KO: How do you manage your Twitter?&lt;/p&gt;

&lt;p&gt;DS: Twitter I do manually in real-time. I don’t do a lot of scheduling except for &lt;a href=&#34;https://twitter.com/rlangtip&#34;&gt;@RLangTip&lt;/a&gt;
which is a feed of daily R tips. And for that I do scheduling through Tweetdeck on the web.&lt;/p&gt;

&lt;p&gt;KO: How many Twitter accounts are you managing?&lt;/p&gt;

&lt;p&gt;DS: I run &lt;a href=&#34;https://twitter.com/revodavid&#34;&gt;@revodavid&lt;/a&gt; which is my personal twitter account, and &lt;a href=&#34;https://twitter.com/rlangtip&#34;&gt;@RLangTip&lt;/a&gt; which is R language tips. I tweet for &lt;a href=&#34;https://twitter.com/R_Forwards&#34;&gt;@R_Forwards&lt;/a&gt; which is the diversity community for R, &lt;a href=&#34;https://twitter.com/RConsortium&#34;&gt;@RConsortium&lt;/a&gt;, the R Consortium, so quite a few.&lt;/p&gt;

&lt;p&gt;KO: How long has this been a core part of your work day?&lt;/p&gt;

&lt;p&gt;DS: The community thing as a focus, maybe five or six years? My career path for a long time was in product management. So I managed S-PLUS as a product for a long time, I managed another product at a different startup, and then I came to Revolution and I did a combination of engineering and product management. But in the last 18 months I’ve been 100% in the community space.&lt;/p&gt;

&lt;p&gt;KO: How did you get into product management to begin with?&lt;/p&gt;

&lt;p&gt;DS: That’s a good question that I’m not sure I know the answer to. I started off my first job after university &amp;ndash; I actually left university specifically to become a support engineer for S-PLUS. When I took on that role, they didn’t really have product management yet at that company, and so when they were looking for somebody to basically steer S-PLUS as a product, it was a good fit for me and an opportunity to move to the States. I took that on and I kind of just learned product management as I did it. I went to a few sort of training/seminar type things, but I didn’t study it.&lt;/p&gt;

&lt;p&gt;KO: Sure. It seems like something that people just kind of get saddled with sometimes?&lt;/p&gt;

&lt;p&gt;DS: Exactly. It’s a discipline that doesn’t really have discipline. But for the various companies I’ve worked for, mostly startups, they all seem to have very different perspectives on what product management is and what the role of a product manager is.&lt;/p&gt;

&lt;p&gt;KO: Yeah, I know what you mean. Are you happy to have sort of moved away from that?&lt;/p&gt;

&lt;p&gt;DS: I am in the sense of &amp;ndash; it was different being in a startup where being a product manager was more like being the shepherd of an entire product ecosystem, whereas in a big company the product manager is a lot more focused and inherently so, a lot more narrow. I happen to prefer the bigger picture I guess.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Honestly, I kind of focus from the point of view of what interests me personally. Which doesn’t sound very community oriented at all… but it’s an exercise in empathy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: What’s your process for deciding what things you talk about and bring to the community?&lt;/p&gt;

&lt;p&gt;DS: Honestly, I kind of focus from the point of view of what interests me personally. Which doesn’t sound very community oriented at all… but it’s an exercise in empathy. If I can write about something, or find a topic that I might find is interesting or exciting and I can communicate that with other people, I’m motivated to write about it and I hope that people are then motivated to learn about it. Kind of the antithesis of this is when I worked in marketing for a while; a lot of that style of writing was the bane of my existence because you’re producing these documents that literally are designed for nobody to read, in this language that nobody engages with. I much prefer blogging and tweeting because it’s much more directly for people.&lt;/p&gt;

&lt;p&gt;KO: What have some of your most popular or successful engagements been about? Feel free to interpret ‘successful&amp;rsquo; in any way.&lt;/p&gt;

&lt;p&gt;DS: Well, from the point of view of what has been the most rewarding part of my job, is finding under-recognized or just these really cool things that people have done that just haven’t had a lot of exposure. And I’ve got a fairly big audience and a fairly wide reach, and it’s always fun for me to find things that people have done that maybe haven’t been seen. And it’s not my work, but I can &amp;ndash; you know &amp;ndash; take an eight page document that somebody’s written that has really cool things in it and just pull out various things. There is so much very cool stuff that people have done, half of the battle is getting it out there.&lt;/p&gt;

&lt;p&gt;KO: What are some of your favorite sources for discovering cool things on the internet?&lt;/p&gt;

&lt;p&gt;DS: There are channels on Reddit that I get a lot of material from, like &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful/&#34;&gt;/r/dataisbeautiful&lt;/a&gt; and things like that. It’s hard to say particular accounts on twitter, but I’ve spent a lot of time following people where I’ve read one of their blog posts and I find their twitter account, and they have just a few followers, I’ll follow them, and then over time it amounts to some good stuff. I have twitter open all day, every day. I don’t read everything on my feed every day, but I certainly keep it open.&lt;/p&gt;

&lt;p&gt;KO: How much of your day is just spent exploring?&lt;/p&gt;

&lt;p&gt;DS: A lot of it. I spend about half of any given day reading. It takes a long time, but every now and then you find this really cool stuff.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It’s one thing to be able to do really cool stuff in R or any other language, but until you can distill that down into something that other people consume, it’s going to be hard to sell yourself.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;KO: Do you have any last nuggets of wisdom for people starting out their careers in R?&lt;/p&gt;

&lt;p&gt;DS: For people starting out their careers, I think one of the most important skills to learn is that communication skill. It’s one thing to be able to do really cool stuff in R or any other language, but until you can distill that down into something that other people consume, it’s going to be hard to sell yourself. And it’s also going to be hard to be valuable. A lot of the people I’ve watched evolve in the community are people who have begun very early in their careers, blogging about what they do. The one piece of advice I could give anyone starting out in their careers is - write what you do, write it in public, and make it so that other people can reproduce what you did.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Governance, Engagement, and Resistance in the Open Science Movement: A Comparative Study</title>
      <link>https://ropensci.org/blog/2017/10/06/sholler-plan/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/06/sholler-plan/</guid>
      <description>
        
        

&lt;p&gt;A growing community of scientists from a variety of disciplines is moving the norms of scientific research toward open practices. Supporters of open science hope to increase the quality and efficiency of research by enabling the widespread sharing of datasets, research software source code, publications, and other processes and products of research. The speed at which the open science community seems to be growing mirrors the rapid development of technological capabilities, including robust open source scientific software, new services for data sharing and publication, and novel data science techniques for working with massive datasets. Organizations like rOpenSci harness such capabilities and deploy various combinations of these research tools, or what I refer to here as open science infrastructures, to facilitate open science.&lt;/p&gt;

&lt;p&gt;As studies of other digital infrastructures have pointed out, developing and deploying the technological capabilities that support innovative work within a community of practitioners constitutes just one part of making innovation happen. As quickly as the technical solutions to improving scientific research may be developing, a host of organizational and social issues are lagging behind and hampering the open science community’s ability to inscribe open practices in the culture of scientific research. Remedying organizational and social issues requires paying attention to open science infrastructures’ human components, such as designers, administrators, and users, as well as the policies, practices, and organizational structures that contribute to the smooth functioning of these systems.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; These elements of infrastructure development have, in the past, proven to be equal to or more important than technical capabilities in determining the trajectory the infrastructure takes (e.g., whether it “succeeds” or “fails”).&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;As a postdoc with rOpenSci, I have begun a qualitative, ethnographic project to explore the organizational and social processes involved in making open science the norm in two disciplines: astronomy and ecology. I focus on these two disciplines to narrow, isolate, and compare the set of contextual factors (e.g., disciplinary histories, research norms, and the like) that might influence perceptions of open science. Specifically, I aim to answer the following research questions (RQ):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;RQ1a: What are the primary motivations of scientists who actively engage with open science infrastructures?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RQ1b: What are the factors that influence resistance to open science among some scientists?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RQ2: What strategies do open science infrastructure leaders use to encourage participation, govern contributions, and overcome resistance to open science infrastructure use?&lt;/p&gt;

&lt;p&gt;a. To what extent do governance strategies balance standardization and flexibility, centralization and decentralization, and voluntary and mandatory contributions?&lt;/p&gt;

&lt;p&gt;b. By what mechanisms are open science policies and practices enforced?&lt;/p&gt;

&lt;p&gt;c. What are the commonalities and differences in the rationale behind choices of governance strategies?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below, I describe how I am systematically investigating these questions in two parts. In Part 1, I am identifying the issues raised by scientists who engage with or resist the open science movement. In Part 2, I am studying the governance strategies open science leaders and decision-makers use to elicit engagement with open science infrastructures in these disciplines.&lt;/p&gt;

&lt;h3 id=&#34;part-1-engagement-with-and-resistance-to-open-science&#34;&gt;Part 1: Engagement with and Resistance to Open Science&lt;/h3&gt;

&lt;p&gt;I am firmly rooted in a research tradition which emphasizes that studying the uptake of a new technology or technological approach, no matter the type of work or profession, begins with capturing how the people charged with changing their activities respond to the change “on the ground.” In this vein, Part 1 of the study aims to lend empirical support or opposition to arguments for and against open science that are commonly found in opinion pieces, on social media, and in organizational mission statements. A holistic reading of such documents reveals several commonalities in the positions for and against open science. Supporters of open science often cite increased transparency, reproducibility, and collaboration as the overwhelming benefits of making scientific research processes and products openly available. Detractors highlight concerns over “scooping,” ownership, and the time costs of curating and publishing code and data.&lt;/p&gt;

&lt;p&gt;I am seeking to verify and test these claims by interviewing and surveying astronomers and ecologists or, more broadly, earth and environmental scientists who fall on various parts of the open science engagement-to-resistance spectrum. I am conducting interviews using a semi-structured interview protocol&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; across all interviewees. I will then use a qualitative data analysis approach based on the grounded theory method&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; to extract themes from the responses, focusing on the factors that promote engagement (e.g., making data available, spending time developing research software, or making publications openly accessible) or resistance (e.g., unwillingness to share code used in a study or protecting access to research data). Similar questions will be asked at scale via a survey.&lt;/p&gt;

&lt;p&gt;Armed with themes from the responses, I will clarify and refine the claims often made in the public sphere about the benefits and drawbacks of open science. I hope to develop this part of the study into actionable recommendations for promoting open science, governing contributions to open science repositories, and addressing the concerns of scientists who are hesitant about engagement.&lt;/p&gt;

&lt;h3 id=&#34;part-2-focusing-on-governance&#34;&gt;Part 2: Focusing on Governance&lt;/h3&gt;

&lt;p&gt;Even with interviews and surveys of scientists on the ground, it is difficult to systematically trace and analyze the totality of social and political processes that support open science infrastructure development because the processes occur across geographic, disciplinary, and other boundaries.&lt;/p&gt;

&lt;p&gt;However, as others have pointed out,&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; the organizational and social elements of digital infrastructure development often become visible and amenable to study through infrastructure governance. Governance refers to the combination of “executive and management roles, program oversight functions organized into structures, and policies that define management principles and decision making.”&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:8&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; Effective governance provides projects with the direction and oversight necessary to achieve desired outcomes of infrastructure development while allowing room for creativity and innovation.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:9&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:10&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:10&#34;&gt;10&lt;/a&gt;&lt;/sup&gt; Studying a project’s governance surfaces the negotiation processes that occur among stakeholders—users, managers, organizations, policymakers, and the like—throughout the development process. Outcomes include agreements about the types of technologies used, standards defining the best practices for technology use, and other policies to ensure that a robust, sustainable infrastructure evolves.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:9&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:11&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:11&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Despite the scientific research community’s increasing reliance on open science infrastructures, few studies compare different infrastructure governance strategies&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and even fewer develop new or revised strategies for governing infrastructure development and use.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:12&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:12&#34;&gt;12&lt;/a&gt;&lt;/sup&gt; The primary goal of this part of the project is to address this gap in our understanding of the governance strategies used to create, maintain, and grow open science infrastructures.&lt;/p&gt;

&lt;p&gt;I am administering this part of the study by conducting in-depth, semi-structured interviews with leaders of various open science infrastructure projects supporting work in astronomy and ecology. I define “leaders” in this context as individuals or small groups of individuals who make decisions about the management of open science infrastructures and their component parts. This set of leaders includes founders and administrators of widely-used scientific software packages and collections of packages, of open data repositories, of open access publication and preprint services, and various combinations of open science tools. Furthermore, I intend to interview the leaders of organizations with which the open science community interacts—top publication editors, for example—to gauge how open science practices and processes are being governed outside of active open science organizations.&lt;/p&gt;

&lt;p&gt;I will conduct qualitative coding as described above to develop themes from the responses of open science leaders. I will then ground these themes in the literature on digital infrastructure governance—which emphasizes gradual, decentralized, and voluntary development—and look for avenues to improve governance strategies.&lt;/p&gt;

&lt;p&gt;Alongside the interview and survey methods, I am actively observing and retaining primary documents from the ongoing discourse around open science in popular scientific communication publications (e.g., &lt;i&gt;Nature&lt;/i&gt; and &lt;i&gt;Science&lt;/i&gt;), conferences and meetings (e.g., OpenCon and discipline-specific hackweeks), and in the popular media/social media (e.g., &lt;i&gt;The New York Times&lt;/i&gt; and Twitter threads).&lt;/p&gt;

&lt;h3 id=&#34;preliminary-themes&#34;&gt;Preliminary Themes&lt;/h3&gt;

&lt;p&gt;I entered this project with a very basic understanding of how open science “works”—the technical and social mechanisms by which scientists make processes and outputs publicly available. In learning about the open science movement, in general and in particular instantiations, I’ve begun to see the intricacies involved in efforts to change scientific research and its modes of communication: research data publication, citation, and access; journal publication availability; and research software development and software citation standards. Within the community trying to sustain these changes are participants and leaders who are facing and tackling several important issues head-on. I list some of the most common engagement, resistance, and governance challenges appearing in interview and observation transcripts below.&lt;/p&gt;

&lt;h4 id=&#34;participation-challenges&#34;&gt;Participation challenges&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Overcoming the fear of sharing code and data, specifically the fear of sharing “messy” code and the fear of being shamed for research errors.&lt;/li&gt;
&lt;li&gt;Defending the time and financial costs of participation in open science—particularly open source software development—to supervisors, collaborators, or tenure and promotion panels who are not engaged with open science.&lt;/li&gt;
&lt;li&gt;Finding time to make code and data usable for others (e.g., through good documentation or complete metadata) and, subsequently, finding a home where code and data can easily be searched and found.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;governance-challenges&#34;&gt;Governance challenges&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Navigating the issue of convincing researchers that software development and data publication/archiving “count” as research products, even though existing funding, publication, and tenure and promotion models may not yet value those contributions.&lt;/li&gt;
&lt;li&gt;Developing guidelines and processes for conducting peer review on research publication, software, and data contributions, especially the tensions involved in “open review.”&lt;/li&gt;
&lt;li&gt;Deciding whose responsibility it is to enforce code and data publication standards or policies, both within open science organizations and in traditional outlets like academic journals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The points raised in this post and the questions guiding my project might seem like discussions you’ve had too many times over coffee during a hackathon break or over beers after a conference session. If so, I’d love to hear from you, even if you are not an astronomer, an ecologist, or an active leader of an open science infrastructure (dsholler at berkeley dot edu). I am always looking for new ideas, both confirming and disconfirming, to refine my approach to this project.&lt;/p&gt;

&lt;!-- references --&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Braa, J., Hanseth, O., Heywood, A., Mohammed, W., Shaw, V. 2007. Developing health information systems in developing countries: The flexible standards strategy. &lt;i&gt;MIS Quarterly&lt;/i&gt;, 31(2), 381-402. &lt;a href=&#34;https://doi.org/10.2307/25148796&#34;&gt;https://doi.org/10.2307/25148796&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Tilson, D., Lyytinen, K., Sørensen, C. 2010 Digital infrastructures: The missing IS research agenda. &lt;i&gt;Information Systems Research&lt;/i&gt;, 21(4), 748-759. &lt;a href=&#34;https://doi.org/10.1287/isre.1100.0318&#34;&gt;https://doi.org/10.1287/isre.1100.0318&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Borgman, C. L. 2010. &lt;i&gt;Scholarship in the digital age: Information, infrastructure, and the Internet.&lt;/i&gt; MIT Press, Cambridge, MA. ISBN: 9780262250863
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Vaast, E., Walsham, G. 2009. Trans-situated learning: Supporting a network of practice with an information infrastructure. &lt;i&gt;Information Systems Research&lt;/i&gt;, 20(4), 547-564. &lt;a href=&#34;https://doi.org/10.1287/isre.1080.0228&#34;&gt;https://doi.org/10.1287/isre.1080.0228&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;Spradley, J. P. (2016). &lt;i&gt;The ethnographic interview&lt;/i&gt;. Longegrove, IL: Waveland Press. ISBN: 0030444969
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;Corbin, J., Strauss, A.,  1990. Grounded theory research: Procedures, canons, and evaluative criteria. &lt;i&gt;Qualitative Sociology&lt;/i&gt;, 13(1), 3-21. &lt;a href=&#34;https://doi.org/10.1007/BF00988593&#34;&gt;https://doi.org/10.1007/BF00988593&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;Barrett, M., Davidson, E., Prabhu, J., Vargo, S. L. 2015. Service innovation in the digital age: Key contributions and future directions. &lt;i&gt;MIS Quarterly&lt;/i&gt;, 39(1) 135-154. DOI: 10.25300/MISQ/2015/39:1.03
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;Hanford, M. 2005. Defining program governance and structure. &lt;i&gt;IBM developerWorks&lt;/i&gt;. Available at: &lt;a href=&#34;https://www.ibm.com/developerworks/rational/library/apr05/hanford/&#34;&gt;https://www.ibm.com/developerworks/rational/library/apr05/hanford/&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;Star, S. L., Ruhleder, K. 1996. Steps toward an ecology of infrastructure: Design and access for large information spaces. &lt;i&gt;Information Systems Research&lt;/i&gt;, 7(1), 111-134. &lt;a href=&#34;https://doi.org/10.1287/isre.7.1.111&#34;&gt;https://doi.org/10.1287/isre.7.1.111&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:9&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;Edwards, P. N., Jackson, S. J., Bowker, G. C., Knobel, C. P. 2007. &lt;i&gt;Understanding infrastructure: Dynamics, tensions, and design. Final report for Workshop on History and Theory of Infrastructure: Lessons for New Scientific Cyberinfrastructures&lt;/i&gt;. NSF Report. Available at: &lt;a href=&#34;https://deepblue.lib.umich.edu/handle/2027.42/49353&#34;&gt;https://deepblue.lib.umich.edu/handle/2027.42/49353&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:10&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;Hanseth, O., Jacucci, E., Grisot, M., Aanestad, M. 2006. Reflexive standardization: Side effects and complexity in standard making. &lt;i&gt;MIS Quarterly&lt;/i&gt;, 30(1), 563-581. &lt;a href=&#34;https://doi.org/10.2307/25148773&#34;&gt;https://doi.org/10.2307/25148773&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:11&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;Hanseth, O., &amp;amp; Lyytinen, K. (2010). Design theory for dynamic complexity in information infrastructures: the case of building internet. &lt;i&gt;Journal of Information Technology&lt;/i&gt;, 25(1), 1-19.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:12&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>googleLanguageR - Analysing language through the Google Cloud Machine Learning APIs</title>
      <link>https://ropensci.org/blog/2017/10/03/googlelanguager/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/03/googlelanguager/</guid>
      <description>
        
        

&lt;!-- open source image taken from: https://upload.wikimedia.org/wikipedia/commons/2/21/Bell_System_switchboard.jpg --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/switchboard.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the greatest assets human beings possess is the power of speech and language, from which almost all our other accomplishments flow. To be able to analyse communication offers us a chance to gain a greater understanding of one another.&lt;/p&gt;

&lt;p&gt;To help you with this, &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt;&lt;/a&gt; is an R package that allows you to perform speech-to-text transcription, neural net translation and natural language processing via the &lt;a href=&#34;https://cloud.google.com/products/machine-learning/&#34;&gt;Google Cloud machine learning services&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An introduction to the package is below, but you can find out more details at the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-s-bet&#34;&gt;Google&amp;rsquo;s bet&lt;/h3&gt;

&lt;p&gt;Google predicts that machine learning is to be a fundamental feature of business, and so they are looking to become the infrastructure that makes machine learning possible. Metaphorically speaking: If machine learning is electricity, then Google wants to be the pylons carrying it around the country.&lt;/p&gt;

&lt;!-- open source image taken from: https://pixabay.com/en/pylon-sky-electricity-tower-2515429/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/pylon.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Google may not be the only company with such ambitions, but one advantage Google has is the amount of data it possesses. Twenty years of web crawling has given it an unprecedented corpus to train its models.  In addition, its recent moves into voice and video gives it one of the biggest audio and speech datasets, all of which have been used to help create machine learning applications within its products such as search and Gmail. Further investment in machine learning is shown by Google&amp;rsquo;s purchase of &lt;a href=&#34;https://deepmind.com/&#34;&gt;Deepmind&lt;/a&gt;, a UK based A.I. research firm that recently was in the news for defeating the top Go champion with its neural network trained Go bot.  Google has also taken an open-source route with the creation and publication of &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt;, a leading machine learning framework.&lt;/p&gt;

&lt;p&gt;Whilst you can create your own machine learning models, for those users who haven&amp;rsquo;t the expertise, data or time to do so, Google also offers an increasing range of machine learning APIs that are pre-trained, such as image and video recognition or job search.  &lt;code&gt;googleLanguageR&lt;/code&gt; wraps the subset of those machine learning APIs that are language flavoured - Cloud Speech, Translation and Natural Language.&lt;/p&gt;

&lt;p&gt;Since they carry complementary outputs that can be used in each other&amp;rsquo;s input, all three of the APIs are included in one package. For example, you can transcribe a recording of someone speaking in Danish, translate that to English and then identify how positive or negative the writer felt about its content (sentiment analysis) then identify the most important concepts and objects within the content (entity analysis).&lt;/p&gt;

&lt;h3 id=&#34;motivations&#34;&gt;Motivations&lt;/h3&gt;

&lt;h4 id=&#34;fake-news&#34;&gt;Fake news&lt;/h4&gt;

&lt;p&gt;One reason why I started looking at this area was the growth of &amp;lsquo;fake news&amp;rsquo;, and its effect on political discourse on social media. I wondered if there was some way to put metrics on how much a news story fuelled one&amp;rsquo;s own bias within your own filter bubble.  The entity API provides a way to perform entity and sentiment analysis at scale on tweets, and by then comparing different users and news sources preferences the hope is to be able to judge how much they are in agreement with your own bias, views and trusted reputation sources.&lt;/p&gt;

&lt;h4 id=&#34;make-your-own-alexa&#34;&gt;Make your own Alexa&lt;/h4&gt;

&lt;p&gt;Another motivating application is the growth of voice commands that will become the primary way of user interface with technology.  Already, &lt;a href=&#34;https://www.thinkwithgoogle.com/data-gallery/detail/google-app-voice-search/&#34;&gt;Google reports up to 20% of search in its app&lt;/a&gt; is via voice search.  I&amp;rsquo;d like to be able to say &amp;ldquo;R, print me out that report for client X&amp;rdquo;.  A Shiny app that records your voice, uploads to the API then parses the return text into actions gives you a chance to create your very own Alexa-like infrastructure.&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;text-align:center&#34;&gt;
&lt;div align=&#34;center&#34;&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-03-googlelanguager/alexa.jpg&#34;&gt;&lt;/div&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The voice activated internet connected speaker, Amazon&amp;rsquo;s Alexa - image from www.amazon.co.uk&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;translate-everything&#34;&gt;Translate everything&lt;/h4&gt;

&lt;p&gt;Finally, I live and work in Denmark.  As Danish is only spoken by less than 6 million people, applications that work in English may not be available in Danish very quickly, if at all.  The API&amp;rsquo;s translation service is the one that made the news in 2016 for &lt;a href=&#34;https://research.googleblog.com/2016/09/a-neural-network-for-machine.html&#34;&gt;&amp;ldquo;inventing its own language&amp;rdquo;&lt;/a&gt;, and offers much better English to Danish translations that the free web version and may make services available in Denmark sooner.&lt;/p&gt;

&lt;h3 id=&#34;using-the-library&#34;&gt;Using the library&lt;/h3&gt;

&lt;p&gt;To use these APIs within R, you first need to do a one-time setup to create a Google Project, add a credit card and authenticate which is &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/#installation&#34;&gt;detailed on the package website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After that, you feed in the R objects you want to operate upon.  The &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/127&#34;&gt;rOpenSci review&lt;/a&gt; helped to ensure that this can scale up easily, so that you can feed in large character vectors which the library will parse and rate limit as required.  The functions also work within &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; pipe syntax.&lt;/p&gt;

&lt;h4 id=&#34;speech-to-text&#34;&gt;Speech-to-text&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/speech.html&#34;&gt;Cloud Speech API&lt;/a&gt; is exposed via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_speech.html&#34;&gt;&lt;code&gt;gl_speech&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It supports multiple audio formats and languages, and you can either feed a sub-60 second audio file directly, or perform asynchrnous requests for longer audio files.&lt;/p&gt;

&lt;p&gt;Example code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)

my_audio &amp;lt;- &amp;quot;my_audio_file.wav&amp;quot;
gl_speech(my_audio)
#  A tibble: 1 x 3
#  transcript confidence                 words
#* &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;                &amp;lt;list&amp;gt;
#1 Hello Mum  0.9227779 &amp;lt;data.frame [19 x 3]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;translation&#34;&gt;Translation&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/translation.html&#34;&gt;Cloud Translation API&lt;/a&gt; lets you translate text via &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_translate.html&#34;&gt;&lt;code&gt;gl_translate&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As you are charged per character, one tip here if you are working with lots of different languages is to perform detection of language offline first using another rOpenSci package, &lt;a href=&#34;https://github.com/ropensci/cld2&#34;&gt;&lt;code&gt;cld2&lt;/code&gt;&lt;/a&gt;.  That way you can avoid charges for text that is already in your target language i.e. English.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
library(cld2)
library(purrr)

my_text &amp;lt;- c(&amp;quot;Katten sidder på måtten&amp;quot;, &amp;quot;The cat sat on the mat&amp;quot;)

## offline detect language via cld2
detected &amp;lt;- map_chr(my_text, detect_language)
# [1] &amp;quot;DANISH&amp;quot;  &amp;quot;ENGLISH&amp;quot;

## get non-English text
translate_me &amp;lt;- my_text[detected != &amp;quot;ENGLISH&amp;quot;]

## translate
gl_translate(translate_me)
## A tibble: 1 x 3
#                 translatedText detectedSourceLanguage                    text
#*                         &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;
#1 The cat is sitting on the mat                     da Katten sidder på måtten
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/nlp.html&#34;&gt;Natural Language API&lt;/a&gt; reveals the structure and meaning of text, accessible via the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/reference/gl_nlp.html&#34;&gt;&lt;code&gt;gl_nlp&lt;/code&gt;&lt;/a&gt; function.&lt;/p&gt;

&lt;p&gt;It returns several analysis:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Entity analysis&lt;/em&gt; - finds named entities (currently proper names and common nouns) in the text along with entity types, salience, mentions for each entity, and other properties. If possible, will also return metadata about that entity such as a Wikipedia URL.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Syntax&lt;/em&gt; - analyzes the syntax of the text and provides sentence boundaries and tokenization along with part of speech tags, dependency trees, and other properties.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sentiment&lt;/em&gt; - the overall sentiment of the text, represented by a magnitude [0, +inf] and score between -1.0 (negative sentiment) and 1.0 (positive sentiment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all useful to get an understanding of the meaning of a sentence, and has potentially the greatest number of applications of the APIs featured.  With entity analysis, auto categorisation of text is possible; the syntax returns let you pull out nouns and verbs for parsing into other actions; and the sentiment analysis allows you to get a feeling for emotion within text.&lt;/p&gt;

&lt;p&gt;A demonstration is below which gives an idea of what output you can generate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleLanguageR)
quote &amp;lt;- &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
nlp &amp;lt;- gl_nlp(quote)

str(nlp)
#List of 6
# $ sentences        :List of 1
#  ..$ :&#39;data.frame&#39;:	1 obs. of  4 variables:
#  .. ..$ content    : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
#  .. ..$ beginOffset: int 0
#  .. ..$ magnitude  : num 0.6
#  .. ..$ score      : num -0.6
# $ tokens           :List of 1
#  ..$ :&#39;data.frame&#39;:	20 obs. of  17 variables:
#  .. ..$ content       : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;things&amp;quot; &amp;quot;are&amp;quot; &amp;quot;infinite&amp;quot; ...
#  .. ..$ beginOffset   : int [1:20] 0 4 11 15 23 25 29 38 42 48 ...
#  .. ..$ tag           : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NOUN&amp;quot; &amp;quot;VERB&amp;quot; &amp;quot;ADJ&amp;quot; ...
#  .. ..$ aspect        : chr [1:20] &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; &amp;quot;ASPECT_UNKNOWN&amp;quot; ...
#  .. ..$ case          : chr [1:20] &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; &amp;quot;CASE_UNKNOWN&amp;quot; ...
#  .. ..$ form          : chr [1:20] &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; &amp;quot;FORM_UNKNOWN&amp;quot; ...
#  .. ..$ gender        : chr [1:20] &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; &amp;quot;GENDER_UNKNOWN&amp;quot; ...
#  .. ..$ mood          : chr [1:20] &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; &amp;quot;INDICATIVE&amp;quot; &amp;quot;MOOD_UNKNOWN&amp;quot; ...
#  .. ..$ number        : chr [1:20] &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;PLURAL&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; &amp;quot;NUMBER_UNKNOWN&amp;quot; ...
#  .. ..$ person        : chr [1:20] &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; &amp;quot;PERSON_UNKNOWN&amp;quot; ...
#  .. ..$ proper        : chr [1:20] &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; &amp;quot;PROPER_UNKNOWN&amp;quot; ...
#  .. ..$ reciprocity   : chr [1:20] &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; &amp;quot;RECIPROCITY_UNKNOWN&amp;quot; ...
#  .. ..$ tense         : chr [1:20] &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; &amp;quot;PRESENT&amp;quot; &amp;quot;TENSE_UNKNOWN&amp;quot; ...
#  .. ..$ voice         : chr [1:20] &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; &amp;quot;VOICE_UNKNOWN&amp;quot; ...
#  .. ..$ headTokenIndex: int [1:20] 1 2 2 2 2 6 2 6 9 6 ...
#  .. ..$ label         : chr [1:20] &amp;quot;NUM&amp;quot; &amp;quot;NSUBJ&amp;quot; &amp;quot;ROOT&amp;quot; &amp;quot;ACOMP&amp;quot; ...
#  .. ..$ value         : chr [1:20] &amp;quot;Two&amp;quot; &amp;quot;thing&amp;quot; &amp;quot;be&amp;quot; &amp;quot;infinite&amp;quot; ...
# $ entities         :List of 1
#  ..$ :Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	6 obs. of  9 variables:
#  .. ..$ name         : chr [1:6] &amp;quot;human stupidity&amp;quot; &amp;quot;things&amp;quot; &amp;quot;universe&amp;quot; &amp;quot;universe&amp;quot; ...
#  .. ..$ type         : chr [1:6] &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; &amp;quot;OTHER&amp;quot; ...
#  .. ..$ salience     : num [1:6] 0.1662 0.4771 0.2652 0.2652 0.0915 ...
#  .. ..$ mid          : Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ wikipedia_url: Factor w/ 0 levels: NA NA NA NA NA NA
#  .. ..$ magnitude    : num [1:6] NA NA NA NA NA NA
#  .. ..$ score        : num [1:6] NA NA NA NA NA NA
#  .. ..$ beginOffset  : int [1:6] 42 4 29 86 29 86
#  .. ..$ mention_type : chr [1:6] &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; &amp;quot;COMMON&amp;quot; ...
# $ language         : chr &amp;quot;en&amp;quot;
# $ text             : chr &amp;quot;Two things are infinite: the universe and human stupidity; and I&#39;m not sure about the universe.&amp;quot;
# $ documentSentiment:Classes ‘tbl_df’, ‘tbl’ and &#39;data.frame&#39;:	1 obs. of  2 variables:
#  ..$ magnitude: num 0.6
#  ..$ score    : num -0.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;This package is 10 times better due to the efforts of the rOpenSci reviewers &lt;a href=&#34;http://enpiar.com/&#34;&gt;Neal Richardson&lt;/a&gt; and &lt;a href=&#34;http://www.juliagustavsen.com/&#34;&gt;Julia Gustavsen&lt;/a&gt;, who have whipped the documentation, outputs and test cases into the form they are today in &lt;code&gt;0.1.0&lt;/code&gt;.  Many thanks to them.&lt;/p&gt;

&lt;p&gt;Hopefully, this is just the beginning and the package can be further improved by its users - if you do give the package a try and find a potential improvement, &lt;a href=&#34;https://github.com/ropensci/googleLanguageR/issues&#34;&gt;raise an issue on GitHub&lt;/a&gt; and we can try to implement it.  I&amp;rsquo;m excited to see what users can do with these powerful tools.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Help us build capacity of open software users and developers with Hacktoberfest</title>
      <link>https://ropensci.org/blog/2017/10/02/hacktoberfest/</link>
      <pubDate>Mon, 02 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/10/02/hacktoberfest/</guid>
      <description>
        
        

&lt;p&gt;One of rOpenSci’s aims is to build capacity of software users and developers and foster a sense of pride in their work. What better way to do that than to encourage you to participate in &lt;a href=&#34;https://hacktoberfest.digitalocean.com/&#34;&gt;Hacktoberfest&lt;/a&gt;, a month-long celebration of open source software!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-10-02-hacktoberfest/hacktoberfest-2017.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;it-doesn-t-take-much-to-get-involved&#34;&gt;It doesn’t take much to get involved&lt;/h3&gt;

&lt;p&gt;Beginners to experts. Contributors and package maintainers welcome. You can get involved by applying the label &lt;code&gt;Hacktoberfest&lt;/code&gt; to issues in your rOpenSci repo (or any project) that are ready for contributors to work on. You can find already-labelled &lt;a href=&#34;https://github.com/search?utf8=%E2%9C%93&amp;amp;q=user%3Aropensci+user%3Aropenscilabs+label%3Ahacktoberfest+state%3Aopen+type%3Aissue&amp;amp;type=&#34;&gt;rOpenSci and ropenscilabs issues here&lt;/a&gt;. A contribution can be anything - fixing typos, improving documentation, writing tests, fixing bugs, or creating new features. Who better to improve a vignette than the person who’s using the package?!&lt;/p&gt;

&lt;p&gt;Consider opening beginner-level issues and label them &lt;code&gt;beginner&lt;/code&gt; and &lt;code&gt;Hacktoberfest&lt;/code&gt;. Remember, everyone had a &lt;a href=&#34;http://firstpr.me/#hadley&#34;&gt;first pull request&lt;/a&gt; so put yourself in their shoes, keep our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md/#code-of-conduct&#34;&gt;code of conduct&lt;/a&gt; in mind and help open up the doors to our community!&lt;/p&gt;

&lt;p&gt;Have a look at this &lt;a href=&#34;https://github.com/ropensci/unconf17/issues/47&#34;&gt;great discussion on first-timers-only issues&lt;/a&gt; with pointers to other discussions and resources. Please chime in in the comments below with your suggestions below for ways to help newcomers get involved.&lt;/p&gt;

&lt;p&gt;Everyone who &lt;a href=&#34;https://hacktoberfest.digitalocean.com/&#34;&gt;signs up&lt;/a&gt; gets stickers, but make four pull requests between October 1–31 and you get a limited edition Hacktoberfest 2017 t-shirt.&lt;/p&gt;

&lt;h3 id=&#34;spread-the-word&#34;&gt;Spread the word&lt;/h3&gt;

&lt;p&gt;Share this blog post and link to your issue or link to &lt;a href=&#34;https://github.com/search?utf8=%E2%9C%93&amp;amp;q=user%3Aropensci+user%3Aropenscilabs+label%3Ahacktoberfest+state%3Aopen+type%3Aissue&amp;amp;type=&#34;&gt;all the issues&lt;/a&gt;! #Hacktoberfest #rstats&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>rrricanes to Access Tropical Cyclone Data</title>
      <link>https://ropensci.org/blog/2017/09/27/rrricanes/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/27/rrricanes/</guid>
      <description>
        
        

&lt;h3 id=&#34;what-is-rrricanes&#34;&gt;What is rrricanes&lt;/h3&gt;

&lt;h4 id=&#34;why-write-rrricanes&#34;&gt;Why Write rrricanes?&lt;/h4&gt;

&lt;p&gt;There is a tremendous amount of weather data available on the internet. Much of it is in raw format and not very easy to obtain. Hurricane data is no different. When one thinks of this data they may be inclined to think it is a bunch of map coordinates with some wind values and not much else. A deeper look will reveal structural and forecast data. An even deeper look will find millions of data points from hurricane reconnaissance, computer forecast models, ship and buoy observations, satellite and radar imagery, &amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; is an attempt to bring this data together in a way that doesn&amp;rsquo;t just benefit R users, but other languages as well.&lt;/p&gt;

&lt;p&gt;I began learning R in 2015 and immediately had wished I had a hurricane-specific dataset when Hurricane Patricia became a harmless, but historic hurricane roaming the Pacific waters. I found this idea revisited again as Hurricane Matthew took aim at Florida and the southeast in 2016. Unable to use R to study and consolidate Matthew&amp;rsquo;s data in R led me to begin learning package development. Thus, the birth of &lt;code&gt;rrricanes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In this article, I will take you on a lengthy tour of the most important features of &lt;code&gt;rrricanes&lt;/code&gt; and what the data means. If you have a background working with hurricane data, most of this will be redundant. My aim here is to cover the big ideas behind the package and explain them under the assumption you, the reader, are unfamiliar with the data offered.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; &lt;strong&gt;is not intended to be used in emergency situations&lt;/strong&gt;. I write this article as areas I have lived or currently live are under the gun from Hurricane Harvey and &lt;code&gt;rrricanes&lt;/code&gt; is unable to obtain data due to external issues (I will describe these later). It is designed with the intent of answering questions and exploring ideas outside of a time-sensitive environment.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; will not be available in CRAN for quite some time. The current schedule is May 15, 2018 (the &amp;ldquo;start&amp;rdquo; of the East Pacific hurricane season). This year is soley for testing under real-time conditions.&lt;/p&gt;

&lt;h4 id=&#34;and-rrricanesdata&#34;&gt;And rrricanesdata&lt;/h4&gt;

&lt;p&gt;The NHC archives text products dating back to at least 1998 (some earlier years exist but yet to be implemented in this package). Accessing this data is a time-consuming process on any computer. A limit of 4 requests per second is put in place to avoid being banned (or restricted) from the archives. So, if a hurricane has 20 text products you wish to pull and parse, this will take 5 seconds. Most cyclones have more and some, far more.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; is a compliment package to &lt;code&gt;rrricanes&lt;/code&gt;. &lt;code&gt;rrricanesdata&lt;/code&gt; contains post-scraped datasets of the archives for all available storms with the exception of advisories issued in the current month.This means you can explore the various datasets without the wait.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; will be updated monthly if an advisory has been issued the previous month. There will be regular monthly updates approximately from May through November - the typical hurricane season. In some cases, a cyclone may develop in the off-season. &lt;code&gt;rrricanesdata&lt;/code&gt; will be updated on the same schedule.&lt;/p&gt;

&lt;h4 id=&#34;eli5-the-data&#34;&gt;ELI5 the Data&lt;/h4&gt;

&lt;p&gt;This package covers tropical cyclones that have developed in the Atlantic basin (north Atlantic ocean) or East Pacific basin (northeast Pacific east of 140#&amp;deg;W). Central Pacific (140#&amp;deg;W - 180#&amp;deg;W) may be mixed in if listed in the NHC archives.&lt;/p&gt;

&lt;p&gt;While traditionally the hurricane season for each basin runs from mid-May or June through November, some cyclones have developed outside of this time frame.&lt;/p&gt;

&lt;p&gt;Every tropical cylone (any tropical low whether classified as a tropical depression, tropical storm or hurricane) contains a core set of text products officially issued from the National Hurricane Center. These products are issued every six hours.&lt;/p&gt;

&lt;p&gt;Much of this data has changed in format over the years. Some products have been discontinued and replaced by new products or wrapped into existing products. Some of these products are returned in raw text format; it is not cleaned and may contain HTML characters. Other products are parsed with every piece of data extracted and cleaned.&lt;/p&gt;

&lt;p&gt;I have done my best to ensure data is high quality. But, I cannot guarantee it is perfect. If you do believe you have found an error, please &lt;a href=&#34;https://github.com/ropensci/rrricanes/issues&#34;&gt;let me know&lt;/a&gt;; even if it seems small. I would rather be notified of a false error than ignore a true one.&lt;/p&gt;

&lt;h4 id=&#34;the-products&#34;&gt;The Products&lt;/h4&gt;

&lt;p&gt;Each advisory product is listed below with an abbreviation in parentheses. Unless otherwise noted, these products are issued every six hours. Generally, the times issued are 03:00, 09:00, 15:00 and 21:00 UTC. Some products may be issued in three-hour increments and, sometimes, two-hour increments. &lt;code&gt;update&lt;/code&gt; can be issued at any time.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Storm Discussion (&lt;code&gt;discus&lt;/code&gt;) - These are technical discussions centered on the current structure of the cyclone, satellite presentation, computer forecast model tendencies and more. These products are not parsed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Forecast/Adivsory (&lt;code&gt;fstadv&lt;/code&gt;) - This data-rich product lists the current location of the cyclone, its wind structure, forecast and forecast wind structure.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Public Advisory (&lt;code&gt;public&lt;/code&gt;) - These are general text statements issued for the public-at-large. Information in these products is a summary of the Forecast/Advisory product along with any watches and warnings issued, changed, or cancelled. Public Advisory products are the only regularly-scheduled product that may be issued intermittently (every three hours and, occasionally, every two hours) when watches and warnings are in effect. These products are not parsed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Wind Speed Probabilities (&lt;code&gt;wndprb&lt;/code&gt;) - These products list the probability of a minimum sustained wind speed expected in a given forecast window. This product replaces the Strike Probabilities product beginning in 2006 (see below).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Updates (&lt;code&gt;update&lt;/code&gt;) - Tropical Cyclone Updates may be issued at any time if a storm is an immediate threat to land or if the cyclone undergoes a significant change of strength or structure. The information in this product is general. These products are not parsed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;discontinued-products&#34;&gt;Discontinued Products&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Strike Probabilities (&lt;code&gt;prblty&lt;/code&gt;) - List the probability of a tropical cyclone passing within 65 nautical miles of a location within a forecast window. Replaced in 2006 by the Wind Speed Probabilities product.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Position Estimates (&lt;code&gt;posest&lt;/code&gt;) - Typically issued as a storm is threatening land but generally rare (see Hurricane Ike 2008, Key AL092008). It is generally just an update of the current location of the cyclone. After the 2011 hurricane season, this product was discontinued; Updates are now issued in their place. These products are not parsed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;primary-key&#34;&gt;Primary Key&lt;/h4&gt;

&lt;p&gt;Every cyclone has a &lt;code&gt;Key&lt;/code&gt;. However, not all products contain this value (&lt;code&gt;prblty&lt;/code&gt;, for example). Products issued during and after the 2005 hurricane season contain this variable.&lt;/p&gt;

&lt;p&gt;Use &lt;code&gt;Key&lt;/code&gt; to tie datasets together. If &lt;code&gt;Key&lt;/code&gt; does not exist, you will need to use a combination of &lt;code&gt;Name&lt;/code&gt; and &lt;code&gt;Date&lt;/code&gt;, depending on your requirements. Keep in mind that, unless a name is retired, names are recycled every seven years. For example, there are multiple cyclones named Katrina but you may want to isolate on Katrina, 2005.&lt;/p&gt;

&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; will not be submitted to CRAN until prior to the hurricane season, 2018. It can be installed via github using &lt;code&gt;devtools&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;devtools::install_github(&amp;quot;ropensci/rrricanes&amp;quot;, build_vignettes = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;optional-supporting-packages&#34;&gt;Optional Supporting Packages&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; uses a drat repository to host the large, pre-processed datasets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;rrricanesdata&amp;quot;,
                 repos = &amp;quot;https://timtrice.github.io/drat/&amp;quot;,
                 type = &amp;quot;source&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To use high resolution tracking charts, you may also wish to install the `rnaturalearthhires&amp;rsquo; package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;rnaturalearthhires&amp;quot;,
                 repos = &amp;quot;http://packages.ropensci.org&amp;quot;,
                 type = &amp;quot;source&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Linux users may also need to install:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;libgdal-dev&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;libproj-dev&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;libxml2-dev&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;get-a-list-of-storms&#34;&gt;Get a List of Storms&lt;/h3&gt;

&lt;p&gt;We start exploring &lt;code&gt;rrricanes&lt;/code&gt; by finding a storm (or storms) we wish to analyze. For this, we use &lt;code&gt;get_storms&lt;/code&gt;. There are two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;years&lt;/code&gt; Between 1998 and current year&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;basins&lt;/code&gt; One or both &amp;ldquo;AL&amp;rdquo; and &amp;ldquo;EP&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An empty call to the function will return storms for both the Atlantic and East Pacific basin for the current year.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
library(rrricanes)
get_storms() %&amp;gt;% print(n = nrow(.))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 33 x 4
##     Year                           Name Basin
##    &amp;lt;dbl&amp;gt;                          &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
##  1  2017          Tropical Storm Arlene    AL
##  2  2017            Tropical Storm Bret    AL
##  3  2017           Tropical Storm Cindy    AL
##  4  2017       Tropical Depression Four    AL
##  5  2017             Tropical Storm Don    AL
##  6  2017           Tropical Storm Emily    AL
##  7  2017             Hurricane Franklin    AL
##  8  2017                 Hurricane Gert    AL
##  9  2017               Hurricane Harvey    AL
## 10  2017 Potential Tropical Cyclone Ten    AL
## 11  2017                 Hurricane Irma    AL
## 12  2017                 Hurricane Jose    AL
## 13  2017                Hurricane Katia    AL
## 14  2017                  Hurricane Lee    AL
## 15  2017                Hurricane Maria    AL
## 16  2017          Tropical Storm Adrian    EP
## 17  2017         Tropical Storm Beatriz    EP
## 18  2017          Tropical Storm Calvin    EP
## 19  2017                 Hurricane Dora    EP
## 20  2017               Hurricane Eugene    EP
## 21  2017             Hurricane Fernanda    EP
## 22  2017            Tropical Storm Greg    EP
## 23  2017    Tropical Depression Eight-E    EP
## 24  2017               Hurricane Hilary    EP
## 25  2017                Hurricane Irwin    EP
## 26  2017   Tropical Depression Eleven-E    EP
## 27  2017            Tropical Storm Jova    EP
## 28  2017              Hurricane Kenneth    EP
## 29  2017           Tropical Storm Lidia    EP
## 30  2017                 Hurricane Otis    EP
## 31  2017                  Hurricane Max    EP
## 32  2017                Hurricane Norma    EP
## 33  2017           Tropical Storm Pilar    EP
## # ... with 1 more variables: Link &amp;lt;chr&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Function &lt;code&gt;get_storms&lt;/code&gt; returns four variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Year - year of the cyclone.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Name - name of the cyclone.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Basin - basin the cyclone developed (AL for Atlantic, EP for east Pacific).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Link - URL to the cyclone&amp;rsquo;s archive page.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The variables &lt;code&gt;Name&lt;/code&gt; and &lt;code&gt;Link&lt;/code&gt; are the only variables that could potentially change. For example, you&amp;rsquo;ll notice a &lt;code&gt;Name&lt;/code&gt; value of &lt;u&gt;Potential Tropical Cyclone Ten&lt;/u&gt;. If this storm became a tropical storm then it would receive a new name and the link to the archive page would change as well.&lt;/p&gt;

&lt;p&gt;For this example we will explore &lt;u&gt;Hurricane Harvey&lt;/u&gt;.&lt;/p&gt;

&lt;h3 id=&#34;text-products&#34;&gt;Text Products&lt;/h3&gt;

&lt;h4 id=&#34;current-data&#34;&gt;Current Data&lt;/h4&gt;

&lt;p&gt;Once we have identified the storms we want to retrieve we can begin working on getting the products. In the earlier discussion of the available products, recall I used abbreviations such as &lt;code&gt;discus&lt;/code&gt;, &lt;code&gt;fstadv&lt;/code&gt;, etc. These are the terms we will use when obtaining data.&lt;/p&gt;

&lt;p&gt;The easiest method to getting storm data is the function &lt;code&gt;get_storm_data&lt;/code&gt;. This function can take multiple storm archive URLs and return multiple datasets within a list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ds &amp;lt;- get_storms() %&amp;gt;%
  filter(Name == &amp;quot;Hurricane Harvey&amp;quot;) %&amp;gt;%
  pull(Link) %&amp;gt;%
  get_storm_data(products = c(&amp;quot;discus&amp;quot;, &amp;quot;fstadv&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This process may take some time (particularly, &lt;code&gt;fstadv&lt;/code&gt; products). This is because the NHC website allows no more than 80 connections every 10 seconds. &lt;code&gt;rrricanes&lt;/code&gt; processes four links every half second.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; uses the &lt;code&gt;dplyr&lt;/code&gt; progress bar to keep you informed of the status. You can turn this off by setting option &lt;code&gt;dplyr.show_progress&lt;/code&gt; to FALSE.&lt;/p&gt;

&lt;p&gt;An additional option is &lt;code&gt;rrricanes.working_msg&lt;/code&gt;; FALSE by default. This option will show a message for each advisory currently being worked. I primarily added it to help find products causing problems but you may find it useful at some point.&lt;/p&gt;

&lt;p&gt;At this point, we have a list - &lt;code&gt;ds&lt;/code&gt; - of dataframes. Each dataframe is named after the product.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(ds)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;discus&amp;quot; &amp;quot;fstadv&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;discus&lt;/code&gt; is one of the products that isn&amp;rsquo;t parsed; the full text of the product is returned.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(ds$discus)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	43 obs. of  6 variables:
##  $ Status  : chr  &amp;quot;Potential Tropical Cyclone&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; ...
##  $ Name    : chr  &amp;quot;Nine&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##  $ Adv     : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Date    : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 21:00:00&amp;quot; ...
##  $ Key     : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Contents: chr  &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nPotential Tropical Cyclone Nine Discussion Number   1\nNWS National&amp;quot;| __truncated__ &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nTropical Storm Harvey Discussion Number   2\nNWS National Hurricane&amp;quot;| __truncated__ &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nTropical Storm Harvey Discussion Number   3\nNWS National Hurricane&amp;quot;| __truncated__ &amp;quot;\nZCZC MIATCDAT4 ALL\nTTAA00 KNHC DDHHMM\n\nTropical Storm Harvey Discussion Number   4\nNWS National Hurricane&amp;quot;| __truncated__ ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;fstadv&lt;/code&gt; dataframes, however, are parsed and contain the bulk of the information for the storm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(ds$fstadv)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	43 obs. of  117 variables:
##  $ Status       : chr  &amp;quot;Potential Tropical Cyclone&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; ...
##  $ Name         : chr  &amp;quot;Nine&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##  $ Adv          : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Date         : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 21:00:00&amp;quot; ...
##  $ Key          : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Lat          : num  13.1 13 13 13.1 13.1 13.4 13.7 13.8 13.9 14.1 ...
##  $ Lon          : num  -54.1 -55.8 -57.4 -59.1 -61.3 -62.9 -64.1 -65.9 -68.1 -70 ...
##  $ Wind         : num  30 35 35 35 35 35 35 35 35 30 ...
##  $ Gust         : num  40 45 45 45 45 45 45 45 45 40 ...
##  $ Pressure     : num  1008 1004 1005 1004 1005 ...
##  $ PosAcc       : num  30 30 30 30 30 40 40 30 30 30 ...
##  $ FwdDir       : num  270 270 270 270 270 275 275 275 275 275 ...
##  $ FwdSpeed     : num  15 16 16 16 18 18 16 18 19 19 ...
##  $ Eye          : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SeasNE       : num  NA 60 60 60 75 150 60 60 45 NA ...
##  $ SeasSE       : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasSW       : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasNW       : num  NA 60 60 60 60 60 45 60 45 NA ...
##  $ NE64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SE64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SW64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NW64         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NE50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SE50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SW50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NW50         : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ NE34         : num  NA 30 50 50 60 60 60 60 0 NA ...
##  $ SE34         : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SW34         : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ NW34         : num  NA 30 50 50 60 60 60 60 60 NA ...
##  $ Hr12FcstDate : POSIXct, format: &amp;quot;2017-08-18 00:00:00&amp;quot; &amp;quot;2017-08-18 06:00:00&amp;quot; ...
##  $ Hr12Lat      : num  13.1 13.1 13.2 13.2 13.3 13.6 14 14 14.1 14.3 ...
##  $ Hr12Lon      : num  -56.4 -58.3 -59.9 -61.7 -63.8 -65.7 -66.8 -68.7 -70.9 -73 ...
##  $ Hr12Wind     : num  30 35 35 35 35 35 35 35 35 30 ...
##  $ Hr12Gust     : num  40 45 45 45 45 45 45 45 45 40 ...
##  $ Hr12NE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12SW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr12NE34     : num  NA 30 50 50 60 60 60 60 60 NA ...
##  $ Hr12SE34     : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ Hr12SW34     : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ Hr12NW34     : num  NA 30 50 50 60 60 60 60 60 NA ...
##  $ Hr24FcstDate : POSIXct, format: &amp;quot;2017-08-18 12:00:00&amp;quot; &amp;quot;2017-08-18 18:00:00&amp;quot; ...
##  $ Hr24Lat      : num  13.2 13.4 13.6 13.5 13.6 13.9 14.3 14.3 14.4 14.6 ...
##  $ Hr24Lon      : num  -59.8 -61.6 -63.3 -65.2 -67.3 -69.3 -70.4 -72.7 -74.9 -77 ...
##  $ Hr24Wind     : num  35 40 40 40 40 40 40 40 40 35 ...
##  $ Hr24Gust     : num  45 50 50 50 50 50 50 50 50 45 ...
##  $ Hr24NE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24SW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr24NE34     : num  50 40 50 50 60 60 60 60 60 60 ...
##  $ Hr24SE34     : num  0 0 0 0 30 0 0 0 0 0 ...
##  $ Hr24SW34     : num  0 0 0 0 30 0 0 0 0 0 ...
##  $ Hr24NW34     : num  50 40 50 50 60 60 60 60 60 60 ...
##  $ Hr36FcstDate : POSIXct, format: &amp;quot;2017-08-19 00:00:00&amp;quot; &amp;quot;2017-08-19 06:00:00&amp;quot; ...
##  $ Hr36Lat      : num  13.5 13.7 13.9 13.9 14 14.2 14.5 14.6 14.9 15.2 ...
##  $ Hr36Lon      : num  -63.2 -65.1 -67 -68.8 -71.1 -73 -74.3 -76.7 -78.7 -80.5 ...
##  $ Hr36Wind     : num  40 45 45 40 40 40 40 45 45 40 ...
##  $ Hr36Gust     : num  50 55 55 50 50 50 50 55 55 50 ...
##  $ Hr36NE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SE64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NW64     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SE50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36SW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NW50     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ Hr36NE34     : num  60 60 60 60 60 60 60 70 70 60 ...
##  $ Hr36SE34     : num  0 30 30 30 30 0 0 0 0 0 ...
##  $ Hr36SW34     : num  0 30 30 30 30 0 0 0 0 0 ...
##  $ Hr36NW34     : num  60 60 60 60 60 60 60 70 70 60 ...
##  $ Hr48FcstDate : POSIXct, format: &amp;quot;2017-08-19 12:00:00&amp;quot; &amp;quot;2017-08-19 18:00:00&amp;quot; ...
##  $ Hr48Lat      : num  13.9 14 14.1 14.1 14.3 14.5 14.8 15.2 15.7 16 ...
##  $ Hr48Lon      : num  -66.7 -68.8 -70.9 -72.7 -75 -76.7 -78.1 -80.1 -82.4 -83.8 ...
##  $ Hr48Wind     : num  45 45 50 50 50 45 45 50 50 45 ...
##  $ Hr48Gust     : num  55 55 60 60 60 55 55 60 60 55 ...
##  $ Hr48NE50     : num  NA NA 30 30 30 NA NA 30 30 NA ...
##  $ Hr48SE50     : num  NA NA 0 0 0 NA NA 0 0 NA ...
##  $ Hr48SW50     : num  NA NA 0 0 0 NA NA 0 0 NA ...
##  $ Hr48NW50     : num  NA NA 30 30 30 NA NA 30 30 NA ...
##  $ Hr48NE34     : num  60 60 60 60 70 60 60 90 90 70 ...
##  $ Hr48SE34     : num  30 30 30 30 30 30 0 50 50 0 ...
##  $ Hr48SW34     : num  30 30 30 30 30 30 0 40 40 0 ...
##  $ Hr48NW34     : num  60 60 60 60 70 60 60 70 70 70 ...
##  $ Hr72FcstDate : POSIXct, format: &amp;quot;2017-08-20 12:00:00&amp;quot; &amp;quot;2017-08-20 18:00:00&amp;quot; ...
##  $ Hr72Lat      : num  14.5 14.5 14.8 15 15 15.5 16.5 17 17.5 18 ...
##  $ Hr72Lon      : num  -74.5 -76.5 -78.6 -80.2 -82 -83.5 -84.7 -86.5 -88 -89 ...
##  $ Hr72Wind     : num  55 55 60 60 60 60 55 55 60 45 ...
##  $ Hr72Gust     : num  65 65 75 75 75 75 65 65 75 55 ...
##   [list output truncated]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each product can also be accessed on its own. For example, if you only wish to view &lt;code&gt;discus&lt;/code&gt; products, use the &lt;code&gt;get_discus&lt;/code&gt; function. &lt;code&gt;fstadv&lt;/code&gt; products can be accessed with &lt;code&gt;get_fstadv&lt;/code&gt;. Every products specific function is preceeded by &lt;code&gt;get_&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To understand the variable definitions, access the help file for each of these functions (i.e., &lt;code&gt;?get_fstadv&lt;/code&gt;). They contain full definitions on the variables and their purpose.&lt;/p&gt;

&lt;p&gt;As you can see, the &lt;code&gt;fstadv&lt;/code&gt; dataframe is very wide. There may be instances you only want to focus on specific pieces of the product. I&amp;rsquo;ve developed tidy functions to help trim these datasets:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_fcst&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_fcst_wr&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_fstadv&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tidy_wr&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These datasets exist in &lt;code&gt;rrricanesdata&lt;/code&gt; as &lt;code&gt;fcst&lt;/code&gt;, &lt;code&gt;fcst_wr&lt;/code&gt;, &lt;code&gt;adv&lt;/code&gt;, and &lt;code&gt;wr&lt;/code&gt;, respectively (see below).&lt;/p&gt;

&lt;p&gt;Most tropical cyclone forecast/advisory products will contain multiple forecast points. Initially, only three-day forecasts were issued. Beginning the with the 2003 season, 96 hour (five-day) forecasts were issued.&lt;/p&gt;

&lt;p&gt;If a storm is not expected to survive the full forecast period, then only relevant forecasts will be issued.&lt;/p&gt;

&lt;p&gt;We use &lt;code&gt;tidy_fcst&lt;/code&gt; to return these forecast points in a tidy fashion from &lt;code&gt;fstadv&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_fcst(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	283 obs. of  8 variables:
##  $ Key     : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv     : num  1 1 1 1 1 1 1 2 2 2 ...
##  $ Date    : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 15:00:00&amp;quot; ...
##  $ FcstDate: POSIXct, format: &amp;quot;2017-08-18 00:00:00&amp;quot; &amp;quot;2017-08-18 12:00:00&amp;quot; ...
##  $ Lat     : num  13.1 13.2 13.5 13.9 14.5 15.5 17 13.1 13.4 13.7 ...
##  $ Lon     : num  -56.4 -59.8 -63.2 -66.7 -74.5 -82 -87.5 -58.3 -61.6 -65.1 ...
##  $ Wind    : num  30 35 40 45 55 65 65 35 40 45 ...
##  $ Gust    : num  40 45 50 55 65 80 80 45 50 55 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wind radius values are issued with parameters of 34, 50 and 64. These values are the radius to which minimum one-minute sustained winds can be expected or exist.&lt;/p&gt;

&lt;p&gt;A tropical depression will not have associated wind radius values since the maximum winds of a depression are 30 knots. If a tropical storm has winds less than 50 knots, then it will only have wind radius values for the 34-knot wind field. If winds are greater than 50 knots, then it will have wind radius values for 34 and 50 knot winds. A hurricane will have all wind radius fields.&lt;/p&gt;

&lt;p&gt;Wind radius values are further seperated by quadrant; NE (northeast), SE, SW and NW. Not all quadrants will have values; particularly if the cyclone is struggling to organize. For example, you will often find a minimal hurricane only has hurricane-force winds (64 knots) in the northeast quadrant.&lt;/p&gt;

&lt;p&gt;When appropriate, a forecast/advisory product will contain these values for the current position and for each forecast position. Use &lt;code&gt;tidy_wr&lt;/code&gt; and &lt;code&gt;tidy_fcst_wr&lt;/code&gt;, respectively, for these variables.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_wr(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	56 obs. of  8 variables:
##  $ Key      : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv      : num  2 3 4 5 6 7 8 9 15 16 ...
##  $ Date     : POSIXct, format: &amp;quot;2017-08-17 21:00:00&amp;quot; &amp;quot;2017-08-18 03:00:00&amp;quot; ...
##  $ WindField: num  34 34 34 34 34 34 34 34 34 34 ...
##  $ NE       : num  30 50 50 60 60 60 60 0 100 80 ...
##  $ SE       : num  0 0 0 0 0 0 0 0 0 30 ...
##  $ SW       : num  0 0 0 0 0 0 0 0 0 20 ...
##  $ NW       : num  30 50 50 60 60 60 60 60 50 50 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_fcst_wr(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	246 obs. of  9 variables:
##  $ Key      : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv      : num  1 1 1 1 1 2 2 2 2 2 ...
##  $ Date     : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 15:00:00&amp;quot; ...
##  $ FcstDate : POSIXct, format: &amp;quot;2017-08-18 12:00:00&amp;quot; &amp;quot;2017-08-19 00:00:00&amp;quot; ...
##  $ WindField: num  34 34 34 34 50 34 34 34 34 34 ...
##  $ NE       : num  50 60 60 80 30 30 40 60 60 80 ...
##  $ SE       : num  0 0 30 40 0 0 0 30 30 40 ...
##  $ SW       : num  0 0 30 40 0 0 0 30 30 40 ...
##  $ NW       : num  50 60 60 80 30 30 40 60 60 80 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, you may only want to focus on current storm details. For this, we use &lt;code&gt;tidy_fstadv&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(tidy_fstadv(ds$fstadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	43 obs. of  18 variables:
##  $ Key     : chr  &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; &amp;quot;AL092017&amp;quot; ...
##  $ Adv     : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Date    : POSIXct, format: &amp;quot;2017-08-17 15:00:00&amp;quot; &amp;quot;2017-08-17 21:00:00&amp;quot; ...
##  $ Status  : chr  &amp;quot;Potential Tropical Cyclone&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; &amp;quot;Tropical Storm&amp;quot; ...
##  $ Name    : chr  &amp;quot;Nine&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##  $ Lat     : num  13.1 13 13 13.1 13.1 13.4 13.7 13.8 13.9 14.1 ...
##  $ Lon     : num  -54.1 -55.8 -57.4 -59.1 -61.3 -62.9 -64.1 -65.9 -68.1 -70 ...
##  $ Wind    : num  30 35 35 35 35 35 35 35 35 30 ...
##  $ Gust    : num  40 45 45 45 45 45 45 45 45 40 ...
##  $ Pressure: num  1008 1004 1005 1004 1005 ...
##  $ PosAcc  : num  30 30 30 30 30 40 40 30 30 30 ...
##  $ FwdDir  : num  270 270 270 270 270 275 275 275 275 275 ...
##  $ FwdSpeed: num  15 16 16 16 18 18 16 18 19 19 ...
##  $ Eye     : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ SeasNE  : num  NA 60 60 60 75 150 60 60 45 NA ...
##  $ SeasSE  : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasSW  : num  NA 0 0 0 0 0 0 0 0 NA ...
##  $ SeasNW  : num  NA 60 60 60 60 60 45 60 45 NA ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In release 0.2.1, &lt;code&gt;tidy_fstadv&lt;/code&gt; will be renamed to &lt;code&gt;tidy_adv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;One final note on the data: all speed variables are measured in knots, distance variables in nautical miles, and pressure variables in millibars. Functions &lt;code&gt;knots_to_mph&lt;/code&gt; and &lt;code&gt;mb_to_in&lt;/code&gt; are available for speed/pressure conversions. Function &lt;code&gt;nm_to_sm&lt;/code&gt; to convert nautical miles to survey miles will be included in release 0.2.1.&lt;/p&gt;

&lt;h4 id=&#34;archived-data&#34;&gt;Archived Data&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;rrricanesdata&lt;/code&gt; was built to make it easier to get pre-processed datasets. As mentioned earlier, &lt;code&gt;rrricanesdata&lt;/code&gt; will be updated the first of every month if any advisory was issued for the previous month. (As I am now writing this portion in September, all of Hurricane Harvey&amp;rsquo;s advisories - the last one issued the morning of August 31 - exist in &lt;code&gt;rrricanesdata&lt;/code&gt; release 0.0.1.4.)&lt;/p&gt;

&lt;p&gt;As with &lt;code&gt;rrricanes&lt;/code&gt;, &lt;code&gt;rrricanesdata&lt;/code&gt; is not available in CRAN (nor will be due to size limitations).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll load all datasets with the call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(rrricanesdata)
data(list = data(package = &amp;quot;rrricanesdata&amp;quot;)$results[,3])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All core product datasets are available. The dataframes &lt;code&gt;adv&lt;/code&gt;, &lt;code&gt;fcst&lt;/code&gt;, &lt;code&gt;fcst_wr&lt;/code&gt; and &lt;code&gt;wr&lt;/code&gt; are the dataframes created by &lt;code&gt;tidy_fstadv&lt;/code&gt;, &lt;code&gt;tidy_fcst&lt;/code&gt;, &lt;code&gt;tidy_fcst_wr&lt;/code&gt; and &lt;code&gt;tidy_wr&lt;/code&gt;, respectively.&lt;/p&gt;

&lt;h3 id=&#34;tracking-charts&#34;&gt;Tracking Charts&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;rrricanes&lt;/code&gt; also comes with helper functions to quickly generate tracking charts. These charts use &lt;code&gt;rnaturalearthdata&lt;/code&gt; (for high resolution maps, use package &lt;code&gt;rnaturalearthhires&lt;/code&gt;). These charts are not required - &lt;a href=&#34;https://twitter.com/hrbrmstr/status/900762714477350913&#34;&gt;Bob Rudis demonstrates&lt;/a&gt; demonstrates succintly - so feel free to experiment.&lt;/p&gt;

&lt;p&gt;You can generate a default plot for the entire globe with &lt;code&gt;tracking_chart&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tracking_chart()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-11-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You may find this handy when examining cyclones that cross basins (from the Atlantic to east Pacific such as Hurricane Otto, 2016).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tracking_chart&lt;/code&gt; takes three parameters (in addition to dots for other &lt;code&gt;ggplot&lt;/code&gt; calls):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;countries&lt;/code&gt; - By default, show country borders&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;states&lt;/code&gt; - By default, show state borders&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;res&lt;/code&gt; - resolution; default is 110nm.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We do not see countries and states in the map above because of the ggplot defaults. Let&amp;rsquo;s try it again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-12-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can &amp;ldquo;zoom in&amp;rdquo; on each basin with helper functions &lt;code&gt;al_tracking_chart&lt;/code&gt; and &lt;code&gt;ep_tracking_chart&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-13-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ep_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-14-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;gis-data&#34;&gt;GIS Data&lt;/h3&gt;

&lt;p&gt;GIS data exists for some cyclones and varies by year. This is a relatively new archive by the NHC and is inconsistent from storm to storm.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;gis&amp;rdquo; functions are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_advisory&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_latest&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_prob_storm_surge&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_windfield&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;gis_wsp&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another area of inconsistency with these products is how they are organized. For example, &lt;code&gt;gis_advisory&lt;/code&gt;, &lt;code&gt;gis_prob_storm_surge&lt;/code&gt; and &lt;code&gt;gis_windfield&lt;/code&gt; can be retrieved with a storm &lt;code&gt;Key&lt;/code&gt; (unique identifier for every cyclone; see &lt;code&gt;fstadv$Key&lt;/code&gt;). Except for &lt;code&gt;gis_prob_storm_surge&lt;/code&gt;, you can even pass an advisory number (see &lt;code&gt;fstadv$Adv&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;gis_wsp&lt;/code&gt; requires a datetime value; to access a specific GIS package for a storm&amp;rsquo;s advisory you would need to use a variable such as &lt;code&gt;fstadv$Date&lt;/code&gt;, subtract three hours and convert to &amp;ldquo;%Y%m%d%H&amp;rdquo; format (&amp;ldquo;%m&amp;rdquo;, &amp;ldquo;%d&amp;rdquo;, and &amp;ldquo;%H&amp;rdquo; are optional).&lt;/p&gt;

&lt;p&gt;All above functions only return URL&amp;rsquo;s to their respective datasets. This was done to allow you to validate the quantity of datasets you wish to retrieve as, in some cases, the dataset may not exist at all or there may be several available. Use &lt;code&gt;gis_download&lt;/code&gt; with the requested URL&amp;rsquo;s to retrieve your datasets.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go through each of these. First, let&amp;rsquo;s get the &lt;code&gt;Key&lt;/code&gt; of Hurricane Harvey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Remember that ds already and only contains data for Hurricane Harvey
key &amp;lt;- ds$fstadv %&amp;gt;% pull(Key) %&amp;gt;% first()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;gis-advisory&#34;&gt;gis_advisory&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;gis_advisory&lt;/code&gt; returns a dataset package containing past and forecast plot points and lines, a forecast cone (area representing where the cyclone could track), wind radius data and current watches and warnings.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;gis_advisory&lt;/code&gt; takes two parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Key&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;advisory&lt;/code&gt; (optional)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we leave out advisory we get all related datasets for Hurricane Harvey:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- gis_advisory(key = key)
length(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 77
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(x, n = 5L)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_001.zip&amp;quot;
## [2] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_001A.zip&amp;quot;
## [3] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_002.zip&amp;quot;
## [4] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_002A.zip&amp;quot;
## [5] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_003.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, there is quite a bit (and why the core gis functions only return URLs rather than the actual datasets). Let&amp;rsquo;s trim this down a bit. Sneaking a peek (&lt;a href=&#34;http://www.nhc.noaa.gov/archive/2017/HARVEY_graphics.php?product=5day_cone_with_line_and_wind&#34;&gt;cheating&lt;/a&gt;) I find advisory 19 seems a good choice.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_advisory(key = key, advisory = 19)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/forecast/archive/al092017_5day_019.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Good; there is a data package available for this advisory. Once you have confirmed the package you want to retrieve, use &lt;code&gt;gis_download&lt;/code&gt; to get the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_advisory(key = key, advisory = 19) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_5day_lin&amp;quot;
## with 1 features
## It has 7 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_5day_pgn&amp;quot;
## with 1 features
## It has 7 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_5day_pts&amp;quot;
## with 8 features
## It has 23 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017-019_ww_wwlin&amp;quot;
## with 5 features
## It has 8 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see what we have.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(gis)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## List of 4
##  $ al092017_019_5day_lin:Formal class &#39;SpatialLinesDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 4 slots
##   .. ..@ data       :&#39;data.frame&#39;:	1 obs. of  7 variables:
##   .. .. ..$ STORMNAME: chr &amp;quot;Harvey&amp;quot;
##   .. .. ..$ STORMTYPE: chr &amp;quot;HU&amp;quot;
##   .. .. ..$ ADVDATE  : chr &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot;
##   .. .. ..$ ADVISNUM : chr &amp;quot;19&amp;quot;
##   .. .. ..$ STORMNUM : num 9
##   .. .. ..$ FCSTPRD  : num 120
##   .. .. ..$ BASIN    : chr &amp;quot;AL&amp;quot;
##   .. ..@ lines      :List of 1
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:8, 1:2] -94.6 -95.6 -96.5 -97.1 -97.3 -97.5 -97 -95 25.2 26.1 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;0&amp;quot;
##   .. ..@ bbox       : num [1:2, 1:2] -97.5 25.2 -94.6 29.5
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
##  $ al092017_019_5day_pgn:Formal class &#39;SpatialPolygonsDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. ..@ data       :&#39;data.frame&#39;:	1 obs. of  7 variables:
##   .. .. ..$ STORMNAME: chr &amp;quot;Harvey&amp;quot;
##   .. .. ..$ STORMTYPE: chr &amp;quot;HU&amp;quot;
##   .. .. ..$ ADVDATE  : chr &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot;
##   .. .. ..$ ADVISNUM : chr &amp;quot;19&amp;quot;
##   .. .. ..$ STORMNUM : num 9
##   .. .. ..$ FCSTPRD  : num 120
##   .. .. ..$ BASIN    : chr &amp;quot;AL&amp;quot;
##   .. ..@ polygons   :List of 1
##   .. .. ..$ :Formal class &#39;Polygons&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. .. .. .. ..@ Polygons :List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Polygon&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. .. .. .. .. .. .. ..@ labpt  : num [1:2] -95.4 29.2
##   .. .. .. .. .. .. .. ..@ area   : num 51.7
##   .. .. .. .. .. .. .. ..@ hole   : logi FALSE
##   .. .. .. .. .. .. .. ..@ ringDir: int 1
##   .. .. .. .. .. .. .. ..@ coords : num [1:1482, 1:2] -94.6 -94.7 -94.7 -94.7 -94.7 ...
##   .. .. .. .. ..@ plotOrder: int 1
##   .. .. .. .. ..@ labpt    : num [1:2] -95.4 29.2
##   .. .. .. .. ..@ ID       : chr &amp;quot;0&amp;quot;
##   .. .. .. .. ..@ area     : num 51.7
##   .. ..@ plotOrder  : int 1
##   .. ..@ bbox       : num [1:2, 1:2] -100 24.9 -91 33
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
##  $ al092017_019_5day_pts:Formal class &#39;SpatialPointsDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 5 slots
##   .. ..@ data       :&#39;data.frame&#39;:	8 obs. of  23 variables:
##   .. .. ..$ ADVDATE  : chr [1:8] &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; ...
##   .. .. ..$ ADVISNUM : chr [1:8] &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; ...
##   .. .. ..$ BASIN    : chr [1:8] &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; ...
##   .. .. ..$ DATELBL  : chr [1:8] &amp;quot;10:00 PM Thu&amp;quot; &amp;quot;7:00 AM Fri&amp;quot; &amp;quot;7:00 PM Fri&amp;quot; &amp;quot;7:00 AM Sat&amp;quot; ...
##   .. .. ..$ DVLBL    : chr [1:8] &amp;quot;H&amp;quot; &amp;quot;H&amp;quot; &amp;quot;M&amp;quot; &amp;quot;M&amp;quot; ...
##   .. .. ..$ FCSTPRD  : num [1:8] 120 120 120 120 120 120 120 120
##   .. .. ..$ FLDATELBL: chr [1:8] &amp;quot;2017-08-24 7:00 PM Thu CDT&amp;quot; &amp;quot;2017-08-25 7:00 AM Fri CDT&amp;quot; &amp;quot;2017-08-25 7:00 PM Fri CDT&amp;quot; &amp;quot;2017-08-26 7:00 AM Sat CDT&amp;quot; ...
##   .. .. ..$ GUST     : num [1:8] 90 115 135 120 85 45 45 45
##   .. .. ..$ LAT      : num [1:8] 25.2 26.1 27.2 28.1 28.6 28.5 28.5 29.5
##   .. .. ..$ LON      : num [1:8] -94.6 -95.6 -96.5 -97.1 -97.3 -97.5 -97 -95
##   .. .. ..$ MAXWIND  : num [1:8] 75 95 110 100 70 35 35 35
##   .. .. ..$ MSLP     : num [1:8] 973 9999 9999 9999 9999 ...
##   .. .. ..$ SSNUM    : num [1:8] 1 2 3 3 1 0 0 0
##   .. .. ..$ STORMNAME: chr [1:8] &amp;quot;Hurricane Harvey&amp;quot; &amp;quot;Hurricane Harvey&amp;quot; &amp;quot;Hurricane Harvey&amp;quot; &amp;quot;Hurricane Harvey&amp;quot; ...
##   .. .. ..$ STORMNUM : num [1:8] 9 9 9 9 9 9 9 9
##   .. .. ..$ STORMSRC : chr [1:8] &amp;quot;Tropical Cyclone&amp;quot; &amp;quot;Tropical Cyclone&amp;quot; &amp;quot;Tropical Cyclone&amp;quot; &amp;quot;Tropical Cyclone&amp;quot; ...
##   .. .. ..$ STORMTYPE: chr [1:8] &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; &amp;quot;MH&amp;quot; &amp;quot;MH&amp;quot; ...
##   .. .. ..$ TCDVLP   : chr [1:8] &amp;quot;Hurricane&amp;quot; &amp;quot;Hurricane&amp;quot; &amp;quot;Major Hurricane&amp;quot; &amp;quot;Major Hurricane&amp;quot; ...
##   .. .. ..$ TAU      : num [1:8] 0 12 24 36 48 72 96 120
##   .. .. ..$ TCDIR    : num [1:8] 315 9999 9999 9999 9999 ...
##   .. .. ..$ TCSPD    : num [1:8] 9 9999 9999 9999 9999 ...
##   .. .. ..$ TIMEZONE : chr [1:8] &amp;quot;CDT&amp;quot; &amp;quot;CDT&amp;quot; &amp;quot;CDT&amp;quot; &amp;quot;CDT&amp;quot; ...
##   .. .. ..$ VALIDTIME: chr [1:8] &amp;quot;25/0000&amp;quot; &amp;quot;25/1200&amp;quot; &amp;quot;26/0000&amp;quot; &amp;quot;26/1200&amp;quot; ...
##   .. ..@ coords.nrs : num(0)
##   .. ..@ coords     : num [1:8, 1:2] -94.6 -95.6 -96.5 -97.1 -97.3 -97.5 -97 -95 25.2 26.1 ...
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : NULL
##   .. .. .. ..$ : chr [1:2] &amp;quot;coords.x1&amp;quot; &amp;quot;coords.x2&amp;quot;
##   .. ..@ bbox       : num [1:2, 1:2] -97.5 25.2 -94.6 29.5
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;coords.x1&amp;quot; &amp;quot;coords.x2&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
##  $ al092017_019_ww_wwlin:Formal class &#39;SpatialLinesDataFrame&#39; [package &amp;quot;sp&amp;quot;] with 4 slots
##   .. ..@ data       :&#39;data.frame&#39;:	5 obs. of  8 variables:
##   .. .. ..$ STORMNAME: chr [1:5] &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; &amp;quot;Harvey&amp;quot; ...
##   .. .. ..$ STORMTYPE: chr [1:5] &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; &amp;quot;HU&amp;quot; ...
##   .. .. ..$ ADVDATE  : chr [1:5] &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; &amp;quot;1000 PM CDT Thu Aug 24 2017&amp;quot; ...
##   .. .. ..$ ADVISNUM : chr [1:5] &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; &amp;quot;19&amp;quot; ...
##   .. .. ..$ STORMNUM : num [1:5] 9 9 9 9 9
##   .. .. ..$ FCSTPRD  : num [1:5] 120 120 120 120 120
##   .. .. ..$ BASIN    : chr [1:5] &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;AL&amp;quot; ...
##   .. .. ..$ TCWW     : chr [1:5] &amp;quot;TWA&amp;quot; &amp;quot;HWA&amp;quot; &amp;quot;TWR&amp;quot; &amp;quot;TWR&amp;quot; ...
##   .. ..@ lines      :List of 5
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] -97.7 -97.4 -97.2 24.3 25.2 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;0&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] -97.2 -97.2 -97.3 26 26.1 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;1&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] -97.2 -97.2 -97.3 26 26.1 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;2&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:10, 1:2] -95.6 -95.3 -95.1 -95.1 -94.8 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;3&amp;quot;
##   .. .. ..$ :Formal class &#39;Lines&#39; [package &amp;quot;sp&amp;quot;] with 2 slots
##   .. .. .. .. ..@ Lines:List of 1
##   .. .. .. .. .. ..$ :Formal class &#39;Line&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. .. .. .. .. ..@ coords: num [1:16, 1:2] -97.3 -97.3 -97.3 -97.4 -97.4 ...
##   .. .. .. .. ..@ ID   : chr &amp;quot;4&amp;quot;
##   .. ..@ bbox       : num [1:2, 1:2] -97.7 24.3 -94.4 29.8
##   .. .. ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;
##   .. .. .. ..$ : chr [1:2] &amp;quot;min&amp;quot; &amp;quot;max&amp;quot;
##   .. ..@ proj4string:Formal class &#39;CRS&#39; [package &amp;quot;sp&amp;quot;] with 1 slot
##   .. .. .. ..@ projargs: chr &amp;quot;+proj=longlat +a=6371200 +b=6371200 +no_defs&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We get four spatial dataframes - points, polygons and lines.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(gis)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;al092017_019_5day_lin&amp;quot; &amp;quot;al092017_019_5day_pgn&amp;quot; &amp;quot;al092017_019_5day_pts&amp;quot;
## [4] &amp;quot;al092017_019_ww_wwlin&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the expection of point spatial dataframes (which can be converted to dataframe using &lt;code&gt;tibble::as_data_frame&lt;/code&gt;, use helper function &lt;code&gt;shp_to_df&lt;/code&gt; to convert the spatial dataframes to dataframes.&lt;/p&gt;

&lt;h4 id=&#34;forecast-track&#34;&gt;Forecast Track&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)
al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_path(data = shp_to_df(gis$al092017_019_5day_lin), aes(x = long, y = lat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-21-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Use &lt;code&gt;geom_path&lt;/code&gt; instead of &lt;code&gt;geom_line&lt;/code&gt; to keep the positions in order.&lt;/p&gt;

&lt;p&gt;You can &amp;ldquo;zoom in&amp;rdquo; even further using &lt;code&gt;ggplot2::coord_equal&lt;/code&gt;. For that, we need to know the limits of our objects (minimum and maximum latitude and longitude) or bounding box. Thankfully, the &lt;code&gt;sp&lt;/code&gt; package can get us this information with the &lt;code&gt;bbox&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;But, we don&amp;rsquo;t want to use the &amp;ldquo;al092017_019_5day_lin&amp;rdquo; dataset. Our &lt;code&gt;gis&lt;/code&gt; dataset contains a forecast cone which expands well beyond the lines dataset.  Take a look:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp::bbox(gis$al092017_019_5day_lin)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##     min   max
## x -97.5 -94.6
## y  25.2  29.5
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sp::bbox(gis$al092017_019_5day_pgn)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##          min       max
## x -100.01842 -90.96327
## y   24.86433  33.01644
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, let&amp;rsquo;s get the bounding box of our forecast cone dataset and zoom in on our map.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$al092017_019_5day_pgn)
al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_path(data = shp_to_df(gis$al092017_019_5day_lin),
            aes(x = long, y = lat)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-24-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s much better. For simplicity I&amp;rsquo;m going to save the base map, &lt;code&gt;bp&lt;/code&gt;, without the line plot.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp &amp;lt;- al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;forecast-points&#34;&gt;Forecast Points&lt;/h4&gt;

&lt;p&gt;Forecast points identify each forecast position along with forecast winds and date. Remember that for point spatial dataframes you use &lt;code&gt;tibble::as_data_frame&lt;/code&gt; rather than &lt;code&gt;sp_to_df&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_point(data = tibble::as_data_frame(gis$al092017_019_5day_pts),
             aes(x = long, y = lat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you ran the code above you would get an error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error in FUN(X[[i]], ...) : object &#39;long&#39; not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Why? The variable &lt;code&gt;long&lt;/code&gt; does not exist as it does in other GIS datasets; it is &lt;code&gt;lon&lt;/code&gt;. This is one of the inconsistencies I was referring to previously. Additionally, the variables are all uppercase.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(gis$al092017_019_5day_pts)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;ADVDATE&amp;quot;   &amp;quot;ADVISNUM&amp;quot;  &amp;quot;BASIN&amp;quot;     &amp;quot;DATELBL&amp;quot;   &amp;quot;DVLBL&amp;quot;
##  [6] &amp;quot;FCSTPRD&amp;quot;   &amp;quot;FLDATELBL&amp;quot; &amp;quot;GUST&amp;quot;      &amp;quot;LAT&amp;quot;       &amp;quot;LON&amp;quot;
## [11] &amp;quot;MAXWIND&amp;quot;   &amp;quot;MSLP&amp;quot;      &amp;quot;SSNUM&amp;quot;     &amp;quot;STORMNAME&amp;quot; &amp;quot;STORMNUM&amp;quot;
## [16] &amp;quot;STORMSRC&amp;quot;  &amp;quot;STORMTYPE&amp;quot; &amp;quot;TCDVLP&amp;quot;    &amp;quot;TAU&amp;quot;       &amp;quot;TCDIR&amp;quot;
## [21] &amp;quot;TCSPD&amp;quot;     &amp;quot;TIMEZONE&amp;quot;  &amp;quot;VALIDTIME&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s try it again.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_point(data = tibble::as_data_frame(gis$al092017_019_5day_pts),
             aes(x = LON, y = LAT))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-28-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Better.&lt;/p&gt;

&lt;h4 id=&#34;forecast-cone&#34;&gt;Forecast Cone&lt;/h4&gt;

&lt;p&gt;A forecast cone identifies the probability of error in a forecast. Forecasting tropical cyclones is tricky business - errors increase the further out a forecast is issued. Theoretically, any area within a forecast cone is at risk of seeing cyclone conditions within the given period of time.&lt;/p&gt;

&lt;p&gt;Generally, a forecast cone package contains two subsets: 72-hour forecast cone and 120-hour forecast cone. This is identified in the dataset under the variable &lt;code&gt;FCSTPRD&lt;/code&gt;. Let&amp;rsquo;s take a look at the 72-hour forecast period:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_polygon(data = shp_to_df(gis$al092017_019_5day_pgn) %&amp;gt;%
                 filter(FCSTPRD == 72),
               aes(x = long, y = lat, color = FCSTPRD))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-29-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nothing there!&lt;/p&gt;

&lt;p&gt;As mentioned earlier, these are experimental products issued by the NHC and they do contain inconsistencies. To demonstrate, I&amp;rsquo;ll use Hurricane Ike advisory 42.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- gis_advisory(key = &amp;quot;AL092008&amp;quot;, advisory = 42) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_5day_lin&amp;quot;
## with 2 features
## It has 9 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_5day_pgn&amp;quot;
## with 2 features
## It has 9 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_5day_pts&amp;quot;
## with 13 features
## It has 20 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092008.042_ww_wwlin&amp;quot;
## with 5 features
## It has 10 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = shp_to_df(df$al092008_042_5day_pgn) %&amp;gt;%
                 filter(FCSTPRD == 72),
                  aes(x = long, y = lat))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-30-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We do, however, have a 120-hour forecast cone for Hurricane Harvey. Let&amp;rsquo;s go ahead and plot that.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_polygon(data = gis$al092017_019_5day_pgn,
               aes(x = long, y = lat), alpha = 0.15)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-31-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s an odd-looking forecast cone, for sure. But this demonstrates the entire area that Harvey could have potentially traveled.&lt;/p&gt;

&lt;h4 id=&#34;watches-and-warnings&#34;&gt;Watches and Warnings&lt;/h4&gt;

&lt;p&gt;Our last dataset in this package is &amp;ldquo;al092017_09_ww_wlin&amp;rdquo;. These are the current watches and warnings in effect. This is a spatial lines dataframe that needs &lt;code&gt;shp_to_df&lt;/code&gt;. Again, we use &lt;code&gt;geom_path&lt;/code&gt; instead of &lt;code&gt;geom_line&lt;/code&gt;. And we want to group our paths by the variable &lt;code&gt;TCWW&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bp +
  geom_path(data = shp_to_df(gis$al092017_019_ww_wwlin),
            aes(x = long, y = lat, group = group, color = TCWW), size = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-32-1.png&#34;  style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The paths won&amp;rsquo;t follow our coastlines exactly but you get the idea. The abbreviations don&amp;rsquo;t really give much information, either. Convert &lt;code&gt;TCWW&lt;/code&gt; to factor and provide better labels for your legend.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ww_wlin &amp;lt;- shp_to_df(gis$al092017_019_ww_wwlin)
ww_wlin$TCWW &amp;lt;- factor(ww_wlin$TCWW,
                              levels = c(&amp;quot;TWA&amp;quot;, &amp;quot;TWR&amp;quot;, &amp;quot;HWA&amp;quot;, &amp;quot;HWR&amp;quot;),
                              labels = c(&amp;quot;Tropical Storm Watch&amp;quot;,
                                         &amp;quot;Tropical Storm Warning&amp;quot;,
                                         &amp;quot;Hurricane Watch&amp;quot;,
                                         &amp;quot;Hurricane Warning&amp;quot;))

bp +
  geom_path(data = ww_wlin,
            aes(x = long, y = lat, group = group, color = TCWW), size = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-33-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;See &lt;a href=&#34;https://ropensci.github.io/rrricanes/articles/articles/forecast_advisory.html&#34;&gt;Forecast/Adivsory GIS&lt;/a&gt; on the &lt;code&gt;rrricanes&lt;/code&gt; website for an example of putting all of this data together in one map.&lt;/p&gt;

&lt;h4 id=&#34;gis-prob-storm-surge&#34;&gt;gis_prob_storm_surge&lt;/h4&gt;

&lt;p&gt;We can also plot the probablistic storm surge for given locations. Again, you will need the storm &lt;code&gt;Key&lt;/code&gt; for this function. There are two additional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;products&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;datetime&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;products&lt;/code&gt; can be one or both of &amp;ldquo;esurge&amp;rdquo; and &amp;ldquo;psurge&amp;rdquo;. esurge shows the probability of the cyclone exceeding the given storm surge plus tide within a given forecast period. psurge shows the probability of a given storm surge within a specified forecast period.&lt;/p&gt;

&lt;p&gt;One or more products may not exist depending on the cyclone and advisory.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;products&lt;/code&gt; parameter expects a list of values for each product. For esurge products, valid values are 10, 20, 30, 40 or 50. For psurge products, valid values are 0, 1, 2, &amp;hellip;, 20.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see if any esurge products exist for Harvey.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(gis_prob_storm_surge(key = key,
                            products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10))))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 150
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And psurge:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(gis_prob_storm_surge(key = key, products = list(&amp;quot;psurge&amp;quot; = 0:20)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 630
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, we have access to a ton of data here. When discussing &lt;code&gt;gis_advisory&lt;/code&gt;, we were able to filter by advisory number. With &lt;code&gt;gis_prob_storm_surge&lt;/code&gt;, this is not an option; we have to use the &lt;code&gt;datetime&lt;/code&gt; parameter to filter. Let&amp;rsquo;s find the &lt;code&gt;Date&lt;/code&gt; for advisory 19.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(d &amp;lt;- ds$fstadv %&amp;gt;% filter(Adv == 19) %&amp;gt;% pull(Date))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2017-08-25 03:00:00 UTC&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;esurge&#34;&gt;esurge&lt;/h5&gt;

&lt;p&gt;Now, let&amp;rsquo;s view all esurge products for date only (exlude time).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_prob_storm_surge(key = key,
                     products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10)),
                     datetime = strftime(d, &amp;quot;%Y%m%d&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082500.zip&amp;quot;
##  [2] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082506.zip&amp;quot;
##  [3] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082512.zip&amp;quot;
##  [4] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082518.zip&amp;quot;
##  [5] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082500.zip&amp;quot;
##  [6] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082506.zip&amp;quot;
##  [7] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082512.zip&amp;quot;
##  [8] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082518.zip&amp;quot;
##  [9] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082500.zip&amp;quot;
## [10] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082506.zip&amp;quot;
## [11] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082512.zip&amp;quot;
## [12] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082518.zip&amp;quot;
## [13] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082500.zip&amp;quot;
## [14] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082506.zip&amp;quot;
## [15] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082512.zip&amp;quot;
## [16] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082518.zip&amp;quot;
## [17] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082500.zip&amp;quot;
## [18] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082506.zip&amp;quot;
## [19] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082512.zip&amp;quot;
## [20] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082518.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s still quite a bit. We can filter it to more by adding hour to the &lt;code&gt;datetime&lt;/code&gt; parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_prob_storm_surge(key = key,
                     products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10)),
                     datetime = strftime(d, &amp;quot;%Y%m%d%H&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This call will give you an error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: No data available for requested storm/advisory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But, this isn&amp;rsquo;t entirely correct. When an advisory package is issued it contains information for the release time. Some of the GIS datasets are based on the release time -3 hours. So, we need to subtract 3 hours from &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There is an additional value that, as of the latest release is not extracted, records the position of the cyclone three hours prior. (As I understand it from the NHC, this is due to the time it takes to collect and prepare the data.) Per &lt;a href=&#34;https://github.com/ropensci/rrricanes/issues/102&#34;&gt;Issue #102&lt;/a&gt;, these values will be added for release 0.2.1. Therefore, instead of subtracting three hours from the &lt;code&gt;Date&lt;/code&gt; variable, you can simply use the &lt;code&gt;PrevPosDate&lt;/code&gt; value for this function.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try it again with the math:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_prob_storm_surge(key = key,
                     products = list(&amp;quot;esurge&amp;quot; = seq(10, 50, by = 10)),
                     datetime = strftime(d - 60 * 60 * 3, &amp;quot;%Y%m%d%H&amp;quot;,
                                         tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge10_2017082500.zip&amp;quot;
## [2] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge20_2017082500.zip&amp;quot;
## [3] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge30_2017082500.zip&amp;quot;
## [4] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge40_2017082500.zip&amp;quot;
## [5] &amp;quot;http://www.nhc.noaa.gov/gis/storm_surge/al092017_esurge50_2017082500.zip&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As I don&amp;rsquo;t want to get all of these datasets, I&amp;rsquo;ll limit my esurge to show surge values with at least a 50% chance of being exceeded:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_prob_storm_surge(key = key,
                            products = list(&amp;quot;esurge&amp;quot; = 50),
                            datetime = strftime(d - 60 * 60 * 3, &amp;quot;%Y%m%d%H&amp;quot;,
                                                tz = &amp;quot;UTC&amp;quot;)) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082500_e50&amp;quot;
## with 97 features
## It has 2 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will bring us a spatial polygon dataframe.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- shp_to_df(gis$al092017_2017082500_e50)
bb &amp;lt;- sp::bbox(gis$al092017_2017082500_e50)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	161313 obs. of  9 variables:
##  $ long   : num  -93.2 -93.2 -93.2 -93.2 -93.2 ...
##  $ lat    : num  29.9 29.9 29.9 29.9 29.9 ...
##  $ order  : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece  : Factor w/ 349 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group  : Factor w/ 7909 levels &amp;quot;0.1&amp;quot;,&amp;quot;0.2&amp;quot;,&amp;quot;0.3&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id     : chr  &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; ...
##  $ POINTID: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ TCSRG50: num  0 0 0 0 0 0 0 0 0 0 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
            aes(x = long, y = lat, group = group, color = TCSRG50)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-43-1.png&#34; title=&#34;plot of chunk unnamed-chunk-43&#34; alt=&#34;plot of chunk unnamed-chunk-43&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This plot tells us that, along the central Texas coast, the expected storm surge along with tides is greater than 7.5 feet and there is a 50% chance of this height being exceeded.&lt;/p&gt;

&lt;h4 id=&#34;psurge&#34;&gt;psurge&lt;/h4&gt;

&lt;p&gt;The psurge product gives us the probabilistic storm surge for a location within the given forecast period.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_prob_storm_surge(key = key,
                            products = list(&amp;quot;psurge&amp;quot; = 20),
                            datetime = strftime(d - 60 * 60 * 3, &amp;quot;%Y%m%d%H&amp;quot;,
                                                tz = &amp;quot;UTC&amp;quot;)) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082500_gt20&amp;quot;
## with 12 features
## It has 2 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will bring us a spatial polygon dataframe.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- shp_to_df(gis$al092017_2017082500_gt20)
bb &amp;lt;- sp::bbox(gis$al092017_2017082500_gt20)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	3293 obs. of  9 variables:
##  $ long     : num  -96.8 -96.8 -96.8 -96.8 -96.8 ...
##  $ lat      : num  28.5 28.5 28.4 28.4 28.4 ...
##  $ order    : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole     : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece    : Factor w/ 54 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group    : Factor w/ 227 levels &amp;quot;0.1&amp;quot;,&amp;quot;0.2&amp;quot;,&amp;quot;0.3&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id       : chr  &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; ...
##  $ POINTID  : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ PSurge20c: num  1 1 1 1 1 1 1 1 1 1 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
            aes(x = long, y = lat, group = group, color = PSurge20c)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-47-1.png&#34; title=&#34;plot of chunk unnamed-chunk-47&#34; alt=&#34;plot of chunk unnamed-chunk-47&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This map shows the cumulative probability that a storm surge of greater than 20 feet will be seen within the highlighted regions.&lt;/p&gt;

&lt;p&gt;This particular map doesn&amp;rsquo;t help much as we&amp;rsquo;ve zoomed in too far. What may provide use is a list of probability stations as obtained from the NHC. For this, you can use &lt;code&gt;al_prblty_stations&lt;/code&gt; (&lt;code&gt;ep_prblty_stations&lt;/code&gt; returns FALSE since, as of this writing, the format is invalid).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;stations &amp;lt;- al_prblty_stations()

al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
            aes(x = long, y = lat, group = group, color = PSurge20c)) +
  geom_label(data = stations, aes(x = Lon, y = Lat, label = Location)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-48-1.png&#34; title=&#34;plot of chunk unnamed-chunk-48&#34; alt=&#34;plot of chunk unnamed-chunk-48&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;gis-windfield&#34;&gt;gis_windfield&lt;/h4&gt;

&lt;p&gt;When possible, there may also be a cyclone wind radius dataset for the current and forecast positions. With this function we can resort back to &lt;code&gt;Key&lt;/code&gt; and an advisory number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_windfield(key = key, advisory = 19) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082503_forecastradii&amp;quot;
## with 15 features
## It has 13 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;al092017_2017082503_initialradii&amp;quot;
## with 3 features
## It has 13 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(gis)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;al092017_2017082503_forecastradii&amp;quot; &amp;quot;al092017_2017082503_initialradii&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s get the bounding box and plot our initialradii dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$al092017_2017082503_initialradii)

al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = shp_to_df(gis$al092017_2017082503_initialradii),
            aes(x = long, y = lat, group = group, fill = factor(RADII)),
            alpha = 0.5) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-51-1.png&#34; title=&#34;plot of chunk unnamed-chunk-51&#34; alt=&#34;plot of chunk unnamed-chunk-51&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And add the forecast wind radii data onto the chart (modifying &lt;code&gt;bb&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$al092017_2017082503_forecastradii)

al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = shp_to_df(gis$al092017_2017082503_initialradii),
            aes(x = long, y = lat, group = group, fill = factor(RADII)),
            alpha = 0.5) +
  geom_polygon(data = shp_to_df(gis$al092017_2017082503_forecastradii),
               aes(x = long, y = lat, group = group, fill = factor(RADII)),
               alpha = 0.5) +
  geom_label(data = stations, aes(x = Lon, y = Lat, label = Location)) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-52-1.png&#34; title=&#34;plot of chunk unnamed-chunk-52&#34; alt=&#34;plot of chunk unnamed-chunk-52&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;gis-wsp&#34;&gt;gis_wsp&lt;/h4&gt;

&lt;p&gt;Our last GIS dataset is wind speed probabilities. This dataset is not storm specific nor even basin-specific; you may get results for cyclones halfway across the world.&lt;/p&gt;

&lt;p&gt;The two parameters needed are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;datetime&lt;/code&gt; - again, using the %Y%m%d%H format (not all values are required)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;res&lt;/code&gt; - Resolution of the probabilities; 5 degrees, 0.5 degrees and 0.1 degrees.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Wind fields are for 34, 50 and 64 knots. Not all resolutions or windfields will be available at a given time.&lt;/p&gt;

&lt;p&gt;Sticking with our variable &lt;code&gt;d&lt;/code&gt;, let&amp;rsquo;s first make sure there is a dataset that exists for that time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_wsp(datetime = strftime(d - 60 * 60 * 3, format = &amp;quot;%Y%m%d%H&amp;quot;, tz = &amp;quot;UTC&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;http://www.nhc.noaa.gov/gis/&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this article, we&amp;rsquo;ll stick to the higher resolution plot.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;we need a temporarily fixed function to replace &lt;code&gt;gis_wsp()&lt;/code&gt;, which will be
fixed in package soon&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis_wsp_2 &amp;lt;- function(datetime, res = c(5, 0.5, 0.1)) {
  res &amp;lt;- as.character(res)
  res &amp;lt;- stringr::str_replace(res, &amp;quot;^5$&amp;quot;, &amp;quot;5km&amp;quot;)
  res &amp;lt;- stringr::str_replace(res, &amp;quot;^0.5$&amp;quot;, &amp;quot;halfDeg&amp;quot;)
  res &amp;lt;- stringr::str_replace(res, &amp;quot;^0.1$&amp;quot;, &amp;quot;tenthDeg&amp;quot;)
  year &amp;lt;- stringr::str_sub(datetime, 0L, 4L)
  request &amp;lt;- httr::GET(&amp;quot;http://www.nhc.noaa.gov/gis/archive_wsp.php&amp;quot;,
                       query = list(year = year))
  contents &amp;lt;- httr::content(request, as = &amp;quot;parsed&amp;quot;, encoding = &amp;quot;UTF-8&amp;quot;)
  ds &amp;lt;- rvest::html_nodes(contents, xpath = &amp;quot;//a&amp;quot;) %&amp;gt;% rvest::html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
    stringr::str_extract(&amp;quot;.+\\.zip$&amp;quot;) %&amp;gt;% .[stats::complete.cases(.)]
  if (nchar(datetime) &amp;lt; 10) {
    ptn_datetime &amp;lt;- paste0(datetime, &amp;quot;[:digit:]+&amp;quot;)
  } else {
    ptn_datetime &amp;lt;- datetime
  }
  ptn_res &amp;lt;- paste(res, collapse = &amp;quot;|&amp;quot;)
  ptn &amp;lt;- sprintf(&amp;quot;%s_wsp_[:digit:]{1,3}hr(%s)&amp;quot;, ptn_datetime,
                 ptn_res)
  links &amp;lt;- ds[stringr::str_detect(ds, ptn)]
  links &amp;lt;- paste0(&amp;quot;http://www.nhc.noaa.gov/gis/&amp;quot;, links)
  return(links)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_wsp_2(
  datetime = strftime(d - 60 * 60 * 3, format = &amp;quot;%Y%m%d%H&amp;quot;, tz = &amp;quot;UTC&amp;quot;),
  res = 5) %&amp;gt;%
  gis_download()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;2017082500_wsp34knt120hr_5km&amp;quot;
## with 11 features
## It has 1 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;2017082500_wsp50knt120hr_5km&amp;quot;
## with 11 features
## It has 1 fields
## OGR data source with driver: ESRI Shapefile
## Source: &amp;quot;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp1wYg2x&amp;quot;, layer: &amp;quot;2017082500_wsp64knt120hr_5km&amp;quot;
## with 11 features
## It has 1 fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All of these datasets are spatial polygon dataframes. Again, we will need to convert to dataframe using &lt;code&gt;shp_to_df&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bb &amp;lt;- sp::bbox(gis$`2017082500_wsp34knt120hr_5km`)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Examine the structure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- shp_to_df(gis$`2017082500_wsp34knt120hr_5km`)
str(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	24182 obs. of  8 variables:
##  $ long      : num  -97.2 -97.2 -97.2 -97.3 -97.3 ...
##  $ lat       : num  20.3 20.3 20.3 20.3 20.4 ...
##  $ order     : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole      : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece     : Factor w/ 8 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group     : Factor w/ 52 levels &amp;quot;0.1&amp;quot;,&amp;quot;0.2&amp;quot;,&amp;quot;0.3&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id        : chr  &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; ...
##  $ PERCENTAGE: chr  &amp;quot;&amp;lt;5%&amp;quot; &amp;quot;&amp;lt;5%&amp;quot; &amp;quot;&amp;lt;5%&amp;quot; &amp;quot;&amp;lt;5%&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
               aes(x = long, y = lat, group = group, fill = PERCENTAGE),
               alpha = 0.25) +
  coord_equal(xlim = c(bb[1,1], bb[1,2]),
              ylim = c(bb[2,1], bb[2,2]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-58-1.png&#34; title=&#34;plot of chunk unnamed-chunk-58&#34; alt=&#34;plot of chunk unnamed-chunk-58&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There aren&amp;rsquo;t many ways we can narrow this down other than using arbitrary longitude values. The observations in the dataset do not have a variable identifying which storm each set of values belongs to. So, I&amp;rsquo;ll remove the &lt;code&gt;coord_equal&lt;/code&gt; call so we&amp;rsquo;re only focused on the Atlantic basin.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;al_tracking_chart(color = &amp;quot;black&amp;quot;, size = 0.1, fill = &amp;quot;white&amp;quot;) +
  geom_polygon(data = df,
               aes(x = long, y = lat, group = group, fill = PERCENTAGE),
               alpha = 0.25)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-27-rrricanes/unnamed-chunk-59-1.png&#34; title=&#34;plot of chunk unnamed-chunk-59&#34; alt=&#34;plot of chunk unnamed-chunk-59&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, you can narrow it down further as you see fit.&lt;/p&gt;

&lt;p&gt;Do not confuse this GIS dataset with the &lt;code&gt;wndprb&lt;/code&gt; product or similar &lt;code&gt;prblty&lt;/code&gt; products; both of which only identify probabilities for given locations.&lt;/p&gt;

&lt;h4 id=&#34;gis-latest&#34;&gt;gis_latest&lt;/h4&gt;

&lt;p&gt;For active cyclones, you can retrieve all available GIS datasets using &lt;code&gt;gis_latest&lt;/code&gt;. Note that, unlike the previous GIS functions, this function will return a list of all GIS dataframes available.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gis &amp;lt;- gis_latest()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have a large list of GIS spatial dataframes. Two things to point out here; first, we now have a &amp;ldquo;windswath&amp;rdquo; GIS dataset. This dataset, to the best of my knowledge, does not exist on it&amp;rsquo;s own. Therefore, no archived &amp;ldquo;windswath&amp;rdquo; datasets are available.&lt;/p&gt;

&lt;p&gt;Second, I have found this data fluctuates even from minute to minute. Earlier this year when attempting to develop automated reporting, I found the return value of the call would vary almost with every call.&lt;/p&gt;

&lt;p&gt;Of course, that doesn&amp;rsquo;t mean it is not valuable, and why it has been included. You can easily perform checks for specific data you are looking for. If it doesn&amp;rsquo;t exist, bail and try again in a few minutes.&lt;/p&gt;

&lt;h3 id=&#34;potential-issues-using-rrricanes&#34;&gt;Potential Issues Using rrricanes&lt;/h3&gt;

&lt;p&gt;I cannot stress enough that &lt;code&gt;rrricanes&lt;/code&gt; &lt;strong&gt;is not intended for use during emergency situations&lt;/strong&gt;, as I myself learned &lt;a href=&#34;https://twitter.com/timtrice/status/901025869367586816&#34;&gt;during Hurricane Harvey&lt;/a&gt;. The package currently relies on the NHC website which, I truly believe, is curated by hand.&lt;/p&gt;

&lt;p&gt;The most common problems I&amp;rsquo;ve noticed are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The NHC website unable to load or slow to respond. This was a hassle in previous releases but seems to be ironed out as of release 0.2.0.6. Nonetheless, there may be instances where response time is slow.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Incorrect storm archive links. Another example would be during Harvey when the link to Harvey&amp;rsquo;s archive page was &lt;a href=&#34;https://twitter.com/PutmanSteve/status/900777826412105729&#34;&gt;listed incorrectly&lt;/a&gt;. If I manually typed the link as it should be, the storm&amp;rsquo;s correct archive page would load. However, the NHC website listed it incorrectly on the annual archives page.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As I become more aware of potential problems, I will look for workarounds. I would be greatly appreciative of any problems being posted to the &lt;a href=&#34;https://github.com/ropensci/rrricanes/issues&#34;&gt;rrricanes repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I will also post known issues beyond my control (such as NHC website issues) to Twitter using the &lt;a href=&#34;https://twitter.com/search?f=tweets&amp;amp;vertical=default&amp;amp;q=%23rrricanes&amp;amp;src=typd&#34;&gt;#rrricanes hashtag&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;future-plans&#34;&gt;Future Plans&lt;/h3&gt;

&lt;p&gt;The following data will be added to &lt;code&gt;rrricanes&lt;/code&gt; as time allows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Reconnaissance data (release 0.2.2)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Computer forecast model data (release 0.2.3)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Archived satellite images (tentative)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ship and buoy data (tentative)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reconnaissance data itself will be a massive project. There are numerous types of products. And, as advisory product formats have changed over the years, so have these. Any help in this task would be tremendously appreciated!&lt;/p&gt;

&lt;p&gt;Some computer forecast models are in the public domain and can certainly be of tremendous use. Some are difficult to find (especially archived). The caution in this area is that many websites post this data but may have limitations on how it can be accessed.&lt;/p&gt;

&lt;p&gt;Additionally, data may be added as deemed fitting.&lt;/p&gt;

&lt;h3 id=&#34;contribute&#34;&gt;Contribute&lt;/h3&gt;

&lt;p&gt;Anyone is more than welcome to contribute to the package. I would definitely appreciate any help. See &lt;a href=&#34;https://github.com/ropensci/rrricanes/blob/master/.github/CONTRIBUTING.md&#34;&gt;Contributions&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;I would ask that you follow the &lt;a href=&#34;http://style.tidyverse.org/&#34;&gt;Tidyverse style guide&lt;/a&gt;. Release 0.2.1 will fully incorporate these rules.&lt;/p&gt;

&lt;p&gt;You do not need to submit code in order to be listed as a contributor. If there is a data source (that can legally be scraped) that you feel should be added, please feel free to submit a request. Submitting bug reports and feature requests are all extremely valuable to the success of &lt;code&gt;rrricanes&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;I want to thank the &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; community for embracing &lt;code&gt;rrricanes&lt;/code&gt; and accepting the package into their vast portfolio. This is my first attempt and putting a project into part of a larger community and the lessons learned have been outstanding.&lt;/p&gt;

&lt;p&gt;I want to thank &lt;a href=&#34;https://github.com/maelle&#34;&gt;Maelle Salmon&lt;/a&gt; who, in a sense, has been like a guiding angel from start to finish during the entire onboarding and review process.&lt;/p&gt;

&lt;p&gt;I want to give a very special thanks to &lt;a href=&#34;https://github.com/robinsones&#34;&gt;Emily Robinson&lt;/a&gt; and &lt;a href=&#34;https://github.com/jsta&#34;&gt;Joseph Stachelek&lt;/a&gt; for taking the time to put &lt;code&gt;rrricanes&lt;/code&gt; to the test, giving valuable insight and recommendations on improving it.&lt;/p&gt;

&lt;p&gt;And a thank-you also to &lt;a href=&#34;https://github.com/jimmylovestea&#34;&gt;James Molyneux&lt;/a&gt;, &lt;a href=&#34;https://github.com/mpadge&#34;&gt;Mark Padgham&lt;/a&gt;, and &lt;a href=&#34;https://github.com/hrbrmstr&#34;&gt;Bob Rudis&lt;/a&gt;, all of whom have offered guidance or input that has helped make &lt;code&gt;rrricanes&lt;/code&gt; far better than it would have been on my own.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Accessing patent data with the patentsview package</title>
      <link>https://ropensci.org/blog/2017/09/19/patentsview/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/19/patentsview/</guid>
      <description>
        
        

&lt;h3 id=&#34;why-care-about-patents&#34;&gt;Why care about patents?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Patents play a critical role in incentivizing innovation, without
which we wouldn&amp;rsquo;t have much of the technology we rely on everyday&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What does your iPhone, Google&amp;rsquo;s PageRank algorithm, and a butter
substitute called Smart Balance all have in common?&lt;/p&gt;

&lt;!-- These are open source images taken from: https://pixabay.com/ --&gt;

&lt;p&gt;&lt;span&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/iphone.png&#34; width=&#34;15%&#34;&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/google.jpg&#34; width=&#34;25%&#34;&gt;
&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/butter.png&#34; width=&#34;25%&#34;&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;They all probably wouldn&amp;rsquo;t be here if not for patents. A patent
provides its owner with the ability to make money off of something that
they invented, without having to worry about someone else copying their
technology. Think Apple would spend millions of dollars developing the
iPhone if Samsung could just come along and &lt;a href=&#34;http://www.reuters.com/article/us-apple-samsung-elec-appeal-idUSKCN1271LF&#34;&gt;rip it
off&lt;/a&gt;?
Probably not.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Patents offer a great opportunity for data analysis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are two primary reasons for this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Patent data is public&lt;/strong&gt;. In return for the exclusive right to
profit off an invention, an individual/company has to publicly
disclose the details of their invention to the rest of the world.
&lt;a href=&#34;http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;amp;Sect2=HITOFF&amp;amp;p=1&amp;amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;amp;r=11&amp;amp;f=G&amp;amp;l=50&amp;amp;co1=AND&amp;amp;d=PTXT&amp;amp;s1=dog&amp;amp;OS=dog&amp;amp;RS=dog&#34;&gt;Examples of those
details&lt;/a&gt;
include the patent&amp;rsquo;s title, abstract, technology classification,
assigned organizations, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patent data can answer questions that people care about&lt;/strong&gt;.
Companies (especially big ones like IBM and Google) have a vested
interest in extracting insights from patents, and spend a lot of
time/resources trying figure out how to best manage their
intellectual property (IP) rights. They&amp;rsquo;re plagued by questions like
&amp;ldquo;who should I sell my underperforming patents to,&amp;rdquo; &amp;ldquo;which technology
areas are open to new innovations,&amp;rdquo; &amp;ldquo;what&amp;rsquo;s going to be the next big
thing in the world of buttery spreads,&amp;rdquo; etc. Patents offer a way to
provide data-driven answers to these questions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Combined, these two things make patents a prime target for data
analysis. However, until recently it was hard to get at the data inside
these documents. One had to either collect it manually using the
official &lt;a href=&#34;https://en.wikipedia.org/wiki/United_States_Patent_and_Trademark_Office&#34;&gt;United States Patent and Trademark
Office&lt;/a&gt;
(USPTO) &lt;a href=&#34;http://patft.uspto.gov/netahtml/PTO/search-adv.htm&#34;&gt;search
engine&lt;/a&gt;, or figure
out a way to download, parse, and model huge XML data dumps. Enter
PatentsView.&lt;/p&gt;

&lt;h3 id=&#34;patentsview-and-the-patentsview-package&#34;&gt;PatentsView and the &lt;code&gt;patentsview&lt;/code&gt; package&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.patentsview.org/web/#viz/relationships&#34;&gt;PatentsView&lt;/a&gt; is one
of USPTO&amp;rsquo;s new initiatives intended to increase the usability and value
of patent data. One feature of this project is a publicly accessible API
that makes it easy to programmatically interact with the data. A few of
the reasons why I like the API (and PatentsView more generally):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The API is free (no credential required) and currently doesn&amp;rsquo;t
impose rate limits/bandwidth throttling.&lt;/li&gt;
&lt;li&gt;The project offers &lt;a href=&#34;http://www.patentsview.org/download/&#34;&gt;bulk downloads of patent
data&lt;/a&gt; on their website (in a
flat file format), for those who want to be closest to the data.&lt;/li&gt;
&lt;li&gt;Both the API and the bulk download data contain disambiguated
entities such as inventors, assignees, organizations, etc. In other
words, the API will tell you whether it thinks that John Smith on
patent X is the same person as John Smith on patent Y. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;patentsview&lt;/code&gt; R package is a wrapper around the PatentsView API. It
contains a function that acts as a client to the API (&lt;code&gt;search_pv()&lt;/code&gt;) as
well as several supporting functions. Full documentation of the package
can be found on its
&lt;a href=&#34;https://ropensci.github.io/patentsview/index.html&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;

&lt;p&gt;You can install the stable version of &lt;code&gt;patentsview&lt;/code&gt; from CRAN:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;patentsview&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or get the development version from GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (!require(devtools)) install.packages(&amp;quot;devtools&amp;quot;)

devtools::install_github(&amp;quot;ropensci/patentsview&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;getting-started&#34;&gt;Getting started&lt;/h3&gt;

&lt;p&gt;The package has one main function, &lt;code&gt;search_pv()&lt;/code&gt;, that makes it easy to
send requests to the API. There are two parameters to &lt;code&gt;search_pv()&lt;/code&gt; that
you&amp;rsquo;re going to want to think about just about every time you call it -
&lt;code&gt;query&lt;/code&gt; and &lt;code&gt;fields&lt;/code&gt;. You tell the API how you want to filter the patent
data with &lt;code&gt;query&lt;/code&gt;, and which fields you want to retrieve with
&lt;code&gt;fields&lt;/code&gt;. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h4 id=&#34;query&#34;&gt;&lt;code&gt;query&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Your query has to use the &lt;a href=&#34;http://www.patentsview.org/api/query-language.html&#34;&gt;PatentsView query
language&lt;/a&gt;, which is
a JSON-based syntax that is similar to the one used by Lucene. You can
write the query directly and pass it as a string to &lt;code&gt;search_pv()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(patentsview)

qry_1 &amp;lt;- &#39;{&amp;quot;_gt&amp;quot;:{&amp;quot;patent_year&amp;quot;:2007}}&#39;
search_pv(query = qry_1, fields = NULL) # This will retrieve a default set of fields
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_id    : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_number: chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_title : chr [1:25] &amp;quot;Sealing device for body suit and sealin&amp;quot;..
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;Or you can use the domain specific language (DSL) provided in the
&lt;code&gt;patentsview&lt;/code&gt; package to help you write the query:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;qry_2 &amp;lt;- qry_funs$gt(patent_year = 2007) # All DSL functions are in the qry_funs list
qry_2 # qry_2 is the same as qry_1
#&amp;gt; {&amp;quot;_gt&amp;quot;:{&amp;quot;patent_year&amp;quot;:2007}}

search_pv(query = qry_2)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_id    : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_number: chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_title : chr [1:25] &amp;quot;Sealing device for body suit and sealin&amp;quot;..
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;qry_1&lt;/code&gt; and &lt;code&gt;qry_2&lt;/code&gt; will result in the same HTTP call to the API. Both
queries search for patents in USPTO that were published after 2007.
There are three gotchas to look out for when writing a query:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Field is queryable.&lt;/strong&gt; The API has 7 endpoints (the default
endpoint is &amp;ldquo;patents&amp;rdquo;), and each endpoint has its own set of fields
that you can filter on. &lt;em&gt;The fields that you can filter on are not
necessarily the same as the ones that you can retrieve.&lt;/em&gt; In other
words, the fields that you can include in &lt;code&gt;query&lt;/code&gt; (e.g.,
&lt;code&gt;patent_year&lt;/code&gt;) are not necessarily the same as those that you can
include in &lt;code&gt;fields&lt;/code&gt;. To see which fields you can query on, look in
the &lt;code&gt;fieldsdf&lt;/code&gt; data frame (&lt;code&gt;View(patentsview::fieldsdf)&lt;/code&gt;) for fields
that have a &amp;ldquo;y&amp;rdquo; indicator in their &lt;code&gt;can_query&lt;/code&gt; column for your given
endpoint.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Correct data type for field.&lt;/strong&gt; If you&amp;rsquo;re filtering on a field in
your query, you have to make sure that the value you are filtering
on is consistent with the field&amp;rsquo;s data type. For example,
&lt;code&gt;patent_year&lt;/code&gt; has type &amp;ldquo;integer,&amp;rdquo; so if you pass 2007 as a string
then you&amp;rsquo;re going to get an error (&lt;code&gt;patent_year = 2007&lt;/code&gt; is good,
&lt;code&gt;patent_year = &amp;quot;2007&amp;quot;&lt;/code&gt; is no good). You can find a field&amp;rsquo;s data type
in the &lt;code&gt;fieldsdf&lt;/code&gt; data frame.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comparison function works with field&amp;rsquo;s data type.&lt;/strong&gt; The comparison
function(s) that you use (e.g., the greater-than function shown
above, &lt;code&gt;qry_funs$gt()&lt;/code&gt;) must be consistent with the field&amp;rsquo;s data
type. For example, you can&amp;rsquo;t use the &amp;ldquo;contains&amp;rdquo; function on fields
of type &amp;ldquo;integer&amp;rdquo; (&lt;code&gt;qry_funs$contains(patent_year = 2007)&lt;/code&gt; will
throw an error). See &lt;code&gt;?qry_funs&lt;/code&gt; for more details.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In short, use the &lt;code&gt;fieldsdf&lt;/code&gt; data frame when you write a query and you
should be fine. Check out the &lt;a href=&#34;https://ropensci.github.io/patentsview/articles/writing-queries.html&#34;&gt;writing queries
vignette&lt;/a&gt;
for more details.&lt;/p&gt;

&lt;h4 id=&#34;fields&#34;&gt;&lt;code&gt;fields&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Up until now we have been using the default value for &lt;code&gt;fields&lt;/code&gt;. This
results in the API giving us some small set of default fields. Let&amp;rsquo;s see
about retrieving some more fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;search_pv(
  query = qry_funs$gt(patent_year = 2007),
  fields = c(&amp;quot;patent_abstract&amp;quot;, &amp;quot;patent_average_processing_time&amp;quot;,
             &amp;quot;inventor_first_name&amp;quot;, &amp;quot;inventor_total_num_patents&amp;quot;)
)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame (with list column(s) inside) on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  3 variables:
#&amp;gt;   ..$ patent_abstract               : chr [1:25] &amp;quot;A sealing device for a&amp;quot;..
#&amp;gt;   ..$ patent_average_processing_time: chr [1:25] &amp;quot;1324&amp;quot; ...
#&amp;gt;   ..$ inventors                     :List of 25
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The fields that you can retrieve depends on the endpoint that you are
hitting. We&amp;rsquo;ve been using the &amp;ldquo;patents&amp;rdquo; endpoint thus far, so all of
these are retrievable:
&lt;code&gt;fieldsdf[fieldsdf$endpoint == &amp;quot;patents&amp;quot;, &amp;quot;field&amp;quot;]&lt;/code&gt;. You can also use
&lt;code&gt;get_fields()&lt;/code&gt; to list the retrievable fields for a given endpoint:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;search_pv(
  query = qry_funs$gt(patent_year = 2007),
  fields = get_fields(endpoint = &amp;quot;patents&amp;quot;, groups = c(&amp;quot;patents&amp;quot;, &amp;quot;inventors&amp;quot;))
)
#&amp;gt; $data
#&amp;gt; #### A list with a single data frame (with list column(s) inside) on the patent data level:
#&amp;gt;
#&amp;gt; List of 1
#&amp;gt;  $ patents:&#39;data.frame&#39;: 25 obs. of  31 variables:
#&amp;gt;   ..$ patent_abstract                       : chr [1:25] &amp;quot;A sealing devi&amp;quot;..
#&amp;gt;   ..$ patent_average_processing_time        : chr [1:25] &amp;quot;1324&amp;quot; ...
#&amp;gt;   ..$ patent_date                           : chr [1:25] &amp;quot;2008-01-01&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_city       : chr [1:25] &amp;quot;Cambridge&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_country    : chr [1:25] &amp;quot;US&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_id         : chr [1:25] &amp;quot;b9fc6599e3d60c&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_assignee_latitude   : chr [1:25] &amp;quot;42.3736&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_location_id: chr [1:25] &amp;quot;42.3736158|-71&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_assignee_longitude  : chr [1:25] &amp;quot;-71.1097&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_assignee_state      : chr [1:25] &amp;quot;MA&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_city       : chr [1:25] &amp;quot;Lucca&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_country    : chr [1:25] &amp;quot;IT&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_id         : chr [1:25] &amp;quot;6416028-3&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_latitude   : chr [1:25] &amp;quot;43.8376&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_location_id: chr [1:25] &amp;quot;43.8376211|10.&amp;quot;..
#&amp;gt;   ..$ patent_firstnamed_inventor_longitude  : chr [1:25] &amp;quot;10.4951&amp;quot; ...
#&amp;gt;   ..$ patent_firstnamed_inventor_state      : chr [1:25] &amp;quot;Tuscany&amp;quot; ...
#&amp;gt;   ..$ patent_id                             : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_kind                           : chr [1:25] &amp;quot;B1&amp;quot; ...
#&amp;gt;   ..$ patent_number                         : chr [1:25] &amp;quot;7313829&amp;quot; ...
#&amp;gt;   ..$ patent_num_cited_by_us_patents        : chr [1:25] &amp;quot;5&amp;quot; ...
#&amp;gt;   ..$ patent_num_claims                     : chr [1:25] &amp;quot;25&amp;quot; ...
#&amp;gt;   ..$ patent_num_combined_citations         : chr [1:25] &amp;quot;35&amp;quot; ...
#&amp;gt;   ..$ patent_num_foreign_citations          : chr [1:25] &amp;quot;0&amp;quot; ...
#&amp;gt;   ..$ patent_num_us_application_citations   : chr [1:25] &amp;quot;0&amp;quot; ...
#&amp;gt;   ..$ patent_num_us_patent_citations        : chr [1:25] &amp;quot;35&amp;quot; ...
#&amp;gt;   ..$ patent_processing_time                : chr [1:25] &amp;quot;792&amp;quot; ...
#&amp;gt;   ..$ patent_title                          : chr [1:25] &amp;quot;Sealing device&amp;quot;..
#&amp;gt;   ..$ patent_type                           : chr [1:25] &amp;quot;utility&amp;quot; ...
#&amp;gt;   ..$ patent_year                           : chr [1:25] &amp;quot;2008&amp;quot; ...
#&amp;gt;   ..$ inventors                             :List of 25
#&amp;gt;
#&amp;gt; $query_results
#&amp;gt; #### Distinct entity counts across all downloadable pages of output:
#&amp;gt;
#&amp;gt; total_patent_count = 100,000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s look at a quick example of pulling and analyzing patent data.
We&amp;rsquo;ll look at patents from the last ten years that are classified below
the &lt;a href=&#34;https://worldwide.espacenet.com/classification#!/CPC=H04L63/02&#34;&gt;H04L63/00 CPC
code&lt;/a&gt;.
Patents in this area relate to &amp;ldquo;network architectures or network
communication protocols for separating internal from external
traffic.&amp;rdquo; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; CPC codes offer a quick and dirty way to find patents of
interest, though getting a sense of their hierarchy can be tricky.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download the data&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(patentsview)

# Write a query:
query &amp;lt;- with_qfuns( # with_qfuns is basically just: with(qry_funs, ...)
  and(
    begins(cpc_subgroup_id = &#39;H04L63/02&#39;),
    gte(patent_year = 2007)
  )
)

# Create a list of fields:
fields &amp;lt;- c(
  c(&amp;quot;patent_number&amp;quot;, &amp;quot;patent_year&amp;quot;),
  get_fields(endpoint = &amp;quot;patents&amp;quot;, groups = c(&amp;quot;assignees&amp;quot;, &amp;quot;cpcs&amp;quot;))
)

# Send HTTP request to API&#39;s server:
pv_res &amp;lt;- search_pv(query = query, fields = fields, all_pages = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;See where the patents are coming from (geographically)&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(leaflet)
library(htmltools)
library(dplyr)
library(tidyr)

data &amp;lt;-
  pv_res$data$patents %&amp;gt;%
    unnest(assignees) %&amp;gt;%
    select(assignee_id, assignee_organization, patent_number,
           assignee_longitude, assignee_latitude) %&amp;gt;%
    group_by_at(vars(-matches(&amp;quot;pat&amp;quot;))) %&amp;gt;%
    mutate(num_pats = n()) %&amp;gt;%
    ungroup() %&amp;gt;%
    select(-patent_number) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(popup = paste0(&amp;quot;&amp;lt;font color=&#39;Black&#39;&amp;gt;&amp;quot;,
                          htmlEscape(assignee_organization), &amp;quot;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Patents:&amp;quot;,
                          num_pats, &amp;quot;&amp;lt;/font&amp;gt;&amp;quot;)) %&amp;gt;%
    mutate_at(vars(matches(&amp;quot;_l&amp;quot;)), as.numeric) %&amp;gt;%
    filter(!is.na(assignee_id))

leaflet(data) %&amp;gt;%
  addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %&amp;gt;%
  addCircleMarkers(lng = ~assignee_longitude, lat = ~assignee_latitude,
                   popup = ~popup, ~sqrt(num_pats), color = &amp;quot;yellow&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Plot the growth of the field&amp;rsquo;s topics over time&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- --&gt;

&lt;pre&gt;&lt;code&gt;library(ggplot2)
library(RColorBrewer)

data &amp;lt;-
  pv_res$data$patents %&amp;gt;%
    unnest(cpcs) %&amp;gt;%
    filter(cpc_subgroup_id != &amp;quot;H04L63/02&amp;quot;) %&amp;gt;% # remove patents categorized into only top-level category of H04L63/02
    mutate(
      title = case_when(
        grepl(&amp;quot;filtering&amp;quot;, .$cpc_subgroup_title, ignore.case = T) ~
          &amp;quot;Filtering policies&amp;quot;,
        .$cpc_subgroup_id %in% c(&amp;quot;H04L63/0209&amp;quot;, &amp;quot;H04L63/0218&amp;quot;) ~
          &amp;quot;Architectural arrangements&amp;quot;,
        grepl(&amp;quot;Firewall traversal&amp;quot;, .$cpc_subgroup_title, ignore.case = T) ~
          &amp;quot;Firewall traversal&amp;quot;,
        TRUE ~
          .$cpc_subgroup_title
      )
    ) %&amp;gt;%
    mutate(title = gsub(&amp;quot;.*(?=-)-&amp;quot;, &amp;quot;&amp;quot;, title, perl = TRUE)) %&amp;gt;%
    group_by(title, patent_year) %&amp;gt;%
    count() %&amp;gt;%
    ungroup() %&amp;gt;%
    mutate(patent_year = as.numeric(patent_year))

ggplot(data = data) +
  geom_smooth(aes(x = patent_year, y = n, colour = title), se = FALSE) +
  scale_x_continuous(&amp;quot;\nPublication year&amp;quot;, limits = c(2007, 2016),
                     breaks = 2007:2016) +
  scale_y_continuous(&amp;quot;Patents\n&amp;quot;, limits = c(0, 700)) +
  scale_colour_manual(&amp;quot;&amp;quot;, values = brewer.pal(5, &amp;quot;Set2&amp;quot;)) +
  theme_bw() + # theme inspired by https://hrbrmstr.github.io/hrbrthemes/
  theme(panel.border = element_blank(), axis.ticks = element_blank())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ropensci.org/assets/blog-images/2017-09-19-patentsview/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;learning-more&#34;&gt;Learning more&lt;/h3&gt;

&lt;p&gt;For analysis examples that go into a little more depth, check out the
&lt;a href=&#34;https://ropensci.github.io/patentsview/articles/citation-networks.html&#34;&gt;data applications
vignettes&lt;/a&gt;
on the package&amp;rsquo;s website. If you&amp;rsquo;re just interested in &lt;code&gt;search_pv()&lt;/code&gt;,
there are
&lt;a href=&#34;https://ropensci.github.io/patentsview/articles/examples.html&#34;&gt;examples&lt;/a&gt;
on the site for that as well. To contribute to the package or report an
issue, check out the &lt;a href=&#34;https://github.com/ropensci/patentsview/issues&#34;&gt;issues page on
GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;d like to thank the package&amp;rsquo;s two reviewers, &lt;a href=&#34;https://github.com/poldham&#34;&gt;Paul
Oldham&lt;/a&gt; and &lt;a href=&#34;http://blog.haunschmid.name/&#34;&gt;Verena
Haunschmid&lt;/a&gt;, for taking the time to review
the package and providing helpful feedback. I&amp;rsquo;d also like to thank
&lt;a href=&#34;http://www.masalmon.eu/&#34;&gt;Maëlle Salmon&lt;/a&gt; for shepherding the package
along the rOpenSci review process, as well &lt;a href=&#34;https://scottchamberlain.info/&#34;&gt;Scott
Chamberlain&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/stefaniebutland&#34;&gt;Stefanie
Butland&lt;/a&gt; for their miscellaneous
help.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;This is both good and bad, as there are errors in the disambiguation. The algorithm that is responsible for the disambiguation was created by the winner of the &lt;a href=&#34;http://www.patentsview.org/workshop/&#34;&gt;PatentsView Inventor Disambiguation Technical Workshop&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;These two parameters end up getting translated into a MySQL query by the API&amp;rsquo;s server, which then gets sent to a back-end database. &lt;code&gt;query&lt;/code&gt; and &lt;code&gt;fields&lt;/code&gt; are used to create the query&amp;rsquo;s &lt;code&gt;WHERE&lt;/code&gt; and &lt;code&gt;SELECT&lt;/code&gt; clauses, respectively.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;There is a slightly more in-depth definition that says that these are patents &amp;ldquo;related to the (logical) separation of traffic/(sub-) networks to achieve protection.&amp;rdquo;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;↩&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>rOpenSci Software Review: Always Improving</title>
      <link>https://ropensci.org/blog/2017/09/11/software-review-update/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/11/software-review-update/</guid>
      <description>
        
        

&lt;p&gt;The R package ecosystem now contains more than 10K packages, and several flagship packages belong under the rOpenSci suite. Some of these are: &lt;a href=&#34;https://github.com/ropensci/magick&#34;&gt;magick&lt;/a&gt; for image manipulation, &lt;a href=&#34;https://github.com/ropensci/plotly&#34;&gt;plotly&lt;/a&gt; for interactive plots, and &lt;a href=&#34;https://github.com/ropensci/git2r&#34;&gt;git2r&lt;/a&gt; for interacting with &lt;code&gt;git&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;rOpenSci is a community of people making software to facilitate open and reproducible science/research. While the rOpenSci team continues to develop and maintain core infrastructure packages, an increasing number of packages in our suite are contributed by members of the extended R community.&lt;/p&gt;

&lt;p&gt;In the early days we accepted contributions to our suite without any formal process for submission or acceptance. When someone wanted to contribute software to our collection, and we could envision scientific applications, we just moved it aboard. But as our community and codebase grew, we began formalizing standards and processes to control quality. This is what became our peer review process.  You can read more about it in our recent &lt;a href=&#34;https://www.numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As our submissions have grown over the past couple of years, our standards around peer review have also changed and continue to evolve in response to changing community needs and updates to the R development infrastructure.&lt;/p&gt;

&lt;p&gt;Although a large number of packages submitted to CRAN could also be part of rOpenSci, our submissions are limited to packages that fit our mission and are able to pass a stringent and time intensive review process.&lt;/p&gt;

&lt;p&gt;Here, we summarize some of the more important changes to peer review at rOpenSci over the past year.  The most recent information can always be found at &lt;a href=&#34;https://onboarding.ropensci.org/&#34;&gt;https://onboarding.ropensci.org/&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;we-ve-expanded-our-scope&#34;&gt;We&amp;rsquo;ve Expanded Our Scope&lt;/h3&gt;

&lt;p&gt;Our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#aims-and-scope&#34;&gt;Aims and Scope&lt;/a&gt; document what types of packages we accept from community contributors. The scope emerges from three main guidelines. First, we accept packages that fit our mission of enabling open and reproducible research. Second, we only accept packages that we feel our editors and community of reviewers are competent to review. Third, we accept packages for which we can reasonably endorse as improving on existing solutions.  In practice, we don&amp;rsquo;t accept  general packages. That&amp;rsquo;s why, for instance, our &amp;ldquo;data munging&amp;rdquo; category only applies to packages designed to work with specific scientific data types.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve refined our focal areas from&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data retrieval&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data visualization&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data deposition&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data munging&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reproducibility&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data retrieval&lt;/strong&gt; - packages for accessing and downloading data from online sources with scientific applications&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data deposition&lt;/strong&gt; - packages that support deposition of data into research repositories, including data formatting and metadata generation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data munging&lt;/strong&gt; - packages for processing data from formats mentioned above&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data extraction&lt;/strong&gt; - packages that aid in retrieving data from unstructured sources such as text, images and PDFs, as well as parsing scientific data types and outputs from scientific equipment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;database access&lt;/strong&gt; - bindings and wrappers for generic database APIs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reproducibility&lt;/strong&gt; - tools that facilitate reproducible research&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;geospatial data&lt;/strong&gt; - accessing geospatial data, manipulating geospatial data, and converting between geospatial data formats&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;text analysis&lt;/strong&gt; (pilot) - we are piloting a sub-specialty area for text analysis which includes implementation of statistical/ML methods for analyzing or extracting text data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You will note that we&amp;rsquo;ve removed data visualization. We&amp;rsquo;ve had some truly excellent data visualization packages come aboard, starting with &lt;a href=&#34;https://github.com/ropensci/plotly&#34;&gt;plotly&lt;/a&gt;.  But since then we&amp;rsquo;ve found data visualization is too general a field for us to confidently evaluate, and at this point have dropped it from our main categories.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve also added geospatial and text analysis as areas where we accept packages that might seem more general or methods-y than we would otherwise.  These are areas where we&amp;rsquo;ve built, among our staff and reviewers, topic-specific expertise.&lt;/p&gt;

&lt;p&gt;Given that we accept packages that improve on existing solutions, in practice we generally avoid accepting more than one package of similar scope. We&amp;rsquo;ve also added &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/policies.md#package-overlap&#34;&gt;clarifying language&lt;/a&gt; about what this entails and how we define overlap with other packages.&lt;/p&gt;

&lt;p&gt;We now strongly encourage &lt;a href=&#34;https://github.com/ropensci/onboarding/issues?q=is%3Aissue+label%3A0%2Fpresubmission&#34;&gt;pre-submission inquiries&lt;/a&gt; to quickly assess whether the package falls into scope. Some of these lead to suggesting the person submit their package, while others are determined out-of-scope. This reduces effort on all sides for packages that be out-of-scope. Many authors do this prior to completing their package so they can decide whether to tailor their development process to rOpenSci.&lt;/p&gt;

&lt;p&gt;To see examples of what has recently been determined to be out-of-scope, see the &lt;a href=&#34;https://github.com/ropensci/onboarding/issues?q=is%3Aissue+is%3Aclosed+label%3Aout-of-scope&#34;&gt;out-of-scope label&lt;/a&gt; in the onboarding repository.&lt;/p&gt;

&lt;p&gt;As always, we&amp;rsquo;d like to emphasize that even when packages are out-of-scope, we&amp;rsquo;re very grateful that authors consider an rOpenSci submission!&lt;/p&gt;

&lt;h3 id=&#34;standards-changes&#34;&gt;Standards changes&lt;/h3&gt;

&lt;p&gt;Our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/packaging_guide.md&#34;&gt;packaging guide&lt;/a&gt; contains both recommended and required best practices. They evolve continually as our community reaches consensus on best practices that we want to encourage and standardize. Here are some of the changes we&amp;rsquo;ve incorporated in the past year.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We now encourage using a &lt;code&gt;object_verb()&lt;/code&gt; function naming scheme to avoid namespace conflicts.&lt;/li&gt;
&lt;li&gt;We now encourage functions to facilitate piping workflows if possible. We don&amp;rsquo;t have an official stance on using pipes in a package.&lt;/li&gt;
&lt;li&gt;We&amp;rsquo;ve clarified and filled out README recommendations&lt;/li&gt;
&lt;li&gt;Documentation: we now recommend inclusion of a package level manual file, and at least one vignette.&lt;/li&gt;
&lt;li&gt;Testing: we clarify that packages should pass &lt;code&gt;devtools::check()&lt;/code&gt; on all major platforms, that each package should have a test suite that covers all major functionality.&lt;/li&gt;
&lt;li&gt;Continuous integration (CI): we now require that packages with compiled code need to run continuous integration on all major platforms; integrate reporting of test coverage; include README badges of CI and coverage.&lt;/li&gt;
&lt;li&gt;We&amp;rsquo;ve clarified our recommended scaffolding suggestions around XML to be more nuanced. Briefly, we recommend the &lt;code&gt;xml2&lt;/code&gt; package in general, but &lt;code&gt;XML&lt;/code&gt; package may be needed in some cases.&lt;/li&gt;
&lt;li&gt;We added a section on CRAN gotchas to help package maintainers avoid common pitfalls encountered during CRAN submission.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Standards changes often take place because we find that both editors and reviewers are making the same recommendations on multiple packages.  Other requirements are added as good practices become accessible to the broader community. For instance, CI and code coverage reporting have gone from recommended to required as the tooling and documentation/tutorials for these have made them more accessible.&lt;/p&gt;

&lt;h3 id=&#34;process-changes&#34;&gt;Process changes&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Editors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the pace of package submissions increases, we&amp;rsquo;ve expanded our editorial team to keep up. &lt;a href=&#34;http://www.masalmon.eu/&#34;&gt;Maëlle Salmon&lt;/a&gt; joined us in February, bringing our &lt;a href=&#34;https://github.com/ropensci/onboarding#-editors-and-reviewers&#34;&gt;team to four&lt;/a&gt;. With four, we need to be more coordinated, so we&amp;rsquo;ve moved to a system of a rotating editor-in-chief, who makes decisions about scope, assigns handling editors, and brings up edge cases for discussion with the whole team.&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Welcome &lt;a href=&#34;https://twitter.com/ma_salmon&#34;&gt;@ma_salmon&lt;/a&gt; to our editorial team for open peer review of &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&#34;&gt;#rstats&lt;/a&gt; software &lt;a href=&#34;https://t.co/KsL0SF1b6K&#34;&gt;https://t.co/KsL0SF1b6K&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;&lt;/p&gt;&amp;mdash; rOpenSci (@rOpenSci) &lt;a href=&#34;https://twitter.com/rOpenSci/status/832228045587099649&#34;&gt;February 16, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The process our editors follow is summarized in our &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/editors_guide.md&#34;&gt;editors&amp;rsquo; guide&lt;/a&gt;, which will help bring editors up to speed when we further expand our team.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As submissions increase, the entire process benefits more from automation. Right now most steps of the review system are manual - we aim to automate as much as possible. Here&amp;rsquo;s a few things we&amp;rsquo;re doing or planning on:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;With every package submission, we run &lt;a href=&#34;https://github.com/MangoTheCat/goodpractice&#34;&gt;goodpractice&lt;/a&gt; on the package to highlight common issues. We do this manually right now, but we&amp;rsquo;re working on an automated system (aka, bot) for automatically running &lt;code&gt;goodpractice&lt;/code&gt; on submissions and reporting back to the issue. Other rOpenSci specific checks, e.g., checking rOpenSci policies, are likely to be added in to this system.&lt;/li&gt;
&lt;li&gt;Reminders: Some readers that have reviewed for rOpenSci may remember the bot that would remind you to get your review in. We&amp;rsquo;ve disabled it for now - but will likely bring it back online soon. Right now, editors do these reminders manually.&lt;/li&gt;
&lt;li&gt;On approval, packages go through a number of housekeeping steps to ensure a smooth transfer. Eventually we&amp;rsquo;d like to automate this process.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Other changes&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://joss.theoj.org/&#34;&gt;JOSS&lt;/a&gt; harmonization/co-submission: For authors wishing to submit their software papers to the Journal of Open Source Software after acceptance, we have also begun streamlining the process. Editors check to make sure that the paper clearly states the scientific application, includes a separate &lt;code&gt;.bib&lt;/code&gt; file and that the accepted version of the software is deposited at Zenodo or Figshare with a DOI. Having these steps completed allows for a fast track acceptance at JOSS.&lt;/li&gt;
&lt;li&gt;Reviewer template and guide: We now have a &lt;a href=&#34;https://github.com/ropensci/onboarding/blob/master/reviewer_template.md&#34;&gt;reviewer template&lt;/a&gt; - making reviews more standardized, and helping reviewers know what to look for. In addition, we have an updated reviewer guide that gives high level guidance, as well as specific things to look for, tools to use, and examples of good reviews. In addition, the guide gives guidance on how to submit reviews.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Badges: We now have badges for rOpenSci review. The badges show whether a package is under review or has been approved. Packages that are undergoing review or have been approved can put this badge in their README.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ropensci/onboarding/issues/86&#34;&gt;&lt;img src=&#34;http://badges.ropensci.org/86_status.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/ropensci/onboarding/issues/116&#34;&gt;&lt;img src=&#34;http://badges.ropensci.org/116_status.svg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Get in touch with us on Twitter (&lt;a href=&#34;https://twitter.com/ropensci&#34;&gt;@ropensci&lt;/a&gt;, or in the comments) if you have any questions or thoughts about our software review policies, scope, or processes.&lt;/p&gt;

&lt;p&gt;To find out more about our software review process join us on the next &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/31/comm-call-v14&#34;&gt;rOpenSci Community Call&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We hope to see you soon in the onboarding repository as a &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/new&#34;&gt;submitter&lt;/a&gt; or as a &lt;a href=&#34;https://ropensci.org/onboarding/&#34;&gt;reviewer&lt;/a&gt;!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Experiences as a first time rOpenSci package reviewer</title>
      <link>https://ropensci.org/blog/2017/09/08/first-review-experiences/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/08/first-review-experiences/</guid>
      <description>
        
        

&lt;p&gt;It all started January 26&lt;sup&gt;th&lt;/sup&gt; this year when I signed up to volunteer as
a reviewer for R packages submitted to rOpenSci. My main motivation for
wanting to volunteer was to learn something new and to
contribute to the R open source community. If you are wondering why the
people behind rOpenSci are doing this, you can read &lt;a href=&#34;https://www.numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science/&#34;&gt;How rOpenSci uses Code Review to Promote Reproducible Science&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Three months later I was contacted by &lt;a href=&#34;https://twitter.com/masalmon&#34;&gt;Maelle Salmon&lt;/a&gt; asking whether I was interested in
reviewing the R package &lt;a href=&#34;https://github.com/ropensci/patentsview&#34;&gt;&lt;code&gt;patentsview&lt;/code&gt;&lt;/a&gt; for rOpenSci. And yes, I
was! To be honest I was a little bit thrilled.&lt;/p&gt;

&lt;p&gt;The packages are submitted for review to rOpenSci via an issue to their
GitHub repository and also the reviews happen there. So you can check out
&lt;a href=&#34;https://github.com/ropensci/onboarding/issues&#34;&gt;all previous package submissions and reviews&lt;/a&gt;.
With all the information you
get from rOpenSci and also the help from the editor it is straightforward
to do the package review. Before I started I read the
reviewer guides (links below) and checked out a few of the existing
reviews. I installed the package &lt;code&gt;patentsview&lt;/code&gt; from GitHub and also
downloaded the source code so I could check out how it was implemented.&lt;/p&gt;

&lt;p&gt;I started by testing core functionality of the package by
running the examples that were mentioned in the README of the
package. I think this is a good
starting point because you get a feeling of what the author wants to
achieve with the package. Later on I came up with my
own queries (side note: this R package interacts with an API from which
you can query patents). During the process I used to switch between
writing queries like a normal user of the package
would do and checking the code. When I saw something in the code that
wasn&amp;rsquo;t quite clear to me or looked wrong I went back to writing new
queries to check whether the behavior of the methods was as expected.&lt;/p&gt;

&lt;p&gt;With this approach I was able to give feedback to the package author
which led to the inclusion of an additional unit test, a helper function
that makes the package easier to use, clarification of an error message
and an improved documentation. You can find the review I did &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/112&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are several R packages that helped me get started with my review,
e.g. &lt;a href=&#34;https://github.com/hadley/devtools&#34;&gt;&lt;code&gt;devtools&lt;/code&gt;&lt;/a&gt; and
&lt;a href=&#34;https://github.com/MangoTheCat/goodpractice&#34;&gt;&lt;code&gt;goodpractice&lt;/code&gt;&lt;/a&gt;. These
packages can also help you when you start writing your own packages. An
example for a very useful method is &lt;code&gt;devtools::spell_check()&lt;/code&gt;, which
performs a spell check on the package description and on manual pages.
At the beginning I had an issue with &lt;code&gt;goodpractice::gp()&lt;/code&gt; but Maelle Salmon
(the editor) helped me resolve it.&lt;/p&gt;

&lt;p&gt;In the rest of this article you can read what I gained personally from doing a
review.&lt;/p&gt;

&lt;h3 id=&#34;contributing-to-the-open-source-community&#34;&gt;Contributing to the open source community&lt;/h3&gt;

&lt;p&gt;When people think about contributing to the open source community, the
first thought is about creating a new R package or contributing to one
of the major packages out there. But not everyone has the resources
(e.g. time) to do so. You also don&amp;rsquo;t have awesome ideas every other day
which can immediately be implemented into new R packages to be used by
the community. Besides contributing with code there are also lots of
other things than can be useful for other R users, for example writing
blog posts about problems you solved, speaking at meetups or reviewing
code to help improve it. What I like much about reviewing code is that
people see things differently and have other experiences. As a reviewer,
you see a new package from the user&amp;rsquo;s perspective which can be hard for
the programmer themselves. Having someone else
review your code helps finding things that are missing because they seem
obvious to the package author or detect code pieces that require more
testing. I had a great feeling when I finished the review, since I had
helped improve an already amazing R package a little bit more.&lt;/p&gt;

&lt;h3 id=&#34;reviewing-helps-improve-your-own-coding-style&#34;&gt;Reviewing helps improve your own coding style&lt;/h3&gt;

&lt;p&gt;When I write R code I usually try to do it in the best way possible.
&lt;a href=&#34;https://google.github.io/styleguide/Rguide.xml&#34;&gt;Google&amp;rsquo;s R Style Guide&lt;/a&gt;
is a good start to get used to coding best practice in R and I also
enjoyed reading &lt;a href=&#34;https://github.com/timoxley/best-practices&#34;&gt;Programming Best Practices
Tidbits&lt;/a&gt;. So normally
when I think some piece of code can be improved (with respect to speed,
readability or memory usage) I check online whether I can find a
better solution. Often you just don&amp;rsquo;t think something can be
improved because you always did it in a certain way or the last time you
checked there was no better solution. This is when it helps to follow
other people&amp;rsquo;s code. I do this by reading their blogs, following many R
users on Twitter and checking their GitHub account. Reviewing an R
package also helped me a great deal with getting new ideas because I
checked each function a lot more carefully than when I read blog posts.
In my opinion, good code does not only use the best package for each
problem but also the small details are well implemented. One thing I
used to do wrong for a long time was filling of data.frames until I
found a better (much faster)
&lt;a href=&#34;https://stackoverflow.com/a/29419402&#34;&gt;solution on stackoverflow&lt;/a&gt;.
And with respect to this you
can learn a lot from someone else&amp;rsquo;s code. What I found really cool in
the package I reviewed was the usage of small helper functions (see
&lt;a href=&#34;https://github.com/ropensci/patentsview/blob/c03e1ab2537873d7a9b76025b0072953efb475c1/R/utils.R&#34;&gt;utils.R&lt;/a&gt;).
Functions like &lt;code&gt;paste0_stop&lt;/code&gt; and &lt;code&gt;paste0_message&lt;/code&gt; make the rest of the
code a lot easier to read.&lt;/p&gt;

&lt;h3 id=&#34;good-start-for-writing-your-own-packages&#34;&gt;Good start for writing your own packages&lt;/h3&gt;

&lt;p&gt;When reviewing an R package, you check the code like a really observant
user. I noticed many things that you usually don&amp;rsquo;t care about when using
an R package, like comments, how helpful the documentation and the
examples are, and also how well unit tests cover the code. I think that
reviewing a few good packages can prepare you very well for writing your
own packages.&lt;/p&gt;

&lt;h3 id=&#34;do-you-want-to-contribute-to-ropensci-yourself&#34;&gt;Do you want to contribute to rOpenSci yourself?&lt;/h3&gt;

&lt;p&gt;If I motivated you to become an rOpenSci reviewer, please sign up! Here
is a list of useful things if you want to become an rOpenSci reviewer
like me.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ropensci.org/onboarding/&#34;&gt;Form to sign up (just takes a minute)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://onboarding.ropensci.org/&#34;&gt;Information for reviewers&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://mozillascience.github.io/codeReview/review.html&#34;&gt;Mozilla reviewing guide&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;While writing this blog post I found a nice article about &lt;a href=&#34;http://www.tidyverse.org/articles/2017/08/contributing/&#34;&gt;contributing
to the tidyverse&lt;/a&gt; which is
mostly also applicable to other R packages in my opinion.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are generally interested in either submitting or reviewing an R package, I would like to invite you to the &lt;a href=&#34;https://ropensci.org/blog/blog/2017/08/31/comm-call-v14&#34;&gt; Community Call on rOpenSci software review and onboarding&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>How rOpenSci uses Code Review to Promote Reproducible Science</title>
      <link>https://ropensci.org/blog/2017/09/01/nf-softwarereview/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ropensci.org/blog/2017/09/01/nf-softwarereview/</guid>
      <description>
        
        

&lt;p&gt;At rOpenSci, we create and curate software to help scientists with the data life cycle. These tools access, download, manage, and archive scientific data in open, reproducible ways. Early on, we realized this could only be a community effort. The variety of scientific data and workflows could only be tackled by drawing on contributions of scientists with field-specific expertise.&lt;/p&gt;

&lt;p&gt;With the community approach came challenges. &lt;strong&gt;How could we ensure the quality of code written by scientists without formal training in software development practices? How could we drive adoption of best practices among our contributors? How could we create a community that would support each other in this work?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We have had great success addressing these challenges via the &lt;em&gt;peer review&lt;/em&gt;.&lt;/strong&gt; We draw elements from a process familiar to our target community – &lt;em&gt;academic peer review&lt;/em&gt; – and a practice from the software development world – &lt;em&gt;production code review&lt;/em&gt; – to create a system that fosters software quality, ongoing education, and community development.&lt;/p&gt;

&lt;h3 id=&#34;an-open-review-process&#34;&gt;An Open Review Process&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Production software review&lt;/strong&gt; occurs within software development teams, open source or not. Contributions to a software project are reviewed by one or more other team members before incorporation into project source code. Contributions are typically small patches, and review serves as a check on quality, as well as an opportunity for training in team standards.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In academic peer review&lt;/strong&gt;, external reviewers critique a complete product – usually a manuscript – with a very broad mandate to address any areas they see as deficient. Academic review is often anonymous and passing through it gives the product, and the author, a public mark of validation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We blend these approaches.&lt;/strong&gt; In our process, authors submit complete R packages to rOpenSci. Editors check that packages fit into our project’s scope, run a series of automated tests to ensure a baseline of code quality and completeness, and then assign two independent reviewers. Reviewers comment on usability, quality, and style of software code as well as documentation. Authors make changes in response, and once reviewers are satisfied with the updates, the package receives a badge of approval and joins our suite.&lt;/p&gt;

&lt;p&gt;This process is quite iterative. After reviewers post a first round of extensive reviews, authors and reviewers chat in an informal back-and-forth, only lightly moderated by an editor. This lets both reviewers and authors pose questions of each other and explain differences of opinion. It can proceed at a much faster pace than typical academic review correspondence. We use the GitHub issues system for this conversation, and responses take minutes to days, rather than weeks to months.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The exchange is also open and public&lt;/strong&gt;. Authors, reviewers, and editors all know each other’s identities. The broader community can view or even participate in the conversation as it happens. This provides an incentive to be thorough and provide non-adversarial, constructive reviews. Both authors and reviewers report that they enjoy and learn more from this open and direct exchange. It also has the benefit of building community. Participants have the opportunity to meaningfully network with new peers, and new collaborations have emerged via ideas spawned during the review process.&lt;/p&gt;

&lt;p&gt;We are aware that open systems can have drawbacks. For instance, in traditional academic review, double-blind peer review &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0169534707002704&#34;&gt;can increase representation of female authors&lt;/a&gt;, suggesting bias in non-blind reviews. It is also possible reviewers are less critical in open review. However, we posit that the openness of the review conversation provides a check on review quality and bias; it’s harder to inject unsupported or subjective comments in public and without the cover of anonymity. Ultimately, we believe the ability of authors and reviewers to have direct but public communication improves quality and fairness.&lt;/p&gt;

&lt;h3 id=&#34;guidance-and-standards&#34;&gt;Guidance and Standards&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;rOpenSci provides guidance on reviewing.&lt;/strong&gt; This falls into two main categories: &lt;strong&gt;high-level best practices&lt;/strong&gt; and &lt;strong&gt;low-level standards&lt;/strong&gt;. High-level best practices are general and broadly applicable across languages and applications. These are practices such as “Write re-usable functions rather than repeating the same code,” “Test edge cases,” or “Write documentation for all of your functions.” Because of their general nature, these can be drawn from other sources and not developed from scratch. Our best practices are based on guidance originally developed by &lt;a href=&#34;https://mozillascience.github.io/codeReview/intro.html&#34;&gt;Mozilla Science Lab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Low-level standards are specific to a language (in our case, R), applications (data interfaces) and user base (researchers). These include specific items such as naming conventions for functions, best choices of dependencies for certain tasks, and adherence to a code style guide. We have an extensive set of standards for our reviewers to check. These change over time as the R software ecosystem evolves, best practices change, and tooling and educational resources make new methods available to developers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Our standards also change based on feedback from reviewers.&lt;/strong&gt; We adopt into our standards suggestions that emerge in multiple reviewers across different packages. Many of these, we’ve found, have to do with with the ease-of-use and consistency of software APIs, and the type and location of information in documentation that make it easiest to find. This highlights one of the advantages of external reviewers – they can provide a fresh perspective on usability, as well as test software under different use-cases than imagined by the author.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As our standards have become more extensive, we have come to rely more on automated tools.&lt;/strong&gt; The R ecosystem, like most languages, has a suite of tools for code linting, function testing, static code analysis and continuous integration. We require authors to use these, and editors run submissions through a suite of tests prior to sending them for review. This frees reviewers from the burden of low-level tasks to focus on high-level critiques where they can add the most value.&lt;/p&gt;

&lt;h3 id=&#34;the-reviewer-community&#34;&gt;The Reviewer Community&lt;/h3&gt;

&lt;p&gt;One of the core challenges and rewards of our work has been developing a community of reviewers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reviewing is a high-skill activity.&lt;/strong&gt; Reviewers need expertise in the programming methods used in a software package and also the scientific field of its application. (“Find me someone who knows sensory ecology and sparse data structures!”) They need good communications skills and the time and willingness to volunteer. Thankfully, the open-science and open-source worlds are filled with generous, expert people. We have been able to expand our reviewer pool as the pace of submissions and the domains of their applications have grown.&lt;/p&gt;

&lt;p&gt;Developing the reviewer pool requires constant recruitment. Our editors actively and broadly engage with developer and research communities to find new reviewers. We recruit from authors of previous submissions, co-workers and colleagues, at conferences, through our other collaborative work and on social media. In the open-source software ecosystem, one can often identify people with particular expertise by looking at their published software or contribution to other projects, and we often will cold-email potential reviewers whose published work suggests they would be a good match for a submission.&lt;/p&gt;

&lt;p&gt;We cultivate our reviewer pool as well as expand it. We bring back reviewers so that they may develop reviewing as a skill, but not so often as to overburden them. We provide guidance and feedback to new recruits. When assigning reviewers to a submission, we aim to pair experienced reviewers with new ones, or reviewers with expertise on a package’s programming methods with those experienced in its field of application. &lt;strong&gt;These reviewers learn from each other, and diversity in perspectives is an advantage&lt;/strong&gt;; less experienced developers often provide insight that more experienced ones do not on software usability, API design, and documentation. More experienced developers will more often identify inefficiencies in code, pitfalls due to edge-cases, or suggest alternate implementation approaches.&lt;/p&gt;

&lt;h3 id=&#34;expanding-peer-review-for-code&#34;&gt;Expanding Peer Review for Code&lt;/h3&gt;

&lt;p&gt;Code review has been one of rOpenSci’s best initiatives. We build software, build skills, and build community, and the peer review process has been a major catalyst for all three. It has made both the software we develop internally and that we accept from outside contributors more reliable, usable, and maintainable. So &lt;strong&gt;we are working to promote open peer review of code by more organizations&lt;/strong&gt; working with scientific software. We helped launch &lt;a href=&#34;http://joss.theoj.org/&#34;&gt;The Journal of Open Source Software&lt;/a&gt;, which uses a version of our review process to provide a developer-friendly publication venue. JOSS’s success has led to a spin-off, the &lt;a href=&#34;http://jose.theoj.org/&#34;&gt;Journal of Open Source Education&lt;/a&gt;, which uses an open, code-review-like processes to provide feedback on curricula and educational materials. We are also developing a pilot program where software papers submitted to a partner academic journal receive a badge for going through rOpenSci review. We are encouraged by other review initiatives like &lt;a href=&#34;https://rescience.github.io/&#34;&gt;ReScience&lt;/a&gt; and &lt;a href=&#34;https://programminghistorian.org/&#34;&gt;The Programming Historian&lt;/a&gt;. &lt;a href=&#34;https://www.bioconductor.org/&#34;&gt;BioConductor&lt;/a&gt;’s code reviews, which predate ours by several years, recently switched to an open model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If your organization is developing or curating scientific code&lt;/strong&gt;, we believe code review, implemented well, can be a great benefit. It can take considerable effort to begin, but &lt;strong&gt;here are some of the key lessons we’ve learned that can help:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Develop standards and guidelines&lt;/strong&gt; for your authors and reviewers to use. Borrow these freely from other projects (feel free to use ours), and modify them iteratively as you go.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use automated tools&lt;/strong&gt; such as code linters, test suites, and test coverage measures to reduce burden on authors, reviewers, and editors as much as possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a clear scope.&lt;/strong&gt; Spell out to yourselves and potential contributors what your project will accept, and why. This will save a lot of confusion and effort in the future.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build a community&lt;/strong&gt; through incentives of networking, opportunities to learn, and kindness.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;rOpenSci is eager to work with other groups interested in developing similar review processes&lt;/strong&gt;, especially if you are interested in reviewing and curating scientific software in languages other than R or beyond our scope of supporting the data life cycle. Software that implements statistical algorithms, for instance, is an area ripe for open peer review of code. Please &lt;a href=&#34;https://ropensci.org/contact.html&#34;&gt;get in touch&lt;/a&gt; if you have questions or wish to co-pilot a review system for your project.&lt;/p&gt;

&lt;p&gt;And of course, if you want to review, we’re always looking for volunteers. Sign up at &lt;a href=&#34;https://ropensci.org/onboarding&#34;&gt;https://ropensci.org/onboarding&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;You can support rOpenSci by &lt;a href=&#34;https://www.numfocus.org/community/donate/&#34;&gt;becoming a NumFOCUS member&lt;/a&gt; or making a &lt;a href=&#34;https://www.numfocus.org/open-source-projects/&#34;&gt;donation to the project&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
